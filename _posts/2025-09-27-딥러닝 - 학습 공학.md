---
layout: post
title: 딥러닝 - 학습 공학
date: 2025-09-27 22:25:23 +0900
category: 딥러닝
---
# 1.11 학습 공학(Training Engineering)
**AMP/BF16 · 체크포인팅 · 시드 고정·재현성 · 분산 학습(DDP 개념) · 혼합정밀 주의사항**

## A. 혼합정밀(AMP/BF16): 개념과 선택 기준

### A-1. 왜 혼합정밀?
- **속도/메모리** 개선: FP32 대비 *연산속도* 증가, *메모리* 감소 → 더 큰 배치·더 큰 모델 가능.
- **자동 혼합정밀(AMP)**: 안전한 연산은 FP16/BF16로, 민감한 연산은 FP32로 **자동 전환**.

### A-2. FP16 vs BF16
- **FP16 (IEEE half)**
  - 비트: 1(sign) + 5(exponent) + 10(mantissa)
  - **정밀도(유효숫자)** ↑, **지수 범위** ↓ → **언더플로우·오버플로우**에 취약  
  - 해결: **`GradScaler`(손실 스케일링)** 필수에 가까움
- **BF16 (bfloat16)**
  - 비트: 1 + **8(exponent)** + 7  
  - FP32와 **동일한 지수 범위** → **수치 안정성** 좋음  
  - 보통 **GradScaler 불필요**, AMP로 `dtype=torch.bfloat16` 사용
- **권장 요약**
  - **NVIDIA Ampere+ / A100/H100**: **BF16** 우선  
  - Ampere 이전, BF16 미지원 → **FP16+GradScaler**  
  - CPU/TPU/MI300 등은 해당 플랫폼 가이드를 따름

---

## B. AMP 실전: 안전 루프 템플릿

### B-1. FP16(autocast + GradScaler)
```python
import torch, torch.nn as nn
from torch.cuda.amp import autocast, GradScaler

model = ... .cuda()
opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.05)
crit = nn.CrossEntropyLoss()
scaler = GradScaler()         # FP16은 보통 필요

for step, (xb, yb) in enumerate(train_loader, start=1):
    xb, yb = xb.cuda(non_blocking=True), yb.cuda(non_blocking=True)
    opt.zero_grad(set_to_none=True)

    with autocast(dtype=torch.float16):   # FP16 autocast
        logits = model(xb)
        loss = crit(logits, yb)

    scaler.scale(loss).backward()         # 1) scale
    scaler.unscale_(opt)                  # 2) unscale(클리핑·검사 전)
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    scaler.step(opt)                      # 3) step (overflow면 skip)
    scaler.update()                       # 4) scale 조정
```
> **순서 중요**: `scale → backward → unscale_(opt) → (clip) → step → update`

### B-2. BF16(autocast만)
```python
from torch.cuda.amp import autocast

model = ... .cuda()
opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)
crit = nn.CrossEntropyLoss()

for xb, yb in train_loader:
    xb, yb = xb.cuda(non_blocking=True), yb.cuda(non_blocking=True)
    opt.zero_grad(set_to_none=True)
    with autocast(dtype=torch.bfloat16):  # BF16은 보통 GradScaler 불필요
        logits = model(xb)
        loss = crit(logits, yb)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    opt.step()
```

### B-3. 어떤 연산이 FP32가 필요한가?
- **감소(reduction)**, **누적(accumulation)**, **정규화 통계(BN running stats)**, **softmax/logsumexp의 내부 핵심 부분** 등은 AMP가 자동으로 FP32로 올림(cast up) 처리.  
- **커스텀 연산**을 만든다면 **수치 스케일**을 반드시 점검(예: 매우 작은 분산/표준편차 나눗셈).

---

## C. 체크포인팅(Checkpointing): “어디까지 했는지”의 기록

### C-1. 무엇을 저장/복원해야 하나?
- **모델 가중치**: `model.state_dict()`  
- **옵티마 상태**: `optimizer.state_dict()` (모멘텀/Adam의 m,v 포함)  
- **스케줄러 상태**: `scheduler.state_dict()`  
- **AMP 스케일러**: `scaler.state_dict()` (FP16일 때)  
- **기타**: `epoch`, `global_step`, **최고 성능 기록(best_metric)** 등

```python
ckpt = {
    "model": model.state_dict(),
    "opt": opt.state_dict(),
    "sched": sched.state_dict() if sched else None,
    "scaler": scaler.state_dict() if scaler else None,
    "epoch": epoch,
    "step": global_step,
    "best": best_metric
}
torch.save(ckpt, "checkpoint.pt")
```

**복원**
```python
ckpt = torch.load("checkpoint.pt", map_location="cuda")
model.load_state_dict(ckpt["model"])
opt.load_state_dict(ckpt["opt"])
if ckpt.get("sched"): sched.load_state_dict(ckpt["sched"])
if ckpt.get("scaler"): scaler.load_state_dict(ckpt["scaler"])
epoch = ckpt.get("epoch", 0)
global_step = ckpt.get("step", 0)
best_metric = ckpt.get("best", float("-inf"))
```

> **주의**: 모델 래핑이 바뀌면(state_dict 키가 어긋나면) 로딩 실패. DDP/EMA 등과의 올바른 저장 위치를 확인(아래 DDP 섹션 참조).

### C-2. “최고 성능” 체크포인트
- 매 에폭(또는 일정 스텝)마다 **검증** → 최고 점수면 `best.pt` 갱신
- 학습 재개 시 **`best.pt`** 도 함께 로딩하여 **EarlyStopping/Plateau** 기준 유지

### C-3. 활성값 체크포인팅(Activation Checkpointing)
- **메모리 절약**: forward 때 일부 중간 활성값을 **저장하지 않고**, backward에서 **재계산**  
- **시간↑, 메모리↓** 트레이드오프

```python
from torch.utils.checkpoint import checkpoint

def block(x):
    x = layer1(x); x = torch.relu(x); x = layer2(x)
    return x

out = checkpoint(block, x)  # 재계산을 허용해 메모리 사용량 감소
```

> 실무 팁: **깊은 Transformer/ViT** 에서 눈에 띄는 메모리 절약. *하지만* 연산시간이 늘어 throughput이 줄 수 있음.

---

## D. 시드 고정 & 재현성(Reproducibility)

### D-1. 왜 어려운가?
- **병렬성/비결정성 커널**, **분산/멀티프로세스**의 랜덤성 등으로 “완전한” 비트 단위 재현은 어려울 수 있음.  
- **실무 목표**: *통계적으로 동일한 결과* 혹은 *같은 환경에서 거의 동일한 결과*.

### D-2. 기본 세팅(단일 GPU)
```python
import os, random, numpy as np, torch

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)  # 구버전 호환
    torch.cuda.manual_seed_all(seed)
    # 결정적 모드(속도↓), 커널 제약 생김
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # CUDA BLAS 결정성(일부 연산) - 환경변수 필요(프로세스 시작 전에)
    os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
    # 알고리즘 강제 결정적(오류 시 예외)
    # torch.use_deterministic_algorithms(True)  # 필요시
```

### D-3. DataLoader 재현성
- **셔플 시드**: `generator=torch.Generator().manual_seed(seed)`  
- **워커 초기화**: `worker_init_fn` 에서 `np.random.seed(seed+worker_id)` 등 설정

```python
def worker_init_fn(worker_id):
    seed = 42 + worker_id
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)

g = torch.Generator()
g.manual_seed(42)
train_loader = torch.utils.data.DataLoader(
    dataset, batch_size=128, shuffle=True, num_workers=4,
    worker_init_fn=worker_init_fn, generator=g, persistent_workers=True
)
```

> **주의**: `persistent_workers=True` 를 쓰면 *첫 로딩 이후 시드가 유지* → epoch마다 동일 셔플을 원하면 **Sampler 레벨**에서 제어.

### D-4. 분산(DDP)에서의 시드
- 랭크별 시드를 **다르게** 주되, **일관된 규칙**으로 설정  
- 예: `base_seed + rank`, `sampler.set_epoch(epoch)` 로 **에폭마다 셔플 변화** 보장

---

## E. 분산 학습(DDP): 개념 → 실전

### E-1. 큰 그림
- **DDP(DistributedDataParallel)**: **GPU마다 프로세스 1개** → 모델 사본을 올리고, `loss.backward()` 때 **AllReduce** 로 그라디언트를 평균  
- 스케일: **노드 수 × GPU 수** 만큼 **배치 분할** → 효율적인 학습

### E-2. 핵심 구성 요소
- **프로세스 그룹 초기화**: `torch.distributed.init_process_group(backend="nccl")`  
- **로컬 디바이스 지정**: `torch.cuda.set_device(local_rank)`  
- **DDP 래핑**: `model = DDP(model, device_ids=[local_rank], output_device=local_rank, broadcast_buffers=True)`  
- **샘플러**: `DistributedSampler(dataset)` 를 DataLoader에 사용  
- **동기화 포인트**: backward 시 자동 평균, `find_unused_parameters` 옵션 주의

### E-3. 실행(권장: `torchrun`)
```bash
# 한 노드 4GPU 예시
torchrun --nproc_per_node=4 train.py --arg1 ... --argN ...
```

### E-4. DDP 학습 루프 골격
```python
# train.py
import os, torch, torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def ddp_setup():
    dist.init_process_group(backend="nccl")
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    return local_rank

def is_main():
    return dist.get_rank() == 0

def main():
    local_rank = ddp_setup()
    model = MyModel().cuda()
    model = DDP(model, device_ids=[local_rank], output_device=local_rank)
    opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)

    train_set = MyDataset(...)
    sampler = torch.utils.data.distributed.DistributedSampler(train_set, shuffle=True)
    loader = torch.utils.data.DataLoader(
        train_set, batch_size=64, sampler=sampler,
        num_workers=4, pin_memory=True, persistent_workers=True
    )

    for epoch in range(1, 101):
        sampler.set_epoch(epoch)  # 에폭별 셔플 시드 변경
        model.train()
        for xb, yb in loader:
            xb, yb = xb.cuda(non_blocking=True), yb.cuda(non_blocking=True)
            opt.zero_grad(set_to_none=True)
            loss = criterion(model(xb), yb)
            loss.backward()     # 여기서 DDP가 AllReduce
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()

        if is_main():   # rank0만 로그/체크포인트
            torch.save({"model": model.module.state_dict(), "epoch": epoch}, "last.pt")

    dist.destroy_process_group()

if __name__ == "__main__":
    main()
```

#### DDP 주의점
- **find_unused_parameters**: 동적으로 어떤 분기에서 파라미터가 **사용되지 않으면** AllReduce 누락 → `find_unused_parameters=True` 가 해결하지만 **오버헤드↑**  
- **broadcast_buffers**: BN 등 버퍼 동기화. 사전학습 고정(Frozen BN) 시 `False` 가능  
- **gradient accumulation**: 누적 스텝 동안 AllReduce가 매 스텝 발생 → *대용량 설정에선* `no_sync()` 블록으로 최적화 가능
```python
accum_steps = 4
for i, (xb, yb) in enumerate(loader):
    with model.no_sync() if (i % accum_steps != accum_steps-1) else nullcontext():
        loss = ...
        (loss/accum_steps).backward()
    if i % accum_steps == accum_steps-1:
        opt.step(); opt.zero_grad(set_to_none=True)
```

- **체크포인트**: `model.module.state_dict()` 로 **원본 모듈** 저장, 로딩 시 `model.module.load_state_dict(...)` 또는 래핑 전에 로드  
- **only rank0 저장**: 파일 충돌 방지. 필요시 `dist.barrier()` 로 동기화

---

## F. 혼합정밀 주의사항(실전 이슈 모음)

1) **클리핑/검증 전 `unscale_`**  
   - FP16에서 `GradScaler.unscale_(opt)` 없이 바로 클리핑하면 **잘못된 클립**  
2) **NaN/Inf 감지**  
   - 스케일러가 오버플로우 감지 시 **스텝을 skip** → 로스가 계속 NaN이면 LR↓, eps↑, 모델/데이터 검사  
3) **정밀도 요구 연산 예외 처리**  
   - 일부 커스텀 레이어(예: `softmax` 앞뒤 특수 정규화)가 FP16에서 불안정 → `with autocast(enabled=False):` 로 강제 FP32
```python
with autocast(enabled=False):
    logits = my_custom_fp32_only_layer(x.float())
```
4) **정규화 통계**  
   - BN 러닝 통계는 FP32 유지. 작은 배치에서는 **SyncBN/LN/GN** 고려  
5) **수치범위와 스케일**  
   - 로짓·손실이 너무 큰 스케일 → 오버플로우 위험. 입력/타깃 스케일링·WD·스케줄 조정  
6) **BF16 가속기 호환성**  
   - 드라이버/라이브러리 버전, 커널 지원 확인. (BF16로 바꾸면 GradScaler 제거하기)

---

## G. 성능 튜닝(입출력·로더·메모리)

- **DataLoader**
  - `num_workers`: CPU 코어수/2 ~ 코어수 탐색
  - `pin_memory=True`: H2D 전송 가속, `non_blocking=True` 와 함께
  - `persistent_workers=True`: epoch 간 워커 재활용
  - `prefetch_factor`: I/O 선행
- **메모리**
  - `torch.backends.cuda.matmul.allow_tf32=True` (A100+에서 **TF32**로 matmul 가속, 정확도 타협)
  - **Gradient Checkpointing**: 메모리↓/시간↑
  - **Mixed Precision**: FP16/BF16
  - `set_to_none=True` 로 grad 초기화
- **프로파일링**
  - `torch.profiler` 로 병목 파악(로더 병목/커널 병목/동기화 지점)

---

## H. 통합 예제: AMP(선택형) + 체크포인트 + 재현성 + (단일/분산) 스위치

```python
# train_core.py
import os, random, numpy as np, torch, torch.nn as nn
from torch.cuda.amp import autocast, GradScaler

def set_seed(seed=1337):
    import os
    os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class Net(nn.Module):
    def __init__(self, d_in=784, d_h=1024, d_out=10, p=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_in, d_h), nn.GELU(), nn.Dropout(p),
            nn.Linear(d_h, d_h), nn.GELU(), nn.Dropout(p),
            nn.Linear(d_h, d_out)
        )
    def forward(self, x): return self.net(x.view(x.size(0), -1))

def save_ckpt(path, **kwargs):
    torch.save(kwargs, path)

def load_ckpt(path, map_location="cuda"):
    return torch.load(path, map_location=map_location)

def train_one_epoch(model, opt, loader, crit, scaler=None, amp_dtype=None, clip=1.0):
    model.train()
    total = n = 0
    for xb, yb in loader:
        xb, yb = xb.cuda(non_blocking=True), yb.cuda(non_blocking=True)
        opt.zero_grad(set_to_none=True)
        if scaler is not None:
            with autocast(dtype=amp_dtype):
                logits = model(xb); loss = crit(logits, yb)
            scaler.scale(loss).backward()
            scaler.unscale_(opt)
            if clip: torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
            scaler.step(opt); scaler.update()
        else:
            with autocast(dtype=amp_dtype):     # BF16이면 scaler=None, dtype=bfloat16
                logits = model(xb); loss = crit(logits, yb)
            loss.backward()
            if clip: torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
            opt.step()
        total += loss.item() * xb.size(0); n += xb.size(0)
    return total/n

@torch.no_grad()
def evaluate(model, loader, crit):
    model.eval()
    tot = n = 0; correct = 0
    for xb, yb in loader:
        xb, yb = xb.cuda(non_blocking=True), yb.cuda(non_blocking=True)
        logits = model(xb); loss = crit(logits, yb)
        tot += loss.item() * xb.size(0); n += xb.size(0)
        correct += (logits.argmax(1) == yb).sum().item()
    return tot/n, correct/n

def run_train(train_loader, val_loader, use_fp16=False, use_bf16=False,
              epochs=20, lr=3e-4, wd=0.05, ckpt_path="last.pt"):
    set_seed(42)
    model = Net().cuda()
    opt = torch.optim.AdamW([
        {"params":[p for n,p in model.named_parameters() if p.ndim>1], "weight_decay":wd},
        {"params":[p for n,p in model.named_parameters() if p.ndim==1 or n.endswith("bias")], "weight_decay":0.0},
    ], lr=lr)
    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs, eta_min=1e-6)
    crit = nn.CrossEntropyLoss(label_smoothing=0.1)

    scaler = None; amp_dtype=None
    if use_fp16:
        scaler = GradScaler()
        amp_dtype = torch.float16
    elif use_bf16:
        amp_dtype = torch.bfloat16

    best = float("inf"); start_ep = 0
    if os.path.exists(ckpt_path):
        ckpt = load_ckpt(ckpt_path)
        model.load_state_dict(ckpt["model"])
        opt.load_state_dict(ckpt["opt"])
        sched.load_state_dict(ckpt["sched"])
        if scaler and ckpt.get("scaler"): scaler.load_state_dict(ckpt["scaler"])
        best = ckpt.get("best", best); start_ep = ckpt.get("epoch", 0)
        print(f"Resumed from {ckpt_path} @ epoch {start_ep}")

    for ep in range(start_ep, epochs):
        tr = train_one_epoch(model, opt, train_loader, crit, scaler, amp_dtype)
        va_loss, va_acc = evaluate(model, val_loader, crit)
        sched.step()
        print(f"[{ep+1:03d}] train {tr:.4f} | val {va_loss:.4f} | acc {va_acc:.3f}")

        save_ckpt(ckpt_path,
                  model=model.state_dict(), opt=opt.state_dict(),
                  sched=sched.state_dict(), scaler=(scaler.state_dict() if scaler else None),
                  epoch=ep+1, best=min(best, va_loss))
        if va_loss < best:
            best = va_loss
            save_ckpt("best.pt", model=model.state_dict(), epoch=ep+1, best=best)
```

> 인자만 바꿔 **FP32**, **FP16(AMP)**, **BF16(AMP)** 를 모두 시험 가능.

---

## I. 디버깅 & 안정성 체크리스트

- [ ] **학습률 너무 큼** → 초기 NaN/발산: LR↓, Warmup↑, WD/Clip 적용, AMP dtype 확인  
- [ ] **손실이 가끔 NaN** → 입력/타깃 범위, 로그/나눗셈 안정화, FP16이면 GradScaler 동작 확인  
- [ ] **DDP Hang/Deadlock** → 모든 rank가 같은 코드 경로로 backward/step 했는지, `find_unused_parameters` 확인  
- [ ] **검증 성능 재현 실패** → 시드·DataLoader·Sampler·drop_last·eval모드·no_grad 확인  
- [ ] **체크포인트 로딩 실패** → 래핑(DDP/EMA)의 `.module` 여부, 키 이름 차이 확인  
- [ ] **작은 배치에서 BN 불안정** → SyncBN/LN/GN 전환 또는 BN Frozen  
- [ ] **성능 느림** → DataLoader 병목(워커↑, pin_memory), AMP/TF32 켜기, 프로파일링으로 커널 확인

---

## J. 자주 받는 질문(FAQ)

**Q1. BF16인데도 NaN이 떠요.**  
A. 보통 **학습률 과대** / **로짓 스케일** 문제입니다. LR↓, 스케줄러 완만화(Cosine), WD↑, 데이터 스케일링을 점검하세요. 커스텀 레이어가 FP32 강제 필요한지 확인하세요.

**Q2. DDP에서 `model.state_dict()` 로 저장하면 되나요?**  
A. DDP 래핑 후라면 **`model.module.state_dict()`** 가 원본. 보통 **저장: rank0에서 `module.state_dict()`**, 로드: 래핑 전 후 모두 가능(키 일치만 보장).

**Q3. 그라디언트 누적과 DDP를 함께 쓰면 느려요.**  
A. 누적 스텝마다 AllReduce가 발생. `no_sync()` 로 **누적 스텝 동안 동기화 off** → 마지막 스텝에만 동기화.

**Q4. 완전 결정론이 필요합니다.**  
A. `torch.use_deterministic_algorithms(True)` + `CUBLAS_WORKSPACE_CONFIG` + cudnn deterministic. 다만 속도/메모리·커널 제약이 큽니다.

**Q5. AMP에서 어떤 연산이 위험합니까?**  
A. 매우 작은 분산/표준편차 나눗셈, 로그-소프트맥스의 커스텀 변형, 일부 정규화/스케일링 층. 이런 곳은 **`autocast(enabled=False)`** 로 FP32 강제.

---

## K. 한 페이지 요약

- **혼합정밀**: BF16(권장, 스케일러 불필요) / FP16(+GradScaler 필수)  
- **체크포인트**: 모델·옵티마·스케줄러·스케일러·메타(에폭/스텝/베스트) 저장·복원  
- **재현성**: 시드 세트 + CUDNN 결정론 + DataLoader 워커 시드 + (DDP) sampler.set_epoch  
- **DDP**: torchrun + per-GPU 프로세스 + DDP 래핑 + DistributedSampler + rank0 save  
- **주의**: FP16은 `unscale_` 후 클립/검증, BN 작은 배치 불안정, find_unused_parameters, no_sync로 누적 최적화

---

### 마무리
학습 공학은 **속도·안정성·재현성**의 삼각 균형 잡기입니다.  
이 장의 템플릿과 체크리스트를 프로젝트에 바로 이식해 보세요. AMP/BF16로 **성능을 끌어올리고**, 체크포인트/시드로 **복원·재현** 가능성을 확보하며, DDP로 **규모를 키우는** 감각을 익히면 다음 장의 대규모 실험도 한결 수월해집니다.