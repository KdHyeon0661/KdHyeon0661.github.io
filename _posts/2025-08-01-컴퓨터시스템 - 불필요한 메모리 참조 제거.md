---
layout: post
title: 컴퓨터시스템 - 불필요한 메모리 참조 제거
date: 2025-08-01 19:20:23 +0900
category: 컴퓨터시스템
---
# 불필요한 메모리 참조 제거

## 왜 중요한가 — 비용 모델과 직관

- **레지스터는 싸고, 메모리는 비싸다.** 동일 값을 **레지스터(스칼라)** 에 잡아두면 L1/L2/LLC/DRAM으로 내려가는 트래픽이 줄어든다.
- 코어는 한 사이클에 수행 가능한 **load/store 포트 수**가 제한된다. 불필요한 참조는 포트 경합과 파이프라인 스톨을 만든다.
- 무의미한 write 도 **캐시 라인을 더럽히고(write-back 유발), RFO(Read-For-Ownership)** 를 일으켜 추가 read를 만든다.

간단한 비용 근사:
$$
\text{CPU time} \approx \text{Instructions} \times \text{CPI} / f,\quad
\Delta \text{CPI} \propto \text{(불필요 loads + stores)}
$$
→ 불필요 참조를 줄이면 **instructions**(특히 메모리 접근)와 **CPI**(미스/경합) 모두가 내려간다.

---

## 대표 패턴 — “불필요”가 생기는 순간들

### 같은 메모리 값을 반복해 읽는다

```c
// before
sum  += node->val;
prod *= node->val;

// after: 한 번만 로드 → 레지스터 재사용
int v = node->val;
sum  += v;
prod *= v;
```

### 곧 덮을 값을 먼저 쓴다(Dead Store)

```c
// before: 바로 덮일 값에 미리 0 저장 (무용)
x = 0;
x = src;

// after
x = src;
```

### 루프 내부의 불변(load) 반복

```c
// before
for (int i=0;i<n;i++) out[i] = in[i] * cfg->scale * table[3];

// after
int s  = cfg->scale;
int t3 = table[3];
for (int i=0;i<n;i++) out[i] = in[i] * s * t3;
```

### store 직후 동일 주소 load

```c
// before
arr[i] = v;
t = arr[i];     // 굳이 메모리 재확인
use(t);

// after
arr[i] = v;
use(v);
```
> 단, **다른 스레드/시그널/간접 호출**이 사이에 값을 바꿀 수 있다면 제거 금지 (§6).

### 함수 경계로 인해 “다시 로드”가 강제되는 경우

```c
// before
extern int f(int* p);
int x = *p;
int y = f(p);     // f가 *p를 바꿀 수 있다고 가정
int z = *p;       // 재로드 발생

// after: API/속성으로 안정성 보장
extern int f_readonly(const int* p) __attribute__((pure));
int x = *p;
int y = f_readonly(p);
int z = x;        // 재사용 가능
```

---

## 실전 기법 — C 코드 변환(전/후)과 설명

### 스칼라 치환(SRA): 자주 쓰는 필드를 레지스터에

```c
// before
for (int i=0;i<n;i++)
    s += a[i].x*a[i].x + a[i].y*a[i].y;

// after
for (int i=0;i<n;i++){
    int x = a[i].x, y = a[i].y;   // 2-load → 레지스터 유지
    s += x*x + y*y;               // 재로드 없음
}
```

### 누적(accumulator)은 메모리가 아니라 레지스터에 유지

```c
// before: 매 반복 load/store
for (int i=0;i<n;i++)
    C[i] = C[i] + A[i]*B[i];

// after: 1 load + 1 store
for (int i=0;i<n;i++){
    int acc = C[i];
    acc += A[i]*B[i];
    C[i] = acc;
}
```
> 더 나아가 내부 루프 전체를 레지스터 누적으로 들고 가서 **루프 종료 시 1회 store** 하면 최적.

### 주소 계산/인덱싱 단순화(컴파일러가 최적화 추정하기 쉽게)

```c
// before
for (int i=0;i<n;i++) sum += a[i];

// after: 포인터 워크 (의미는 같고 패턴이 단순)
for (int *p=a, *e=a+n; p!=e; ++p) sum += *p;
```

### Dead Store to whole object: 전체 덮어쓸 버퍼의 사전 memset 제거

```c
// before
void fill(int *buf, size_t n){
    memset(buf, 0, n*sizeof *buf);   // 전부 덮일 예정
    for (size_t i=0;i<n;i++) buf[i]=i;
}

// after
void fill(int *buf, size_t n){
    for (size_t i=0;i<n;i++) buf[i]=i;
}
```
> 반대로 **정말 0이 필요한** 큰 버퍼라면 `calloc`이 더 유리할 수 있다(제로 페이지/초기화 비용).

### 별칭 제거: `restrict`/`const`/속성으로 “안전”을 알려주기

```c
// before
void saxpy(int n, float *x, float *y, float a){
  for (int i=0;i<n;i++) y[i] += a*x[i]; // x,y alias 가능 → 매번 보수적
}

// after
void saxpy(int n, float *restrict x, float *restrict y, float a){
  float aa = a;                        // 스칼라화
  #pragma omp simd                    // 벡터화 힌트
  for (int i=0;i<n;i++) y[i] += aa*x[i];
}
```
- 함수가 메모리를 읽기만 한다면 **`__attribute__((pure))`**, 인자만 쓰면 **`__attribute__((const))`**(GCC/Clang)로 **재로드 억제**를 유도.

### store→load forwarding 실패 회피(같은 폭·정렬 유지)

```c
// bad: 1바이트 store 직후 4바이트 load (폭 불일치 → 전방 실패 가능)
*(uint8_t*)p = 7;
u32 x = *(uint32_t*)p;

// better: 동일 폭 접근
*(uint32_t*)p = 7u;
uint32_t x = *(uint32_t*)p;
```
- **폭/정렬/부분 겹침**이 다르면 하드웨어 전방이 깨져 **스톨**이 난다. 가능한 한 **같은 폭/정렬**로.

### 함수 경계 넘는 스칼라 유지: 인라인/`static`/LTO

```c
// header
static inline int inc(int x){ return x+1; } // 인라인되면 주변 재로드 없음
```
- 번역 단위(TU) 경계 때문에 막히면 **LTO(-flto)** 나 **PGO**를 활용.

---

## 고급: RFO/NT-store/쓰기 병합과 “불필요 참조”의 미세조정

- **Write-allocate(RFO)**: 쓰기 미스 시 라인을 먼저 **읽어** 와야 하는 정책. 의미 없는 write 도 **추가 read**를 만든다.
- **Non-temporal store(스트리밍 store)**: 결과를 캐시에 남길 필요가 없을 때, RFO를 피하며 **쓰기 병합 버퍼**를 통해 메모리로 스트림.
```c
// x86 예시(플랫폼 의존): C 배열에 큰 스트림 쓰기
#include <immintrin.h>

void store_stream(float *dst, float v, int n){
  __m256 xv = _mm256_set1_ps(v);
  for (int i=0;i<n;i+=8)
    _mm256_stream_ps(&dst[i], xv); // NT-store: 캐시 오염+RFO 감소
  _mm_sfence(); // 완료 보장
}
```
- **주의**: 작은 데이터/재사용 있는 데이터엔 역효과. **정말 재사용 없을 때**만.

---

## “불필요”가 **필요**해지는 순간 — 제거 금지 규칙

1. **멀티스레드 가시성**
   - 다른 스레드가 데이터를 바꿀 수 있으면, 이전 로드를 **캐시**해 두면 안 된다.
   - 해결: C/C++ **atomics**와 올바른 **메모리 오더** 사용.
2. **`volatile`/MMIO**
   - 하드웨어 레지스터 접근은 **항상 실제 메모리**를 건드려야 한다. 재배치/제거 금지.
3. **함수 호출/시그널/롱점프**
   - 호출된 함수/시그널 핸들러가 메모리를 바꿀 수 있다면 재로드 필요. API 계약/속성으로 “변경 없음”을 알려주지 않는 한 제거 금지.
4. **엄격 별칭/미정의 동작(UB)**
   - `char*` 펀닝, 잘못된 캐스팅은 컴파일러의 가정을 깨뜨려 **예상 밖 재로드/비최적화**를 만든다. 코드를 **표준 준수**로 정리.

---

## 컴파일러가 이미 잘하는 것 vs 우리가 도와야 하는 것

- 자동: **DCE/DSE(죽은 코드/스토어 제거)**, **CSE**, **LICM**(루프 불변 호이스팅), **SRA**(스칼라 대체), **SLP/벡터화**.
- 막히는 원인: **별칭 불명확**, **외부 효과(함수/글로벌/asm)**, **번역 단위 경계**, **보수적 플래그**.
- 우리가 줄 것: `restrict`, `const`, `pure/const` 속성, **인라인/`static`**, **LTO/PGO**, **명시적 정렬/연속성**, **깨끗한 별칭 규약**.

---

## 마이크로 예제 — 어셈블리 차이로 확인하기

```c
// before
int f(int *p){
  int s = *p;     // load
  *p = s + 1;     // store
  return *p;      // 다시 load (불필요)
}

// after
int f_opt(int *p){
  int s = *p;     // load
  s = s + 1;
  *p = s;         // store
  return s;       // 레지스터 반환
}
```
- 최적화 후에는 마지막 `load` 가 사라져 **mov/ldr 감소**. `perf stat`의 `L1-dcache-loads`/`-stores` 감소로 확인 가능.

---

## 대용량 루프 — 메모리 참조 최소화 묶음 레시피

### 블로킹 + 레지스터 누적(행렬-벡터)

```c
void gemv(int N, int K, const double* A, const double* x, double* y){
  for (int i=0;i<N;i++){
    double acc = y[i];                // 1 load
    const double* Ai = &A[i*(size_t)K];
    for (int k=0;k<K;k++) acc += Ai[k]*x[k]; // 레지스터 누적
    y[i] = acc;                       // 1 store
  }
}
```

### 불변·반복 로드 제거 + 별칭 금지

```c
void scale_add(int n, float *restrict y, const float *restrict x, float a){
  float aa = a;                       // 스칼라화(레지스터 유지)
  #pragma omp simd
  for (int i=0;i<n;i++)
    y[i] += aa * x[i];
}
```

### 경계 분리(peeling)로 정렬 맞추기(벡터화/전방 성공률 ↑)

```c
void copy_aligned(float *restrict dst, const float *restrict src, int n){
  int i=0; for(; ((uintptr_t)&dst[i])%32 && i<n; ++i) dst[i]=src[i]; // 정렬까지 프롤로그
  for(; i+7<n; i+=8){ // 32B 정렬 구간
    // 컴파일러가 _mm256_load/store 로 바꿀 여지↑
    dst[i+0]=src[i+0]; dst[i+1]=src[i+1]; /* ... */ dst[i+7]=src[i+7];
  }
  for(; i<n; ++i) dst[i]=src[i]; // 에필로그
}
```

---

## 도구로 “정말 줄었나” 검증하기

- **perf stat**:
  - 핵심 카운터: `instructions`, `cycles`, `L1-dcache-loads`, `L1-dcache-load-misses`, `L1-dcache-stores`, `branches`, `branch-misses`.
  - 불필요 참조 제거 후 `loads/stores`가 **눈에 띄게 감소**해야 한다.
- **perf record / report**: 핫 루프 어셈블리에서 `mov/ldr/str` 패턴이 줄었는지 확인.
- **Compiler Explorer(godbolt)**: -O0/-O3 차이, `restrict`/속성 추가 전후 **기계어 비교**.
- **Sanitizers(ASan/UBSan/TSan)**: 최적화가 가리는 **UB/레이스**를 사전에 제거.

---

## 체크리스트 — 바로 적용

- [ ] 동일 메모리 값을 **두 번 이상 읽는가?** → 로컬 스칼라에 담아 재사용.
- [ ] **읽히지 않을 값**을 먼저 쓰는가? → dead store 제거(특히 루프/초기화).
- [ ] 루프 내부의 **불변 로드**가 있는가? → 루프 밖으로 호이스팅.
- [ ] **store 직후 동일 주소 load**가 있는가? → 레지스터 값 사용.
- [ ] **별칭 가능성** 때문에 재로드가 발생하는가? → `restrict`/`const`/함수 속성/인라인/LTO.
- [ ] **폭/정렬** 불일치로 전방 실패가 나는가? → 동일 폭/정렬로 정리.
- [ ] 멀티스레드/`volatile`/MMIO/시그널 등 **실제 접근이 필요한가?** → 제거 금지.
- [ ] 빌드: `-O3 -march=native -flto`(+PGO)로 최적화 토대 확보.
- [ ] 변경 후 **perf 카운터**로 loads/stores/미스율이 줄었는가?

---

## 보너스: 미세 팁(현업에서 자주 마주치는 함정)

- **함수의 “은근한” 부수효과**: 로깅/통계 갱신/글로벌 플래그 접근이 있으면 컴파일러는 보수적으로 재로드한다. **순수성 명시**나 **경계 분리**(핵심 계산을 작은 `static inline` 함수로)로 최적화 통로를 만들어라.
- **C의 `restrict` vs C++**: C++ 표준엔 `restrict`가 없다. 컴파일러 확장 `__restrict`/`__restrict__` 사용을 고려하되, 라이브러리 API라면 **문서로 별칭 금지 계약**을 명확히.
- **메모리 장벽과 최적화**: `atomic_thread_fence`/`asm volatile("" ::: "memory")` 같은 장벽은 재배치를 제한한다. 꼭 필요한 최소 위치에만.
- **UB가 최적화를 막는다**: 경계 밖 접근, 잘못된 정렬 캐스팅, 수명 규칙 위반 등은 **컴파일러가 재로드/비최적화**를 하게 만든다. **정상성 먼저**.

---

## 요약

1) **같은 값을 다시 메모리에서 읽지 말고, 레지스터로 승격**하라.
2) **곧 덮을 값을 미리 쓰지 말라**(dead store 제거).
3) 컴파일러가 막히는 이유는 대개 **별칭/함수 경계/동시성/volatile**이다 — `restrict`/속성/인라인/LTO/올바른 원자성으로 **길을 터 주자**.
4) 마지막은 항상 **측정**: loads/stores/미스율이 진짜 줄었는지 **perf/어셈블리**로 확인하라.

위 원칙과 패턴만 지켜도, “메모리 접근이 비싼 순간”을 상당수 없앨 수 있다. 남는 것은 **연산**과 **캐시 히트**뿐이다.
