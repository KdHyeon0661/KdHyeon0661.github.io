---
layout: post
title: 기계학습 - 의사결정나무
date: 2025-08-19 17:25:23 +0900
category: 기계학습
---
# 🌳 의사결정나무(Decision Tree)

## 1. 개요
**의사결정나무(Decision Tree)**는 데이터를 분류(Classification)하거나 회귀(Regression)하는 데 사용되는 **트리 기반 모델**입니다.  
트리 구조에서 **질문(조건)**을 반복적으로 분할하면서, 최종적으로 **잎 노드(leaf node)**에서 예측 값을 출력합니다.

- **분류(Classification Tree)**: 클래스 레이블 예측.
- **회귀(Regression Tree)**: 연속적인 수치 예측.

> 핵심 아이디어: 데이터를 분할할 때, **가장 불순도(impurity)를 줄이는 질문**을 찾아 트리를 성장시킨다.

---

## 2. 기본 구조
- **루트 노드(Root Node)**: 전체 데이터를 포함.
- **내부 노드(Decision Node)**: 특정 특징(feature)에 대한 조건(예: \(X_1 > 5\))으로 분할.
- **가지(Branch)**: 조건에 따른 데이터의 흐름.
- **잎 노드(Leaf Node)**: 최종 예측 결과(클래스 레이블 or 평균값).

---

## 3. 학습 과정

### 3.1 분할 기준 (Split Criterion)
데이터를 나눌 때는 **불순도(Impurity)**를 최소화하는 기준을 사용합니다.

1. **지니 불순도(Gini Impurity)**
   - 분류 문제에서 주로 사용.
   $$
   Gini = \sum_{k=1}^{K} p_k (1 - p_k) = 1 - \sum_{k=1}^{K} p_k^2
   $$
   (\(p_k\): 노드에 속한 샘플 중 클래스 \(k\)의 비율)

   - 값 범위: 0 (순수, 한 클래스만 있음) ~ 0.5 (최대 혼합).

2. **엔트로피(Entropy)**
   - 정보 이론 기반 불순도.
   $$
   H = - \sum_{k=1}^{K} p_k \log_2(p_k)
   $$
   - 엔트로피가 낮을수록 불확실성이 작음.
   - 정보 이득(Information Gain):
     $$
     IG = H(\text{부모}) - \sum_i \frac{n_i}{n} H(\text{자식}_i)
     $$

3. **분산 감소(Variance Reduction)**
   - 회귀 문제에서 사용.
   $$
   Var(S) = \frac{1}{|S|} \sum_{i \in S} (y_i - \bar{y})^2
   $$
   - 부모 노드의 분산이 자식 노드 분산의 가중 평균보다 얼마나 줄었는지 계산.

---

### 3.2 학습 알고리즘 (ID3, C4.5, CART)
1. **ID3**  
   - 분할 기준: 정보 이득(Entropy 기반).
   - 단점: 속성 값이 많은 특징을 선호(Overfitting 가능).

2. **C4.5**  
   - ID3 개선 → **정보 이득 비율(Information Gain Ratio)** 사용.
   - 연속형 변수 처리 가능.

3. **CART(Classification and Regression Tree)**  
   - 분류: 지니 불순도.
   - 회귀: 분산 감소.
   - 대부분의 라이브러리(sklearn 등)에서 사용되는 표준 알고리즘.

---

## 4. 수학적 예시 (분류)

데이터: 날씨에 따라 "운동 여부"를 예측.

| 날씨 | 온도 | 습도 | 바람 | 운동 |
|------|------|------|------|------|
| 맑음 | 덥다 | 높음 | 약함 | X |
| 맑음 | 덥다 | 높음 | 강함 | X |
| 흐림 | 덥다 | 높음 | 약함 | O |
| 비 | 온화 | 높음 | 약함 | O |
| 비 | 서늘 | 보통 | 약함 | O |
| 비 | 서늘 | 보통 | 강함 | X |
| 흐림 | 서늘 | 보통 | 강함 | O |

### 단계
1. 부모 노드의 엔트로피:
   $$
   H_{parent} = -\left(\frac{4}{7}\log_2 \frac{4}{7} + \frac{3}{7}\log_2 \frac{3}{7}\right) \approx 0.985
   $$

2. "날씨"로 분할했을 때 엔트로피 계산 → 정보 이득(IG) 최대 확인.

3. IG가 가장 큰 특성으로 분할 → 트리 성장.

---

## 5. 트리의 정지 조건 (Stopping Criteria)
트리가 무한히 커지는 것을 방지하기 위해:
- 최대 깊이 제한 (`max_depth`).
- 최소 샘플 개수(`min_samples_split`, `min_samples_leaf`).
- 분할로 인한 불순도 감소량이 임계값 이하일 경우 중단.

---

## 6. 가지치기(Pruning)
트리가 깊어질수록 **과적합(Overfitting)** 문제가 발생합니다.  
이를 방지하기 위해 **가지치기(Pruning)**를 수행합니다.

- **사전 가지치기 (Pre-Pruning)**: 트리 성장 중 조건 만족 시 조기 중단.
- **사후 가지치기 (Post-Pruning)**: 트리를 모두 만든 후, 성능 향상 없는 가지 제거.

---

## 7. 장단점

✅ 장점
- 직관적이고 해석이 쉬움(화이트박스 모델).
- 수치형/범주형 변수 모두 사용 가능.
- 전처리(스케일링, 정규화) 필요 없음.

❌ 단점
- 과적합에 취약.
- 데이터 작은 변화에도 구조가 크게 달라질 수 있음(불안정성).
- 앙상블(Random Forest, Gradient Boosting)보다 성능이 떨어짐.

---

## 8. 파이썬 예제 (Scikit-Learn)

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# 데이터 불러오기
iris = load_iris()
X, y = iris.data, iris.target

# 모델 학습
clf = DecisionTreeClassifier(criterion="entropy", max_depth=3, random_state=42)
clf.fit(X, y)

# 트리 시각화
plt.figure(figsize=(12,8))
plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()
```

---

## 📌 요약
- **의사결정나무**: 조건 분할을 통해 예측하는 트리 모델.
- 분할 기준: **지니, 엔트로피, 분산 감소**.
- 알고리즘: **ID3, C4.5, CART**.
- 장점: 해석 가능성, 단순성.
- 단점: 과적합, 불안정성 → 보통 **랜덤 포레스트, XGBoost** 같은 앙상블로 개선.