---
layout: post
title: 기계학습 - 의사결정나무
date: 2025-08-19 17:25:23 +0900
category: 기계학습
---
# 의사결정나무(Decision Tree)

## 개요와 표기

의사결정나무는 **축 정렬(axis-aligned)** 분할을 반복해 입력 공간을 직사각형(고차원에서는 직교 다면체)으로 나누고, 각 영역(잎)에서 **상수 예측**(클래스 비율 또는 평균)을 한다.

- 분류(Classification): 잎에서 **클래스 확률** \( \hat{p}_k \) 추정 후 다수결.
- 회귀(Regression): 잎에서 **타깃 평균** \( \bar{y} \) (또는 중앙값 등) 예측.

표기:
- 학습 데이터 \( \{(x_i, y_i)\}_{i=1}^n,\; x_i\in\mathbb{R}^d \).
- 노드 \(t\)의 샘플 집합 \(S_t\), 크기 \(|S_t|\).
- 분할(split): 특징 \(j\)와 임계값 \(\theta\)로 좌/우 자식 \(S_L=\{x\mid x_j\le \theta\}\), \(S_R=\{x\mid x_j>\theta\}\).

---

## 불순도(Impurity)와 분할 기준

### 분류용 불순도

1) **지니 불순도(Gini)**
노드 \(t\)의 클래스 비율 \(p_{t,k}\)에 대해
$$
G(t)=\sum_{k=1}^{K} p_{t,k}(1-p_{t,k}) \;=\;1-\sum_{k=1}^{K} p_{t,k}^2.
$$
- 0이면 완전 순수(단일 클래스), 이진 균등혼합이면 최대.

2) **엔트로피(Entropy)**
$$
H(t)=-\sum_{k=1}^{K} p_{t,k}\log_2 p_{t,k}.
$$
- 정보이득(Information Gain) \(IG = H(\text{부모})-\sum_{c\in\{L,R\}} \frac{|S_c|}{|S_t|}H(c)\).

3) **오분류율(Misclassification Rate)**
$$
\mathrm{Err}(t)=1-\max_k p_{t,k}.
$$
- 분할 선택에는 둔감하여 거의 쓰지 않음(잎의 최종 지표로는 사용 가능).

**지니 vs 엔트로피**
- 둘 다 **불순도 감소**를 선호하며 실무 성능 차이는 크지 않다.
- 엔트로피는 이론적으로 정보이득과 연결, 지니는 계산이 빠르고 중간 클래스 비율 변화에 조금 더 민감.

---

### 회귀용 불순도

1) **SSE/분산(Variance) 감소**
노드 \(t\)에서
$$
\mathrm{SSE}(t)=\sum_{i\in S_t}(y_i-\bar{y}_t)^2,\qquad \bar{y}_t=\frac{1}{|S_t|}\sum_{i\in S_t} y_i.
$$
분할 점수:
$$
\Delta(t\to L,R)=\mathrm{SSE}(t)-\mathrm{SSE}(L)-\mathrm{SSE}(R).
$$

> **정리(평균 최적성)**: 고정 집합에서 \(\mathrm{SSE}\)를 최소화하는 상수 예측은 \(\bar{y}\).
> 증명: \(\sum (y_i-c)^2\)를 \(c\)로 미분하면 \(c=\bar{y}\)에서 0.

2) **MAE/중앙값 기반**
절대오차 기준이면 잎 예측치는 중앙값이 최적. 하지만 학습 시 MAE 기준 분할은 계산이 까다로워 SSE를 주로 사용.

---

### 분할 점수의 일반형

부모 노드 \(t\), 자식 \(L,R\). 임의의 불순도 \(I(\cdot)\)에 대해
$$
\text{Gain}(t;j,\theta)=I(t)-\left(\frac{|S_L|}{|S_t|}I(L)+\frac{|S_R|}{|S_t|}I(R)\right).
$$
최적 분할은 \(\arg\max_{j,\theta} \text{Gain}\).

---

## 연속/범주 특징의 분할

### 연속형 특징(Thresholding)

- 각 특징 \(j\)에 대해 \(x_{(1),j}\le \dots\le x_{(m),j}\) 정렬 후 후보 \(\theta\)를 인접한 값의 중간으로 스캔.
- 누적 통계(클래스 카운트, 타깃 합·제곱합)를 유지하면 한 번의 선형 스캔으로 각 \(\theta\)의 Gain 계산 가능.
- 한 특징당 \(O(n\log n)\) (정렬) + \(O(n)\) (스캔).

### 범주형 특징(Categorical)

- **다중분기**(C4.5): 카테고리별 자식 노드. 고유값 개수가 많을수록 **분할 정보량(Split Info)**으로 패널티 → **Gain Ratio** 사용.
- **이진화**(CART): 부분집합 분할 \(A\subset \mathcal{C}\) vs \(\mathcal{C}\setminus A\).
  - 이진 분할 최적화는 NP-hard이지만 클래스별 정렬 휴리스틱(이진, K 작은 경우) 가능.
- 라이브러리(sklearn)는 대체로 원-핫 인코딩 후 연속형 분할(축정렬)을 적용.
  - 고카디널리티 범주는 **타깃 누수**와 **과적합** 위험 → **타깃 인코딩/평활화**는 **CV 내부**에서만 적합.

---

## 학습 알고리즘 패밀리

### ID3 → C4.5 → CART

| 알고리즘 | 분할 기준 | 분기 수 | 연속형 | 가지치기 |
|---|---|---|---|---|
| ID3 | 정보이득 | 다중 | 제한적 | 사후(간단) |
| C4.5 | **Gain Ratio** | 다중 | 지원(임계값) | **오류 기반 가지치기(EbP)**, Subtree raising |
| CART | **지니/분산** | **이진** | 완전 지원 | **비용-복잡도 프루닝(α-CCP)** |

### CART 의사코드(개략)

```
GrowTree(S):
  if stop(S): return Leaf(S)
  for each feature j:
    for each candidate threshold θ on j:
      compute Gain(S; j, θ)
  choose (j*, θ*) with max Gain
  S_L, S_R = split(S, j*, θ*)
  return Node(j*, θ*, GrowTree(S_L), GrowTree(S_R))
```

`stop(S)`는 최대깊이, 최소 샘플 수, 최소 불순도 감소 등.

---

## 정지 기준과 프루닝(Pruning)

### 사전 가지치기(Pre-pruning)

- `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_leaf_nodes`, `min_impurity_decrease`.
- 효과: **분산 감소**(안정화), **속도/메모리** 절약.

### 사후 가지치기(Post-pruning)

1) **CART 비용-복잡도 프루닝(CCP)**
트리 \(T\)의 리프 개수 \(|T|\), 리프들의 잔차 합(분류: 불순도 합, 회귀: SSE 합) \(R(T)\).
복잡도 비용:
$$
R_\alpha(T)=R(T)+\alpha |T|.
$$
\(\alpha\)를 늘리며 **약한 링크**부터 가지 제거 → **\( \alpha\)-경로** 생성 → **CV**로 최적 \(\alpha\) 선택.

2) **C4.5 오류 기반 가지치기(EbP)**
서브트리의 재귀 추정 오류율과 잎 대체의 오류율 비교, 신뢰구간 보정 후 단순화.

> 실무 팁: sklearn의 `cost_complexity_pruning_path`로 \(\alpha\) 후보를 얻고, CV 점수로 선택.

---

## 편향-분산, 안정성, 정규화 노브

| 하이퍼파라미터 | 영향(요약) |
|---|---|
| `max_depth` ↓ | **편향↑**, **분산↓** (과적합 억제) |
| `min_samples_leaf` ↑ | 경계 매끄러움↑, 이상치 민감↓ |
| `min_impurity_decrease` ↑ | 미세한 분할 억제 |
| 클래스 가중(`class_weight`) | 불균형에서 소수 클래스 **재현율↑** |
| 특징 수 제한(`max_features`) | 분할 다양화(앙상블에서 중요), 단일 트리에서는 효과 제한 |

---

## 누락값·이상치·불균형·연속성 제약

- **결측치**: 전통 CART는 **서로게이트 분할(surrogate split)**로 대체 분기(동일 경향 특징) 사용. sklearn `DecisionTree*`는 결측을 직접 지원하지 않아 **Imputer 파이프라인** 필수.
- **이상치**: `min_samples_leaf`/`min_impurity_decrease`로 극단 분할 억제. 회귀는 **Huber 손실** 같은 대체를 쓰는 GBM류가 유리.
- **클래스 불균형**: `class_weight='balanced'` 혹은 사용자 비용 매트릭스 기반 임계값 조정.
- **단조 제약**: 순수 CART는 미지원(GBDT 구현들 일부 지원).

---

## 시간복잡도·메모리

- 한 노드에서 특징 \(d\), 샘플 \(n_t\)일 때 분할 탐색 \(O(d\,n_t\log n_t)\) (정렬 포함).
- 전체 트리 평균은 \(O(d\,n\log n)\) 근방(정확한 상수는 정지기준/데이터 분포에 의존).
- 메모리: 노드 저장(인덱스/통계/자식 포인터), 대용량 데이터는 **히스토그램 기반**(GBDT류) 고려.

---

## 해석: 규칙 추출·특징 중요도·부분의존

- **규칙(rule) 집합**: 루트→잎 경로를 논리식으로 변환 → **화이트박스** 설명.
- **특징 중요도**
  - **불순도 기반**: 분할에 의한 불순도 감소량의 합(빠르나 편향 위험: 고카디널리티·연속형 선호).
  - **퍼뮤테이션 중요도**: 검증셋에서 해당 특징만 섞어 **성능 저하**를 측정(권장).
- **부분의존(PDP) / ICE**: 트리 모델은 상호작용에 민감 → **ICE**로 개별 곡선 확인 권장.
- **확률 캘리브레이션**: 트리의 잎 확률은 **계단형으로 과신**되기 쉬움 → Platt/isotonic 보정.

---

## Python 실전 — 분류 트리

### 기본 학습·시각화

```python
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

X, y = load_wine(return_X_y=True)
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)

clf = DecisionTreeClassifier(
    criterion="gini", max_depth=4,
    min_samples_leaf=5, random_state=42
)
clf.fit(X_tr, y_tr)

print(classification_report(y_te, clf.predict(X_te), digits=3))

plt.figure(figsize=(14,8))
plot_tree(clf, filled=True)
plt.title("Decision Tree (Wine, depth<=4)")
plt.show()
```

### 프루닝 경로(CCP α) + 교차검증 선택

```python
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier

path = clf.cost_complexity_pruning_path(X_tr, y_tr)
alphas = path.ccp_alphas

best_score, best_alpha = -1, None
for a in alphas:
    m = DecisionTreeClassifier(random_state=42, ccp_alpha=a, max_depth=None)
    score = cross_val_score(m, X_tr, y_tr, cv=5).mean()
    if score > best_score:
        best_score, best_alpha = score, a

pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)
pruned.fit(X_tr, y_tr)
print("Best α:", best_alpha)
print("Test accuracy:", pruned.score(X_te, y_te))
```

### 불균형 + 파이프라인(결측/스케일 불필요하나 결측은 보정)

```python
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=4000, n_features=10, weights=[0.95, 0.05],
                           random_state=0)
# 일부 결측 생성

rng = np.random.RandomState(0)
mask = rng.rand(*X.shape) < 0.02
X[mask] = np.nan

pipe = Pipeline([
    ("imp", SimpleImputer(strategy="median")),
    ("tree", DecisionTreeClassifier(class_weight="balanced",
                                    min_samples_leaf=20, random_state=0))
])
pipe.fit(X, y)
print(classification_report(y, pipe.predict(X), digits=3))
```

---

## Python 실전 — 회귀 트리

### 기본 회귀 + 성능

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

X, y = fetch_california_housing(return_X_y=True, as_frame=False)
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)

reg = DecisionTreeRegressor(max_depth=6, min_samples_leaf=20, random_state=42)
reg.fit(X_tr, y_tr)

pred = reg.predict(X_te)
rmse = mean_squared_error(y_te, pred, squared=False)
print("RMSE:", rmse)
```

### `min_samples_leaf` 효과(매끄러움·과적합 완화)

```python
for m in [1, 5, 20, 100]:
    r = DecisionTreeRegressor(max_depth=None, min_samples_leaf=m, random_state=42)
    r.fit(X_tr, y_tr)
    rmse = mean_squared_error(y_te, r.predict(X_te), squared=False)
    print(f"min_samples_leaf={m}: RMSE={rmse:.4f}")
```

---

## 정보이득 비율(Gain Ratio)와 다중분기(C4.5 세부)

- **SplitInfo**: 속성 자체의 분할 정보량
$$
\mathrm{SplitInfo}(A)=-\sum_{v\in \mathcal{V}_A} \frac{|S_v|}{|S|}\log_2\frac{|S_v|}{|S|}.
$$
- **GainRatio**:
$$
\mathrm{GR}(A)=\frac{\mathrm{InfoGain}(A)}{\mathrm{SplitInfo}(A)}.
$$
> 값이 많은 속성(예: ID)에 과도한 가산점을 방지.

- **Subtree raising**: 하위 서브트리를 상위로 올려 가지 간소화(일부 구현에서 제공).

---

## 규칙 추출과 반사실(counterfactual) 해석

- 루트→잎 경로를 AND 규칙으로 변환:
  ```
  if (sepal_length <= 5.45) and (petal_width > 1.65) and ... then class=Iris-virginica (p=0.92, n=37)
  ```
- **반사실**: 특정 샘플이 다른 클래스로 분류되려면 **최소 변경**은 무엇인가?
  - 경계 임계값에 대한 **한두 특징 이동**으로 규칙 교차를 유도 → 설명 제공.

---

## 검증·통계적 신뢰

- **Stratified K-fold**로 성능 평균 및 표준오차, **부트스트랩**으로 지표 신뢰구간 제시.
- 모델 비교 시 **McNemar**(이진 라벨), **DeLong**(ROC-AUC 차이) 등으로 유의성 점검.

---

## 한계와 대안

- **불안정성**: 데이터 작은 변화 → 다른 트리.
  - 대안: **랜덤 포레스트**(배깅+특징 무작위), **GBDT류(XGBoost/LightGBM/CatBoost)**.
- **축 정렬 한계**: 경계가 비축 정렬 시 많은 분할 필요.
  - 대안: **Oblique Tree**(선형 분할)·**Rotation Forest** 등.

---

## 하이퍼파라미터 튜닝 가이드(분류 예)

```python
from sklearn.model_selection import GridSearchCV
grid = {
    "max_depth": [3, 5, 8, None],
    "min_samples_leaf": [1, 5, 20, 50],
    "min_impurity_decrease": [0.0, 1e-4, 1e-3],
    "class_weight": [None, "balanced"]
}
gs = GridSearchCV(DecisionTreeClassifier(random_state=0), grid,
                  scoring="f1_macro", cv=5, n_jobs=-1)
gs.fit(X_tr, y_tr)
print(gs.best_params_, gs.best_score_)
```

> 분류 평가지표는 **F1(불균형)** 또는 **ROC-AUC/PR-AUC**(확률 출력) 권장. 회귀는 **RMSE/MAE**.

---

## 수학적 보충: 지니·엔트로피의 경계 성질

- 이진 \(p\in[0,1]\)에서
$$
\mathrm{Gini}(p)=2p(1-p),\quad
H(p)\le \log 2 \cdot \sqrt{\mathrm{Gini}(p)} \;\;(\text{상수 차})
$$
- 둘 다 \(p=\tfrac12\)에서 최대, \(p\to 0,1\)에서 0으로 수렴.
- 분할 시 **가중 평균 불순도**는 결합 불확실성의 볼록 조합 → **Gain ≥ 0**.

---

## 체크리스트

- [ ] 결측값 처리(파이프라인/서로게이트)
- [ ] 불균형 시 `class_weight` 또는 비용 기반 임계값
- [ ] 프루닝(CCP α) 또는 보수적 정지기준
- [ ] 검증전략(Stratified CV, 그룹/시계열이면 전용 Split)
- [ ] 중요도는 **퍼뮤테이션**으로 재검증
- [ ] 확률 사용 시 **캘리브레이션**(Platt/Isotonic)

---

## 요약

- **의사결정나무**는 불순도 감소를 최대화하는 **국소 최적 분할**을 반복하여 간명하고 해석 가능한 규칙을 만든다.
- 분류에서는 **지니/엔트로피**, 회귀에서는 **SSE 감소**가 핵심 기준.
- **사전/사후 가지치기**로 복잡도를 제어하고, **프루닝 경로(CCP α)**를 CV로 선택하라.
- 해석은 **경로 규칙·퍼뮤 중요도·PDP/ICE**로 보완하고, 확률 출력은 **캘리브레이션**하라.
- 단일 트리의 한계를 넘어서면 **랜덤 포레스트/GBDT**가 실무 기본 대안이다.
