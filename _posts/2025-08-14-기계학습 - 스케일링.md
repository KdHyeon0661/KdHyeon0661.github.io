---
layout: post
title: 기계학습 - 스케일링
date: 2025-08-14 16:20:23 +0900
category: 기계학습
---
# 스케일링(Scaling)

## 1. 왜 스케일링이 필요한가?

### (1) 거리 기반 알고리즘의 왜곡
- 서로 다른 단위/범위(예: cm vs 원)로 인해 **스케일 큰 피처가 거리/유사도 계산을 지배**한다.
- KNN/K-Means/커널 SVM 등에서 군집/결정경계가 왜곡된다.

### (2) 경사하강법의 수렴 속도
- 선형회귀의 목적함수 \( L(w)=\frac{1}{2n}\|Xw - y\|_2^2 \) 의 **기울기 리프시츠 상수**는
$$
L_{\text{GD}}=\frac{1}{n}\lambda_{\max}(X^\top X).
$$
- **조건수** \( \kappa(X)=\frac{\sigma_{\max}(X)}{\sigma_{\min}(X)} \) 가 클수록 등고선은 **가늘고 긴 타원**이 되어 **지그재그 수렴**을 유발한다.
- 열별 표준화로 \(X\) 의 각 열 분산을 1로 맞추면 \( \kappa(X) \) 완화 → **스텝 크기 선택 여유** 및 **수렴 가속**.

### (3) 해석 용이성
- 특징들이 동일 스케일이면 **계수의 상대적 크기** 비교나 **정규화 패널티(릿지/라쏘)** 해석이 일관적이다.

---

## 2. 스케일링 방법 총정리

> 표준 기호: 입력 \(x\), 변환 후 \(x'\). 데이터셋 열(특징) 단위의 통계량은 \(x_{\min}, x_{\max}, \mu, \sigma, \text{median}, \text{IQR}\) 등.

### (1) 정규화(Min–Max Scaling)
- **정의**:
$$
x'=\frac{x - x_{\min}}{x_{\max}-x_{\min}}\ \in [0,1]\quad(\text{또는 선형 변환으로 }[a,b])
$$
- **장점**: 값 범위를 고정 → 신경망·픽셀(0–1) 입력에 적합.  
- **단점**: 이상치에 매우 민감(분모가 커지거나 분자가 비정상).
- **팁**: 이상치 클리핑(예: 상하위 1% 절단)과 함께 사용.

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
X_scaled = scaler.fit_transform(X_train)  # test에는 transform만
```

---

### (2) 표준화(Z-score)
- **정의**:
$$
x'=\frac{x-\mu}{\sigma},\qquad \mu=\mathbb{E}[x],\ \sigma=\sqrt{\mathbb{V}[x]}
$$
- **장점**: 평균 0, 분산 1 → **조건수 완화** 및 **GD 수렴 가속**.  
- **단점**: 극단 이상치에 영향.  
- **팁**: 희소행렬은 중앙화(평균 빼기)가 비효율 → `with_mean=False`.

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()  # sparse면 with_mean=False
X_scaled = scaler.fit_transform(X_train)
```

---

### (3) Robust Scaling (median & IQR)
- **정의**:
$$
x'=\frac{x-\mathrm{median}(x)}{\mathrm{IQR}(x)},\quad \mathrm{IQR}=Q_3-Q_1
$$
- **장점**: 이상치에 강함. 중앙값 0, IQR 1.  
- **단점**: 극단 분포/다봉 분포에서는 분리력이 약할 수 있음.

```python
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler(quantile_range=(25,75))
X_scaled = scaler.fit_transform(X_train)
```

---

### (4) MaxAbs Scaling (희소데이터 친화)
- **정의**:
$$
x'=\frac{x}{\max |x|}
$$
- **장점**: 0 중심 이동 없이 크기만 조정 → **희소성 유지**.  
- **사용처**: 원-핫, TF-IDF와 같은 고차 희소 피처.

```python
from sklearn.preprocessing import MaxAbsScaler
scaler = MaxAbsScaler()
Xs = scaler.fit_transform(X_sparse)
```

---

### (5) Unit-norm Scaling ( \(L_1/L_2/L_\infty\) 정규화 )
- **정의** (샘플 벡터 \( \mathbf{x} \) 기준):
$$
\mathbf{x}'=\frac{\mathbf{x}}{\|\mathbf{x}\|_p}
$$
- **장점**: 코사인 유사도 기반 모델(텍스트/추천)에서 길이 표준화.  
- **주의**: 샘플 정규화(행 기준) vs 특징 정규화(열 기준)를 구분.

```python
from sklearn.preprocessing import Normalizer
norm = Normalizer(norm="l2")   # 행 단위
X_row_normalized = norm.fit_transform(X)
```

---

### (6) 로그/파워 변환 (분포 안정화)
- **로그**: \( x'>=\log(1+x) \) ( \(x> -1\), 보통 \(x\ge0\) ). 긴 꼬리/양의 치우침 완화.  
- **Box–Cox**( \(x>0\) 필요 ):
$$
x'=\begin{cases}
\frac{x^\lambda - 1}{\lambda}, & \lambda\ne 0\\
\log x, & \lambda=0
\end{cases}
$$
- **Yeo–Johnson**( \(x\in\mathbb{R}\) 허용, 실무 권장 ).

```python
from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method="yeo-johnson", standardize=True)
X_pt = pt.fit_transform(X_train)
```

---

### (7) Quantile Transformer (순위-가우시안/균등화)
- **정의**: 각 값의 **순위**를 이용해 목표 분포(균등/정규)에 맵핑.  
- **장점**: 이상치 영향 완화, 비정규 분포를 표준정규로 변환.  
- **단점**: **거리**/**상대크기**가 크게 변형(순위만 보존) → KNN 등 주의.

```python
from sklearn.preprocessing import QuantileTransformer
qt = QuantileTransformer(output_distribution="normal", n_quantiles=1000, random_state=42)
X_q = qt.fit_transform(X_train)
```

---

### (8) Whitening (공분산 정규화)
- **정의**: 중심화 후 공분산을 단위행렬로 만드는 선형변환.
- **PCA Whitening**: \( X_c=U\Sigma V^\top \Rightarrow X_{\text{white}}=X_c V \Sigma^{-1} \).
- **장점**: 상관 제거, 등방성.  
- **단점**: 노이즈 증폭 가능 → **\(\epsilon\)** 가중 역행렬로 안정화.

---

## 3. 어떤 경우에 어떤 스케일링?

| 알고리즘 | 권장 스케일링 | 이유/메모 |
|---|---|---|
| KNN / K-Means | **표준화/Min–Max** | 거리 민감. 각 축 영향 균형 |
| SVM (RBF/Linear) | **표준화/Power** | 마진/커널 폭, 정규화 페널티 일관성 |
| 로지스틱/선형회귀(+정규화) | **표준화/Robust** | 패널티의 공정성, 조건수 개선 |
| PCA/ICA | **표준화** | 분산/공분산 계산의 균형 |
| 신경망 | **Min–Max/표준화** | 활성함수/배치정규화 안정화 |
| 트리/랜덤포레스트/GBDT | **대체로 불필요** | 순위/분기 기준은 스케일 불변 |
| 나이브 베이즈(카운트) | **불필요~제한적** | 확률모형 기반, 단 TF-IDF는 별도 스케일 |
| 추천/텍스트(코사인) | **Unit-norm/TF-IDF** | 길이 표준화, 빈도 보정 |

---

## 4. 데이터 유형별 전략

### (1) 수치 vs 범주
- **수치형**: 위 스케일링 적용.  
- **범주형**: 원-핫/임베딩 → **보통 스케일링 불필요**(0/1). 단, **고유값 가중**이 필요한 특수 상황 제외.

### (2) 희소행렬(NLP/원-핫/TF-IDF)
- **중앙화 금지**(희소성 붕괴).  
- `MaxAbsScaler` 또는 `StandardScaler(with_mean=False)`.  
- TF-IDF 자체가 빈도 스케일 보정이므로 추가 스케일링은 **보수적** 접근.

### (3) 이미지
- **채널별 평균/표준편차**(dataset-level) 정규화, 또는 [0, 1] 범위 정규화.  
- 전이학습 시 **사전 학습 모델의 통계**(예: ImageNet mean/std) 사용.

### (4) 시계열
- **학습 구간으로만 fit**(미래 정보 누수 방지).  
- **롤링/확장 윈도우** 스케일링(운영 시점 적용).

### (5) 멀티도메인/그룹별 데이터
- 사용자·기기·지역 등 **그룹 단위 통계**로 스케일 → 그룹 간 분포 차 유지 + 누수 방지.  
- 교차검증은 `GroupKFold`/`TimeSeriesSplit`과 결합.

---

## 5. 누수 방지와 교차검증

- **원칙**: 어떤 통계량도 **학습 세트에서만** 추정하고 검증/테스트에는 **transform만**.  
- 교차검증 시 스케일러의 fit은 **fold 내부**에서 수행.
- 구현은 **Pipeline/ColumnTransformer**로 자동화.

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression

num_cols = ["age","income","height"]
cat_cols = ["city","gender"]

preproc = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

pipe = Pipeline([
    ("prep", preproc),
    ("clf", LogisticRegression(max_iter=500))
])

# cross_val_score는 fold마다 prep.fit이 다시 수행되어 누수 방지
```

---

## 6. 타깃 스케일링(Target Scaling)

- 회귀에서 타깃 \(y\)가 매우 치우치면 **\(\log(1+y)\)** 또는 **표준화**로 안정화 후 학습, 예측 후 **역변환**.
- 평가 시에는 **원래 스케일 기준 지표**(RMSE/MAE)를 사용.

```python
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

ysc = StandardScaler()
y_tr_scaled = ysc.fit_transform(y_tr.reshape(-1,1)).ravel()

model = Ridge(alpha=1.0).fit(X_tr, y_tr_scaled)
y_pred_scaled = model.predict(X_te)
y_pred = ysc.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()
```

---

## 7. 수치·최적화 관점: 조건수·사전조건화·정규화와의 상호작용

### (1) 사전조건화(Preconditioning)로서의 스케일링
- 열 스케일러 \(D=\mathrm{diag}(s_1,\dots,s_d)\) 를 두고 \(X' = X D^{-1},\ w' = D w\) 라 하면
$$
\min_w \|Xw-y\|^2 \equiv \min_{w'} \|X' w' - y\|^2.
$$
- \(D\) 를 적절히 택해 \(X'\) 의 열 크기를 맞추면 \( \kappa(X') \downarrow \) → **GD 수렴 가속**.

### (2) 정규화(릿지/라쏘)와의 상호작용
- 페널티 \( \lambda\|w\|_2^2 \) 는 **스케일에 민감**. 스케일이 크면 해당 축의 **과도한 수축** 발생 → **표준화 후 정규화**가 기본.

---

## 8. 실전 코드 모음

### 8.1 스케일링별 간단 비교 (KMeans 예)
```python
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import silhouette_score

rng = np.random.default_rng(0)
X1 = rng.normal(loc=[0,0], scale=[1,10], size=(400,2))
X2 = rng.normal(loc=[5,50], scale=[1,10], size=(400,2))
X = np.vstack([X1, X2])

def run(name, transformer=None):
    X_in = transformer.fit_transform(X) if transformer else X
    km = KMeans(n_clusters=2, n_init=10, random_state=0)
    labels = km.fit_predict(X_in)
    s = silhouette_score(X_in, labels)
    print(f"{name:15s} silhouette={s:.3f}")
    
run("No scaling", None)
run("Standard", StandardScaler())
run("MinMax", MinMaxScaler())
```

### 8.2 SGD 수렴 속도 비교(표준화 전/후)
```python
import numpy as np
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

rng = np.random.default_rng(42)
n, d = 5000, 30
scales = np.logspace(0, 6, d)        # 큰 스케일 차이
X = rng.normal(size=(n,d)) * scales
w_true = rng.normal(size=d)
y = X @ w_true + rng.normal(0, 1.0, size=n)

Xtr, Xte, ytr, yte = train_test_split(X,y,test_size=0.3,random_state=0)

def train(Xtr, Xte, tag):
    sgd = SGDRegressor(alpha=1e-4, learning_rate='invscaling', eta0=0.1, random_state=0, max_iter=1000, tol=1e-3)
    sgd.fit(Xtr, ytr)
    ypred = sgd.predict(Xte)
    print(tag, "MSE:", mean_squared_error(yte, ypred), "n_iter_:", sgd.n_iter_)

train(Xtr, Xte, "raw")
ss = StandardScaler()
train(ss.fit_transform(Xtr), ss.transform(Xte), "standardized")
```

### 8.3 ColumnTransformer + 파이프라인 (수치/범주 혼합)
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

num_cols = ["age","income","height","spending"]
cat_cols = ["city","channel"]

preproc = ColumnTransformer([
    ("num", RobustScaler(), num_cols),  # 이상치 견고
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

pipe = Pipeline([
    ("prep", preproc),
    ("clf", LogisticRegression(max_iter=500))
])

scores = cross_val_score(pipe, X_df, y, cv=5, scoring="roc_auc")
print("AUC (5-fold):", scores.mean(), scores.std())
```

### 8.4 희소행렬 처리 (MaxAbs 또는 표준화 with_mean=False)
```python
from scipy.sparse import csr_matrix
from sklearn.preprocessing import MaxAbsScaler, StandardScaler
Xs = csr_matrix(X_dense)  # 예시

# 방법 1: MaxAbs
ma = MaxAbsScaler()
Xs1 = ma.fit_transform(Xs)

# 방법 2: StandardScaler with_mean=False
ss = StandardScaler(with_mean=False)
Xs2 = ss.fit_transform(Xs)
```

### 8.5 시계열: 롤링 스케일링(누수 방지)
```python
import numpy as np

def rolling_standardize(x, win=100, eps=1e-8):
    # x: (T, d)
    Xs = np.empty_like(x)
    mean = np.zeros((x.shape[1],))
    var = np.ones((x.shape[1],))
    for t in range(x.shape[0]):
        lo = max(0, t-win)
        seg = x[lo:t+1]
        mean = seg.mean(axis=0)
        var  = seg.var(axis=0)
        Xs[t] = (x[t] - mean) / np.sqrt(var + eps)
    return Xs

Xs = rolling_standardize(X_time, win=200)
```

### 8.6 대상(y) 로그 스케일링 + 역변환
```python
import numpy as np
from sklearn.ensemble import RandomForestRegressor

y_tr_log = np.log1p(y_tr)
rf = RandomForestRegressor().fit(X_tr, y_tr_log)
y_pred_log = rf.predict(X_te)
y_pred = np.expm1(y_pred_log)
```

### 8.7 간단 구현(NumPy) — MinMax/Z-score/Robust
```python
import numpy as np

def minmax_scale(X, feature_range=(0,1)):
    a,b = feature_range
    xmin = X.min(axis=0); xmax = X.max(axis=0)
    denom = np.where(xmax>xmin, xmax-xmin, 1.0)
    return (X - xmin) / denom * (b-a) + a

def zscore(X, eps=1e-8):
    mu = X.mean(axis=0); std = X.std(axis=0)
    return (X - mu) / (std + eps)

def robust_scale(X, q=(25,75), eps=1e-8):
    q1, q3 = np.percentile(X, q, axis=0)
    iqr = np.maximum(q3 - q1, eps)
    med = np.median(X, axis=0)
    return (X - med) / iqr
```

---

## 9. 체크리스트 & 요약

### 체크리스트
- [ ] **훈련 데이터로만 fit** 하고, 검증/테스트에는 transform만 (CV 내에서도 fold-wise).  
- [ ] **알고리즘 특성 반영**: 거리 기반/정규화 모델에는 스케일링 필수, 트리류는 대체로 불필요.  
- [ ] **정규화와 병행**: 릿지/라쏘/엘라스틱넷 전에 표준화(특히 다중공선성).  
- [ ] **희소데이터**: 중앙화 금지 → MaxAbs 또는 Standard(with_mean=False).  
- [ ] **시계열/그룹**: 시간/그룹 경계 넘어 fit 금지(누수).  
- [ ] **스케일 후 파생특징** 순서 주의: 일반적으로 **파생 생성 → 스케일링**.  
- [ ] **타깃 스케일링** 사용 시 반드시 **역변환**로 해석/평가.  
- [ ] **수치안정성**: 아주 큰/작은 값은 로그/파워 변환 후 스케일링.

### 요약
- 스케일링은 **거리 계산의 공정성**, **최적화 수렴 가속**, **정규화 해석 일관성**을 위해 필수다.  
- 표준화/Robust/Min–Max/MaxAbs/Unit-norm/파워/Quantile/Whitening 중 **데이터 분포와 모델**에 맞추어 선택하라.  
- 올바른 스케일링은 단지 “예쁘게 맞추는 것”이 아니라, **조건수 개선이라는 강력한 사전조건화**로 **학습 안정성과 성능**을 좌우한다.

---
**부록: 핵심 수식 모음**

- **Min–Max**:  
$$
x'=\frac{x-x_{\min}}{x_{\max}-x_{\min}}\cdot(b-a)+a
$$

- **Z-score**:  
$$
x'=\frac{x-\mu}{\sigma}
$$

- **Robust**:  
$$
x'=\frac{x-\mathrm{median}(x)}{\mathrm{IQR}(x)}
$$

- **Unit-norm (행 기준)**:  
$$
\mathbf{x}'=\frac{\mathbf{x}}{\|\mathbf{x}\|_p}
$$

- **사전조건화 해석**:  
\(X'=X D^{-1},\ w'=D w\) 에 대해  
$$
\min_w\|Xw-y\|^2 \equiv \min_{w'}\|X' w' - y\|^2,\quad 
\kappa(X')\ll \kappa(X)\ \text{(적절한 }D\text{ 선택 시)}.
$$