---
layout: post
title: 기계학습 - 스케일링
date: 2025-08-14 16:20:23 +0900
category: 기계학습
---
# 스케일링(Scaling)

머신 러닝에서 **스케일링(Scaling)**은 서로 다른 크기와 단위를 가진 피처(feature) 값을 일정한 범위나 분포로 변환하는 과정입니다.  
스케일링은 특히 **거리 기반 알고리즘**(KNN, K-Means, SVM)과 **경사하강법(Gradient Descent)**을 사용하는 모델에서 학습 속도와 성능에 큰 영향을 줍니다.

---

## 1. 왜 스케일링이 필요한가?

### (1) 변수 간 단위 차이 문제
예:
- `키`: 150 ~ 200 cm
- `몸무게`: 40 ~ 100 kg
- `연봉`: 2,000,000 ~ 200,000,000 원

위 데이터를 그대로 사용하면, **스케일이 큰 변수**(연봉)가 거리를 지배하게 됩니다.  
→ 모델이 실제 중요한 특징보다 값의 크기에 민감해져서 **왜곡된 결과**가 발생할 수 있습니다.

---

### (2) 경사하강법의 수렴 속도
- 스케일이 맞지 않으면 비용 함수(cost function)의 등고선이 **타원형**으로 변형되어, 경사하강법이 지그재그로 수렴하게 됩니다.
- 스케일링을 하면 등고선이 **원형**에 가까워져 수렴 속도가 빨라집니다.

---

### (3) 해석 및 비교 용이성
- 스케일링을 하면 모든 변수의 값이 같은 기준 범위나 분포를 가지므로, 변수 중요도 비교가 쉬워집니다.

---

## 2. 주요 스케일링 방법

### (1) 정규화(Normalization, Min-Max Scaling)
- **데이터를 [0, 1] 또는 [-1, 1] 범위로 변환**
- **공식**:
  $$
  x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
  $$
- **특징**:
  - 값의 범위를 고정
  - 이상치(outlier)에 민감
- **적용 예시**:
  - 신경망(입력 범위를 제한)
  - 픽셀 값 처리(0~255 → 0~1)

```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

data = np.array([[1], [2], [3], [4], [5]])
scaler = MinMaxScaler()
scaled = scaler.fit_transform(data)
print(scaled)
```

출력:
```plaintext
[[0.  ]
 [0.25]
 [0.5 ]
 [0.75]
 [1.  ]]
```

---

### (2) 표준화(Standardization, Z-score Scaling)
- **평균을 0, 표준편차를 1로 맞추는 변환**
- **공식**:
  $$
  x' = \frac{x - \mu}{\sigma}
  $$
  - \(\mu\): 평균
  - \(\sigma\): 표준편차
- **특징**:
  - 데이터의 분포를 유지하면서 평균 0, 분산 1
  - 이상치 영향이 정규화보다 적음
- **적용 예시**:
  - SVM, 로지스틱 회귀, 선형 회귀
  - PCA(주성분 분석)

```python
from sklearn.preprocessing import StandardScaler

data = np.array([[1], [2], [3], [4], [5]])
scaler = StandardScaler()
scaled = scaler.fit_transform(data)
print(scaled)
```

출력:
```plaintext
[[-1.26491106]
 [-0.63245553]
 [ 0.        ]
 [ 0.63245553]
 [ 1.26491106]]
```

---

### (3) Robust Scaling
- **중앙값(median)과 IQR(Interquartile Range, 사분위 범위)**을 이용한 스케일링
- **공식**:
  $$
  x' = \frac{x - \text{median}(x)}{\text{IQR}(x)}
  $$
- **특징**:
  - 이상치에 강함
  - 데이터의 중앙값을 0, IQR을 1로 맞춤
- **적용 예시**:
  - 이상치가 많은 데이터셋
  - 금융 거래, 센서 데이터

```python
from sklearn.preprocessing import RobustScaler

data = np.array([[1], [2], [3], [100], [5]])
scaler = RobustScaler()
scaled = scaler.fit_transform(data)
print(scaled)
```

출력:
```plaintext
[[-0.5 ]
 [ 0.  ]
 [ 0.5 ]
 [48.5 ]
 [ 1.5 ]]
```

---

## 3. 어떤 경우에 어떤 스케일링을 쓸까?

| 알고리즘 | 권장 스케일링 | 이유 |
|----------|--------------|------|
| KNN, K-Means | 정규화 / 표준화 | 거리 계산에 스케일 중요 |
| SVM, 로지스틱 회귀 | 표준화 | 가중치 계산 안정화 |
| PCA | 표준화 | 주성분 방향 계산 시 분산 동일화 |
| 신경망 | 정규화 | 입력 범위 제한 시 안정적 학습 |
| 이상치 많은 데이터 | Robust Scaling | 중앙값과 IQR 사용 |

---

## 4. 실전 팁
1. **훈련 데이터로만 스케일러 fit**
   - 데이터 누수(Data Leakage) 방지
   ```python
   scaler.fit(X_train)
   X_train_scaled = scaler.transform(X_train)
   X_test_scaled = scaler.transform(X_test)
   ```
2. **스케일링 방법 비교**
   - 동일 모델에서 스케일링 방식만 바꿔서 교차 검증 성능 비교
3. **파이프라인(Pipeline) 활용**
   - 전처리 + 학습 과정 자동화

---

## 📌 정리
- **정규화**: 데이터 범위를 일정 구간([0,1])으로 조정 → 거리 기반, 신경망에 적합
- **표준화**: 평균 0, 표준편차 1로 조정 → 대부분의 통계·선형 모델에 적합
- **Robust Scaling**: 이상치에 강함 → 극단값 많은 데이터에 적합