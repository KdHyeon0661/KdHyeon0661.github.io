---
layout: post
title: 딥러닝 - 실험관리
date: 2025-10-07 15:25:23 +0900
category: 딥러닝
---
# 실험관리(MLflow/W&B) 베스트 프랙티스 — **파이프라인 로그 · 아티팩트 전부 담기 (PyTorch 중심)**

## 0) 한눈에 비교: MLflow vs W&B

| 항목 | MLflow | W&B |
|---|---|---|
| 추적 서버/스토리지 | 자체 호스팅/로컬/클라우드, **Model Registry 제공** | 관리형 SaaS(기본), 로컬 오프라인/프록시도 가능, **Artfiacts & Registry** |
| 러닝곡선/메트릭 시각화 | 기본 플롯 + 커스텀 아티팩트 | 강력한 실시간 대시보드, **Tables/Reports/Sweeps** |
| 파이프라인 러닝 구조 | **Nested Run**(상위 파이프라인→하위 스테이지) | **Groups/Job Types/Artifacts** |
| 모델 패키징 | **mlflow.<flavor>**(pyfunc/torch 등), Conda/requirements 자동 기록 | Artifact로 모델/데이터 버전화, Registry(Org 플랜 이상) |
| 하이퍼파라미터 탐색 | 별도 HPO 도구(Optuna, Ray) 연동 or Projects | **W&B Sweeps**(Random/Bayes/Grid) 내장 |
| 선택 가이드 | **자체 인프라 + 레지스트리**가 중요 | **협업 대시보드 + HPO**가 중요 |

> 권장: **둘을 동시에** 쓰되, 로그/아티팩트를 **양쪽에 동시 기록**(경량 오버헤드) → 장점만 취합.

---

## 1) 프로젝트 스캐폴딩(추천)

```
exp-demo/
├─ data/                   # (DVC 등으로 관리 가능)
├─ src/
│  ├─ config.py            # 공통 설정/하이퍼
│  ├─ data.py              # Dataset/DataLoader
│  ├─ model.py             # PyTorch 모듈
│  ├─ train.py             # 학습 루프(MLflow/W&B 동시 로깅)
│  ├─ eval.py              # 평가/혼동행렬/곡선
│  ├─ pipeline.py          # 파이프라인(전처리→학습→평가) Nested Run
│  └─ utils_logging.py     # 공통 로깅 헬퍼(이미지/테이블/시그니처 등)
├─ params.yaml             # 하이퍼 기본값(선택)
├─ requirements.txt        # 의존성
├─ mlflow_tracking_uri.txt # 추적 URI 관리(로컬/서버)
└─ README.md
```

---

## 2) 재현성 베이스라인(필수)

- **시드 고정**:  
  $$\text{seed}\Rightarrow \text{DataLoader 셔플},\ \text{PyTorch 연산},\ \text{NumPy},\ \text{Python random}$$  
- **환경 캡처**: Git SHA, `requirements.txt`/`conda.yaml`, CUDA/CUDNN 버전  
- **데이터 지문**: 입력 데이터 **해시/행수/통계**를 런에 기록

```python
# src/config.py
import os, random, numpy as np, torch, subprocess, json, hashlib
from datetime import datetime

def set_seed(seed: int = 42):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def git_sha():
    try:
        return subprocess.check_output(["git","rev-parse","HEAD"], text=True).strip()
    except Exception:
        return "unknown"

def file_sha256(path):
    h=hashlib.sha256()
    with open(path,"rb") as f:
        for ch in iter(lambda:f.read(1<<20), b""):
            h.update(ch)
    return h.hexdigest()

def env_snapshot():
    return {
        "time": datetime.utcnow().isoformat()+"Z",
        "git_sha": git_sha(),
        "python": f"{os.sys.version_info.major}.{os.sys.version_info.minor}",
        "cuda": torch.version.cuda if torch.cuda.is_available() else "cpu",
        "torch": torch.__version__,
    }

def save_json(obj, path):
    with open(path,"w",encoding="utf-8") as f: json.dump(obj,f,indent=2,ensure_ascii=False)
```

---

## 3) 데이터/모델/평가 템플릿 (PyTorch)

```python
# src/data.py
import torch, torchvision
from torchvision import transforms
from torch.utils.data import DataLoader

def build_dataloaders(data_root, batch_size=64, num_workers=4):
    tf_train = transforms.Compose([
        transforms.Resize((128,128)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
    ])
    tf_val = transforms.Compose([
        transforms.Resize((128,128)),
        transforms.ToTensor(),
        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
    ])
    train = torchvision.datasets.FakeData(size=2000, image_size=(3,128,128), num_classes=5, transform=tf_train)
    val   = torchvision.datasets.FakeData(size=400, image_size=(3,128,128), num_classes=5, transform=tf_val)
    return (
        DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True),
        DataLoader(val, batch_size=batch_size*2, shuffle=False, num_workers=num_workers, pin_memory=True),
        5
    )
```

```python
# src/model.py
import torch, torch.nn as nn, torchvision.models as models

class SmallCNN(nn.Module):
    def __init__(self, n_cls=5):
        super().__init__()
        m = models.resnet18(weights=None)
        m.fc = nn.Linear(m.fc.in_features, n_cls)
        self.m = m
    def forward(self, x): return self.m(x)
```

```python
# src/utils_logging.py
import io, numpy as np, torch, sklearn.metrics as skm
from PIL import Image

def make_confmat_fig(y_true, y_pred, labels=None):
    import matplotlib.pyplot as plt
    cm = skm.confusion_matrix(y_true, y_pred, labels=None)
    fig = plt.figure(figsize=(4,4))
    plt.imshow(cm, interpolation='nearest'); plt.title("Confusion Matrix"); plt.colorbar()
    tick = np.arange(cm.shape[0])
    plt.xticks(tick, labels if labels else tick, rotation=45); plt.yticks(tick, labels if labels else tick)
    plt.tight_layout()
    return fig

def fig_to_png_bytes(fig):
    import matplotlib.pyplot as plt
    buf = io.BytesIO()
    fig.savefig(buf, format="png", dpi=150, bbox_inches="tight")
    plt.close(fig); buf.seek(0)
    return buf.getvalue()

def sample_grid(images, max_n=16):
    # [N,3,H,W] tensor → PIL grid
    images = images[:max_n].detach().cpu()
    images = (images - images.min())/(images.max()-images.min()+1e-6)
    B,C,H,W = images.shape
    n = int(np.ceil(np.sqrt(B)))
    canvas = torch.zeros(3, n*H, n*W)
    k=0
    for i in range(n):
        for j in range(n):
            if k>=B: break
            canvas[:, i*H:(i+1)*H, j*W:(j+1)*W]=images[k]
            k+=1
    img = (canvas*255).byte().permute(1,2,0).numpy()
    return Image.fromarray(img)
```

---

## 4) **MLflow + W&B 동시 로깅** 트레이닝 루프

- **MLflow**: `mlflow.set_experiment`, `mlflow.start_run(nested=True)`, `log_param`, `log_metric`, `log_artifact`, **모델 저장/서명**  
- **W&B**: `wandb.init(project=..., group=..., job_type=...)`, `wandb.config.update`, `wandb.log`, **Artifacts**(`wandb.Artifact`)

```python
# src/train.py
import os, time, json, mlflow, wandb, torch, torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score
from pathlib import Path
from src.config import set_seed, env_snapshot, save_json
from src.data import build_dataloaders
from src.model import SmallCNN
from src.utils_logging import make_confmat_fig, fig_to_png_bytes, sample_grid

def train_one_epoch(model, dl, optim, device):
    model.train()
    crit = nn.CrossEntropyLoss()
    tot_loss, tot_n = 0.0, 0
    t0=time.time()
    for step,(x,y) in enumerate(dl,1):
        x,y = x.to(device), y.to(device)
        optim.zero_grad()
        out = model(x); loss = crit(out,y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optim.step()

        tot_loss += loss.item()*len(x); tot_n += len(x)
        if step % 20 == 0:
            mlflow.log_metric("train/step_loss", loss.item(), step=step)
            wandb.log({"train/step_loss": loss.item(), "train/step": step})
    return tot_loss/tot_n, time.time()-t0

@torch.no_grad()
def evaluate(model, dl, device):
    model.eval()
    ys, ps, loss_sum, n = [], [], 0.0, 0
    crit = nn.CrossEntropyLoss()
    for x,y in dl:
        x,y = x.to(device), y.to(device)
        out = model(x)
        loss = crit(out,y)
        p = out.argmax(1)
        ys.append(y.cpu()); ps.append(p.cpu())
        loss_sum += loss.item()*len(x); n += len(x)
    y_true = torch.cat(ys).numpy(); y_pred = torch.cat(ps).numpy()
    acc = accuracy_score(y_true, y_pred)
    return loss_sum/n, acc, (y_true, y_pred)

def main():
    # ------- 구성 -------
    cfg = {
        "seed": 42, "epochs": 5, "lr": 3e-4,
        "batch_size": 64, "num_workers": 4,
        "data_root": "data", "project": "exp-best-practice",
        "exp_name": "cnn-fakedata", "tags": ["demo","cnn"],
    }
    set_seed(cfg["seed"])
    device = "cuda" if torch.cuda.is_available() else "cpu"
    Path("outputs").mkdir(exist_ok=True)

    # ------- MLflow 세팅 -------
    tracking_uri = Path("mlflow_tracking_uri.txt").read_text().strip() if Path("mlflow_tracking_uri.txt").exists() else "mlruns"
    mlflow.set_tracking_uri(tracking_uri)
    mlflow.set_experiment(cfg["project"])

    with mlflow.start_run(run_name=cfg["exp_name"], tags={t:t for t in cfg["tags"]}) as run:
        run_id = run.info.run_id
        mlflow.log_params({k:v for k,v in cfg.items() if k not in ["project","exp_name","tags"]})
        mlflow.log_dict(env_snapshot(), "env.json")

        # ------- W&B 세팅 -------
        wandb.init(
            project=cfg["project"], name=cfg["exp_name"],
            tags=cfg["tags"], config=cfg, save_code=True
        )

        # ------- 데이터/모델 -------
        tr, va, n_cls = build_dataloaders(cfg["data_root"], cfg["batch_size"], cfg["num_workers"])
        model = SmallCNN(n_cls).to(device)
        optim = AdamW(model.parameters(), lr=cfg["lr"])

        # 샘플 이미지 아티팩트
        xb, yb = next(iter(tr))
        grid = sample_grid(xb, max_n=16)
        grid_path = Path("outputs/sample_grid.png"); grid.save(grid_path)
        mlflow.log_artifact(str(grid_path), artifact_path="samples")
        wandb.log({"samples/grid": wandb.Image(str(grid_path))})

        # ------- 학습 루프 -------
        best = -1.0
        for ep in range(1, cfg["epochs"]+1):
            tr_loss, tr_time = train_one_epoch(model, tr, optim, device)
            va_loss, va_acc, (y_true, y_pred) = evaluate(model, va, device)

            # 로그(스칼라)
            step = ep
            mlflow.log_metrics({
                "train/epoch_loss": tr_loss,
                "val/loss": va_loss,
                "val/acc": va_acc,
                "time/epoch_sec": tr_time,
                "lr": optim.param_groups[0]["lr"]
            }, step=step)
            wandb.log({"train/epoch_loss": tr_loss, "val/loss": va_loss, "val/acc": va_acc, "epoch": ep, "lr": optim.param_groups[0]["lr"]})

            # 혼동행렬 이미지
            fig = make_confmat_fig(y_true, y_pred)
            png = fig_to_png_bytes(fig)
            cm_path = Path(f"outputs/confmat_ep{ep}.png"); cm_path.write_bytes(png)
            mlflow.log_artifact(str(cm_path), artifact_path="eval")
            wandb.log({"eval/confusion_matrix": wandb.Image(str(cm_path)), "epoch": ep})

            # 체크포인트 저장(베스트만)
            if va_acc > best:
                best = va_acc
                ckpt = {"model_state": model.state_dict(), "config": cfg}
                torch.save(ckpt, "outputs/best.pt")
                mlflow.log_artifact("outputs/best.pt", artifact_path="checkpoints")
                wandb.save("outputs/best.pt", base_path=".")
                wandb.run.summary["best_val_acc"] = best

        # 최종 메트릭
        mlflow.log_metric("final/best_val_acc", best)
        wandb.run.summary["final/best_val_acc"] = best

        # MLflow 모델(선택) 저장 & 시그니처 기록
        # 간단 데모: torchscript로 저장 후 pyfunc로 래핑하는 패턴도 가능
        # 여기선 아티팩트만 기록. 실무는 mlflow.pytorch.log_model 권장.
        # mlflow.pytorch.log_model(model, "model", registered_model_name="cnn-fakedata")

        wandb.finish()
        print(f"MLflow run_id: {run_id}")

if __name__=="__main__":
    main()
```

**포인트**
- **스칼라 메트릭**은 에폭/스텝 단계별 모두 기록.  
- **이미지/표**는 **아티팩트** 또는 **W&B Image/Table**로 기록.  
- **체크포인트**는 최소 **best**와 **last**(선택) 기록 → 복원/재개 쉽게.

---

## 5) 파이프라인 전단계 로깅(전처리→학습→평가) — **Nested Runs & Job Types**

- **MLflow**: `with mlflow.start_run():` 아래 **스테이지별 `nested=True`**로 하위 런  
- **W&B**: `job_type`(`"preprocess"`, `"train"`, `"eval"`)과 `group`(파이프라인 ID)로 묶기

```python
# src/pipeline.py
import mlflow, wandb, os, time
from pathlib import Path
from src.config import env_snapshot, save_json
from src.train import main as train_main

def preprocess_stage():
    time.sleep(1)  # 전처리 예시(실제는 스크립트 실행/통계 산출)
    Path("outputs").mkdir(exist_ok=True)
    Path("outputs/pre_stats.json").write_text('{"rows":2000,"cols":128}', encoding="utf-8")
    mlflow.log_artifact("outputs/pre_stats.json", artifact_path="preprocess")
    wandb.log({"preprocess/rows": 2000, "preprocess/cols": 128})

def eval_stage():
    # 평가 산출물 업로드(예: 예측 CSV, PR/ROC 곡선 이미지)
    # 여기서는 train.py에서 이미 주요 아티팩트 업로드했다고 가정
    pass

def pipeline_run():
    mlflow.set_experiment("exp-best-practice")
    with mlflow.start_run(run_name="full-pipeline") as parent:
        mlflow.log_dict(env_snapshot(), "env.json")

        wandb.init(project="exp-best-practice", name="full-pipeline", job_type="pipeline", tags=["pipeline"])
        group_id = wandb.run.id  # 그룹 식별자
        wandb.finish()

        # Stage 1: Preprocess (nested)
        with mlflow.start_run(run_name="preprocess", nested=True):
            wandb.init(project="exp-best-practice", name="preprocess", group=group_id, job_type="preprocess")
            preprocess_stage()
            wandb.finish()

        # Stage 2: Train (nested)
        with mlflow.start_run(run_name="train", nested=True):
            wandb.init(project="exp-best-practice", name="train", group=group_id, job_type="train")
            from src.train import main as train_stage
            train_stage()
            wandb.finish()

        # Stage 3: Eval (nested)
        with mlflow.start_run(run_name="eval", nested=True):
            wandb.init(project="exp-best-practice", name="eval", group=group_id, job_type="eval")
            eval_stage()
            wandb.finish()

if __name__=="__main__":
    pipeline_run()
```

**베스트 프랙티스**
- **각 스테이지**가 생성하는 **산출물(아티팩트)**, **스칼라 통계**, **환경 스냅샷**을 **각 하위 런**에 기록  
- 상위 런(파이프라인)은 **요약/메타**만 보유 → DAG 관점에서 추적 용이

---

## 6) **Artifacts**: 무엇을, 어떻게 남길까?

**공통 원칙**
1. **재현과 디버깅에 필요한 최소셋**(config, 코드 스냅샷, 데이터 지문, 학습曲선).  
2. **모델/체크포인트**: `best.pt`, (선택)`last.pt`, (선택)`epoch_k.pt`  
3. **평가 산출물**: 예측 CSV, 혼동행렬 PNG, PR/ROC 곡선, 오답 샘플 이미지  
4. **환경/의존**: `env.json`, `requirements.txt`/`conda.yaml`, git SHA  
5. **데이터 지문**: 입력 데이터 **파일 해시 목록**, 통계(행수, 결측, 클래스 분포)

**MLflow 업로드**: `mlflow.log_artifact(path, artifact_path=...)`  
**W&B 업로드**: `wandb.save(path)`, **대형/버전관리**는 `wandb.Artifact`

```python
# 예: W&B Artifact로 예측 테이블 버전관리
import pandas as pd, wandb
preds = pd.DataFrame({"id":[1,2,3], "y_true":[0,1,2], "y_pred":[0,1,1], "prob":[0.9,0.7,0.4]})
preds.to_csv("outputs/preds.csv", index=False)
artifact = wandb.Artifact("predictions", type="inference")
artifact.add_file("outputs/preds.csv")
wandb.log_artifact(artifact)
```

> **Artifact 타입**을 명확히(`"dataset"`, `"model"`, `"metrics"`, `"inference"`) → 팀에서 소비/DI 관리가 쉬워짐.

---

## 7) 하이퍼파라미터 탐색(HPO)

### 7.1 **W&B Sweeps**(권장: 간단 강력)
```yaml
# sweep.yaml
program: src/train.py
method: bayes
metric:
  name: val/acc
  goal: maximize
parameters:
  lr:
    min: 1e-5
    max: 1e-3
  batch_size:
    values: [32, 64, 128]
  seed:
    values: [7, 21, 42]
```

```bash
wandb sweep sweep.yaml                # 스윕 생성(리턴된 ID 확인)
wandb agent <SWEEP_ID> --count 20     # 20회 탐색
```

- 각 에이전트 실행 = **개별 런** → MLflow도 같이 켜두면 **양쪽 모두 기록**  
- **베스트 설정**은 W&B 스윕 대시보드에서 쉽게 확인

### 7.2 MLflow와 Optuna 연동(옵션)
```python
# 간단 예시: Optuna로 HPO → 각 trial을 MLflow run으로
import optuna, mlflow

def objective(trial):
    with mlflow.start_run(nested=True):
        lr = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
        bs = trial.suggest_categorical("batch_size", [32,64,128])
        # ... 호출부에서 cfg 갱신 후 train() 실행, 최종 val_acc 반환
        mlflow.log_params({"lr":lr,"batch_size":bs})
        val_acc = 0.8  # 데모
        mlflow.log_metric("val/acc", val_acc)
        return val_acc

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)
```

---

## 8) 모델 패키징 & 레지스트리

### 8.1 MLflow 모델 저장/서명
- 입력/출력 시그니처를 정의하면 **서빙/배포 호환성**↑  
- **예측 함수 래퍼(pyfunc)**로 커스텀 전처리 포함 가능

```python
# 예: mlflow.pytorch.log_model
# (train.py의 마지막 부분에)
import mlflow.pytorch
sig = mlflow.models.signature.infer_signature(
    model_input=torch.randn(1,3,128,128), model_output=torch.randn(1,5)
)
mlflow.pytorch.log_model(model, artifact_path="model", signature=sig, registered_model_name="cnn-fakedata")
```

- **Model Registry**에서 Staging→Production 승격, **모델 버전** 관리

### 8.2 W&B Model Registry(Org 플랜 이상)
- 모델 아티팩트로 버전 관리 → **리포트/대시보드에서 바로 비교**  
- 배포 스택과의 연결은 MLflow 만큼 표준화되진 않았지만 **CI/CD 통합은 용이**

---

## 9) 지표/곡선 로깅 베스트 프랙티스

- **스칼라**: `loss`, `acc`, `lr`, `grad_norm`, `throughput(img/s)`, `gpu_mem(MiB)`  
- **분포**: 가중치/그래디언트 히스토그램(샘플링)  
- **곡선**:  
  - **ROC/PR**:  
    $$\text{TPR}=\frac{TP}{TP+FN},\quad \text{FPR}=\frac{FP}{FP+TN}$$  
    $$\text{Precision}=\frac{TP}{TP+FP},\quad \text{Recall}=\frac{TP}{TP+FN}$$  
  - 학습/검증 손실/정확도 vs 에폭  
- **테이블/케이스 스터디**: 오답 Top-K, 엣지 케이스 스크린샷  
- **데이터 드리프트 신호**: 클래스 분포/요약 통계 변화

```python
# 예: ROC/PR 계산 후 이미지로 저장해 업로드
import numpy as np, matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve

def log_curves(probs, y_true):
    fpr, tpr, _ = roc_curve(y_true, probs)
    roc_auc = auc(fpr, tpr)
    plt.figure(); plt.plot(fpr,tpr); plt.title(f"ROC AUC={roc_auc:.3f}")
    Path("outputs/roc.png").write_bytes(fig_to_png_bytes(plt.gcf()))
    mlflow.log_artifact("outputs/roc.png", "eval"); wandb.log({"eval/roc": wandb.Image("outputs/roc.png")})

    pr, rc, _ = precision_recall_curve(y_true, probs)
    plt.figure(); plt.plot(rc, pr); plt.title("PR Curve")
    Path("outputs/pr.png").write_bytes(fig_to_png_bytes(plt.gcf()))
    mlflow.log_artifact("outputs/pr.png", "eval"); wandb.log({"eval/pr": wandb.Image("outputs/pr.png")})
```

---

## 10) 데이터/아티팩트 **버전 상호참조**(강력 추천)

- 각 런에 **데이터 해시/스냅샷 ID**를 **태그/파라미터**로 기록  
- 아티팩트 내 `manifest.json`에 **원본→파생→특성** 버전 체인 기록  
- 이렇게 하면 **“이 모델은 어떤 데이터/파이프라인으로 만들어졌는가?”**가 즉시 역추적됨

```python
manifest = {
  "raw_sha": "…",
  "processed_sha": "…",
  "features_version": "r18-2025-10-28",
  "code_git_sha": "…",
}
save_json(manifest, "outputs/manifest.json")
mlflow.log_artifact("outputs/manifest.json", "meta")
artifact = wandb.Artifact("manifest","metadata"); artifact.add_file("outputs/manifest.json")
wandb.log_artifact(artifact)
```

---

## 11) CI/CD & 오프라인/비공개 환경

- **오프라인 W&B**: `WANDB_MODE=offline` → 후에 `wandb sync`로 업로드  
- **비공개 MLflow**: 사내 Tracking Server(+ S3/GCS Artifact Store)  
- **GitHub Actions**: MLflow는 로컬 디렉토리 추적/아티팩트 업로드, W&B는 `WANDB_API_KEY` 시크릿 사용

```yaml
# .github/workflows/train.yaml (요지)
name: train
on: [push]
jobs:
  run:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v5
      with: { python-version: "3.11" }
    - run: pip install -r requirements.txt
    - run: echo "mlruns" > mlflow_tracking_uri.txt
    - env:
        WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
      run: python -m src.train
    - run: python -m src.pipeline
```

---

## 12) 품질 회귀(Regression) 감시

- 기준 **Baseline Run ID** 고정 → 주기적으로 **메트릭 diff**  
- **허용 편차**를 넘으면 **실패 태그**(W&B) 또는 **빌드 실패**(CI)  
- 데이터/코드 버전 변화에 따른 **원인 분석**:  
  - R-MISS(리트리벌 미스), PK-DROP(패킹 누락), NLI-UNSUP(근거 불충분)…(RAG 맥락)  
  - 분류 문제는 **클래스별 F1** 저하 감지

---

## 13) 체크리스트 요약

- [ ] **시드/환경/데이터 지문** 기록  
- [ ] **Nested Runs / job_type**로 파이프라인 단계별 로그  
- [ ] 스칼라/곡선/이미지/테이블/예측 CSV 등 **아티팩트 풀세트**  
- [ ] **best/last** 체크포인트 + **manifest.json**  
- [ ] **Model Signature**와 **Registry**(MLflow) or **W&B Artifacts**  
- [ ] **HPO**: W&B Sweeps or Optuna(+MLflow nested run)  
- [ ] **오프라인/비공개** 모드 고려, 시크릿 안전하게  
- [ ] **회귀 감시**: 기준과 diff 자동화

---

## 14) 흔한 문제 & 해결

- **중복 로그/아티팩트 난립** → `run_name` 규칙, 날짜/브랜치/sha 포함, 오래된 런 정리 정책  
- **이미지/표 대형화** → 썸네일/샘플링 저장, 전체는 압축 아카이브  
- **OOM/시간 초과** → 로그 빈도 조절(스텝→에폭), 아티팩트는 **베스트/요약본** 위주  
- **W&B 방화벽/내부망** → 오프라인 모드+사후 `wandb sync`, 또는 **on-prem 프록시**  
- **MLflow 경로 충돌** → 실험별 artifact_location 분리, 커스텀 S3 prefix

---

## 15) API 치트시트

**MLflow**
```python
import mlflow
mlflow.set_tracking_uri("mlruns")                 # or http://server:5000
mlflow.set_experiment("my-exp")
with mlflow.start_run(run_name="trial-1", tags={"stage":"train"}):
    mlflow.log_param("lr", 1e-3)
    mlflow.log_metric("val/acc", 0.88, step=1)
    mlflow.log_artifact("outputs/best.pt", "checkpoints")
    mlflow.log_dict({"a":1},"meta/info.json")
    # mlflow.pytorch.log_model(model, "model", registered_model_name="my-model")
```

**W&B**
```python
import wandb
wandb.init(project="my-exp", name="trial-1", tags=["train"], config={"lr":1e-3})
wandb.log({"val/acc":0.88, "epoch":1})
artifact = wandb.Artifact("best-ckpt","model"); artifact.add_file("outputs/best.pt")
wandb.log_artifact(artifact)
wandb.finish()
```

---

# 마무리

- **핵심 메시지**: “**데이터 지문**과 **파이프라인 단계별 로그** 그리고 **아티팩트**를 **일관된 스키마**로 남겨라.”  
- MLflow와 W&B를 **동시에 얇게 얹으면** 레지스트리·대시보드·협업·HPO 강점을 **모두** 얻습니다.  
- 본문 템플릿을 복붙해 **바로 실행**하고, 프로젝트에 맞춰 **스테이지/아티팩트 스키마**만 커스터마이즈하세요.  