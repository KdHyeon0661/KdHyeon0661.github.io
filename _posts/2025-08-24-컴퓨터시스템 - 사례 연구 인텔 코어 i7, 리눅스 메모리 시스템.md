---
layout: post
title: 컴퓨터시스템 - 사례 연구 - 인텔 코어 i7, 리눅스 메모리 시스템
date: 2025-08-24 15:20:23 +0900
category: 컴퓨터시스템
---
# 사례 연구: 인텔 Core i7 / 리눅스 메모리 시스템

> **핵심 요약**
> - **하드웨어**: Core i7 계열은 64B 캐시라인, 다단 캐시(L1/L2/LLC), 다단 TLB, 하드웨어 프리페처, PCID/INVPCID를 통해 문맥 전환 비용을 줄인다.
> - **리눅스**: 페이지 기반 가상메모리(4 KB 기본, 2 MB/1 GB 대페이지), COW, THP, NUMA “first-touch” 정책으로 성능/효율 균형을 잡는다.
> - **프로그램**: **지역성(시간/공간)**, **TLB 도달범위(TLB reach)**, **NUMA 로컬리티**, **폴트/프리페치/MLP**를 의식한 자료구조와 접근 패턴이 성패를 가른다.

> **주의(세대별 차이)**
> Core i7은 세대에 따라 캐시/ TLB 크기, LLC(inclusive/non-inclusive) 정책이 조금씩 다릅니다. 아래 값들은 **전형적 범위/관례**를 제시하며, 정밀 수치는 사용 중인 CPU의 CPUID/SDM를 확인하세요.

---

## 1. Core i7 메모리 하드웨어 구조 (개요와 현재적 관례)

### 1.1 캐시 계층(64B 라인, write-back, MESI 일관성)
| 레벨 | 전형적 크기(코어당/소켓공유) | 공유범위 | 접근지연(개략) | 참고
|---|---|---|---|---|
| L1I/L1D | 각 32 KB | 코어 전용 | ~4–5 cyc | VIPT(virtually-indexed, physically-tagged) 계열
| L2     | 256–512 KB | 코어 전용 | ~10–14 cyc | 세대별 256/512 KB
| L3(LLC) | 수 MB(예: 8–30 MB) | 소켓 내 코어 공유 | 수십 cyc | 보통 PIPT/LLC; 세대에 따라 (비)포괄성 정책 상이

- **프리페처**: DCU(데이터), IP 기반, L2 스트라이드/스트리밍 등. **연속 스트라이드**에 강함.
- **동일 캐시라인 경쟁**은 MESI(Coherent) 프로토콜로 조정. **거짓 공유(false sharing)**를 유발하면 코어 간 invalidation 폭증.

### 1.2 TLB 구조(전형)
- **L1 DTLB**: 4 KB 페이지 수십 엔트리(예: 64), 2 MB/1 GB용 별도 소수 엔트리.
- **STLB(2차 TLB)**: 수백~천여 엔트리(세대별 상이).
- **PCID**: 컨텍스트 전환 시 **TLB/페이지구조 캐시 유지**(KPTI 부담 완화). Linux는 PCID/INVPCID 가능 시 적극 활용.

### 1.3 메모리 채널·NUMA
- 단일 소켓 i7: **UMA**(균등 접근). 멀티소켓/특수 플랫폼: **NUMA**(노드 간 지연/대역 차).
- DDR4/DDR5 **2채널 이상**. 채널/랭크/은행 병렬성으로 **MLP**(Memory Level Parallelism) 확보.

---

## 2. 리눅스 가상메모리: 주소공간·페이지·폴트

### 2.1 주소공간(64비트, 4/5-레벨 페이징)
```
사용자 :  0x0000_0000_0000_0000  ───────────────────────────────────► ~ 수십–수백 TB
커널   :  0xffff_8000_0000_0000  ───────────────────────────────────► 최상위 영역
```
- **ASLR**: mmap/스택/라이브러리 베이스 랜덤화.
- **5-레벨 페이징(LA57)**: 일부 최신 플랫폼/커널에서 활성화 가능(가상주소 확장).

### 2.2 페이지 크기
- **4 KB**(기본), **2 MB**(Huge/THP), **1 GB**(HugeTLB, 지원 시).
- **THP(Transparent Huge Page)**: 커널이 4KB 묶음을 2MB로 자동 승격(정책/defrag 의존).
- **명시적 HugeTLB**: `MAP_HUGETLB`, hugetlbfs, 사전 예약 필요.

### 2.3 페이지 폴트
1) **소프트 폴트(minor)**: 이미 메모리에 있으나 PTE 미설정(매핑만 생성).
2) **메이저 폴트(major)**: 디스크에서 읽어와야 함(매우 느림).
3) **COW 폴트**: fork 이후 첫 쓰기 시 사본 생성.

---

## 3. 주소변환 경로와 AMAT(평균 메모리 접근시간) 모델

### 3.1 단계
1. VA → **TLB 조회**(히트 시 물리주소)
2. **캐시 계층 조회**(L1→L2→LLC→메모리)
3. **TLB 미스**면 하드웨어 페이지 워커가 페이지테이블 트리 탐색 → PTE 없으면 커널로 트랩(폴트 핸들러)

### 3.2 분석식(개략)
- **TLB-AMAT**
  $$
  T_{\text{TLB}} = h_{\text{TLB}} \cdot t_{\text{TLB}} + (1-h_{\text{TLB}})\cdot t_{\text{walk}}
  $$
  여기서 \(h_{\text{TLB}}\): TLB 히트율, \(t_{\text{walk}}\): 페이지워크 비용.

- **캐시-AMAT**
  $$
  T_{\text{cache}} = h_1 t_1 + (1-h_1)\!\big[h_2 t_2 + (1-h_2)(h_3 t_3 + (1-h_3)t_{\text{mem}})\big]
  $$

- **통합 개념**
  $$
  T_{\text{access}} \approx T_{\text{TLB}} + T_{\text{cache}}
  $$
  (실제는 파이프라이닝/겹침·MLP로 단순합보다 작은 경우 다수)

### 3.3 **TLB 도달범위(Reach)**
- 4 KB L1 DTLB 64엔트리 → **256 KB** 커버
- 2 MB TLB 32엔트리 → **64 MB** 커버
- **대페이지**로 TLB 리치 급상승 ⇒ **페이지워크 감소**가 큰 이득

---

## 4. 병목 유형과 원인-대책 매핑

문제 | 전형적 원인 | 핵심 대책
---|---|---
**TLB 미스↑** | 큰 워킹셋·랜덤 접근 | **2 MB THP/HugeTLB**, 자료구조 압축/슬라이스
**LLC 미스↑** | 비연속 대량 스캔, 충돌 | **스트라이드/블록** 최적화, SoA, 프리페치
**NUMA 비지역성** | 스레드/데이터 배치 분산 | **first-touch**, `numactl/mbind`, 페이지 migrate
**메이저 폴트** | 콜드 파일/스왑 압박 | 워밍업, `readahead/madvise`, 워킹셋 축소
**거짓 공유** | 공유라인에 다른 스레드 필드 | `alignas(64)` 패딩, **write-combining** 설계
**컨텍스트 전환 비용** | 빈번한 switch | PCID 활성, 스레드/핫 루프 고정(`taskset`)

---

## 5. 계측/관찰 도구: 최소 주력 세트

```bash
# 5.1 핵심 PMU 카운터(요약)
perf stat -e cycles,instructions,cache-misses,cache-references \
           -e dTLB-load-misses,dTLB-loads \
           -e L1-dcache-loads,L1-dcache-load-misses \
           -e LLC-loads,LLC-load-misses \
           -e page-faults,major-faults \
           ./app

# 5.2 NUMA/페이지 배치
numactl --hardware
numastat -p <pid>
cat /proc/<pid>/numa_maps

# 5.3 매핑/상주/THP
cat /proc/<pid>/maps
cat /proc/<pid>/smaps        # Private_Dirty/AnonHugePages 등
cat /sys/kernel/mm/transparent_hugepage/enabled

# 5.4 폴트 트레이싱(bpftrace)
bpftrace -e 'tracepoint:exceptions:page_fault_user { @[comm] = count(); }'
```

---

## 6. 실험 A — 캐시/ TLB 감도 마이크로벤치

### 6.1 스트라이드 접근 vs 캐시/TLB
```c
// cache_tlb_stride.c : 워킹셋/스트라이드에 따른 CPI/시간 변화 관찰
#define _GNU_SOURCE
#include <x86intrin.h>
#include <sched.h>
#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <string.h>
#include <time.h>
#include <unistd.h>

static inline uint64_t rdtsc(){ unsigned a,d; asm volatile("rdtsc":"=a"(a),"=d"(d)); return ((uint64_t)d<<32)|a; }

void pin_to_core(int c){
  cpu_set_t set; CPU_ZERO(&set); CPU_SET(c,&set);
  if (sched_setaffinity(0,sizeof(set),&set)) perror("sched_setaffinity");
}

int main(int argc,char**argv){
  size_t bytes = (argc>1)? strtoull(argv[1],0,0) : (size_t)256<<20; // 256MB
  size_t stride= (argc>2)? strtoull(argv[2],0,0) : 64;              // stride in bytes
  int core     = (argc>3)? atoi(argv[3]) : 0;

  pin_to_core(core);
  size_t n = bytes/sizeof(uint64_t);
  uint64_t *a;
  if (posix_memalign((void**)&a, 4096, n*sizeof(uint64_t))) return 1;
  for(size_t i=0;i<n;i++) a[i]=i;

  size_t step = stride/sizeof(uint64_t);
  uint64_t s=0;
  uint64_t t0=rdtsc();
  for(size_t k=0;k<10;k++){            // 10x 반복으로 노이즈 완화
    for(size_t i=0;i<n;i+=step) s+=a[i];
  }
  uint64_t t1=rdtsc();
  double cycles = (double)(t1-t0);
  double ops = (double)(10*(n/step));
  printf("bytes=%zu stride=%zuB cycles/iter=%.2f, cycles/op=%.2f, sum=%lu\n",
         bytes, stride, cycles/10.0, cycles/ops, (unsigned long)s);
  free(a);
  return 0;
}
```
**사용 예**
```bash
gcc -O2 cache_tlb_stride.c -o stride
perf stat -e cycles,cache-misses,dTLB-load-misses ./stride $((256<<20)) 64
perf stat -e cycles,cache-misses,dTLB-load-misses ./stride $((256<<20)) $((4*1024))
```
- **64B** 스트라이드는 프리페처/캐시 친화.
- **4 KB** 스트라이드는 **TLB thrash** 유발(각 접근이 다른 페이지).

### 6.2 THP 효과 관찰
```bash
# THP 권유: 프로세스 힙에 대해 대페이지 시도
MADV=always   # 또는 "advise"
sudo sh -c 'echo always > /sys/kernel/mm/transparent_hugepage/enabled'
# 실행 전후 dTLB-load-misses, major-faults 비교
```

---

## 7. 실험 B — NUMA 로컬리티(First-Touch)와 이주

### 7.1 first-touch 데모
```c
// numa_first_touch.c : 초기화 스레드의 노드에 페이지가 할당됨
#define _GNU_SOURCE
#include <numa.h>
#include <pthread.h>
#include <sched.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

static void pin(int cpu){ cpu_set_t set; CPU_ZERO(&set); CPU_SET(cpu,&set); sched_setaffinity(0,sizeof(set),&set); }

void* worker(void*arg){
  int cpu = *(int*)arg; pin(cpu);
  size_t N = (size_t)512<<20; // 512MB
  char *buf = aligned_alloc(4096, N);
  memset(buf, 1, N);          // first-touch: cpu의 노드에 배치
  // ... compute ...
  free(buf);
  return NULL;
}
int main(){
  if(numa_available() < 0){ puts("no numa"); return 0; }
  int cpu0=0, cpu1=1;
  pthread_t t0,t1;
  pthread_create(&t0,NULL,worker,&cpu0);
  pthread_create(&t1,NULL,worker,&cpu1);
  pthread_join(t0,NULL); pthread_join(t1,NULL);
}
```
```bash
numactl --hardware
gcc -O2 -lnuma numa_first_touch.c -o nft
numastat -p $$        # 다른 터미널에서 NUMA 로컬/원격 통계 관찰
```

### 7.2 페이지 이주(move_pages)
```bash
# pid의 특정 주소범위를 강제로 노드0로 이주
sudo ./your_app &
PID=$!
sudo bash -c 'printf "%p %x\n" 0x7f0000000000 0x8000000'  # 예시범위
sudo move_pages $PID ...   # (util: numactl/mbind/migratepages 참고)
```

---

## 8. 실전 레시피 — 병목별 처방

### 8.1 TLB thrash 완화
- **2 MB 페이지 사용**:
  - **자동**: THP enabled + `madvise(..., MADV_HUGEPAGE)`
  - **수동**: hugetlbfs + `MAP_HUGETLB`(사전 예약)
- **자료구조 재배치**: SoA(Structure of Arrays), **페이지 로컬** 청크 처리.
- **랜덤 접근 줄이기**: 재정렬, 캐시/배치 알고리즘.

### 8.2 LLC 미스·대역 최적화
- **스트라이드/블록**: 64–256B 단위 묶음 처리.
- **프리페치**: `__builtin_prefetch(p + k)`로 소프트 힌트.
- **MLP 최대화**: 독립 miss 다발로 in-flight 요청 수 늘리기(루프 풀/언롤).

### 8.3 NUMA 로컬리티
- **스레드-데이터 결혼**: 스레드가 사용할 메모리는 그 스레드가 **초기화**(first-touch).
- **고정**: `taskset`, `pthread_setaffinity_np`.
- **정책**: `numactl --cpunodebind --membind`, `mbind()`.

### 8.4 폴트/IO
- 콜드스타트 워밍업: **프리터치**(순회), `madvise(WILLNEED)`.
- 스왑 압박 회피: cgroup 메모리 상한 조정/스왑 비활성(업무 성격에 맞게).
- 파일 매핑: 랜덤/대용량이면 `mmap` + `MADV_RANDOM`/`SEQUENTIAL` 튜닝.

### 8.5 거짓 공유 방지
```c
#include <stdalign.h>
typedef struct {
  alignas(64) volatile uint64_t counter;
  char pad[64 - sizeof(uint64_t)];
} padded_counter;
```

---

## 9. “체크 무기고”: 현장에서 바로 쓰는 명령/코드

### 9.1 퍼포먼스 카운터 단골
```bash
perf stat -e cycles,instructions,IPC,LLC-load-misses,dTLB-load-misses \
           -e mem_load_retired.l1_miss,mem_load_retired.l2_miss \
           -e page-faults,major-faults \
           ./app
```

### 9.2 페이지/THP/NUMA 확인
```bash
grep -i huge /proc/meminfo
cat /sys/kernel/mm/transparent_hugepage/enabled
numactl --show
```

### 9.3 bpftrace 스니펫
```bash
# 유저 폴트 상위 커맨드
bpftrace -e 'tracepoint:exceptions:page_fault_user { @[comm] = count(); }'
# 메이저 폴트 상위 PID
bpftrace -e 'tracepoint:exceptions:page_fault_user /args->fault_major/ { @[pid] = count(); }'
```

### 9.4 코드: 소프트 프리페치 도움
```c
for (size_t i=0; i<n; i+=step){
  __builtin_prefetch(&a[i+64/sizeof*a[0]], 0, 1);
  s+=a[i];
}
```

---

## 10. 메모리 수학(간단 공식 모음)

- **TLB 리치**
  $$ \text{Reach} = (\#\text{entries}_{4K})\cdot 4\text{KB} + (\#\text{entries}_{2M})\cdot 2\text{MB} + \dots $$
  Reach가 워킹셋보다 크면 페이지워크 빈도↓.

- **Roofline 개념(간이)**
  $$ P \le \min\big(P_{\text{peak}},\, B_{\text{mem}}\cdot I\big) $$
  여기서 \(I=\frac{\text{연산}}{\text{바이트}}\) (연산집약도). 메모리 지배 구간에서는 **I**를 키우도록 재구성.

---

## 11. 자주 묻는 질문(FAQ)

**Q1. THP를 “always”로 두면 항상 좋은가?**
A. 아니요. 조각화/defrag 비용·메모리 풋프린트가 커질 수 있다. **“advise”** + **핫 경로에 `madvise(HUGEPAGE)`**가 현실적.

**Q2. `mmap` vs `read` 성능?**
A. 랜덤 접근/부분 갱신/공유 데이터엔 `mmap`이 유리한 경우가 많다. 반대로 I/O 경계를 명확히 추적·제어하려면 `read`가 예측성에서 낫다.

**Q3. PCID는 어떻게 확인?**
A. `grep pcid /proc/cpuinfo`. 켜져 있으면 컨텍스트 전환/ KPTI 비용을 줄이는 데 기여.

---

## 12. 심화: VIPT·동의어(synonym)·포괄성 정책(한 페이지 설명)

- **VIPT** L1: 인덱스는 VA 일부, 태그는 PA. 페이지 컬러링으로 synonym 문제를 회피.
- **LLC 포괄성**: 일부 세대는 **inclusive**, 일부는 **non-inclusive**/“mostly” 정책. 포괄성은 코어 간 스누프/무효화 전략에 영향.
- **결론**: 애플리케이션 레벨에서는 **라인 크기(64B)·지역성·동시성** 관리가 훨씬 큰 효과.

---

## 13. 종합 튜닝 체크리스트

- [ ] **워킹셋** 추정(핫 데이터 크기/패턴).
- [ ] **TLB 히트** 높이기(THP/HugeTLB/슬라이스).
- [ ] **NUMA first-touch** 보장, 스레드/메모리 바인딩.
- [ ] **거짓 공유** 제거(패딩/파티셔닝).
- [ ] **프리페치/스트라이드** 최적화, MLP를 고려한 루프 변환.
- [ ] 콜드스타트 **워밍업**·`madvise(WILLNEED)`.
- [ ] PMU/`perf`/`bpftrace`로 **증거 기반** 검증.
- [ ] THP 정책을 **워크로드별**로(전역 always는 주의).

---

## 14. 부록 — 실무 스니펫 모음

### 14.1 THP 힌트 주기
```c
#include <sys/mman.h>
void *p = aligned_alloc(2<<20, N);           // 2MB 정렬 권장
madvise(p, N, MADV_HUGEPAGE);
```

### 14.2 HugeTLB(명시) 매핑
```c
// mount -t hugetlbfs nodev /mnt/huge ; echo 512 > /proc/sys/vm/nr_hugepages
int fd = open("/mnt/huge/buf", O_CREAT|O_RDWR, 0644);
ftruncate(fd, 1<<30); // 1GB
void* p = mmap(NULL, 1<<30, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_HUGETLB, fd, 0);
```

### 14.3 NUMA 바인딩(런처)
```bash
numactl --cpunodebind=0 --membind=0 ./app
```

### 14.4 거짓 공유 제거(파티션 큐)
```c
typedef struct { alignas(64) int head,tail; char pad[64-8]; } qidx_t;
```

---

## 15. 결론

- **Core i7×Linux 메모리 스택**을 **TLB/캐시/NUMA/폴트/프리페처/THP** 관점에서 보면,
  성능은 “**지역성 유지 + 페이지워크 억제 + 로컬 접근 + 병렬 메모리 요청**”의 네 축이 좌우한다.
- 진단은 **perf/bpftrace/numa stat/smaps**로 **측정-가설-재설계**의 루프를 굴리는 것이 정답이다.
- 마지막으로, **대페이지(2 MB)**와 **first-touch**만 정확히 지켜도 많은 워크로드에서 **한 등급 성능**을 얻는다.
