---
layout: post
title: 기계학습 - 미분과 경사하강법
date: 2025-08-14 20:20:23 +0900
category: 기계학습
---
# 미분과 경사하강법(Gradient Descent)

## 1. 미분 기초

### 1. 정의와 직관
- 한 변수 함수 \(f:\mathbb{R}\to\mathbb{R}\)의 미분:
$$
f'(x) = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}.
$$
- 머신러닝에서 **손실 함수가 파라미터 변화에 얼마나 민감한지** 정량화한다.

### 2. 다변수와 편미분
- \(f:\mathbb{R}^d\to\mathbb{R}\), \(w=(w_1,\dots,w_d)\):
$$
\frac{\partial f}{\partial w_j}(w)=\lim_{h\to 0}\frac{f(w_1,\dots,w_j+h,\dots,w_d)-f(w)}{h}.
$$

### 3. 연쇄법칙(Chain Rule)
- 합성 \(f(g(x))\):
$$
\frac{d}{dx}f(g(x))=f'(g(x))\cdot g'(x).
$$
- 벡터형:
$$
\nabla_x\, h(x)=J_g(x)^\top \nabla f(g(x)),\quad J_g=\frac{\partial g}{\partial x}\ (\text{야코비안}).
$$

---

## 2. Gradient·Jacobian·Hessian

### 1. 기울기(Gradient)
- 스칼라 함수 \(f:\mathbb{R}^d\to\mathbb{R}\)의 **가장 가파른 증가 방향**:
$$
\nabla f(w)=\begin{bmatrix}\frac{\partial f}{\partial w_1}\\ \vdots\\ \frac{\partial f}{\partial w_d}\end{bmatrix}.
$$

### 2. 헤시안(Hessian)
- 곡률 정보(2계 미분):
$$
H_f(w)=\left[\frac{\partial^2 f}{\partial w_i\,\partial w_j}\right]_{i,j=1}^d.
$$
- **볼록성**: \(f\)가 이차 미분 가능이고 \(H_f(w)\succeq 0\)이면 볼록.
- **로컬 2차 근사**(테일러):
$$
f(w+\Delta)\approx f(w)+\nabla f(w)^\top\Delta+\tfrac{1}{2}\Delta^\top H_f(w)\Delta.
$$

---

## 3. 경사하강법(Gradient Descent)

### 1. 기본 업데이트
$$
w_{t+1}=w_t-\alpha_t \nabla L(w_t).
$$
- \(\alpha_t\): 학습률(step size).
- **손실 감소**: \(\alpha\)가 적절하고 \(L\)이 매끄러우면 \(L(w_{t+1})<L(w_t)\)가 보장될 수 있다.

### 2. 종류
- **배치 GD**: 전체 데이터로 \(\nabla L\). 안정적이나 느림.
- **SGD**: 샘플 하나(또는 작은 배치)로 \(\nabla L\) 추정. 빠르지만 노이즈 큼.
- **미니배치**: 타협(현실 표준).

### 3. 의사코드(미니배치)
```python
w = w0
for epoch in range(E):
    for batch in loader:
        grad = grad_estimate(L, w, batch)  # ∇의 추정
        w = w - alpha * grad
```

---

## 4. 학습률과 수렴 이론 (필수 개념만)

### 1. Lipschitz-연속 기울기
- \(\nabla L\)이 \(L\)-Lipschitz이면(매끄러움):
$$
\|\nabla L(u)-\nabla L(v)\|\le L\|u-v\|.
$$
- **고정 학습률** \(\alpha\le 1/L\)에서
$$
L(w_{t+1})\le L(w_t)-\tfrac{\alpha}{2}\|\nabla L(w_t)\|^2.
$$

### 2. 강볼록(strongly convex)일 때
- \(\mu\)-강볼록이면서 \(\nabla L\)이 \(L\)-Lipschitz일 때, \(\alpha=1/L\):
$$
\|w_t-w^*\|\le \left(1-\frac{\mu}{L}\right)^t \|w_0-w^*\|.
$$
(선형 수렴률)

### 3. 라인서치 개념(Armijo/Wolfe)
- 고정 \(\alpha\) 대신, 조건을 만족하는 \(\alpha_t\)를 **백트래킹**으로 선택해 안정/수렴 개선.

---

## 5. 실습 A: 1D/2D 함수의 경사하강 경로

```python
import numpy as np

# 1D: f(w) = w^2 + 2w + 1 = (w+1)^2
def f(w): return w**2 + 2*w + 1
def g(w): return 2*w + 2

w, alpha = 5.0, 0.2
for i in range(10):
    w = w - alpha*g(w)
    print(i+1, w, f(w))

# 2D: f(w) = 0.5 * w^T A w + b^T w  (볼록 2차)
A = np.array([[3.0, 0.5],[0.5, 1.0]])  # 대칭 양의 정부호
b = np.array([-1.0, 2.0])
def f2(w): return 0.5*w@A@w + b@w
def g2(w): return A@w + b

w = np.array([2.0, -3.0])
alpha = 0.4
for i in range(20):
    w = w - alpha*g2(w)
print("final w:", w, "f(w):", f2(w))
```

---

## 6. 실습 B: 선형/로지스틱 회귀 — 수식과 구현

### 6.1 선형 회귀(MSE) — 벡터형 유도
- 데이터 \(X\in\mathbb{R}^{n\times d},\ y\in\mathbb{R}^{n}\), 예측 \(\hat y=Xw\). 손실:
$$
L(w)=\frac{1}{n}\|y-Xw\|_2^2.
$$
- 기울기:
$$
\nabla L(w)= -\frac{2}{n}X^\top(y-Xw)=\frac{2}{n}X^\top(Xw-y).
$$

```python
import numpy as np
from sklearn.metrics import mean_squared_error

def linreg_gd(X, y, lr=0.1, iters=200):
    n, d = X.shape
    w = np.zeros(d)
    for t in range(iters):
        grad = (2.0/n) * X.T @ (X @ w - y)
        w -= lr * grad
    return w

# 테스트
rng = np.random.default_rng(0)
X = rng.normal(size=(200,3))
w_true = np.array([1.0, -2.0, 0.5])
y = X@w_true + rng.normal(0,0.3,size=200)
w_hat = linreg_gd(X,y,lr=0.2,iters=300)
print("MSE:", mean_squared_error(y, X@w_hat), "||w-w*||:", np.linalg.norm(w_hat - w_true))
```

### 6.2 로지스틱 회귀 — 수식 유도
- 확률:
$$
p_i=\sigma(z_i),\quad z_i = x_i^\top w,\quad \sigma(z)=\frac{1}{1+e^{-z}}.
$$
- 음의 로그우도(로지스틱 손실):
$$
L(w)= -\frac{1}{n}\sum_{i=1}^n \left[y_i\log p_i+(1-y_i)\log(1-p_i)\right].
$$
- 기울기:
$$
\nabla L(w)=\frac{1}{n} X^\top(\hat p - y),\quad \hat p = \sigma(Xw).
$$

```python
import numpy as np
from sklearn.metrics import log_loss, accuracy_score

def sigmoid(z): return 1.0/(1.0+np.exp(-z))

def logreg_gd(X, y, lr=0.5, iters=500, l2=0.0):
    n, d = X.shape
    w = np.zeros(d)
    for t in range(iters):
        p = sigmoid(X@w)
        grad = (1.0/n)*X.T@(p - y) + l2*w  # L2 정규화(AdamW와 다르게 표준 L2)
        w -= lr * grad
    return w

# toy
rng = np.random.default_rng(1)
X = rng.normal(size=(300,2))
true_w = np.array([2.0, -1.5])
logit = X@true_w
y = (rng.random(300) < sigmoid(logit)).astype(float)

w = logreg_gd(X,y,lr=0.5,iters=1000,l2=0.01)
p = sigmoid(X@w); yhat = (p>=0.5).astype(float)
print("logloss:", log_loss(y, p), "acc:", accuracy_score(y, yhat))
```

**정규화 주석**  
- 표준 L2는 손실에 \(\lambda\|w\|_2^2\)를 더한다(위 코드의 `l2*w`).  
- **AdamW**는 가중치 감쇠(Decoupled Weight Decay)로 구현(7장 참조).

---

## 7. 고급 최적화 기법

### 7.1 모멘텀(Momentum)
- 속도 \(v\) 도입:
$$
v_{t+1}=\beta v_t+(1-\beta)\nabla L(w_t),\quad
w_{t+1}=w_t-\alpha v_{t+1}.
$$
- 지그재그 감소, 협곡형 곡면에서 수렴 가속.

### 7.2 네스테로프 가속 경사(NAG)
- **미리 한 발 예측**:
$$
v_{t+1}=\beta v_t+(1-\beta)\nabla L(w_t-\alpha \beta v_t),\quad
w_{t+1}=w_t-\alpha v_{t+1}.
$$

### 7.3 적응적 방법
- **Adagrad**:
$$
G_t=\sum_{k=1}^t g_k\odot g_k,\quad w_{t+1}=w_t-\alpha \frac{g_t}{\sqrt{G_t}+\epsilon}.
$$
- **RMSProp**(지수평활):
$$
E[g^2]_t=\rho E[g^2]_{t-1}+(1-\rho)g_t^2,\quad
w_{t+1}=w_t-\alpha \frac{g_t}{\sqrt{E[g^2]_t}+\epsilon}.
$$
- **Adam**(1·2차 모멘트 추정 + 바이어스 보정):
$$
m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t,\quad
v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2,
$$
$$
\hat m_t=\frac{m_t}{1-\beta_1^t},\quad \hat v_t=\frac{v_t}{1-\beta_2^t},\quad
w_{t+1}=w_t-\alpha \frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon}.
$$
- **AdamW**: 가중치 감쇠를 **손실의 L2**가 아니라 **업데이트에 분리 적용**:
$$
w_{t+1}=w_t-\alpha\left(\frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon}+ \lambda w_t\right).
$$

```python
# Adam/AdamW 스켈레톤
def adam_step(w, g, m, v, t, lr=1e-3, b1=0.9, b2=0.999, eps=1e-8, weight_decay=0.0, decoupled=True):
    m = b1*m + (1-b1)*g
    v = b2*v + (1-b2)*(g*g)
    mhat = m/(1-b1**t); vhat = v/(1-b2**t)
    if decoupled and weight_decay>0:
        w = w - lr*weight_decay*w  # AdamW
    w = w - lr*(mhat/(np.sqrt(vhat)+eps))
    if (not decoupled) and weight_decay>0:
        w = w - lr*weight_decay*w  # 표준 L2
    return w, m, v
```

---

## 8. 2차/준2차 방법(개요)

### 8.1 뉴턴법
- 업데이트:
$$
w_{t+1}=w_t - H^{-1}(w_t)\nabla L(w_t).
$$
- 2차 정보로 **빠른 수렴**(로컬). 대규모 딥넷에는 **비현실적**(Hessian/역행렬 비용↑).

### 8.2 (L)BFGS
- **헤시안의 역 근사**를 메모리 효율적으로 갱신(준2차).  
- 중소규모/전이학습의 마지막 레이어 최적화 등에서 유용.

---

## 9. 비미분/제약: 서브그래디언트·프로젝션·프로시멀

### 9.1 서브그래디언트
- 볼록 비미분 함수(예: \(\ell_1\), 힌지)의 일반화 기울기 \(g\in\partial f\).
- 업데이트: \(w\leftarrow w-\alpha g\).

### 9.2 프로젝션(제약)
- 제약 \(w\in\mathcal{C}\)에 대해, **투영**:
$$
w_{t+1}=\Pi_{\mathcal{C}}\big(w_t-\alpha \nabla L(w_t)\big).
$$
- 예: 비음수 \(w\ge 0\), L2-볼 \( \|w\|\le R \).

### 9.3 프로시멀(라쏘)
- 목적 \(L(w)+\lambda\|w\|_1\)에 대해, **ProxGD**:
$$
w_{t+1}=\text{prox}_{\alpha\lambda\|\cdot\|_1}\big(w_t-\alpha\nabla L(w_t)\big),
$$
- **soft-threshold**:
$$
\text{prox}_{\tau\|\cdot\|_1}(z)=\text{sign}(z)\cdot\max(|z|-\tau,\,0).
$$

---

## 10. 역전파와 자동미분(Autodiff)

### 1. 연쇄법칙의 계산 그래프화
- **Reverse-mode**(역전파): 스칼라 출력–다수 파라미터 구조에서 효율적(딥러닝 표준).
- 프레임워크(PyTorch/JAX)는 그래프를 자동 구성하고 \(\nabla\)를 계산.

```python
# PyTorch 예시(개념용)
import torch
X = torch.randn(200, 3)
w = torch.zeros(3, requires_grad=True)
y = X@torch.tensor([1.0,-2.0,0.5]) + 0.3*torch.randn(200)

for _ in range(200):
    yhat = X@w
    loss = ((yhat - y)**2).mean()
    loss.backward()
    with torch.no_grad():
        w -= 0.2*w.grad
        w.grad.zero_()
print("w:", w.detach())
```

### 2. 수치미분으로 그래디언트 체크
- 유한차분:
$$
\frac{\partial f}{\partial w_j}\approx \frac{f(w+he_j)-f(w-he_j)}{2h}.
$$

```python
def grad_check(f, w, g, h=1e-5):
    num = np.zeros_like(w)
    for j in range(len(w)):
        e = np.zeros_like(w); e[j]=1.0
        num[j]=(f(w+h*e)-f(w-h*e))/(2*h)
    print("||num-grad||:", np.linalg.norm(num - g(w)))
```

---

## 11. 일반화와 배치 크기

- **SGD 노이즈**는 종종 **협곡에서 탈출/평평한 최소값** 선호 → 일반화 유리.  
- **배치 크기↑**는 추정 분산↓, 그러나 **일반화 저하** 보고도 많음.  
- 실무: 학습 초기에 큰 LR + 워밍업, 이후 코사인 감쇠/스케줄링 조합 자주 사용.

---

## 12. 실무 실패 패턴 & 디버깅 체크리스트

1) **학습률 부적합**  
- 증상: 손실 발산/진동 → \( \alpha\downarrow \) 또는 라인서치/스케줄.  

2) **특성 스케일 문제(조건수 악화)**  
- 해결: 표준화/정규화, 배치정규화/LayerNorm, 초기화(Xavier/He).  

3) **기울기 폭발/소실**  
- 해결: **그래디언트 클리핑**, 잔차연결(ResNet), 정규화, 게이트형 구조(LSTM).  

4) **데이터 누수**  
- 파이프라인에서 **fit/transform 분리**, 시계열은 시간 분할, 타겟 인코딩 K-fold.  

5) **정규화 해석 혼동**  
- L2(패널티) vs AdamW(감쇠) 구분.  

6) **중간 관측**  
- 학습 초기에 **훈련 손실 ↓** 확인, \(\|\nabla\|\) 로그, LR finder로 대략 범위 추정.

### 간단 LR Finder
```python
# 학습률을 지수적으로 키우면서 손실 관찰 → 적절범위 선택
```

---

## 부록: 라인서치(Armijo) 스켈레톤

```python
def backtracking_line_search(f, g, w, p, alpha0=1.0, c=1e-4, rho=0.5):
    # Armijo: f(w+αp) ≤ f(w) + c α ∇f(w)^T p
    alpha = alpha0
    f0 = f(w); g0 = g(w)
    while f(w + alpha*p) > f0 + c * alpha * (g0 @ p):
        alpha *= rho
        if alpha < 1e-12: break
    return alpha
```

---

## 수식 모음(핵심 정리)

- **GD 업데이트**:
$$
w_{t+1}=w_t-\alpha_t \nabla L(w_t).
$$

- **선형 회귀(벡터)**:
$$
\nabla L(w)=\frac{2}{n}X^\top(Xw-y).
$$

- **로지스틱 회귀**:
$$
\nabla L(w)=\frac{1}{n}X^\top(\sigma(Xw)-y),\quad \sigma(z)=\frac{1}{1+e^{-z}}.
$$

- **모멘텀/Adam(요약)**:
$$
v\!\leftarrow\!\beta v+(1-\beta)g,\quad
m\!\leftarrow\!\beta_1 m+(1-\beta_1)g,\ 
v\!\leftarrow\!\beta_2 v+(1-\beta_2)g^2.
$$

- **프로시멀(라쏘)**:
$$
\text{prox}_{\tau\|\cdot\|_1}(z)=\text{sign}(z)\max(|z|-\tau,0).
$$

---

## 현업 체크리스트

- [ ] 손실/지표 정의 정합성(예: 분류는 CE/ROC-AUC 등)  
- [ ] 데이터 파이프라인: 누수 방지, 스케일링·인코딩 fit 범위 확인  
- [ ] 초기화/정규화: Xavier/He, BN/LN, Dropout/WD(AdamW)  
- [ ] 최적화: SGD(+모멘텀)/AdamW 선택, 배치 크기와 스케줄 설계  
- [ ] 학습률 탐색: LR finder / 코사인/스텝 감쇠 / 워밍업  
- [ ] 모니터링: 훈련/검증 손실·정확도·\(\|\nabla\|\)·학습률·그라디언트 클립  
- [ ] 조기 종료/체크포인트/시드 고정(재현성)  
- [ ] 디버깅: 수치미분으로 그래디언트 체크, 작은 문제에서 수렴 확인  
- [ ] 배포: 입력 스케일 가드, 모델/스키마 버전, 드리프트 알람

---

## 요약
- **미분/기울기**는 손실 변화의 국소 구조를 드러내고, **경사하강법**은 그 반대 방향으로 손실을 줄인다.  
- 수렴 이론의 핵심은 **매끄러움(Lipschitz)**·**(강)볼록성**·**학습률**이며, 실무에서는 **모멘텀/AdamW/스케일링/정규화**가 필수다.  
- 비미분/제약은 **서브그래디언트/프로시멀/프로젝션**으로 다루며, 딥러닝은 **역전파(자동미분)**로 기울기를 효과적으로 계산한다.  
- **데이터 파이프라인/누수 방지/모니터링**까지 포함하는 엔드투엔드 전략이 **성공적인 학습과 일반화**를 보장한다.