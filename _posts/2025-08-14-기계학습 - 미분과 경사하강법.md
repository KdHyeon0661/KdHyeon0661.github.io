---
layout: post
title: 기계학습 - 미분과 경사하강법
date: 2025-08-14 20:20:23 +0900
category: 기계학습
---
# 미분과 경사하강법(Gradient Descent)

머신 러닝에서 **모델 학습의 핵심**은 **손실 함수(Loss Function)**를 최소화하는 것입니다.  
이를 위해 손실 함수의 기울기(Gradient)를 계산하고, 기울기가 가리키는 방향을 이용해 파라미터를 업데이트하는 과정이 필요합니다.  
이 핵심 도구가 바로 **미분(Differentiation)**과 **경사하강법(Gradient Descent)**입니다.

---

## 1. 미분(Differentiation)

### (1) 정의
미분은 **함수의 순간 변화율**을 구하는 연산입니다.  
기계학습에서는 **손실 함수가 가중치 변화에 따라 어떻게 변하는지**를 측정하는 데 사용됩니다.

---

### (2) 기본 개념
- 함수 \( f(x) \)의 미분:
$$
f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$
- 의미: **x가 아주 조금 변했을 때, f(x)가 얼마나 변하는지**

---

### (3) 머신 러닝에서의 예시
선형 회귀에서 손실 함수(MSE):
$$
L(w) = \frac{1}{n} \sum_{i=1}^n (y_i - w x_i)^2
$$
이를 \(w\)에 대해 미분하면:
$$
\frac{\partial L}{\partial w} = -\frac{2}{n} \sum_{i=1}^n x_i (y_i - w x_i)
$$
→ 기울기를 알면 \(w\)를 업데이트하는 방향을 정할 수 있음.

---

### (4) 편미분(Partial Derivative)
- 다변수 함수에서 특정 변수에 대해서만 미분
- 예:
$$
f(w_1, w_2) = w_1^2 + 3w_2^2
$$
$$
\frac{\partial f}{\partial w_1} = 2w_1, \quad \frac{\partial f}{\partial w_2} = 6w_2
$$

---

## 2. 경사(Gradient)

### (1) 정의
- 다변수 함수에서 **각 변수의 편미분 값을 모아 놓은 벡터**
- 기울기 벡터(Gradient Vector):
$$
\nabla f(\mathbf{w}) =
\begin{bmatrix}
\frac{\partial f}{\partial w_1} \\
\frac{\partial f}{\partial w_2} \\
\vdots \\
\frac{\partial f}{\partial w_n}
\end{bmatrix}
$$
- 방향: 함수가 가장 **가파르게 증가하는 방향**
- 머신 러닝에서 **손실 함수를 줄이려면 이 방향의 반대(-)**로 이동

---

## 3. 경사하강법(Gradient Descent)

### (1) 정의
- 손실 함수를 최소화하기 위해 **기울기의 반대 방향**으로 파라미터를 반복적으로 업데이트하는 최적화 알고리즘

---

### (2) 기본 공식
$$
\mathbf{w} := \mathbf{w} - \alpha \cdot \nabla_{\mathbf{w}} L(\mathbf{w})
$$
- \(\mathbf{w}\): 모델 파라미터(가중치)
- \(\alpha\): 학습률(Learning Rate) — 이동하는 스텝 크기
- \(\nabla_{\mathbf{w}} L(\mathbf{w})\): 손실 함수의 기울기

---

### (3) 절차
1. 파라미터 초기화 (임의값)
2. 손실 함수의 기울기 계산
3. 기울기의 반대 방향으로 이동
4. 수렴 조건(기울기 크기 작아짐, 손실 변화 미미)을 만족할 때까지 반복

---

### (4) 경사하강법 종류
1. **배치 경사하강법(Batch Gradient Descent)**
   - 전체 데이터셋을 사용해 기울기 계산
   - 수렴 안정적, 하지만 대규모 데이터에서는 느림
2. **확률적 경사하강법(Stochastic Gradient Descent, SGD)**
   - 샘플 1개로 기울기 계산
   - 빠르지만 손실 값이 요동침
3. **미니배치 경사하강법(Mini-batch Gradient Descent)**
   - 소규모 배치로 기울기 계산
   - 효율성과 안정성 균형

---

### (5) 학습률(α)의 영향
- α가 너무 크면 → 발산(수렴하지 않음)
- α가 너무 작으면 → 수렴 속도 느림
- Adaptive 방법: Adam, RMSProp, Adagrad 등

---

## 4. 파이썬 예제 (단일 변수 함수 최소화)
```python
import numpy as np

# 손실 함수: f(w) = w^2 + 2w + 1
def loss(w):
    return w**2 + 2*w + 1

# 기울기: f'(w) = 2w + 2
def gradient(w):
    return 2*w + 2

w = 5.0          # 초기값
alpha = 0.1      # 학습률

for i in range(20):
    grad = gradient(w)
    w = w - alpha * grad
    print(f"반복 {i+1}: w = {w:.4f}, 손실 = {loss(w):.4f}")
```

---

## 5. 머신 러닝에서의 활용
- **선형 회귀**: MSE 최소화
- **로지스틱 회귀**: 로그 손실 최소화
- **신경망**: 역전파(Backpropagation)로 기울기 계산 → 경사하강법으로 가중치 업데이트

---

## 📌 정리
- **미분**: 함수의 변화율, 경사하강법의 기초
- **기울기(Gradient)**: 다변수 함수에서의 변화율 벡터
- **경사하강법**: 손실 함수를 최소화하기 위해 기울기의 반대 방향으로 이동
- 학습률, 배치 크기, 최적화 알고리즘 선택이 성능에 큰 영향

미분과 경사하강법을 이해하면 머신 러닝 모델이 **"데이터를 보고 스스로 더 나은 파라미터를 찾는 과정"**을 수학적으로 명확히 설명할 수 있습니다.