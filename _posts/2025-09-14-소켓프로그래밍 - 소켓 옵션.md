---
layout: post
title: 소켓프로그래밍 - 소켓 옵션
date: 2025-09-14 21:25:23 +0900
category: 소켓프로그래밍
---
# 소켓 옵션 — 리눅스/C++23

## 0. 전체 그림: 소켓 옵션은 “정책 레버”

TCP 소켓의 성능·안정성을 결정하는 축은 크게 네 가지다.

1. **주소/포트 재사용 정책** — 재기동, 멀티 프로세스, TIME\_WAIT  
2. **버퍼 크기·윈도우 정책** — BDP, 지연·처리량·버퍼블로트  
3. **패킷화·지연 정책** — Nagle, Delayed ACK, NODELAY, CORK  
4. **수명·타임아웃 정책** — Keepalive, USER\_TIMEOUT, LINGER, DEFER\_ACCEPT, FASTOPEN

그리고 관측 도구:

- **`TCP_INFO` / `ss` / `tcpdump`** → 지금 연결이 실제로 어떻게 동작하는지 “숫자”로 검증

옵션 하나하나를 따로 외우기보다,

- “어떤 문제를 해결하려고 쓰는지”
- “어떤 수식/지표와 연결되는지(BDP, RTT, 손실률)”
- “어떤 부작용이 있는지(버퍼블로트, 헤더 오버헤드, RST 종료 등)”

를 같이 기억하는 쪽이 실무에서 훨씬 유용하다.

---

## 1. 공통 유틸: 옵션 세팅/조회 래퍼 (C++23, `sockopt.hpp`)

먼저, **모든 예제에서 재사용할 수 있는** 소켓 옵션 래퍼를 만든다.  
매번 `setsockopt`/`getsockopt`를 직접 쓰면 에러 처리·타입 캐스팅이 지저분해지기 때문에,  
C++23의 `std::expected`와 `std::error_code`를 활용해 **일관된 패턴**을 만든다.

```cpp
// sockopt.hpp
#pragma once
#include <sys/socket.h>
#include <netinet/in.h>
#include <netinet/tcp.h>
#include <unistd.h>
#include <cstring>
#include <expected>
#include <system_error>
#include <string>
#include <print>

inline std::error_code last_errno() {
    return { errno, std::generic_category() };
}

template <class T>
inline std::expected<void, std::error_code>
setopt(int fd, int level, int optname, const T& v) {
    if (::setsockopt(fd, level, optname, &v, sizeof(v)) == 0) {
        return {};
    }
    return std::unexpected(last_errno());
}

template <class T>
inline std::expected<T, std::error_code>
getoptv(int fd, int level, int optname) {
    T v{};
    socklen_t len = sizeof(v);
    if (::getsockopt(fd, level, optname, &v, &len) == 0) {
        return v;
    }
    return std::unexpected(last_errno());
}

inline void print_errno(std::string_view tag) {
    std::print(stderr, "[{}] {}\n", tag, std::strerror(errno));
}
```

이제 이후 코드에서:

```cpp
int one = 1;
if (auto r = setopt(fd, SOL_SOCKET, SO_REUSEADDR, one); !r) {
    print_errno("SO_REUSEADDR");
}
```

같은 식으로 매우 깔끔하게 옵션을 다룰 수 있다.

---

## 2. 주소 재사용: `SO_REUSEADDR` / `SO_REUSEPORT`

### 2.1 TCP 4-튜플과 EADDRINUSE

TCP 연결은 다음 4-튜플로 식별된다.

$$
(\text{local\_addr}, \text{local\_port}, \text{remote\_addr}, \text{remote\_port})
$$

서버는 보통 `(local_addr, local_port)` 만 정해놓고,  
클라이언트에서 들어오는 각 연결이 나머지 두 항목을 채우는 구조다.

문제는:

- 서버 프로세스를 재기동하거나,
- 동일 포트로 여러 워커를 띄우고 싶을 때,

이미 TIME\_WAIT / FIN\_WAIT 등에 남아 있는 커널 엔트리 때문에 `bind()`가 **EADDRINUSE** 로 실패할 수 있다는 것이다.

이 상황을 다루는 대표 옵션이 `SO_REUSEADDR`, `SO_REUSEPORT`다.

---

### 2.2 `SO_REUSEADDR` — 재기동/포트 회수의 기본

#### 2.2.1 리눅스에서의 의미

리눅스에서 `SO_REUSEADDR`는 다음과 같은 의미를 가진다.

- 동일 포트로 **재바인드**를 허용하는 유연성을 높여 준다.
- 특히 **TIME\_WAIT에 있는 구 연결** 때문에 새 서버가 `bind()` 에 실패하는 상황을 완화한다.
- 단, 이미 **완전히 같은 4-튜플**로 활성/listening 상태인 소켓이 있으면 여전히 실패한다.

C++23 래퍼로 사용 예:

```cpp
int s = ::socket(AF_INET6, SOCK_STREAM, 0);
if (s < 0) { print_errno("socket"); /* 에러 처리 */ }

int yes = 1;
if (auto r = setopt(s, SOL_SOCKET, SO_REUSEADDR, yes); !r) {
    print_errno("SO_REUSEADDR");
}
```

#### 2.2.2 BSD/macOS에서의 의미 차이

BSD 계열(FreeBSD, macOS 등)에서는:

- `SO_REUSEADDR`가 **동일 (IP, PORT)에 대한 중복 바인드**에 더 관대하게 작동하는 경향이 있다.
- 잘못 사용하면 “서로 다른 프로세스가 같은 포트의 연결을 나눠 갖는” 이상한 상황이 생길 수 있다.

그래서 크로스 플랫폼 프로젝트에서는:

- “하나의 포트에 하나의 리스너만 띄운다” 는 정책이라면 `SO_REUSEADDR` 정도는 안전하지만,
- 멀티 코어 분산을 위해 여러 리스너를 띄울 땐 **`SO_REUSEPORT`** 를 쓰는 편이 더 명확하다.

---

### 2.3 `SO_REUSEPORT` — 커널 레벨 로드밸런싱

#### 2.3.1 목적

`SO_REUSEPORT`는  
**같은 (IP, PORT)에 여러 소켓을 동시에 `bind()` 하게 한 뒤**, 커널이 **새 연결을 분산**하도록 하는 옵션이다.

- 서버 프로세스를 **코어 수만큼** 띄우고,
- 각 프로세스가 **동일한 포트**에 `bind()`하여 `listen()`한 뒤,
- 커널이 들어오는 연결을 각 리스너에 **로드밸런싱**하는 패턴을 만들 수 있다.

리눅스 코드 예:

```cpp
int lfd = ::socket(AF_INET6, SOCK_STREAM, 0);

int one = 1;
setopt(lfd, SOL_SOCKET, SO_REUSEADDR, one);
setopt(lfd, SOL_SOCKET, SO_REUSEPORT, one);

sockaddr_in6 addr{};
addr.sin6_family = AF_INET6;
addr.sin6_addr   = in6addr_any;
addr.sin6_port   = htons(9000);

::bind(lfd, (sockaddr*)&addr, sizeof(addr));
::listen(lfd, 512);
```

이 코드를 여러 프로세스에서 동일하게 실행하면, 각 프로세스가  
`accept()` 를 통해 들어오는 연결을 나누어 받게 된다.

#### 2.3.2 주의 사항

- 모든 리스너가 **동일한 옵션** (`SO_REUSEADDR`, `SO_REUSEPORT`, `backlog`) 을 갖고 있어야 한다.
- 리눅스 커널은 **해시 기반 분배**를 한다. 완전한 균등은 아니지만 일반적인 부하에서는 충분히 괜찮다.
- `SO_REUSEPORT` 없이 **fork 후 accept** 를 돌리는 패턴도 있지만,
  - FD를 공유하는 구조라서 **락 경합**과 **캐시 친화성** 측면에서 불리할 수 있다.

---

### 2.4 예제: 멀티 프로세스/멀티 스레드 리스너 패턴

간단한 패턴:

1. 부모 프로세스에서 리스너 소켓을 열고 `SO_REUSEADDR`만 적용.
2. `fork()` 로 자식들을 만들고, 모두 같은 `lfd`에 대해 `accept()`를 수행.

보다 현대적 패턴:

- 코어 수만큼 **독립 프로세스**를 띄우고,  
  각 프로세스가 `SO_REUSEADDR | SO_REUSEPORT`를 세팅한 뒤 **각자 `bind()` / `listen()`**.

두 패턴을 모두 시험해 보면서:

- **CPU 사용 균형**
- **컨텍스트 스위칭**
- **캐시 미스/성능**

을 `perf` 등의 도구로 비교하면, `SO_REUSEPORT`의 장단점을 몸으로 느낄 수 있다.

---

## 3. 버퍼 크기: `SO_RCVBUF` / `SO_SNDBUF` 와 BDP

### 3.1 TCP 커널 버퍼와 BDP 복습

TCP에서 한 시점에 네트워크 “파이프” 안에 떠 있는 데이터 양은 **대역폭–지연곱(BDP)** 로 근사할 수 있다.

$$
\text{BDP} = \text{Bandwidth} \times \text{RTT}
$$

예를 들어:

- 1 Gbps 링크, RTT = 100 ms:

$$
\text{BDP} = 1{,}000 \times 10^6 \,\text{b/s} \times 0.1\,\text{s}
          = 100 \times 10^6\,\text{b}
          \approx 12.5\,\text{MB}
$$

즉, **한 번에 약 12.5MB의 인플라이트 데이터** 가 있어야 링크를 포화시키는 수준까지 쓸 수 있다.

이때 실제 인플라이트 상한은:

- 송신 측 **혼잡 윈도우(cwnd)**  
- 수신 측 **수신 윈도우(rwnd)**  
- 양쪽 커널의 **송/수신 버퍼 크기(sndbuf, rcvbuf)**

에 의해 결정된다.

---

### 3.2 `SO_RCVBUF` / `SO_SNDBUF` 의 의미

- `SO_RCVBUF`
  - 커널 **수신 버퍼** 크기 힌트.
  - 이 버퍼가 꽉 차면 **수신 측**은 `rwnd`를 줄이거나 0으로 만들어 **백프레셔**를 건다.
- `SO_SNDBUF`
  - 커널 **송신 버퍼** 크기 힌트.
  - 이 버퍼가 작으면, 애플리케이션이 분명히 더 보내고 싶어도 `send()`가 **블록**되거나 EAGAIN을 리턴할 수 있다.

리눅스에서는:

- `SO_RCVBUF`, `SO_SNDBUF` 세팅값에 **내부 배율(대개 2배)** 이 적용된다.
- `getsockopt`로 조회해 보면, 내가 설정한 값과 다르게 나오는 이유가 여기 있다.

```cpp
int rcv_hint = 4 << 20; // 4MiB
int snd_hint = 4 << 20;

setopt(fd, SOL_SOCKET, SO_RCVBUF, rcv_hint);
setopt(fd, SOL_SOCKET, SO_SNDBUF, snd_hint);

auto gr = getoptv<int>(fd, SOL_SOCKET, SO_RCVBUF);
auto gs = getoptv<int>(fd, SOL_SOCKET, SO_SNDBUF);

std::print("[buf] rcv={} snd={}\n",
           gr ? *gr : -1,
           gs ? *gs : -1);
```

실행 결과는 대략 `rcv=8388608 snd=8388608` 같은 식으로 나올 수 있다.

---

### 3.3 오토튜닝과 상한 (`tcp_rmem` / `tcp_wmem`)

리눅스 TCP 스택은 **오토튜닝(autotuning)** 기능을 가진다.

- `/proc/sys/net/ipv4/tcp_rmem`
- `/proc/sys/net/ipv4/tcp_wmem`

에 **min, default, max** 값이 들어 있다.

커널은:

- 연결 상황에 따라 `SO_RCVBUF`, `SO_SNDBUF`를 **필요시 자동 확대**하면서,
- 해당 *max* 값을 넘지 않도록 제한한다.

옵션으로 힌트를 주더라도:

- 커널 정책이 우선이기 때문에,
- 실제로는 **더 크게** 혹은 **더 작게** 쓰일 수 있다.

그래서 실무에서는:

1. 우선 BDP를 계산해 **최소한 필요한 크기**를 추정하고,
2. `SO_RCVBUF`, `SO_SNDBUF`에 그 정도 값을 힌트로 준 뒤,
3. `TCP_INFO`로 실제 인플라이트, `rcv_space`, 재전송, RTT 변화를 관찰하면서
4. `/proc` 상한까지 포함하여 전체 정책을 조정하는 식으로 접근하는 것이 좋다.

---

### 3.4 UDP에서의 의미 차이

UDP에서도 `SO_RCVBUF` / `SO_SNDBUF`는 적용되지만 의미가 약간 다르다.

- TCP처럼 재전송·혼잡 제어를 하지 않기 때문에,
- 버퍼가 꽉 찌면 그냥 **패킷 드롭**으로 이어진다.
- 특히 수신 측 `SO_RCVBUF`가 작으면, burst traffic 동안 패킷 유실이 크게 늘어난다.

UDP 기반 프로토콜(예: 자체 재전송·FEC를 구현한 스트리밍)에서는:

- 예상되는 burst 크기, RTT, 패킷당 크기를 감안해
- **수신 버퍼 / 송신 버퍼를 넉넉히** 잡아주는 것이 중요하다.

---

### 3.5 버퍼 튜닝 전략 요약

1. **링크 특성 파악**: RTT, 대역폭
2. **BDP 계산**:
   - $$ \text{BDP} = \text{BW} \times \text{RTT} $$
3. `SO_RCVBUF`, `SO_SNDBUF` (및 `/proc` 상한)을  
   **BDP 이상의 값** + 약간 여유 있게 잡는다.
4. `TCP_INFO` 로 `rcv_space`, `tcpi_snd_cwnd`, `rtt/rttvar` 를 보면서
   - 처리량이 **BDP 제한**에 막힌 것인지
   - 아니면 **손실/지연/CPU** 쪽 문제인지 구분한다.
5. 지나치게 크게 잡았을 때 RTT가 비정상적으로 늘어나면, **버퍼블로트**를 의심하고 다시 줄인다.

---

## 4. 지연/패킷화: `TCP_NODELAY` / `TCP_CORK` 와 Nagle, Delayed ACK

### 4.1 Nagle 알고리즘과 Delayed ACK

TCP는 기본적으로:

- 작은 패킷을 **합쳐서** 보내고 싶어한다(Nagle).
- ACK도 가능한 한 **모아서** 보내고 싶어한다(Delayed ACK).

이 두 가지는 **대역폭 효율**을 높이는 데는 도움이 되지만,  
**짧은 요청/응답 패턴(RPC, 채팅, 게임 패킷)** 에서는 **눈에 띄는 지연**을 만들어낸다.

Nagle 알고리즘(간략 개념):

- 아직 ACK 되지 않은 소량의 데이터가 인플라이트에 있다면,
- 새로운 작은 write를 **바로 보내지 않고** 버퍼에 쌓았다가,
- 기존 데이터에 대한 ACK가 오거나 충분히 많은 데이터가 모이면 전송.

Delayed ACK:

- 수신 측은 매 패킷마다 ACK를 보내지 않고,
- **미리 정해진 짧은 시간(수 ms)** 동안 ACK를 지연시킨 뒤,
- 같이 보낼 데이터가 있으면 **데이터와 합쳐서** 또는 여러 세그먼트를 묶어 ACK를 보낸다.

두 가지가 동시에 작동하면:

- “작은 요청 → 작은 응답” 패턴에서,
- 양쪽이 서로 상대의 패킷/ACK을 기다리느라 **불필요한 수 ms ~ 수십 ms 지연**이 생길 수 있다.

---

### 4.2 `TCP_NODELAY` — Nagle 끄기 (지연 감소)

`TCP_NODELAY`는 **Nagle 알고리즘을 끄는 옵션**이다.

```cpp
int one = 1;
setopt(fd, IPPROTO_TCP, TCP_NODELAY, one);
```

효과:

- 애플리케이션의 `send()`가 호출될 때,
- 커널은 “작은 패킷이더라도 바로 보내라”고 해석한다.
- 특히 작은 메시지(수십 바이트 수준)를 자주 주고받는 RPC/채팅/게임 프로토콜에서 **왕복 지연을 줄이는 효과**가 크다.

단점:

- 세그먼트 수 증가 → 헤더 오버헤드 증가
- NIC, 커널, 인터럽트 처리 등에서 CPU 부담 증가

그래서 “무조건 켠다” 보다는:

- **작은 메시지가 매우 잦고 지연에 민감**한 연결에는 켠다.
- 큰 덩어리 전송이나 bulk file transfer 중심이라면 기본값을 유지해도 충분하다.

---

### 4.3 `TCP_CORK` (Linux) — 묶어서 내보내기

리눅스 고유 옵션인 `TCP_CORK`는 `NODELAY`와 반대 방향의 정책을 제공한다.

- “지금은 보내지 말고, **커널이 적당하다고 판단하거나 CORK를 풀 때까지** 한 번에 몰아서 보내라.”

사용 예:

```cpp
int on = 1, off = 0;
setopt(fd, IPPROTO_TCP, TCP_CORK, on);

// 여러 번 write()
::send(fd, header, header_len, 0);
::send(fd, body_part1, len1, 0);
::send(fd, body_part2, len2, 0);

// 이제야 flush
setopt(fd, IPPROTO_TCP, TCP_CORK, off);
```

대표적인 시나리오:

- HTTP 응답에서:
  - 헤더와 바디를 여러 번 나눠 쓰더라도,
  - 한 번의 큰 세그먼트로 보내고 싶은 경우
- `sendfile()`을 이용해 파일을 보낼 때,
  - 그 전에 쓴 헤더와 합쳐서 깔끔하게 전송하고 싶을 때

주의:

- `TCP_NODELAY`와 동시에 쓰는 건 **정책 충돌**이다.
- 일반적으로 **한 소켓에 대해 둘 중 하나만** 사용한다.

---

### 4.4 예제: RPC 패턴에서 `TCP_NODELAY` 전/후 비교

간단한 “ping 서버”를 생각해 보자.

- 클라이언트: 32바이트짜리 요청을 보내고, 32바이트 응답을 기다린다.
- 서버: 요청 하나를 받으면, 즉시 32바이트 응답을 보낸다.

요청/응답을 1만 번 반복하면서:

- `TCP_NODELAY` OFF, ON일 때의 왕복 시간 평균을 비교해 보면,
- RTT가 수 ms ~ 수십 ms 인 링크에서 **체감 차이**가 분명히 드러난다.

실험 코드는 앞에서 만든 `opt_server` / `opt_client` 에서  
작은 메시지 반복 전송 부분만 따로 떼어 사용하면 된다.

---

## 5. 종료 정책: `SO_LINGER` — close() 의 의미를 바꾸는 옵션

### 5.1 기본 TCP 종료 동작 (linger OFF)

기본적으로 `SO_LINGER` 를 설정하지 않으면:

- 애플리케이션에서 `close(fd)` 를 호출하는 순간,
- 커널은 가능하면 **남은 데이터를 보내고**,  
  정상적인 **FIN/ACK 종료(handshake)** 를 수행한다.
- 애플리케이션 입장에서는 `close()` 가 **거의 즉시 반환**된다.
- 실제 네트워크에서 FIN/ACK 교환이 완료되기 전이라도,  
  그 이후 관리는 **커널의 책임**으로 진행된다.

---

### 5.2 `SO_LINGER` 구조와 두 가지 극단

`SO_LINGER`는 다음 구조체로 관리된다.

```cpp
struct linger {
    int l_onoff;   // 0: 기본 동작, 1: 커스텀
    int l_linger;  // 초 단위
};
```

#### 5.2.1 즉시 RST 종료 (`l_onoff=1, l_linger=0`)

```cpp
linger lg{ .l_onoff = 1, .l_linger = 0 };
setopt(fd, SOL_SOCKET, SO_LINGER, lg);
```

의미:

- `close()` 를 호출하는 순간, 커널은 **즉시 RST(Reset) 세그먼트** 를 보낸다.
- 큐에 남아있던 송신 데이터는 **모두 폐기**된다.
- 상대는 다음과 같은 식으로 에러를 보게 된다.
  - `recv()` → `ECONNRESET`
  - 이미 일부 데이터를 읽은 후 뒤에서 추가 데이터를 기대했다면, 프로토콜상 **데이터 유실·파손** 문제가 생긴다.

용도:

- **프로토콜 위반**이나 **보안 상 즉시 끊어야 하는 상황**에서만 사용.
- 예: 잘못된 헤더, 공격적인 요청, 인증 실패 등에서
  - 의도적으로 상대에게 “이 연결은 잘못됐다”는 강한 시그널을 보내고 싶을 때.

일반적인 HTTP/DB 연결 같은 곳에서는 **절대 기본값으로 쓰면 안 된다**.

#### 5.2.2 블로킹 종료 (`l_onoff=1, l_linger > 0`)

```cpp
linger lg{ .l_onoff = 1, .l_linger = 10 };
setopt(fd, SOL_SOCKET, SO_LINGER, lg);
```

의미:

- `close()` 가 최대 `10초`까지 **블록**하면서  
  큐에 남아 있는 데이터를 전송하려고 시도한다.
- 그 시간 안에 전송이 완료·ACK되면 정상 종료.
- 그렇지 않으면, 타임아웃 후 **강제로 소켓을 닫고** 에러를 반환할 수 있다.

용도:

- 동기 I/O 기반의 단순 프로그램에서  
  “종료 전에 꼭 이만큼의 데이터를 보내야 한다” 는 정책이 필요할 때.
- 하지만 요즘은:
  - 논블로킹 I/O + 이벤트 루프 구조가 많고,
  - 종료 처리도 별도의 상태 머신으로 다루는 경우가 많기 때문에,
  - **기본 linger OFF + 명시적 flush** 구조가 더 많이 사용된다.

---

### 5.3 정리: `SO_LINGER`는 특수 상황에서만

실무에서 보통:

- 기본값(OFF) 유지
- 프로토콜 설계에서 절대 필요한 경우에만,
  - RST 종료(`l_onoff=1, l_linger=0`) 혹은
  - 제한 시간 종료(`l_onoff=1, l_linger>0`)를 **명확한 의도** 하에 사용

이 글 전체에서 할 수 있는 가장 중요한 결론 중 하나는:

> **“`SO_LINGER`를 잘못 건드리면, 문제가 생겼을 때 추적하기 어렵다.”**

는 것이다.

---

## 6. Keepalive — 유휴 연결의 생존/감지

### 6.1 왜 Keepalive 인가?

현대 네트워크 환경에서:

- NAT 게이트웨이
- 방화벽
- 로드밸런서
- 클라우드 인프라의 연결 관리 로직

들은 **유휴 TCP 연결**을 일정 시간 후 자동으로 끊어 버리는 경우가 많다.

예를 들어:

- 아무 데이터도 오가지 않는 소켓을 10분만 유지하려고 했는데,
- 중간의 NAT 장비가 2분 유휴 후 삭제해 버려서,
- 애플리케이션은 여전히 연결이 살아 있다고 믿고 있다가,
- 실제로는 어느 시점 이후 네트워크 패킷이 모두 버려져 버리는 상황.

이를 완화하기 위해 TCP 레벨에서:

- **정기적으로 작은 probe 패킷을 보내고**,
- 상대가 응답하지 않으면 연결이 깨졌다고 판단하는 기능이 **Keepalive**다.

---

### 6.2 리눅스 TCP Keepalive 옵션 4개

1. `SO_KEEPALIVE` (켜기/끄기)
2. `TCP_KEEPIDLE`   — 유휴 시간 (첫 probe까지 대기 시간)
3. `TCP_KEEPINTVL`  — probe 간격
4. `TCP_KEEPCNT`    — 허용할 최대 실패 횟수

설정 예:

```cpp
int ka = 1;
setopt(fd, SOL_SOCKET, SO_KEEPALIVE, ka);

int idle  = 30; // 30초 유휴 후 첫 probe
int intvl = 10; // probe 간 10초
int cnt   = 3;  // 최대 3회 실패

setopt(fd, IPPROTO_TCP, TCP_KEEPIDLE,  idle);
setopt(fd, IPPROTO_TCP, TCP_KEEPINTVL, intvl);
setopt(fd, IPPROTO_TCP, TCP_KEEPCNT,   cnt);
```

동작:

- 마지막 데이터 송수신 이후 **30초** 동안 아무 트래픽이 없는 경우,
- 커널이 **Keepalive probe**를 보낸다.
- 응답이 없으면 **10초 간격**으로 최대 3회까지 재시도.
- 여전히 응답이 없으면, 연결을 **실패 상태**로 보고  
  이후 애플리케이션의 `read()/write()`가 에러를 반환하게 된다.

---

### 6.3 macOS/FreeBSD 와의 차이

- macOS에서는 `TCP_KEEPALIVE`(idle 시간), `TCP_KEEPINTVL`, `TCP_KEEPCNT` 같은 이름으로 제공된다.
- 정확한 이름과 단위를 OS별로 확인해야 한다.
  - idle 시간 단위가 초인지, 다른 단위인지
  - 디폴트 값이 얼마인지 등

크로스 플랫폼 코드를 짤 때는:

- OS별 `#ifdef` 블록으로 상수명을 분리하거나,
- CMake/빌드 스크립트에서 feature detection을 해서 조건부 컴파일을 하는 것이 안전하다.

---

### 6.4 애플리케이션 레벨 Heartbeat 와의 관계

많은 시스템에서:

- TCP Keepalive와는 별도로,
- 애플리케이션 레벨 heartbeat(예: ping/pong 메시지)를 주고받는다.

둘은 역할이 다르다.

- **TCP Keepalive**
  - 커널 레벨에서 **연결 상태**만 확인.
  - 애플리케이션 로직이 죽었는지 여부는 모른다.
- **애플리케이션 heartbeat**
  - 서비스를 제공하는 **애플리케이션 자체**가 살아 있는지,
  - 메시지를 올바른 형식으로 처리하고 있는지까지 확인.

실무에서는:

- TCP Keepalive는 **NAT/방화벽 타임아웃**에 맞춰 “너무 길지 않게” 설정하고,
- 애플리케이션 heartbeat는 **서비스 요구사항**에 맞춰 훨씬 짧은 주기로 설정해  
  두 레이어가 서로 보완하는 형태를 취하는 경우가 많다.

---

## 7. 송신 타임아웃: `TCP_USER_TIMEOUT`

### 7.1 개념

`TCP_USER_TIMEOUT`은 **송신 측 관점에서의 “마지막 ACK 대기 한계시간”** 을 의미한다.

- 응용층은 데이터를 `send()`로 밀어 넣는다.
- 커널은 이를 세그먼트로 만들어 네트워크에 보낸 뒤,
- 상대 측의 ACK를 기다린다.
- 이때 일정 시간 동안 책임지고 재전송을 반복하다가,
  - 그 시간을 넘어서도 ACK가 오지 않으면
  - 해당 연결을 **실패**로 간주하고 에러를 반환한다.

`TCP_USER_TIMEOUT`은 바로 이 **최대 대기 시간**을 정의한다.

```cpp
int ms = 20000; // 20초
setopt(fd, IPPROTO_TCP, TCP_USER_TIMEOUT, ms);
```

효과:

- 네트워크 장애, 상대 호스트 정지 등의 상황에서,
- 기본 TCP 재전송 정책(수 분에 이를 수도 있음)에 의존하지 않고,
- 애플리케이션이 원하는 시간 내에 **실패를 감지**할 수 있다.

---

### 7.2 `SO_SNDTIMEO` 와의 차이

`SO_SNDTIMEO` 는:

- `send()` 호출이 **블록되는 시간**의 상한을 지정한다.
- 그 시간이 지나면 `send()` 가 `EAGAIN` 혹은 `EWOULDBLOCK` 같은 에러로 돌아올 수 있다.

반면 `TCP_USER_TIMEOUT`은:

- 전송된 데이터가 **ACK되지 못한 채 머무는 최대 시간**이다.
- 재전송을 포함한 전체 lifespan의 상한.

두 옵션은 목적이 다르다.

- `SO_SNDTIMEO`는 **단일 `send()` 호출 레벨**에서의 “함수 호출 시간”.
- `TCP_USER_TIMEOUT`은 **연결 전체 관점**에서의 “데이터 전달 보장 시간”.

실무에서는:

- 네트워크 장애 시 몇 분씩 블록되는 것을 피하고 싶다면,  
  **`TCP_USER_TIMEOUT`**을 통해 상한을 정해 주는 것이 훨씬 깔끔하다.

---

## 8. 수신 ACK 정책 힌트: `TCP_QUICKACK` (Linux)

`TCP_QUICKACK`는 리눅스 고유의 **Delayed ACK 완화 힌트**다.

```cpp
int on = 1;
setopt(fd, IPPROTO_TCP, TCP_QUICKACK, on);
```

특징:

- 이 옵션은 **지속적인 모드**가 아니라,  
  “지금부터 당분간 빠르게 ACK 보내라” 정도의 **힌트**다.
- 커널 상황에 따라 즉시 다시 Delayed ACK 정책으로 되돌아갈 수 있다.
- RFC로 표준화된 것이 아니라, Linux TCP 구현에 의존적인 동작이다.

그래서:

- 성능 실험이나 문제 해결 과정에서  
  한시적으로 써 볼 수는 있지만,
- 장기적으로 유지해야 하는 핵심 옵션으로 잡기는 애매한 면이 있다.

---

## 9. accept/초기 연결: `TCP_DEFER_ACCEPT` / `TCP_FASTOPEN`

### 9.1 SYN 큐 / accept 큐 / backlog

리눅스 TCP 스택에는 크게 두 종류의 큐가 있다.

1. **SYN 큐 (half-open queue)**  
   - 아직 3-way handshake가 완전히 끝나지 않은 연결
   - SYN-RECV 상태 등
2. **accept 큐 (완료 큐)**  
   - handshake가 끝나고 애플리케이션에서 `accept()`를 할 수 있는 상태

`listen(fd, backlog)` 의 `backlog` 값은  
**완료 큐 크기에 대한 힌트**이며, 커널은 이를

- `net.core.somaxconn`

으로 **캡**한다.

SYN 큐 크기는:

- `net.ipv4.tcp_max_syn_backlog`

로 별도로 관리된다.

`TCP_DEFER_ACCEPT`, `TCP_FASTOPEN`은  
이 초기 연결 과정에서의 행동을 바꾸는 옵션이다.

---

### 9.2 `TCP_DEFER_ACCEPT` — 데이터가 올 때까지 accept 지연

`TCP_DEFER_ACCEPT`는 리눅스에서 제공하는 옵션으로,

> “SYN/ACK이 오고 ACK까지 완료되었어도,  
>  **데이터가 실제로 올 때까지** `accept()`를 잠시 미뤄라.”

는 정책을 설정한다.

```cpp
int sec = 5; // 최대 5초 대기
setopt(lfd, IPPROTO_TCP, TCP_DEFER_ACCEPT, sec);
```

효과:

- 단순히 연결만 맺고 데이터를 안 보내는 클라이언트(SYN flood, idle connection)를  
  어느 정도 **거르고**, 의미 있는 요청이 오는 연결에만  
  애플리케이션 리소스를 할당하게 할 수 있다.
- HTTP 같은 프로토콜에서:
  - 클라이언트가 SYN을 보내고 나서,
  - 실제 HTTP 요청 헤더를 보내는 연결만 `accept()` 에서 보게 된다.

주의:

- 프로토콜 설계에 따라, **연결만 맺고 한동안 아무 것도 안 보내는 정상 클라이언트**가 있을 수도 있다.
- 이런 경우 `TCP_DEFER_ACCEPT`가 의도치 않게 정상 연결을 늦추거나 실패시킬 수 있다.

---

### 9.3 `TCP_FASTOPEN` — 3-way와 동시에 데이터 보내기

`TCP_FASTOPEN` (TFO)은:

- **3-way handshake 과정에서 데이터를 미리 보내는 기법**이다.
- 일반적인 TCP에서는:
  - 클라이언트 SYN
  - 서버 SYN+ACK
  - 클라이언트 ACK
  - 그 다음부터 데이터 전송
- TFO에서는:
  - 클라이언트 SYN에 **데이터를 함께 실어** 보낼 수 있다.
  - 서버가 이를 바로 수신해 처리하면,
  - 왕복 지연 한 번을 줄일 수 있다.

서버 측 설정:

```cpp
int qlen = 5;
setopt(lfd, IPPROTO_TCP, TCP_FASTOPEN, qlen);
```

클라이언트 측:

```cpp
int on = 1;
setopt(fd, IPPROTO_TCP, TCP_FASTOPEN, on);
// connect 없이 sendto(MSG_FASTOPEN) 등으로 초기 데이터 전송 가능 (리눅스 API)
```

주의:

- 중간 방화벽/로드밸런서가 TFO를 제대로 이해하지 못하면,  
  일부 환경에서 **연결 실패/지연**이 발생할 수 있다.
- 커널/배포판마다 기본 설정과 지원 수준이 다르므로,  
  실제 사용할 때는 환경별 측정이 필수다.

---

## 10. 서비스 품질 힌트: `IP_TOS` (DSCP/ECN)

### 10.1 DSCP/ECN 기본 개념

IPv4 헤더의 TOS 필드(IPv6에서는 Traffic Class)는  
상위 6비트에 **DSCP** (Differentiated Services Code Point),  
하위 2비트에 **ECN** (Explicit Congestion Notification)을 담는다.

$$
\text{TOS} = (\text{DSCP} \ll 2) \,\vert\, \text{ECN}
$$

DSCP는:

- 네트워크 장비(라우터, 스위치 등)가  
  트래픽 우선순위를 구분하는 데 사용할 수 있는 **힌트**다.

예시 코드:

```cpp
int tos = 0x28; // 예: DSCP=0x0A (AF11 등), ECN=0
setopt(fd, IPPROTO_IP, IP_TOS, tos);
```

하지만 실제로:

- 네트워크 장비가 DSCP를 존중할지 여부는 **환경 정책**에 따라 다르다.
- 공용 인터넷에서는 중간 장비가 DSCP를 **무시**하거나 **지워버리는** 경우도 많다.
- 기업 내부/데이터센터 네트워크에서는 DSCP 기반 QoS 정책이 적극적으로 활용되기도 한다.

따라서 `IP_TOS`는:

- “이렇게 설정하면 QoS가 보장된다” 는 의미가 아니라,
- “네트워크 팀이 필요 시 활용할 수 있는 **클래스 힌트**를 제공”하는 용도로 이해하는 편이 좋다.

IPv6에서는:

- `IPV6_TCLASS` 옵션으로 같은 역할을 할 수 있다.

---

## 11. 관측: `TCP_INFO` 와 `ss`/`tcpdump`

### 11.1 `TCP_INFO` 구조체

리눅스의 `TCP_INFO` 옵션은 TCP 연결의 상태를  
**정수 값으로 자세히** 알려준다.

```cpp
#include <netinet/tcp.h>
#include <print>

void log_tcp_info(int fd, std::string_view tag) {
    tcp_info info{};
    socklen_t len = sizeof(info);
    if (::getsockopt(fd, IPPROTO_TCP, TCP_INFO, &info, &len) == 0) {
        std::print("[tcp_info][{}] "
                   "state={} rtt={}us rttvar={}us cwnd={} "
                   "snd_ssthresh={} rcv_space={} "
                   "retrans={} total_retrans={}\n",
                   tag,
                   info.tcpi_state,
                   info.tcpi_rtt, info.tcpi_rttvar,
                   info.tcpi_snd_cwnd,
                   info.tcpi_snd_ssthresh,
                   info.tcpi_rcv_space,
                   info.tcpi_retrans, info.tcpi_total_retrans);
    }
}
```

자주 보는 필드:

- `tcpi_rtt` / `tcpi_rttvar`  
  → RTT 평균/분산(마이크로초 단위)
- `tcpi_snd_cwnd`  
  → 송신 혼잡 윈도우 (세그먼트 단위)
- `tcpi_rcv_space`  
  → 수신 버퍼 여유 공간 (바이트)
- `tcpi_total_retrans`  
  → 전체 재전송 횟수

이를 통해:

- BDP에 비해 `cwnd`가 충분히 큰지,
- 손실률이 비정상적으로 높은지,
- 버퍼 튜닝이 제대로 되었는지

를 **수치로** 판단할 수 있다.

---

### 11.2 `ss`, `tcpdump` 와의 조합

- `ss -tinp`  
  → 각 연결의 큐 크기, cwnd, rtt, send/recv-Q 등을 요약해서 보여준다.
- `tcpdump` / Wireshark  
  → 실제 패킷 흐름을 보면서
  - Nagle/Delayed ACK 상호작용
  - Zero-window / Window update
  - RST/FIN 패턴
  등을 관찰하기 좋다.

옵션 튜닝 전/후에:

- **`TCP_INFO` / `ss` / `tcpdump` 세 개를 모두 써서**  
  “연결이 실제로 어떻게 달라졌는가”를 확인하면,  
  이론과 실전 지표가 일치하는지 검증할 수 있다.

---

## 12. TIME_WAIT / 재기동 / 백로그 — 연결 수명과 큐의 의미

### 12.1 TIME_WAIT: 누가 들어가나?

TCP에서 **TIME\_WAIT** 상태는:

- 보통 **능동 종료(먼저 FIN 보낸 쪽)** 가 들어간다.
- 목적:
  - 지연된 패킷이 뒤늦게 도착하는 것을 막기 위해
  - 재사용된 포트에서 **이전 연결의 패킷**과 **새 연결의 패킷**이 섞이지 않도록

TIME\_WAIT 상태는 일정 시간(보통 최대 세그먼트 라이프타임의 2배 정도) 유지되며,  
그동안 같은 4-튜플을 사용하는 새로운 연결은 제한될 수 있다.

---

### 12.2 재기동과 `SO_REUSEADDR` 의 관계

서버 재기동 시:

- 이전 서버가 사용하던 포트에 대한 여러 TIME\_WAIT 엔트리가 남아 있으면,
- 새 서버의 `bind()`가 `EADDRINUSE`로 실패할 수 있다.

`SO_REUSEADDR`는 이 상황을 상당 부분 완화해 준다.

- “이전에 쓰던 연결 때문에 생긴 TIME\_WAIT 엔트리들이 있더라도,
  **가능하면 새로 bind 하고 싶다**” 는 의도를 커널에 전달.

하지만 모든 상황을 100% 해결해 주는 것은 아니므로:

- 서버 재기동 로직에서 **재시도/backoff** 를 준비해 두는 것이 안전하다.

---

### 12.3 백로그의 두 종류: accept 큐 / SYN 큐

다시 정리하면:

1. **SYN 큐 (half-open queue)**  
   - 클라이언트 SYN 수신 후  
     3-way handshake가 완료되기 전까지의 연결.
   - 크기: `net.ipv4.tcp_max_syn_backlog`
2. **accept 큐 (완료 큐)**  
   - handshake 완료 후,  
     애플리케이션이 `accept()` 호출을 통해 가져갈 수 있는 연결.
   - 크기: `listen(fd, backlog)` 의 `backlog`,
     다만 `net.core.somaxconn` 으로 상한 제한.

서버에서 갑자기 부하가 몰릴 때:

- SYN flood 공격
- 애플리케이션 처리 지연으로 `accept()`가 늦어지는 상황

등을 구분해서 보려면:

- SYN 큐가 꽉 찼는지,
- accept 큐가 꽉 찼는지,
- 커널 로그(`dmesg`)에서 관련 메시지가 있는지

를 함께 확인해야 한다.

`TCP_DEFER_ACCEPT`, SYN cookies, `somaxconn`, `tcp_max_syn_backlog` 조합으로  
이 부분을 미세하게 조정할 수 있다.

---

## 13. 통합 실습: 옵션 전/후 비교용 서버·클라이언트 하네스

앞에서 제시한 코드를 기반으로,  
**옵션 조합이 실제 성능에 어떤 영향을 주는지** 확인할 수 있는  
간단한 테스트 프로그램을 다시 한번 정리해 보자.

### 13.1 서버: `opt_server.cpp`

- 리스너 소켓에:
  - `SO_REUSEADDR` / `SO_REUSEPORT`
  - `TCP_DEFER_ACCEPT`
- 수락 소켓에:
  - `TCP_NODELAY`
  - `SO_RCVBUF` / `SO_SNDBUF`
  - `SO_LINGER`
- 그리고 각 루프마다 `TCP_INFO`를 찍는다.

```cpp
// opt_server.cpp
// 빌드: g++ -std=c++23 -O2 opt_server.cpp -o opt_server
#include "sockopt.hpp"
#include <netdb.h>
#include <sys/socket.h>
#include <unistd.h>
#include <vector>
#include <print>
#include <string>
#include <cstring>

int main(int argc, char** argv){
    // args: <port> [reuseport=0|1] [defer=0|sec] [rcvbuf_k=0] [sndbuf_k=0] [nodelay=0|1] [linger=off|rst|sec]
    if (argc < 2) {
        std::print(stderr,
            "usage: {} <port> [reuseport] [defer_sec] [rcvbuf_k] [sndbuf_k] [nodelay] [linger]\n",
            argv[0]);
        return 1;
    }
    const char* port = argv[1];
    int reuseport = (argc>2)? std::stoi(argv[2]) : 0;
    int defer_sec = (argc>3)? std::stoi(argv[3]) : 0;
    int rcvbuf_k  = (argc>4)? std::stoi(argv[4]) : 0;
    int sndbuf_k  = (argc>5)? std::stoi(argv[5]) : 0;
    int nodelay   = (argc>6)? std::stoi(argv[6]) : 0;
    std::string linger_arg = (argc>7)? argv[7] : "off";

    addrinfo hints{}, *res=nullptr;
    hints.ai_family   = AF_UNSPEC;
    hints.ai_socktype = SOCK_STREAM;
    hints.ai_flags    = AI_PASSIVE | AI_ADDRCONFIG | AI_NUMERICSERV;

    if (getaddrinfo(nullptr, port, &hints, &res) != 0) {
        perror("gai");
        return 1;
    }

    int lfd = -1;
    for (auto* ai = res; ai; ai = ai->ai_next) {
        lfd = ::socket(ai->ai_family, ai->ai_socktype, ai->ai_protocol);
        if (lfd < 0) continue;

        int one = 1;
        setopt(lfd, SOL_SOCKET, SO_REUSEADDR, one);
        if (reuseport) setopt(lfd, SOL_SOCKET, SO_REUSEPORT, one);

        if (ai->ai_family == AF_INET6) {
            int v6only = 0; // dual-stack 허용
            setopt(lfd, IPPROTO_IPV6, IPV6_V6ONLY, v6only);
        }
        if (defer_sec > 0) {
            setopt(lfd, IPPROTO_TCP, TCP_DEFER_ACCEPT, defer_sec);
        }

        if (::bind(lfd, ai->ai_addr, ai->ai_addrlen) == 0 &&
            ::listen(lfd, 512) == 0) {
            break;
        }
        ::close(lfd);
        lfd = -1;
    }
    freeaddrinfo(res);

    if (lfd < 0) {
        perror("listen");
        return 1;
    }

    std::print("[server] listening on :{}\n", port);

    for (;;) {
        sockaddr_storage ss{};
        socklen_t slen = sizeof(ss);
        int cfd = ::accept(lfd, (sockaddr*)&ss, &slen);
        if (cfd < 0) {
            if (errno == EINTR) continue;
            perror("accept");
            continue;
        }

        // per-connection options
        if (nodelay) {
            int on = 1;
            setopt(cfd, IPPROTO_TCP, TCP_NODELAY, on);
        }
        if (rcvbuf_k > 0) {
            int sz = rcvbuf_k * 1024;
            setopt(cfd, SOL_SOCKET, SO_RCVBUF, sz);
        }
        if (sndbuf_k > 0) {
            int sz = sndbuf_k * 1024;
            setopt(cfd, SOL_SOCKET, SO_SNDBUF, sz);
        }
        if (linger_arg == "rst") {
            linger lg{ .l_onoff = 1, .l_linger = 0 };
            setopt(cfd, SOL_SOCKET, SO_LINGER, lg);
        } else if (linger_arg != "off") {
            int sec = std::stoi(linger_arg);
            linger lg{ .l_onoff = 1, .l_linger = sec };
            setopt(cfd, SOL_SOCKET, SO_LINGER, lg);
        }

        std::vector<char> buf(64 << 10);

        for (;;) {
            auto info = getoptv<tcp_info>(cfd, IPPROTO_TCP, TCP_INFO);
            if (info) {
                std::print("[tcp_info] rtt={}us cwnd={} rcv_space={} retrans={}\n",
                           info->tcpi_rtt,
                           info->tcpi_snd_cwnd,
                           info->tcpi_rcv_space,
                           info->tcpi_total_retrans);
            }

            ssize_t n = ::recv(cfd, buf.data(), buf.size(), 0);
            if (n > 0) {
                ssize_t w = 0;
                while (w < n) {
                    ssize_t m = ::send(cfd, buf.data() + w, n - w, 0);
                    if (m > 0) {
                        w += m;
                    } else if (m < 0 && errno == EINTR) {
                        continue;
                    } else {
                        break;
                    }
                }
                continue;
            } else if (n == 0) {
                ::close(cfd);
                break;
            } else if (errno == EINTR) {
                continue;
            } else {
                perror("recv");
                ::close(cfd);
                break;
            }
        }
    }
}
```

---

### 13.2 클라이언트: `opt_client.cpp`

- 연결 소켓에:
  - `TCP_NODELAY`
  - `SO_SNDBUF` / `SO_RCVBUF`
  - Keepalive 3종
- 짧은 메시지 여러 번 + 큰 메시지 bulk 전송으로 부하 생성

```cpp
// opt_client.cpp
// 빌드: g++ -std=c++23 -O2 opt_client.cpp -o opt_client
#include "sockopt.hpp"
#include <netdb.h>
#include <sys/socket.h>
#include <unistd.h>
#include <print>
#include <vector>
#include <string>
#include <chrono>

int main(int argc, char** argv){
    // args: <host> <port> [nodelay=0|1] [sndbuf_k=0] [rcvbuf_k=0] [keepalive=0|1] [idle] [intvl] [cnt]
    if (argc < 3) {
        std::print(stderr,
            "usage: {} <host> <port> [nodelay] [sndbuf_k] [rcvbuf_k] [keepalive] [idle] [intvl] [cnt]\n",
            argv[0]);
        return 1;
    }

    const char* host = argv[1];
    const char* port = argv[2];
    int nodelay  = (argc > 3) ? std::stoi(argv[3]) : 0;
    int sndbuf_k = (argc > 4) ? std::stoi(argv[4]) : 0;
    int rcvbuf_k = (argc > 5) ? std::stoi(argv[5]) : 0;
    int keepalive= (argc > 6) ? std::stoi(argv[6]) : 0;
    int idle     = (argc > 7) ? std::stoi(argv[7]) : 60;
    int intvl    = (argc > 8) ? std::stoi(argv[8]) : 10;
    int cnt      = (argc > 9) ? std::stoi(argv[9]) : 3;

    addrinfo hints{}, *res=nullptr;
    hints.ai_family   = AF_UNSPEC;
    hints.ai_socktype = SOCK_STREAM;
    hints.ai_flags    = AI_ADDRCONFIG | AI_NUMERICSERV;

    if (getaddrinfo(host, port, &hints, &res) != 0) {
        perror("gai");
        return 1;
    }

    int s = -1;
    for (auto* ai = res; ai; ai = ai->ai_next) {
        s = ::socket(ai->ai_family, ai->ai_socktype, ai->ai_protocol);
        if (s < 0) continue;

        if (nodelay) {
            int on = 1;
            setopt(s, IPPROTO_TCP, TCP_NODELAY, on);
        }
        if (sndbuf_k > 0) {
            int sz = sndbuf_k * 1024;
            setopt(s, SOL_SOCKET, SO_SNDBUF, sz);
        }
        if (rcvbuf_k > 0) {
            int sz = rcvbuf_k * 1024;
            setopt(s, SOL_SOCKET, SO_RCVBUF, sz);
        }
        if (keepalive) {
            int on = 1;
            setopt(s, SOL_SOCKET, SO_KEEPALIVE, on);
            setopt(s, IPPROTO_TCP, TCP_KEEPIDLE,  idle);
            setopt(s, IPPROTO_TCP, TCP_KEEPINTVL, intvl);
            setopt(s, IPPROTO_TCP, TCP_KEEPCNT,   cnt);
        }

        if (::connect(s, ai->ai_addr, ai->ai_addrlen) == 0) {
            break;
        }
        ::close(s);
        s = -1;
    }
    freeaddrinfo(res);

    if (s < 0) {
        perror("connect");
        return 1;
    }

    // 1) 작은 메시지 1만 개
    std::vector<char> small(64, 's');
    for (int i = 0; i < 10000; ++i) {
        ::send(s, small.data(), small.size(), 0);
    }

    // 2) 큰 덩어리 bulk 전송
    std::vector<char> big(1 << 20, 'B'); // 1 MiB
    auto t0 = std::chrono::steady_clock::now();
    size_t total = 0;
    for (int i = 0; i < 64; ++i) {       // 64 MiB
        ssize_t n = ::send(s, big.data(), big.size(), 0);
        if (n > 0) {
            total += (size_t)n;
        } else if (n < 0 && errno == EINTR) {
            --i;
            continue;
        } else {
            break;
        }
    }
    auto t1 = std::chrono::steady_clock::now();
    double sec = std::chrono::duration<double>(t1 - t0).count();

    std::print("[client] bulk={} bytes in {:.3f}s → {:.3f} MB/s\n",
               total, sec,
               (total / 1e6) / sec);

    // 서버 echo drain
    std::vector<char> buf(256 << 10);
    while (::recv(s, buf.data(), buf.size(), 0) > 0) {}
    ::close(s);
}
```

---

### 13.3 실험 시나리오 (전/후 비교)

1. **기본 옵션**
   - 서버:  
     `./opt_server 9000 0 0 0 0 0 off`
   - 클라이언트:  
     `./opt_client 127.0.0.1 9000`
   - 결과: 기본 커널 설정에서의 작은 메시지 지연, bulk 처리량, `TCP_INFO`를 기록.

2. **`TCP_NODELAY` ON**
   - 클라이언트:  
     `./opt_client 127.0.0.1 9000 1`
   - 비교:
     - 작은 메시지 응답 지연 변화
     - 세그먼트 수 변화를 `tcpdump` 로 확인

3. **버퍼 확장**
   - 서버:  
     `./opt_server 9000 0 0 4096 4096 0 off`
   - 클라:  
     `./opt_client 127.0.0.1 9000 0 4096 4096`
   - 비교:
     - RTT가 큰 환경(예: `tc netem`으로 50~100ms 지연)에서  
       bulk 처리량이 버퍼 크기에 따라 어떻게 바뀌는지.

4. **Keepalive 설정**
   - 클라:  
     `./opt_client server.example.com 9000 0 0 0 1 30 10 3`
   - NAT/방화벽 넘는 환경에서
     - 유휴 상태를 길게 유지했을 때 끊어지지 않는지 확인.
     - `TCP_INFO` 재전송 카운트 변화를 기록.

5. **`SO_LINGER` RST 모드**
   - 서버:  
     `./opt_server 9000 0 0 0 0 0 rst`
   - 클라:
     - 데이터를 보내는 중 서버에서 연결을 끊었을 때
     - 클라 측 `recv()` 에러 코드, 데이터 유실 여부를 관찰.

이런 실험을 통해:

- 각 옵션이 “이론상 이렇게 된다” 라는 설명을 넘어,
- 실제 숫자/로그/패킷으로 어떻게 드러나는지 체감할 수 있다.

---

## 14. 옵션 개요 표 (리마인드용)

| 옵션 | 레벨 | 효과/목적 | 주의사항 |
|------|------|----------|---------|
| `SO_REUSEADDR` | SOL_SOCKET | 재기동/포트 회수 완화 | BSD에서 의미 상이(중복 바인드 주의) |
| `SO_REUSEPORT` | SOL_SOCKET | 동일 (IP,PORT) 다중 리스너 → 커널 로드밸런싱 | 옵션/백로그 일관성 필요 |
| `SO_RCVBUF` / `SO_SNDBUF` | SOL_SOCKET | 커널 수신·송신 버퍼 힌트(오토튜닝) | BDP 기준으로 잡되, 과도하면 버퍼블로트 |
| `TCP_NODELAY` | IPPROTO_TCP | Nagle 비활성화 → 작은 메시지 지연 감소 | 세그먼트 수 증가, CPU·헤더 오버헤드 |
| `TCP_CORK` (Linux) | IPPROTO_TCP | 여러 write를 묶어 한 번에 전송 | NODELAY와 동시 사용 지양 |
| `SO_LINGER` | SOL_SOCKET | close() 동작 제어(FIN vs RST, 블록 여부) | 잘못 쓰면 데이터 유실/ECONNRESET |
| `SO_KEEPALIVE` | SOL_SOCKET | TCP keepalive 사용 여부 | 하위 3옵션(TCP_KEEP*)와 함께 튜닝 |
| `TCP_KEEPIDLE` / `TCP_KEEPINTVL` / `TCP_KEEPCNT` | IPPROTO_TCP | idle/probe 간격/재시도 횟수 | OS별 이름·단위 차이 있음 |
| `TCP_USER_TIMEOUT` | IPPROTO_TCP | 송신 데이터 ACK 대기 한계시간 | 네트워크 장애 시 무한 블록 방지 |
| `TCP_DEFER_ACCEPT` (Linux) | IPPROTO_TCP | 데이터가 올 때까지 accept 지연 | 정상 idle 클라이언트 영향 주의 |
| `TCP_FASTOPEN` (Linux) | IPPROTO_TCP | 3-way와 동시에 데이터 전송 | 중간 장비 호환성, 커널 설정 필요 |
| `IP_TOS` | IPPROTO_IP | DSCP/ECN 기반 QoS 힌트 | 네트워크 정책에 따라 무시될 수 있음 |
| `TCP_QUICKACK` (Linux) | IPPROTO_TCP | Delayed ACK 완화 힌트 | 지속 보장 아님, Linux 구현 의존 |
| `TCP_INFO` | IPPROTO_TCP | RTT, cwnd, 재전송 등 상태 관측 | 튜닝 전/후 비교에 필수 도구 |

---

## 15. 수식으로 다시 보는 튜닝 직관

앞에서 언급한 핵심 수식 세 개를 다시 적어 보자.

1. **대역폭–지연곱(BDP)**

$$
\text{BDP} = \text{Bandwidth} \times \text{RTT}
$$

2. **필요 윈도우 조건 (직관)**

$$
\min\{\text{cwnd},\ \text{rwnd}\} \gtrsim \text{BDP}
$$

3. **손실 환경 처리량(직관식)**

$$
\text{Throughput} \approx \frac{\text{MSS}}{\text{RTT}\sqrt{p}}
$$

여기서:

- $$\text{cwnd}$$ : 송신 혼잡 윈도우 (세그먼트 * MSS)
- $$\text{rwnd}$$ : 수신 윈도우 (수신 버퍼 여유)
- $$p$$ : 패킷 손실 확률

소켓 옵션의 역할을 이 수식에 대입해 보면:

- `SO_RCVBUF` / `SO_SNDBUF`  
  → `rwnd`, sndbuf의 상한을 키워 **BDP에 맞게 파이프를 채우는 역할**
- `TCP_NODELAY` / `TCP_CORK` / `TCP_QUICKACK`  
  → 작은 메시지 패턴에서 **효율 vs 지연** trade-off 조정
- `SO_LINGER` / `TCP_USER_TIMEOUT` / Keepalive  
  → 연결 수명과 실패 감지 지점 조정
- `SO_REUSEADDR` / `SO_REUSEPORT` / 백로그  
  → **동시 접속 수**, 재기동, 멀티코어 확장성에 영향

결국 소켓 옵션은 위 세 수식과:

- **TIME\_WAIT 수명**
- **SYN 큐 / accept 큐 상태**

를 조합해서, “이 시스템이 **어떤 지점에서 느려지고, 어떤 지점에서 끊어지도록 할 것인가**”를 설계하는 레버라고 볼 수 있다.

---

## 16. 현장용 체크리스트

실제 프로젝트에서 사용할 수 있는 체크리스트를 마지막으로 정리해 보자.

1. **재기동/포트 회수**
   - [ ] 리스너 소켓에 `SO_REUSEADDR` 적용
   - [ ] 멀티 워커/멀티 프로세스 구조라면 `SO_REUSEPORT` 도입 검토

2. **버퍼/BDP**
   - [ ] 링크 대역폭/RTT 파악
   - [ ] BDP 계산 후 `SO_RCVBUF` / `SO_SNDBUF` 힌트 설정
   - [ ] `/proc/sys/net/ipv4/tcp_rmem` / `tcp_wmem` 상한 확인
   - [ ] `TCP_INFO` 로 `rcv_space`, `cwnd`, `rtt` 모니터링

3. **지연/패킷화**
   - [ ] 작은 메시지·지연 민감: `TCP_NODELAY` ON 검토
   - [ ] 헤더+바디 묶기/파일 전송: `TCP_CORK` (Linux) 검토
   - [ ] `tcpdump` 로 세그먼트 패턴 확인

4. **종료/타임아웃**
   - [ ] 기본적으로 `SO_LINGER` OFF 유지
   - [ ] 특수 상황에서만 RST 종료(`l_onoff=1,l_linger=0`) 사용
   - [ ] 장시간 네트워크 장애 대응: `TCP_USER_TIMEOUT` 설정 검토

5. **유휴 연결**
   - [ ] NAT/방화벽 환경: `SO_KEEPALIVE` + `TCP_KEEP*` 조합 튜닝
   - [ ] 애플리케이션 레벨 heartbeat와 역할 분리

6. **초기 연결/부하**
   - [ ] 부하 급증 시 `somaxconn` / `tcp_max_syn_backlog` 값 확인
   - [ ] 특정 시나리오에서 `TCP_DEFER_ACCEPT`, SYN cookies 등 활용 검토
   - [ ] TFO(`TCP_FASTOPEN`)는 환경 테스트 후 제한적으로 사용

7. **관측/디버깅**
   - [ ] `TCP_INFO` 정기 로깅 (RTT, cwnd, retrans)
   - [ ] `ss -tinp`, `tcpdump` 로 큐/패킷 수준 확인
   - [ ] 전/후 옵션 조합별 지표를 표로 정리해 비교

이 체크리스트와 함께, 이 글의 코드들을 **직접 컴파일해 실험**해 보면  
“소켓 옵션은 무언가 마법 같은 설정”이 아니라,  
**명확한 목적과 수식 위에서 동작하는 정책 스위치**라는 감각이 만들어질 것이다.
