---
layout: post
title: 기계학습 - 지도학습(회귀)
date: 2025-08-14 22:20:23 +0900
category: 기계학습
---
# 지도학습(회귀, Regression)

## 회귀 문제의 수학적 정식화

- 목표: **연속 타깃** \(y\)를 입력 \(x\in\mathbb{R}^d\)로부터 예측
$$
y = f^\star(x) + \epsilon,\qquad \mathbb{E}[\epsilon\mid x]=0
$$
- 학습은 함수족 \(\mathcal{F}\) 위에서 **경험위험 최소화**
$$
\hat f=\arg\min_{f\in\mathcal{F}}\ \frac{1}{n}\sum_{i=1}^n L\!\big(y_i, f(x_i)\big)
$$
- 회귀의 표준 손실은 **MSE**
$$
L_{\text{MSE}}(y,\hat y)=(y-\hat y)^2
$$

---

## 회귀 vs 분류 (요약표)

| 구분 | 회귀(Regression) | 분류(Classification) |
|---|---|---|
| 출력 | 연속값 \(\mathbb{R}\) | 이산 클래스 \(\{1,\dots,C\}\) |
| 전형 손실 | MSE/MAE/Huber/Quantile | CE/BCE/Focal/Hinge |
| 지표 | RMSE, MAE, \(R^2\), MAPE | Accuracy, F1, AUC |
| 예 | 가격·온도·수요 예측 | 스팸, 이미지 라벨 |

---

## 회귀 모델 카탈로그 — 언제 무엇을 고를까?

### 선형계열

- **단순/다중 선형 회귀(OLS)**
  $$\hat y = x^\top w + b,\quad w=\arg\min_w \|y-Xw-b\mathbf{1}\|_2^2$$
  빠르고 해석 용이. 선형성 가정·이분산/이상치 민감.
- **정규화 회귀**
  릿지(L2): \( \|w\|_2^2 \), 라쏘(L1): \( \|w\|_1 \), 엘라스틱넷: 혼합.
  다중공선성/고차원 \(p\gg n\) 대응, 라쏘는 **변수 선택**.
- **다항/스플라인/상호작용**
  특징 변환으로 비선형 근사(선형 learner + 비선형 특징).

### 비모수·커널

- **kNN 회귀**: 국소 평균. 데이터 밀도 높고 잡음 적을 때.
- **SVR(ε-Insensitive, 커널)**: 마진 기반, RBF/폴리/선형 커널.
- **가우시안 프로세스(GPR)**: 베이지안 비모수; 작은 \(n\), 불확실성 추정 강력.

### 트리·앙상블

- **의사결정나무 회귀**: 해석력, 비선형/상호작용 자동 포착.
- **랜덤포레스트(RF)**: 분산 감소, 튜닝 쉬움.
- **그레디언트 부스팅(GBDT/XGBoost/LightGBM/CatBoost)**: SOTA 전천후. 불균형·복잡 비선형·이질 특징에 강함.

### 신경망 회귀

- MLP/1D CNN/RNN/Transformer 등. 대량 데이터·복잡 패턴.
- 출력 활성함수/손실 선택으로 음수 불가/양수 제한/분위수 예측 등 제약 표현 가능.

---

## 주요 회귀 손실과 평가 지표

### 손실 (학습 목적)

- **MSE**
  $$
  \text{MSE}=\frac{1}{n}\sum_i (y_i-\hat y_i)^2
  $$
- **MAE**
  $$
  \text{MAE}=\frac{1}{n}\sum_i |y_i-\hat y_i|
  $$
- **Huber** (이상치 견고)
$$
L_\delta(e)=\begin{cases}\frac12 e^2,&|e|\le \delta\\ \delta|e|-\frac12\delta^2,&\text{else}\end{cases},\ e=y-\hat y
$$
- **Quantile(분위수)**
  $$L_\tau(e)=\max(\tau e,(\tau-1)e)$$

### 지표 (평가)

- **RMSE**: \(\sqrt{\text{MSE}}\) (원 단위)
- **MAE**: 이상치 견고
- **\(R^2\)**:
$$
R^2=1-\frac{\sum_i (y_i-\hat y_i)^2}{\sum_i(y_i-\bar y)^2}
$$
- **MAPE/SMAPE**: 0 근처 값에 민감 → 사용시 절사/보정.

---

## 전처리·특징 엔지니어링

### 스케일링

- **표준화** \(x'=(x-\mu)/\sigma\): 선형/커널/SGD 수렴 안정.
- **Robust/Min–Max/MaxAbs**: 분포·희소성에 맞게 선택.

### 결측·이상치

- **수치**: 중앙값/모델 기반(IterativeImputer).
- **이상치**: Huber/RANSAC, 분위수 절사, 로깅/파워 변환.

### 범주형

- 원-핫(희소), 타깃 인코딩(누수 주의; CV 기반), CatBoost 인코딩.

### 파생특징

- **다항/상호작용**(PolynomialFeatures), **스플라인**, **주기형(사인/코사인)**, **어그리게이션(그룹별 통계)**.

---

## 가정·진단·로버스트

### OLS 기본 가정(요지)

- 선형성, 독립성, 등분산성, 정규성(추론용). 깨지면: WLS/GLS, 로버스트 손실, 변환.

### 잔차 진단

- 잔차 vs 예측 산점(패턴/이분산 확인), Q–Q plot, 영향점(레버리지 \(h_{ii}\), Cook’s D).

### 다중공선성

- **VIF**: \( \text{VIF}_j=\frac{1}{1-R_j^2} \) ↑ → 릿지/PCA/변수 통합.

---

## 불확실성 추정(예측구간·분위수·컨포멀)

### 선형모형 예측구간

- 표준오차 \(s\) 추정 후
$$
\hat y_\* \pm t_{n-p,1-\alpha/2}\cdot s\sqrt{x_\*^\top (X^\top X)^{-1}x_\*}
$$

### 분위수 회귀(하위/상위 대역)

- \(\tau\in\{0.1,0.9\}\) 두 모델 → **예측 대역**.

### 컨포멀 예측(모형 무관)

1. 학습/검증 분할, 검증 잔차의 \((1-\alpha)\)-분위수 \(q\).
2. 새 입력 \(\hat y\pm q\)로 구간. (가법 오차 가정)

---

## 모델 선택·튜닝·검증

- **KFold/GroupKFold/TimeSeriesSplit**: 데이터 구조에 맞게.
- **파이프라인**으로 **누수 방지**(스케일러/인코더는 **학습 폴드**에서만 fit).
- **그리드/랜덤/베이즈 최적화**.
- **학습곡선/검증곡선**으로 과소/과적합 확인.

---

## 수식·예: 대표 회귀식 모음

### 단순 선형 회귀 (재확인)

$$
\hat{y}=wx+b,\quad
MSE=\frac{1}{n}\sum_{i=1}^n (y_i - (w x_i + b))^2
$$

### 다중 선형 회귀

$$
\hat y=Xw+b\mathbf{1},\quad
\hat w=(X^\top X)^{-1}X^\top (y-b\mathbf{1})
$$

### 릿지/라쏘

$$
\min_w \frac{1}{n}\|y-Xw\|_2^2 + \lambda\|w\|_2^2,\qquad
\min_w \frac{1}{n}\|y-Xw\|_2^2 + \lambda\|w\|_1
$$

### SVR(개념식)

$$
\min_{w,b}\ \frac12\|w\|^2 + C\sum_i(\xi_i+\xi_i^\*)\quad
\text{s.t. }|y_i-w^\top\phi(x_i)-b|\le \epsilon+\xi_i
$$

---

## 파이프라인 & 코드 — 실전 스니펫

### 기본: 단순 선형 회귀 (재현)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

X = np.array([1,2,3,4,5]).reshape(-1,1)
y = np.array([1.2,1.9,3.0,3.9,5.1])

m = LinearRegression().fit(X,y)
yhat = m.predict(X)

print("w:", m.coef_, "b:", m.intercept_)
plt.scatter(X,y,label="Actual"); plt.plot(X,yhat,'r',label="Pred")
plt.legend(); plt.show()
```

### 수치+범주 혼합 데이터: ColumnTransformer + RidgeCV

```python
import pandas as pd
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import RidgeCV
from sklearn.metrics import mean_squared_error

# 예시 데이터프레임

df = pd.DataFrame({
    "size":[40,60,80,55,120,95,70,85],
    "rooms":[1,2,3,2,4,3,2,3],
    "city":["A","A","B","B","A","B","B","A"],
    "price":[140,210,300,195,520,420,265,355]
})
X = df[["size","rooms","city"]]; y = df["price"]

num_cols = ["size","rooms"]; cat_cols = ["city"]
pre = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

pipe = Pipeline([
    ("prep", pre),
    ("model", RidgeCV(alphas=[1e-3,1e-2,1e-1,1,10], cv=KFold(5,shuffle=True,random_state=42)))
])

scores = cross_val_score(pipe, X, y, cv=5, scoring="neg_root_mean_squared_error")
print("CV RMSE:", -scores.mean())
pipe.fit(X,y)
print("Chosen alpha:", pipe.named_steps["model"].alpha_)
```

### 다항 회귀 + 규제 (누수 방지 파이프라인)

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import ElasticNetCV

poly_ridge = Pipeline([
    ("prep", StandardScaler()),
    ("poly", PolynomialFeatures(degree=3, include_bias=False)),
    ("enet", ElasticNetCV(l1_ratio=[.2,.5,.8,1.0], alphas=None, cv=5, max_iter=5000, random_state=42))
])
poly_ridge.fit(X_num_train, y_train)
y_pred = poly_ridge.predict(X_num_test)
```

### — 스케일 필수

```python
from sklearn.svm import SVR

svr = Pipeline([
    ("sc", StandardScaler()),
    ("svr", SVR(kernel="rbf", C=10, epsilon=0.1, gamma="scale"))
])
svr.fit(X_train, y_train)
```

### 트리 앙상블: 랜덤포레스트·GBDT

```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

rf = RandomForestRegressor(n_estimators=400, max_depth=None, random_state=42, n_jobs=-1)
gbr = GradientBoostingRegressor(loss="squared_error", learning_rate=0.05, max_depth=3, n_estimators=600, random_state=42)

rf.fit(X_train, y_train); gbr.fit(X_train, y_train)
```

### 분위수 회귀(예측 구간)

```python
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor

q10 = GradientBoostingRegressor(loss="quantile", alpha=0.10)
q50 = GradientBoostingRegressor(loss="quantile", alpha=0.50)
q90 = GradientBoostingRegressor(loss="quantile", alpha=0.90)

for m in (q10,q50,q90): m.set_params(n_estimators=500, learning_rate=0.05, max_depth=3, random_state=42)
for m in (q10,q50,q90): m.fit(X_train, y_train)

y_lo, y_mid, y_hi = q10.predict(X_test), q50.predict(X_test), q90.predict(X_test)
interval_width = np.mean(y_hi - y_lo)
print("Avg 80% PI width:", interval_width)
```

### 컨포멀 예측(간단 버전; 모델 무관 예측구간)

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge

Xtr, Xcal, ytr, ycal = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
base = Pipeline([("sc", StandardScaler()), ("ridge", Ridge(alpha=1.0))]).fit(Xtr, ytr)
cal_resid = np.abs(ycal - base.predict(Xcal))
q = np.quantile(cal_resid, 0.9)  # 약 80%~90% 커버리지 설정
yhat = base.predict(X_test)
lower, upper = yhat - q, yhat + q
```

### 시계열 회귀: TimeSeriesSplit + 누수 방지

```python
from sklearn.model_selection import TimeSeriesSplit, cross_val_score

tscv = TimeSeriesSplit(n_splits=5)
pipe = Pipeline([("sc", StandardScaler()), ("gbr", GradientBoostingRegressor(random_state=42))])
scores = cross_val_score(pipe, X_time, y_time, cv=tscv, scoring="neg_mean_absolute_error")
print("TimeCV MAE:", -scores.mean())
```

---

## 문제 해결 가이드(증상 → 처방)

| 증상 | 원인 후보 | 처방 |
|---|---|---|
| 학습·검증 간 성능 격차 큼 | 과적합 | 정규화↑, 단순화, 데이터↑, 조기종료, Dropout/앙상블 |
| 모든 점예측이 평균으로 모임 | 과소적합/모델 용량 부족 | 특징 확장(다항/스플라인), 비선형 모델, 하이퍼파라미터↑ |
| 잔차의 분산이 예측과 함께 증가 | 이분산 | 로그/파워 변환, WLS, 분위수 회귀 |
| 이상치 몇 개가 전체를 왜곡 | 영향점 | 로버스트(Huber/RANSAC), 이상치 처리 |
| SVR/SGD 수렴 불안·느림 | 스케일 불량 | 표준화, 학습률 스케줄 조정 |
| 범주 고카디널리티 | 희소·누수 | 타깃 인코딩(CV), CatBoost, 해시 인코딩 |

---

## 요약

- 회귀는 \( y \in \mathbb{R} \) 예측 문제로, **손실(대개 MSE)** 최소화가 핵심이다.
- **모델 선택**은 데이터의 구조·크기·설명가능성 요구에 달림: 선형(빠르고 해석), 커널/비모수(국소/유연), 트리 앙상블(범용 성능), NN(대규모·복잡).
- **전처리**(스케일링·인코딩·결측), **교차검증**, **정규화**는 기본기.
- 성능 평가 지표(RMSE/MAE/\(R^2\))와 함께 **불확실성(분위수·컨포멀)**을 제공하면 **신뢰 가능한 예측**이 된다.
- **잔차 진단**과 **문제해결 가이드**를 루틴화하라. 올바른 파이프라인/검증이 없는 높은 점수는 **착시**일 수 있다.

---
**부록 A — 미니 레퍼런스 수식**

- OLS 해(절편 포함):
$$
\tilde w=(\tilde X^\top \tilde X)^{-1}\tilde X^\top y,\quad \tilde X=[\mathbf{1},X]
$$
- 릿지:
$$
\tilde w=(\tilde X^\top \tilde X+\lambda \tilde R)^{-1}\tilde X^\top y,\ \tilde R=\mathrm{diag}(0,1,\dots,1)
$$
- 예측구간(선형):
$$
\hat y_\*\pm t_{n-p,1-\alpha/2}\cdot s\sqrt{x_\*^\top (X^\top X)^{-1}x_\*}
$$

**부록 B — 실전 체크리스트**

- [ ] 스케일링 fit은 **학습 세트/폴드**만.
- [ ] 범주 인코딩은 **파이프라인**에.
- [ ] 정규화 회귀 전 **표준화**.
- [ ] 시계열은 **TimeSeriesSplit**.
- [ ] 불확실성: **분위수/컨포멀** 중 하나 이상 제공.
- [ ] 진단: 잔차·레버리지·VIF 확인.
- [ ] 리포트는 **지표 + 구간 + 피처 영향(트리 SHAP/PDP)**까지.
