---
layout: post
title: 기계학습 - 다항 회귀
date: 2025-08-14 23:25:23 +0900
category: 기계학습
---
# 다항 회귀(Polynomial Regression)

## 1. 개념 정리

### 1. 정의
- 입력 \(x\)와 출력 \(y\)의 관계를 **다항식**으로 근사:
$$
\hat{y} = w_0 + w_1 x + w_2 x^2 + \cdots + w_d x^d
$$
- \(d\): 다항 차수, \(w_i\): 가중치. 차수가 커질수록 표현력 ↑, 과적합 위험 ↑.

### 2. 선형 회귀와의 관계
- “선형”의 의미는 **파라미터 \(w\)** 에 대한 선형성이다.
- 다항 회귀는 **특성 변환** \(\phi(x)=[1, x, x^2,\dots,x^d]\) 후 일반 선형회귀를 적용하는 것:
$$
\hat{y} = \phi(x)^\top w
$$

### 3. 왜 쓰는가?
- 단순 직선으로는 설명 어려운 **곡선형 패턴**을 간단한 베이스라인으로 빠르게 포착.
- 파이프라인과 정규화를 결합하면 튜닝·배포까지 일관된 실무 흐름 구성 가능.

---

## 2. 수학적 표현과 해법

### 1. 설계행렬(반더몬드 행렬)
- 표본 \((x_i,y_i)_{i=1}^n\) 에 대해:
$$
X=
\begin{bmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^d\\
1 & x_2 & x_2^2 & \cdots & x_2^d\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & x_n & x_n^2 & \cdots & x_n^d
\end{bmatrix},\quad
w=
\begin{bmatrix}
w_0\\w_1\\\vdots\\w_d
\end{bmatrix},\quad
y=
\begin{bmatrix}
y_1\\y_2\\\vdots\\y_n
\end{bmatrix}.
$$
- 최소제곱 해:
$$
\hat{w} = \arg\min_w \|y - Xw\|_2^2.
$$

### 2. 정규방정식·SVD
- 정규방정식(수치적으로 불안정할 수 있음):
$$
(X^\top X)\hat{w} = X^\top y.
$$
- 실무에서는 **QR 분해** 또는 **SVD**가 수치안정성 측면에서 권장:
$$
X = U\Sigma V^\top \Rightarrow \hat{w} = V\Sigma^+ U^\top y.
$$

### 3. 손실과 일반화
- 경험적 위험 최소화(ERM):
$$
\min_{w}\ \frac{1}{n}\sum_{i=1}^n (y_i - \phi(x_i)^\top w)^2.
$$
- 정규화(릿지/라쏘)를 추가하면 과적합 억제:
$$
\min_{w}\ \frac{1}{n}\sum_{i=1}^n (y_i - \phi(x_i)^\top w)^2 + \lambda \|w\|_2^2
\quad\text{(Ridge)}
$$
$$
\min_{w}\ \frac{1}{n}\sum_{i=1}^n (y_i - \phi(x_i)^\top w)^2 + \lambda \|w\|_1
\quad\text{(Lasso)}
$$

---

## 3. 장단점·모델링 직관

### 장점
- 간단한 코드로 **비선형 패턴** 근사.
- 파이프라인(스케일링→다항변환→회귀)로 **재현성/배포** 용이.

### 단점/주의
- 차수↑ → **과적합**/외삽 불안정(범위 밖 폭주).
- **다중공선성**(고차항 간 상관↑) → 분산 확대 → 계수 해석 어려움.
- **스케일** 문제: \(x^d\) 크기 폭증 → 수치 불안정 → **표준화/중심화** 필수.

---

## 4. 차수 선택과 검증

- **과소적합**(차수 너무 낮음): 편향↑, 훈련/검증 모두 성능 낮음.
- **과적합**(차수 너무 높음): 분산↑, 훈련 성능↑·검증 성능↓.
- 해결: **교차검증**으로 최적 차수·정규화 강도(\(\lambda\)) 동시 탐색.
- 보조지표: **학습곡선**, **검증곡선**, **AIC/BIC**, **잔차진단**.

---

## 5. 실습 ① — 1차원 다항 회귀(기초)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

# 1. 데이터(비선형) 생성
rng = np.random.default_rng(42)
X = np.linspace(0, 6, 60).reshape(-1, 1)
y_true = 0.5*X**3 - 2*X**2 + 1.0*X + 4.0
y = y_true + rng.normal(0, 3.0, size=y_true.shape)

# 2. 학습/평가 분할
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, random_state=42)

# 3. 파이프라인(스케일링 → 다항(3차) → 선형회귀)
pipe = Pipeline([
    ("scaler", StandardScaler(with_mean=True, with_std=True)),
    ("poly", PolynomialFeatures(degree=3, include_bias=False)),
    ("lr", LinearRegression())
])

pipe.fit(X_tr, y_tr)
y_hat = pipe.predict(X)

# 4. 시각화
plt.scatter(X, y, s=20, label="Noisy data")
plt.plot(X, y_true, linewidth=2, label="True function")
plt.plot(X, y_hat, linewidth=2, label="Poly deg=3 (OLS)")
plt.legend(); plt.xlabel("x"); plt.ylabel("y"); plt.title("Polynomial Regression (degree=3)")
plt.show()

# 5. 간단한 성능
from sklearn.metrics import mean_squared_error, r2_score
rmse_tr = np.sqrt(mean_squared_error(y_tr, pipe.predict(X_tr)))
rmse_te = np.sqrt(mean_squared_error(y_te, pipe.predict(X_te)))
print("RMSE train:", rmse_tr, "RMSE test:", rmse_te, "R2 test:", r2_score(y_te, pipe.predict(X_te)))
```

**설명 포인트**
- **표준화**를 (x → z-score)로 선행해 수치 안정성 확보.
- 3차는 진실 모델과 동일; 하지만 노이즈·표본 수·분할 방식에 따라 최적 차수는 달라질 수 있으므로 **CV로 확인** 필요.

---

## 6. 실습 ② — 차수 선택 + 정규화(그리드서치)

```python
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import KFold, GridSearchCV

degrees = [1, 2, 3, 4, 6, 8, 10]
alphas = [0.0, 1e-3, 1e-2, 1e-1, 1.0, 10.0]  # 0.0은 사실상 OLS

def build_pipe(model, deg):
    return Pipeline([
        ("scaler", StandardScaler()),
        ("poly", PolynomialFeatures(degree=deg, include_bias=False)),
        ("reg", model)
    ])

param_grid = {
    "poly__degree": degrees,
    "reg__alpha": alphas,
}

models = {
    "ridge": Ridge(random_state=42),
    "lasso": Lasso(max_iter=5000, random_state=42)
}

best_models = {}
cv = KFold(n_splits=5, shuffle=True, random_state=42)

for name, base_model in models.items():
    pipe = Pipeline([("scaler", StandardScaler()),
                     ("poly", PolynomialFeatures(include_bias=False)),
                     ("reg", base_model)])
    g = GridSearchCV(pipe, param_grid, cv=cv,
                     scoring="neg_root_mean_squared_error", n_jobs=-1)
    g.fit(X_tr, y_tr.ravel())
    best_models[name] = g.best_estimator_
    print(f"[{name}] best params:", g.best_params_, " CV RMSE:", -g.best_score_)

# 테스트 성능 비교
for name, m in best_models.items():
    from sklearn.metrics import mean_squared_error
    rmse = np.sqrt(mean_squared_error(y_te, m.predict(X_te)))
    print(f"[{name}] Test RMSE:", rmse)
```

**핵심 포인트**
- **차수와 \(\alpha\)** 를 동시에 탐색 → **편향-분산 균형** 자동 탐색.
- 라쏘는 불필요한 고차항 계수를 0으로 만들어 **특성 선택** 효과를 낸다.

---

## 7. 실습 ③ — 과적합 시연(학습/검증 곡선)

```python
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

deg_list = range(1, 16)
rmse_trs, rmse_tes = [], []

for d in deg_list:
    model = Pipeline([
        ("scaler", StandardScaler()),
        ("poly", PolynomialFeatures(degree=d, include_bias=False)),
        ("lr", LinearRegression())
    ])
    model.fit(X_tr, y_tr)
    rmse_trs.append(np.sqrt(mean_squared_error(y_tr, model.predict(X_tr))))
    rmse_tes.append(np.sqrt(mean_squared_error(y_te, model.predict(X_te))))

plt.plot(deg_list, rmse_trs, marker="o", label="Train RMSE")
plt.plot(deg_list, rmse_tes, marker="o", label="Test RMSE")
plt.xlabel("polynomial degree"); plt.ylabel("RMSE")
plt.title("Under/Overfitting across degrees")
plt.legend(); plt.show()
```

- 보통 특정 차수 이후 **Train RMSE ↓** 계속 하락하지만 **Test RMSE ↑** 반등 → 과적합 구간.

---

## 8. 다변량(다특성) 다항 회귀와 상호작용

### 1. 수식
- \(p\)차원 입력 \(x=(x_1,\dots,x_p)\)를 \(d\)차 다항으로 확장하면 항의 수는
$$
\text{항 개수}=\binom{p+d}{d}.
$$
- 예: \(p=5, d=4 \Rightarrow \binom{9}{4}=126\) → **특성 폭증**에 유의.

### 2. 코드(상호작용 포함)
```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline

data = fetch_california_housing(as_frame=True)
X_df = data.frame.drop(columns=["MedHouseVal"])
y_arr = data.frame["MedHouseVal"].values

X_tr, X_te, y_tr, y_te = train_test_split(X_df, y_arr, test_size=0.2, random_state=42)

pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("poly", PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)),
    ("ridge", Ridge(alpha=1.0, random_state=42))
])
pipe.fit(X_tr, y_tr)
from sklearn.metrics import mean_squared_error, r2_score
print("Test RMSE:", np.sqrt(mean_squared_error(y_te, pipe.predict(X_te))),
      "R2:", r2_score(y_te, pipe.predict(X_te)))
```

- **상호작용 항**(예: \(x_1x_2\))은 현실에서 매우 유용. 다만 항 수가 빠르게 증가 → **정규화·특성선택** 필수.

---

## 9. 수치 안정성과 스케일링, 중심화

### 1. 왜 불안정해지는가?
- 반더몬드 행렬은 \(x^d\) 항이 커져 **조건수(Cond. number)**가 급증.
- 데이터 범위가 크면(예: \(x\in[0,10^3]\)) 고차항이 폭주 → 계수 추정 불안정.

### 2. 예방책
- **표준화**: \(z=(x-\mu)/\sigma\).
- **중심화**: 데이터 평균을 0 근처로 이동 → 다항 베이스의 상관 약화.
- **직교 다항**(체비쇼프/르장드르) 사용 시 수치안정성·해석성 향상.

### 3. 직접 해보기(정규방정식 vs SVD)
```python
import numpy as np

def vandermonde(x, d):
    return np.column_stack([x**k for k in range(d+1)])  # bias 포함

rng = np.random.default_rng(0)
x = np.linspace(-3, 3, 50)
Xv = vandermonde(x, d=10)
w_true = np.array([2, -1, 0, 0.5] + [0]*8)
y = Xv @ w_true + rng.normal(0, 0.5, size=len(x))

# (1) 정규방정식
w_ne = np.linalg.solve(Xv.T@Xv, Xv.T@y)

# (2) SVD
U, S, Vt = np.linalg.svd(Xv, full_matrices=False)
w_svd = Vt.T @ ( (U.T @ y) / S )

print("||w_ne - w_svd||:", np.linalg.norm(w_ne - w_svd))
```

- 잡음·조건수에 따라 **정규방정식이 불안정**할 수 있음을 실제 데이터에서 관찰 가능.

---

## 10. 모델 진단: 잔차·가정 위반·이상치

### 1. 잔차 점검
- **패턴**이 보이면 모델 미스펙(차수 부족, 누락 상호작용).
- **이분산**(x 커질수록 분산 커짐) → 가중회귀/변환(로그 등) 고려.

```python
import matplotlib.pyplot as plt
y_pred = pipe.predict(X_tr)
resid = (y_tr - y_pred)
plt.scatter(y_pred, resid, s=12)
plt.axhline(0, color="k", linewidth=1)
plt.xlabel("Predicted"); plt.ylabel("Residual"); plt.title("Residual vs Fitted")
plt.show()
```

### 2. 이상치/영향점
- **RANSAC** 등 강건회귀를 보조로 고려.
- **쿠크 거리(Cook’s distance)**, 레버리지 점검.

---

## 11. 정보 기준(AIC/BIC)과 모델 선택

- 가정: 잔차가 정규, 등분산일 때 근사 가능.
$$
\text{AIC} = 2k - 2\ln\hat{L},\quad
\text{BIC} = k\ln n - 2\ln\hat{L}
$$
- \(k\): 유효 파라미터 수(다항 계수 수), \(\hat{L}\): 최대우도.
- **BIC는 복잡한 모델에 더 가혹** → 과적합 억제.

---

## 12. 직교 다항(체비쇼프)와 수치적 장점

- 체비쇼프 다항 \(T_m(t)=\cos(m\arctan2(\sqrt{1-t^2}, t))\), \(t\in[-1,1]\).
- 데이터 \(x\)를 \([-1,1]\)로 선형 변환 후, \(\phi(x)=[T_0(t),T_1(t),\dots,T_d(t)]\) 사용 시 **상호 상관이 낮아져** 안정적.

```python
import numpy as np
def to_minus1_1(x):
    a,b = x.min(), x.max()
    return 2*(x-a)/(b-a) - 1

def cheb_features(t, d):
    T = [np.ones_like(t), t]
    for k in range(2, d+1):
        T.append(2*t*T[-1] - T[-2])
    return np.column_stack(T[:d+1])
```

---

## 13. 스플라인·구간 다항 vs 전역 다항

- 전역 다항(본 글 주제)은 **전체 범위**에 하나의 고차식.
- **스플라인**은 구간별 저차 다항을 **연속/매끄러움 제약**으로 이어붙임 → 외삽 안정·유연성 우수.
- 실무 곡선 적합에서 **Cubic Spline, B-spline, 자연스플라인**이 더 강력한 경우가 많다.

---

## 14. 커널 리지/가우시안 프로세스와의 관계

- 다항 회귀는 **명시적** 다항 베이스.
- **커널 방식**(예: 다항 커널 \(k(x,x')=(1+x^\top x')^d\))은 **암묵적** 특성공간에서 선형모델을 학습.
- 고차 다항을 넓게 쓰고 싶다면 **커널 리지/SVR**이 수치·표현 면에서 유리할 수 있다.

---

## 15. 실습 ④ — FastAPI 배포 스텁(예)

```python
# 학습 코드에서 저장
import joblib
joblib.dump(best_models["ridge"], "poly_ridge.joblib")
```

```python
# app.py : uvicorn app:api --reload
from fastapi import FastAPI
import numpy as np
import joblib

api = FastAPI()
model = joblib.load("poly_ridge.joblib")

@api.post("/predict")
def predict(payload: dict):
    # payload: {"X": [[1.2], [3.4], ...]}  # 1D 예시
    X = np.array(payload["X"], dtype=float)
    yhat = model.predict(X).tolist()
    return {"pred": yhat}
```

---

## 16. 자주 하는 실수와 예방법

1) **스케일링 누락**: 고차항 폭주 → 항상 **표준화/중심화**.
2) **검증 누락**: 훈련 성능만 보고 차수 확장 금지 → **CV·학습/검증곡선** 필수.
3) **과도한 차수**: 외삽 위험. 배포 입력 범위 밖에서 폭주 → 입력 범위 가드·클리핑.
4) **다중공선성**: 라쏘/릿지·직교다항·차원축소(PCA)로 완화.
5) **누수(Leakage)**: 다항변환/스케일링은 반드시 **훈련 세트에서 fit, 전체에 transform**.

---

## 17. 종합 실무 파이프라인(템플릿)

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV, KFold

# 예: 수치형만 다항화, 결측/스케일링 포함
num_cols = [0]  # 1D 예제, 다변량이면 리스트 확장
num_pipe = Pipeline([
    ("imp", SimpleImputer(strategy="median")),
    ("sc", StandardScaler()),
    ("poly", PolynomialFeatures(include_bias=False)),
])

pre = ColumnTransformer([
    ("num", num_pipe, num_cols),
], remainder="drop")

full = Pipeline([
    ("pre", pre),
    ("reg", Ridge(random_state=42))
])

param_grid = {
    "pre__num__poly__degree": [1,2,3,4,6,8],
    "reg__alpha": [1e-3,1e-2,1e-1,1,10],
}

cv = KFold(n_splits=5, shuffle=True, random_state=42)
g = GridSearchCV(full, param_grid, cv=cv, scoring="neg_root_mean_squared_error", n_jobs=-1)
g.fit(X_tr, y_tr)
print("Best:", g.best_params_, "CV RMSE:", -g.best_score_)
```

---

## 18. 수학 보충

### 1. 편향–분산 직관
$$
\mathbb{E}\big[(\hat f(x)-f(x))^2\big] = \underbrace{\text{Bias}^2}_{d\ \text{낮음}} + \underbrace{\text{Var}}_{d\ \text{높음}} + \sigma^2.
$$
- 차수 \(d\)가 커지면 Bias↓ Var↑ → 정규화·CV로 타협점을 찾는다.

### 2. 복잡도(항 수)
$$
\#\text{features}=\binom{p+d}{d}=\frac{(p+d)!}{p!\,d!}.
$$
- 다변량·고차에서는 **연산/메모리** 부담이 급증 → 정규화·차수 제한·커널/스플라인 검토.

---

## 19. 확장 논의: 로버스트 회귀·가중회귀·변환

- **로버스트**(Huber, RANSAC): 이상치에 둔감.
- **가중 최소제곱(WLS)**: 이분산에서 관측치별 가중.
- **변환**: 로그/Box-Cox로 분산 안정화·선형화.

---

## 20. 체크리스트(현업용)

- [ ] 입력 범위/스케일 확인 → **표준화/중심화** 적용
- [ ] 다항 차수·정규화 하이퍼파라미터는 **CV로 선택**
- [ ] 학습/검증/테스트 **명확 분리**, 누수 금지(변환 fit/transform 분리)
- [ ] **잔차 플롯**으로 패턴/이분산/영향점 진단
- [ ] 외삽 방지 가드(입력 범위 체크, 경고/클리핑)
- [ ] 필요 시 **스플라인/커널**로 대체 검토
- [ ] 배포 전 **고정 난수/버전**으로 재현성 확보, **드리프트** 모니터링

---

## 21. 요약

- 다항 회귀는 **특성 다항변환 + 선형회귀**로 비선형 관계를 손쉽게 근사.
- 핵심은 **차수 선택·정규화·스케일링·검증**이다.
- 수치안정성과 해석을 위해 **표준화/직교다항/릿지·라쏘**를 적극 활용.
- 다변량·고차로 갈수록 항 수가 폭증하므로 **복잡도 관리**가 중요하다.
- 한계(외삽 불안정)가 뚜렷하므로 **스플라인·커널**도 함께 고려하면 실무 내구성이 높다.
