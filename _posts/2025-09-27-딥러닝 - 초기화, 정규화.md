---
layout: post
title: 딥러닝 - 초기화, 정규화
date: 2025-09-27 19:25:23 +0900
category: 딥러닝
---
# / 정규화(Normalization)

**Xavier/He 초기화 · 정규화(BN/LN/GN) & 배치 크기 이슈 · 드롭아웃 · 가중치 감쇠(Weight Decay)**

## A. 왜 초기화·정규화인가?

### A-1. 문제 배경

깊은 네트워크에서 **활성값/그라디언트의 분산**이 층을 지날수록 소실/폭주하기 쉽다.
- 입력 $$\mathbf{x}$$ 의 분산이 $$\mathrm{Var}[\mathbf{x}]$$ 라면, 선형층/활성함수를 통과하며 분산이 크게 변한다.
- 불안정: **수렴 느림**, **Dead ReLU**, **NaN/Inf**, **훈련 실패**.

### A-2. 해결 축

1) **초기화**: 층을 통과해도 분산이 **보존**되도록 가중치 분포를 설계.
2) **정규화**: 배치/피처 차원 통계로 **스케일/평균**을 안정화.
3) **정규화형 규제**: 드롭아웃/감쇠로 **과적합** 억제 & **폭주 억제**.

---

## / He(Kaiming) 초기화

### B-1. 선형층에서의 분산 전파(핵심 직관)

선형층 $$\mathbf{y} = \mathbf{W}\mathbf{x}$$, 독립 가정과 대칭 가정 하에
$$
\mathrm{Var}[y_j] \approx \mathrm{fan\_in}\,\mathrm{Var}[w]\ \mathrm{Var}[x].
$$
활성함수의 비선형성까지 고려하여 **입출력 분산이 유지**되도록 $$\mathrm{Var}[w]$$ 를 선택한다.

### — tanh/선형 계열

$$
\mathrm{Var}[w] \approx \frac{2}{\mathrm{fan\_in} + \mathrm{fan\_out}}.
$$
- **권장 사용**: **tanh**, **Sigmoid**(요즘은 은닉층에 잘 쓰지 않지만), **선형**.
- PyTorch: `nn.init.xavier_uniform_`, `nn.init.xavier_normal_`.

### — ReLU 계열

ReLU의 반파 특성(절반이 0) 반영 → **fan_in 기준**
$$
\mathrm{Var}[w] \approx \frac{2}{\mathrm{fan\_in}}.
$$
- **권장 사용**: **ReLU/LeakyReLU/PReLU/GELU**(일반적으로 He가 잘 맞음).
- LeakyReLU(기울기 $$\alpha$$)에 대한 gain:
$$
\text{gain} \approx \sqrt{\frac{2}{1+\alpha^2}}.
$$

### B-4. Convolution에서의 fan_in/out

Conv 필터 $$\mathbf{W}\in\mathbb{R}^{C_{\text{out}}\times C_{\text{in}}\times k_h\times k_w}$$
$$
\mathrm{fan\_in} = C_{\text{in}} \cdot k_h \cdot k_w,\quad
\mathrm{fan\_out}= C_{\text{out}} \cdot k_h \cdot k_w.
$$

### B-5. 코드: Xavier/He 초기화 유틸

```python
import torch
import torch.nn as nn
import torch.nn.init as init

def init_layer(m, nonlin='relu', leaky_slope=0.01):
    if isinstance(m, (nn.Linear, nn.Conv2d)):
        if nonlin in ['tanh', 'sigmoid', 'linear']:
            init.xavier_uniform_(m.weight)  # 또는 xavier_normal_
        elif nonlin in ['relu', 'gelu', 'elu', 'leaky_relu', 'prelu']:
            a = leaky_slope if nonlin=='leaky_relu' else 0.0
            init.kaiming_normal_(m.weight, a=a, nonlinearity='leaky_relu' if nonlin=='leaky_relu' else 'relu')
        else:
            init.kaiming_normal_(m.weight, nonlinearity='relu')
        if m.bias is not None:
            nn.init.zeros_(m.bias)

class SmallCNN(nn.Module):
    def __init__(self, act='relu'):
        super().__init__()
        self.act = dict(relu=nn.ReLU(), gelu=nn.GELU(), elu=nn.ELU())[act]
        self.net = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            self.act,
            nn.Conv2d(64, 64, 3, padding=1),
            self.act,
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(64, 10)
        )
        self.net.apply(lambda m: init_layer(m, nonlin=act))

    def forward(self, x): return self.net(x)
```

### 팁

- **마지막 BN의 scale(감마)을 0으로 초기화**하면 처음엔 잔차분기가 거의 0 → **안정 수렴** (pre-activation ResNet에서 흔함).
- **Residual scaling**: 잔차 출력에 0.1 같은 작은 스케일을 곱해 초반 폭주 예방.

```python
def zero_last_bn_gamma(module):
    for m in module.modules():
        if isinstance(m, nn.BatchNorm2d):
            if m.weight is not None:
                nn.init.zeros_(m.weight)  # 마지막 BN에만 적용하도록 조건 분기 필요
```

### B-7. Orthogonal 초기화 (RNN/선형에 종종 유용)

$$
\mathbf{W}^\top \mathbf{W} = \mathbf{I}.
$$
- **특징**: 변환에서 **에너지 보존** → 깊은 네트워크에서 유리할 때가 있음.
```python
for m in model.modules():
    if isinstance(m, nn.Linear):
        nn.init.orthogonal_(m.weight, gain=1.0)
```

---

## C. 정규화층: BN / LN / GN (수식·직관·코드)

### C-1. BatchNorm (BN)

- 채널별로 **배치 통계**(평균/분산)로 정규화:
  - 2D Conv의 경우 피처맵(배치×공간) 위 평균/분산.
$$
\mu_B = \frac{1}{m}\sum_{i=1}^m x_i,\quad
\sigma_B^2 = \frac{1}{m}\sum_{i=1}^m (x_i - \mu_B)^2
$$
정규화 후 학습가능 **스케일/시프트** $$\gamma,\beta$$:
$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \varepsilon}},\qquad
y_i = \gamma \hat{x}_i + \beta.
$$
- **장점**: 스케일 안정, **큰 배치**에서 매우 효과적.
- **주의**: **작은 배치/분산 학습**에서 통계 불안정(노이즈).

```python
bn = nn.BatchNorm2d(num_features=64, eps=1e-5, momentum=0.1)
# 학습 시: 배치 통계, 추론 시: 러닝 평균/분산 사용

model.train();  # BN은 train/eval 모드에 민감
model.eval();
```

### C-2. LayerNorm (LN)

- **샘플 내부의 피처 축 전부**(채널·시퀀스 길이 등)에 대해 평균/분산 계산:
$$
\mu = \frac{1}{H}\sum_{j=1}^H x_j,\quad
\sigma^2=\frac{1}{H}\sum_{j=1}^H (x_j-\mu)^2.
$$
- **배치 크기 영향 없음** → Transformer에서 표준.
```python
ln = nn.LayerNorm(normalized_shape=hidden_dim, eps=1e-5)
```

### C-3. GroupNorm (GN)

- 채널을 **G개의 그룹**으로 나눠 그룹별로 정규화.
- **중간 지점**: 배치 크기 영향 줄이면서 **공간·채널 구조** 반영.
```python
gn = nn.GroupNorm(num_groups=32, num_channels=64, eps=1e-5, affine=True)
```

### C-4. InstanceNorm (참고)

- 채널별로 **샘플 단위** 정규화(배치 독립). 스타일 트랜스퍼 등에서 활용.

### C-5. 어디에 두나? (Pre-activation vs Post)

- **Pre-activation ResNet**: `BN → Act(ReLU) → Conv` (He et al.)
  장점: 잔차 연결을 **아이덴티티**에 가깝게 유지, **그라디언트 흐름** 개선.
- MLP/CNN 일반적 패턴: `Conv/Linear → BN → Act`.

### C-6. Bias와 BN

- BN 직후에 **β(shift)** 가 있으므로, **이전 레이어 bias는 보통 꺼둔다**(효율/중복 제거).

```python
# 권장 (BN이 shift 제공)

nn.Conv2d(64, 64, 3, padding=1, bias=False)
```

---

## D. 배치 크기 이슈 & 해결책

### D-1. BN의 작은 배치 문제

- per-GPU 배치가 작을 때 **μ/σ 추정 오차**가 커짐 → **손실 진동** · **러닝 평균 불안**.

### D-2. 해결책

1) **SyncBN**: 여러 GPU의 배치를 **동기화**해 더 큰 유효 통계.
```python
# DDP 환경에서

model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
```
2) **Ghost BN**: 큰 배치를 **가상 소그룹**으로 나눠 통계를 구하는 기법(논리적 개념).
3) **Frozen BN**: 사전학습 가중치에서 BN의 러닝 통계를 **고정**(학습 중 `eval()`처럼 동작).
4) **LN/GN로 교체**: 배치 독립 정규화 사용(Transformer/작은 배치 CNN).
5) **유효 배치 확장**: **그라디언트 누적**로 per-step 배치는 작아도 **BN에는 큰 배치**가 필요 → 이 부분은 누적으로 해결되지 않음(**BN은 forward 배치를 씀**). 따라서 SyncBN/LN/GN이 더 근본적.

### D-3. Frozen BN 코드

```python
def set_bn_eval(m):
    if isinstance(m, torch.nn.modules.batchnorm._BatchNorm):
        m.eval()            # running stats 사용
        m.weight.requires_grad_(False)
        m.bias.requires_grad_(False)

model.apply(set_bn_eval)
```

---

## — 확률적 뉴런 끄기

### E-1. 정의(역방향 스케일 포함한 Inverted Dropout)

학습에서 뉴런을 확률 $$p$$ 로 끄고, **켜진 뉴런을 $$1/(1-p)$$ 로 스케일**:
$$
\tilde{\mathbf{h}} = \frac{1}{1-p}\,\mathbf{m}\odot \mathbf{h},\quad m_i\sim\mathrm{Bernoulli}(1-p).
$$
추론에서는 **드롭아웃 비활성**(스케일 불필요).

### E-2. 어디에 쓰나

- **MLP/Transformer**: 피드포워드 중간층에 `p=0.1~0.5`.
- **CNN**: BN과 증강이 강하면 p를 작게(0~0.3) 혹은 생략.
- **RNN**: 순환 연결에 직접 드롭아웃은 불안정 → **Recurrent Dropout**(특수 구현) 또는 **입력/출력/층간**에 사용.

```python
ff = nn.Sequential(
    nn.Linear(512, 2048),
    nn.GELU(),
    nn.Dropout(p=0.1),
    nn.Linear(2048, 512),
    nn.Dropout(p=0.1)   # Transformer FFN 스타일
)
```

> **BN과 Dropout 순서**: 일반적으로 **BN → 활성 → Dropout**. BN이 확률적 마스크 영향을 받으면 러닝통계 불안.

---

## — AdamW의 분리(decoupled)

### F-1. L2 정규화 vs AdamW

- 고전 **L2**: 손실에 $$\lambda\|\theta\|^2$$ 를 추가 → 그라디언트에 $$+\lambda\theta$$ 가 더해짐.
- **AdamW**: 모멘텀/적응항과 분리하여 **업데이트 후 별도 감쇠**:
$$
\theta \leftarrow \theta - \eta \frac{\hat{m}}{\sqrt{\hat v}+\epsilon} \quad;\quad
\theta \leftarrow (1-\eta\lambda)\,\theta.
$$
- **효과**: 더 일관된 규제, 일반화 좋음.

### F-2. no_decay 파라미터 그룹

- **Bias, Norm(γ/β)** 는 보통 **감쇠 제외**: 과도한 규제가 성능 저하.
```python
decay, no_decay = [], []
for n,p in model.named_parameters():
    if p.ndim == 1 or n.endswith('bias'):
        no_decay.append(p)
    else:
        decay.append(p)
opt = torch.optim.AdamW(
    [{'params': decay, 'weight_decay': 0.05},
     {'params': no_decay, 'weight_decay': 0.0}],
    lr=3e-4, betas=(0.9,0.999), eps=1e-8
)
```

---

## G. 조합 레시피(Architecture-wise)

### G-1. CNN(ResNet류)

- **초기화**: He(kaiming, fan_in)
- **정규화**: BN(큰 배치), 작은 배치면 **SyncBN or GN**
- **드롭아웃**: 적거나 없음(강력한 증강/WD로 대체)
- **WD**: 1e-4 ~ 1e-2 (SGD), AdamW(0.01~0.1)
- **트릭**: 마지막 BN 감마 0, Residual scaling 0.1

### G-2. Transformer

- **초기화**: 일반적으로 **Kaiming/Truncated Normal**(프레임워크 기본), QKV/프로젝션에 fan_in 기준
- **정규화**: **LayerNorm**(Pre-LN 권장: LN → sublayer → Residual)
- **드롭아웃**: FFN/Attn 드롭아웃 0.1~0.2, attn probs dropout 0.0~0.1
- **WD**: AdamW 0.05~0.1, **no_decay** 그룹 적용
- **트릭**: Residual에 작은 스케일(0.1) 또는 **수정된 Pre-LN**로 안정화

### G-3. 작은 배치·고해상도 비전

- **정규화**: **GroupNorm(num_groups=32)**
- **증강**: 강력(CutMix/MixUp/RandAugment) → 드롭아웃 줄이거나 제거
- **초기화**: He 계열 유지

---

## H. 실험: 분산 전파/정규화 효과 확인

### H-1. 서로 다른 초기화로 활성 분산 비교

```python
import torch, torch.nn as nn, torch.nn.init as init

class ToyDeep(nn.Module):
    def __init__(self, init_name='xavier'):
        super().__init__()
        layers = []
        for _ in range(10):
            layers += [nn.Linear(512, 512), nn.ReLU()]
        self.net = nn.Sequential(*layers)
        self.apply(lambda m: self._init(m, init_name))

    def _init(self, m, init_name):
        if isinstance(m, nn.Linear):
            if init_name=='xavier':
                init.xavier_normal_(m.weight); nn.init.zeros_(m.bias)
            elif init_name=='kaiming':
                init.kaiming_normal_(m.weight, nonlinearity='relu'); nn.init.zeros_(m.bias)

    def forward(self, x): return self.net(x)

x = torch.randn(256, 512)
for name in ['xavier','kaiming']:
    model = ToyDeep(init_name=name)
    with torch.no_grad():
        h = x
        stds = []
        for layer in model.net:
            h = layer(h)
            if isinstance(layer, nn.ReLU): stds.append(h.std().item())
    print(name, 'avg std:', sum(stds)/len(stds))
```
> 관찰: ReLU 계열에선 **He**가 깊어질 때 분산 유지에 유리.

### H-2. BN vs LN (작은 배치에서)

```python
import torch, torch.nn as nn

class CNN_BN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3,64,3,padding=1,bias=False), nn.BatchNorm2d(64), nn.ReLU(),
            nn.Conv2d(64,64,3,padding=1,bias=False), nn.BatchNorm2d(64), nn.ReLU()
        )
    def forward(self,x): return self.net(x)

class CNN_LN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3,64,3,padding=1,bias=True)
        self.conv2 = nn.Conv2d(64,64,3,padding=1,bias=True)
        self.ln1 = nn.GroupNorm(1,64)  # LN 대용(채널만 정규화)
        self.ln2 = nn.GroupNorm(1,64)
        self.act = nn.ReLU()
    def forward(self,x):
        x = self.act(self.ln1(self.conv1(x)))
        x = self.act(self.ln2(self.conv2(x)))
        return x

x_small = torch.randn(4,3,64,64)   # 작은 배치
print(CNN_BN()(x_small).mean().item(), CNN_LN()(x_small).mean().item())
```
> 작은 배치에서는 **BN 출력 통계**가 흔들릴 수 있으나, LN/GN은 **안정**.

---

## I. 구현 템플릿

### I-1. Pre-activation Residual Block (GN 버전)

```python
class PreActResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, groups=32, stride=1, drop=0.0):
        super().__init__()
        self.norm1 = nn.GroupNorm(groups, in_ch)
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)
        self.norm2 = nn.GroupNorm(groups, out_ch)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.drop = nn.Dropout2d(drop) if drop>0 else nn.Identity()
        self.act = nn.ReLU(inplace=True)
        self.skip = (nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False)
                     if (stride!=1 or in_ch!=out_ch) else nn.Identity())
        self.apply(lambda m: init_layer(m, nonlin='relu'))

    def forward(self, x):
        h = self.act(self.norm1(x))
        out = self.conv1(h)
        out = self.drop(self.conv2(self.act(self.norm2(out))))
        return out + self.skip(x)
```

### I-2. Transformer-style Block (Pre-LN)

```python
class TransformerBlock(nn.Module):
    def __init__(self, d, nhead=8, d_ff=4, p_drop=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(d)
        self.attn = nn.MultiheadAttention(d, nhead, dropout=p_drop, batch_first=True)
        self.ln2 = nn.LayerNorm(d)
        self.ff = nn.Sequential(
            nn.Linear(d, d*d_ff), nn.GELU(), nn.Dropout(p_drop),
            nn.Linear(d*d_ff, d), nn.Dropout(p_drop)
        )
        self.apply(lambda m: init_layer(m, nonlin='gelu'))

    def forward(self, x, attn_mask=None, key_padding_mask=None):
        h = self.ln1(x)
        attn_out, _ = self.attn(h,h,h, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)
        x = x + attn_out
        h = self.ln2(x)
        x = x + self.ff(h)
        return x
```

---

## J. 드롭아웃·정규화·감쇠의 상호작용

- **BN + Dropout**: Dropout이 BN의 통계를 흔들 수 있어 **BN → Act → Dropout** 순서 권장, p는 낮게.
- **LN/GN + Dropout**: 배치 독립이므로 상호작용 깔끔.
- **Weight Decay vs Norm 파라미터**: γ/β(Scale/Shift)와 bias는 **감쇠 제외** 권장.
- **강한 증강**(MixUp/CutMix/RandAug): 드롭아웃 강도를 낮춰도 정규화 충분.

---

## K. 문제 해결 체크리스트

1) **훈련 초반 불안정/NaN**
- 초기화 확인(He/Xavier), LR↓, AdamW `eps` 1e-8→1e-6, Grad Clip, Pre-LN, Warmup
- BN 통계: 작은 배치면 SyncBN/LN/GN 전환

2) **수렴 느림·Dead ReLU**
- He 초기화, LeakyReLU/GELU 사용, 마지막 BN 감마 0, Residual scaling

3) **일반화 나쁨(오버핏)**
- WD↑, 드롭아웃↑, 라벨 스무딩, 증강 강도↑, Early stopping

4) **분산/멀티GPU 재현성**
- BN → SyncBN, `sampler.set_epoch`, `torch.backends.cudnn.deterministic=True`

---

## L. 수식 큐시트(빠른 참조)

- **Xavier**
  $$\mathrm{Var}[w]=\frac{2}{\mathrm{fan\_in}+\mathrm{fan\_out}}$$

- **He (ReLU)**
  $$\mathrm{Var}[w]=\frac{2}{\mathrm{fan\_in}}$$

- **BN**
  $$\hat{x}=\frac{x-\mu_B}{\sqrt{\sigma_B^2+\varepsilon}},\quad y=\gamma\hat{x}+\beta$$

- **LN/GN**
  $$\hat{x}=\frac{x-\mu_{\text{feat}}}{\sqrt{\sigma_{\text{feat}}^2+\varepsilon}}$$
  (정규화 축이 샘플 내부)

- **Dropout (inverted)**
  $$\tilde{h}=\frac{1}{1-p}\,m\odot h,\quad m\sim\text{Bern}(1-p)$$

- **AdamW 감쇠(분리형)**
  $$\theta\leftarrow \theta - \eta \frac{\hat m}{\sqrt{\hat v}+\epsilon},\quad
    \theta\leftarrow (1-\eta\lambda)\theta$$

---

## M. 미니 프로젝트: “안정 블록” 만들기

### M-1. 문제 설정

- CIFAR-10 소형 CNN을 **BN+ReLU** vs **GN+GELU** vs **LN+GELU** 로 비교.
- 초기화는 He, 드롭아웃 0.1, AdamW 3e-4, WD 0.05, Cosine+Warmup.

### M-2. 공용 학습 루프(요지)

```python
import torch, torch.nn as nn
from torch.optim.lr_scheduler import CosineAnnealingLR

def train(model, train_loader, val_loader, epochs=100, warmup=500):
    opt = torch.optim.AdamW([
        {'params':[p for n,p in model.named_parameters() if p.ndim>1], 'weight_decay':0.05},
        {'params':[p for n,p in model.named_parameters() if p.ndim==1 or n.endswith('bias')], 'weight_decay':0.0}
    ], lr=3e-4, betas=(0.9,0.999), eps=1e-8)
    sch = CosineAnnealingLR(opt, T_max=epochs)
    step = 0
    for ep in range(epochs):
        model.train()
        for xb,yb in train_loader:
            step += 1
            # linear warmup
            if step < warmup:
                scale = step / warmup
                for g in opt.param_groups: g['lr'] = 3e-4*scale
            opt.zero_grad(set_to_none=True)
            logits = model(xb)
            loss = nn.CrossEntropyLoss(label_smoothing=0.1)(logits, yb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()
        sch.step()
        # validation …
```

> 관찰: **작은 배치**일수록 GN/LN 쪽이 손실 곡선이 매끈하고 **일관된 수렴**을 보이는 경향.

---

## N. 실무 요약(한 페이지)

- **초기화**
  - ReLU/GELU: **He(kaiming, fan_in)**
  - tanh/선형: **Xavier**
  - Residual: **마지막 BN 감마 0** or **residual scaling**
  - Bias는 **0**로 시작

- **정규화**
  - 큰 배치 CNN: **BN**
  - 작은 배치/Transformer: **LN** / **GN**(32 그룹 권장)
  - BN 사용 시 **bias=False**, `model.train()/eval()` 엄수, SyncBN 검토

- **드롭아웃**
  - MLP/Transformer: 0.1~0.3
  - CNN: 증강/WD가 강하면 0~0.2
  - 순서: **Norm → Act → Dropout**

- **가중치 감쇠**
  - **AdamW** 사용, **no_decay** 그룹( bias & Norm 파라미터 )
  - WD: 0.01~0.1(언어/비전 대형), 1e-4~1e-3(고전 CNN/SGD)

- **작은 배치 이슈**
  - BN 불안정 → **SyncBN/LN/GN/Frozen BN**
  - 그라디언트 누적은 **BN 문제를 직접 해결하지 못함**(forward 배치 통계)

---

### 마무리

초기화는 **출발선**, 정규화는 **균형추**, 드롭아웃/감쇠는 **가드레일**이다.
세 요소를 올바르게 조합하면 **훈련 안정성·속도·일반화**가 동시에 개선된다. 다음 장에서는 이 기반 위에 **평가/신뢰성(메트릭·캘리브레이션)** 을 얹는다.
