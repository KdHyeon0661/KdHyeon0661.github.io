---
layout: post
title: 기계학습 - 선형 회귀
date: 2025-08-14 23:20:23 +0900
category: 기계학습
---
# 선형 회귀(Linear Regression)

## 1. 개념과 표기

### (1) 모델과 벡터/행렬 표기
- 데이터 \(X\in\mathbb{R}^{n\times d}\), 타깃 \(y\in\mathbb{R}^n\), 가중치 \(w\in\mathbb{R}^d\), 절편 \(b\in\mathbb{R}\).
- 예측:
$$
\hat y = Xw + b\mathbf{1}
$$
- **절편 처리**: 설계행렬 \( \tilde X = [\mathbf{1}, X] \in \mathbb{R}^{n\times (d+1)} \), 계수 \( \tilde w=[b;w] \) 로 합치면
$$
\hat y=\tilde X\,\tilde w.
$$

### (2) 목적함수(평균제곱오차, MSE)
$$
\min_{w,b}\ \frac{1}{n}\,\|y-(Xw+b\mathbf{1})\|_2^2.
$$
- **중심화(centering)**: \( \bar x=\frac{1}{n}X^\top\mathbf{1},\ \bar y=\frac{1}{n}\mathbf{1}^\top y \) 로 열평균 제거 시,
$$
b=\bar y-\bar x^\top w.
$$
즉, \(X\) 를 평균 0으로 만들면 절편은 자동으로 \( \bar y \)에 맞춰진다.

---

## 2. OLS의 기하: 정사영, Hat 행렬, 잔차

### (1) 정사영으로서의 OLS
- 목표:
$$
\min_{\tilde w}\ \|y-\tilde X\tilde w\|_2^2.
$$
- 해는 \(y\)를 \(\mathcal{R}(\tilde X)\) (**열공간**)에 **직교 정사영**한 점:
$$
\hat y = P y,\quad P=\tilde X(\tilde X^\top \tilde X)^{-1}\tilde X^\top.
$$
여기서 \(P\)는 **Hat 행렬**(프로젝터): \(P^2=P,\ P^\top=P\).

### (2) 잔차와 직교성
- 잔차 \( r=y-\hat y \)는 열공간에 직교:
$$
\tilde X^\top r = 0.
$$

### (3) 레버리지와 LOOCV(빠른 검증)
- 대각원소 \(h_{ii}=P_{ii}\) 는 **레버리지**(영향력)로, 큰 \(h_{ii}\)는 이례치/고립 점.
- LOOCV 예측:
$$
\hat y_{(i)}=\hat y_i-\frac{r_i}{1-h_{ii}},\quad
\text{PRESS}=\sum_{i=1}^n\left(\frac{r_i}{1-h_{ii}}\right)^2.
$$

---

## 3. 확률적 관점: MLE/MAP, 정규화의 베이지안 해석

### (1) 가우시안 노이즈 가정
- 생성모형:
$$
y\mid X,w,b \sim \mathcal{N}(Xw+b\mathbf{1},\,\sigma^2 I).
$$
- **MLE**는 OLS와 동치:
$$
\arg\min_{w,b}\ \|y-(Xw+b\mathbf{1})\|_2^2.
$$

### (2) MAP과 정규화
- **릿지**(L2): \( w\sim\mathcal{N}(0,\tau^2 I) \Rightarrow \) MAP이
$$
\min_{w,b}\ \|y-(Xw+b\mathbf{1})\|_2^2 + \lambda\|w\|_2^2,\quad \lambda=\sigma^2/\tau^2.
$$
- **라쏘**(L1): \( w_j \sim \text{Laplace}(0,\beta)\Rightarrow \) MAP이
$$
\min_{w,b}\ \|y-(Xw+b\mathbf{1})\|_2^2 + \lambda\|w\|_1.
$$

---

## 4. OLS 해: 정규방정식 vs QR/SVD

### (1) 정규방정식(직접식)
$$
\tilde w = (\tilde X^\top \tilde X)^{-1}\tilde X^\top y.
$$
- **주의**: \( \tilde X^\top \tilde X \)이 ill-conditioned/특이하면 수치 불안정.

### (2) QR 분해(권장)
- \( \tilde X=QR\ (Q^\top Q=I,\ R\ \text{상삼각}) \)
$$
\tilde w = R^{-1}Q^\top y.
$$

### (3) SVD/의사역행렬(가장 안정)
- \( \tilde X=U\Sigma V^\top \Rightarrow \tilde X^+=V\Sigma^+ U^\top \)
$$
\tilde w = \tilde X^+ y.
$$

---

## 5. 경사하강법(GBD)·미니배치·수렴

### (1) 기울기(절편 포함)
$$
\nabla_{\tilde w}\ \frac{1}{2n}\|y-\tilde X\tilde w\|_2^2
= -\frac{1}{n}\tilde X^\top(y-\tilde X\tilde w)
= \frac{1}{n}\tilde X^\top(\tilde X\tilde w-y).
$$

### (2) 업데이트
$$
\tilde w \leftarrow \tilde w - \alpha \cdot \frac{1}{n}\tilde X^\top(\tilde X\tilde w-y).
$$

### (3) 스텝 크기와 수렴
- Lipschitz 상수 \(L=\frac{1}{n}\lambda_{\max}(\tilde X^\top \tilde X)\).  
- \( \alpha\in(0,\frac{2}{L}) \)면 단조 감소(볼록 이차).

### (4) SGD/미니배치
- 배치 대신 샘플/소배치 기울기. 지그재그하지만 대규모에서 효율적.

---

## 6. 정규화 회귀: 릿지·라쏘·엘라스틱넷

### (1) 릿지의 닫힌형 해
$$
\tilde w = (\tilde X^\top \tilde X + \lambda \tilde R)^{-1}\tilde X^\top y,
$$
- 보통 절편에는 패널티를 주지 않음 → \(\tilde R=\mathrm{diag}(0,1,\dots,1)\).
- 주성분 방향 \(v_j\)에 대한 **수축**:
$$
\text{proj 계수}\ \frac{\sigma_j^2}{\sigma_j^2+\lambda}.
$$

### (2) 라쏘의 희소성
- 해가 **0으로 정확히** 떨어져 변수 선택이 가능.
- 직교 설계행렬일 때 **소프트 임계값화**:
$$
\hat w_j=\mathrm{sign}(z_j)\cdot \max(|z_j|-\tfrac{\lambda}{2},0),\quad z=X^\top y.
$$

### (3) 엘라스틱넷
$$
\min_{w,b}\ \|y-Xw-b\mathbf{1}\|_2^2+\lambda_1\|w\|_1+\lambda_2\|w\|_2^2.
$$
- **강한 상관** 변수 집합을 함께 선택하는 경향(라쏘의 불안정성 완화).

---

## 7. 진단과 가정 점검

### (1) 등분산성/정규성
- 잔차 vs 예측 산점, Q–Q plot, 브루쉬–파건/화이트 테스트(개념적 언급).

### (2) 다중공선성
- VIF(분산 팽창 계수): \( \mathrm{VIF}_j=\frac{1}{1-R_j^2} \) (특징 \(x_j\)를 나머지로 회귀했을 때 결정계수 \(R_j^2\))  
- 완화: **릿지**, 차원축소(PCA), 변수 결합/제거.

### (3) 영향점/이상치
- 레버리지 \(h_{ii}\), Cook’s distance \( D_i=\frac{r_i^2}{p\,\hat\sigma^2}\cdot\frac{h_{ii}}{(1-h_{ii})^2} \) (개념식)  
- 대응: 강건 손실(허버), RANSAC, 도메인 점검.

---

## 8. WLS/GLS, 강건 회귀, p≫n, 커널 릿지

### (1) WLS(가중 최소제곱)
- 이분산일 때 가중치 \(W=\mathrm{diag}(w_i)\):
$$
\min\ (y-\tilde X\tilde w)^\top W (y-\tilde X\tilde w),
\quad \tilde w=(\tilde X^\top W\tilde X)^{-1}\tilde X^\top W y.
$$

### (2) GLS(상관 오차)
- 공분산 \(\Sigma\) (비대각)일 때:
$$
\tilde w=(\tilde X^\top \Sigma^{-1}\tilde X)^{-1}\tilde X^\top \Sigma^{-1} y.
$$

### (3) 강건 회귀
- **허버 손실**: 작은 잔차는 L2, 큰 잔차는 L1로 처리.  
- **RANSAC**: 내부자 집합으로 반복 적합.

### (4) p≫n (특징이 샘플보다 많은 상황)
- OLS 해 무한/불안정 → **릿지/라쏘/드롭컬럼/PCA** 필수.

### (5) 커널 릿지(dual, 비선형 확장)
- Gram \(K=X X^\top\) 대신 일반 **커널** \(K_{ij}=k(x_i,x_j)\).  
- **이원식 해**:
$$
\alpha=(K+\lambda I)^{-1}y,\quad \hat y_*=K(X,X_*)^\top \alpha.
$$

---

## 9. 실전 파이프라인(스케일링·CV·특징공학)

- 스케일러(표준화) → 다항/상호작용 → 릿지/라쏘/엘라스틱넷 → **CV로 \(\lambda\)** 선택.  
- **주의**: 스케일/다항 **fit은 학습세트만**으로! (누수 금지)

```python
# scikit-learn 파이프라인 예시
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import RidgeCV, LassoCV
from sklearn.model_selection import KFold

ridge_pipe = Pipeline([
    ("sc", StandardScaler(with_mean=True, with_std=True)),
    ("poly", PolynomialFeatures(degree=2, include_bias=False)),
    ("ridge", RidgeCV(alphas=[1e-3,1e-2,1e-1,1,10], cv=KFold(5, shuffle=True, random_state=42)))
])

lasso_pipe = Pipeline([
    ("sc", StandardScaler()),
    ("lasso", LassoCV(alphas=None, cv=5, random_state=42, max_iter=5000))  # CV로 자동 그리드
])
```

---

## 10. 코드 모음

### 10.1 Numpy: OLS(의사역행렬, QR, SVD 비교)
```python
import numpy as np

rng = np.random.default_rng(0)
n, d = 200, 5
X = rng.normal(size=(n, d))
w_true = np.array([2., -1.5, 0., 0.5, 3.0])
b_true = 1.2
y = X @ w_true + b_true + rng.normal(0, 0.3, size=n)

# 설계행렬과 절편 합치기
Xtil = np.c_[np.ones(n), X]

# 1. 의사역행렬(SVD 내부)
w_pinv = np.linalg.pinv(Xtil) @ y

# 2. QR
Q, R = np.linalg.qr(Xtil, mode="reduced")
w_qr = np.linalg.solve(R, Q.T @ y)

# 3. 정규방정식(테스트용)
w_ne = np.linalg.solve(Xtil.T @ Xtil, Xtil.T @ y)

print("pinv:", w_pinv[:3], "...", "b:", w_pinv[0])
print("qr  :", w_qr[:3],  "...", "b:", w_qr[0])
print("ne  :", w_ne[:3],  "...", "b:", w_ne[0])
```

### 10.2 Numpy: GD로 OLS 학습(학습률/수렴 확인)
```python
import numpy as np

n, d = 1000, 10
X = rng.normal(size=(n, d))
w_true = rng.normal(size=d)
b_true = 0.5
y = X @ w_true + b_true + rng.normal(0, 0.5, size=n)

Xtil = np.c_[np.ones(n), X]
w = np.zeros(d+1)
lr = 1e-2
for t in range(1000):
    pred = Xtil @ w
    grad = (Xtil.T @ (pred - y)) / n     # ∇ = X^T(Xw - y)/n (절편 포함)
    w -= lr * grad
    if t % 100 == 0:
        mse = np.mean((pred - y)**2)
        print(f"iter {t:4d} mse {mse:.4f}")
print("GD w[:3]:", w[:3], "b:", w[0])
```

### 10.3 Hat 행렬/레버리지/LOOCV(빠른 PRESS)
```python
import numpy as np

n, d = 80, 3
X = rng.normal(size=(n, d))
b_true = -0.7
w_true = rng.normal(size=d)
y = X @ w_true + b_true + rng.normal(0, 1.0, size=n)

Xtil = np.c_[np.ones(n), X]
P = Xtil @ np.linalg.pinv(Xtil)     # = Xtil (Xtil^+)
yhat = P @ y
r = y - yhat
h = np.diag(P)
press = np.sum((r/(1 - h))**2)
print("PRESS:", press, "MSE:", np.mean(r**2))
print("max leverage:", h.max(), "mean leverage ~ p/n:", h.mean(), "p:", Xtil.shape[1])
```

### 10.4 Sklearn: 선형/릿지/라쏘/엘라스틱넷 + CV
```python
import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV
from sklearn.metrics import mean_squared_error, r2_score

X = rng.normal(size=(1000, 20))
true_w = np.r_[np.ones(5), np.zeros(15)]  # 희소
y = X @ true_w + rng.normal(0, 0.5, size=1000)

Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)

lin = Pipeline([("sc", StandardScaler()), ("lr", LinearRegression())])
ridge = Pipeline([("sc", StandardScaler()), ("rg", RidgeCV(alphas=np.logspace(-3,2,20), cv=5))])
lasso = Pipeline([("sc", StandardScaler()), ("ls", LassoCV(cv=5, random_state=42, max_iter=5000))])
enet = Pipeline([("sc", StandardScaler()), ("en", ElasticNetCV(l1_ratio=[.2,.5,.8,1.0], cv=5, random_state=42, max_iter=5000))])

for name, m in [("Linear", lin), ("Ridge", ridge), ("Lasso", lasso), ("ElasticNet", enet)]:
    m.fit(Xtr, ytr)
    p = m.predict(Xte)
    print(name, "MSE:", mean_squared_error(yte, p), "R2:", r2_score(yte, p))
```

### 10.5 WLS(단순 예)
```python
import numpy as np

n, d = 200, 3
X = rng.normal(size=(n, d))
w_true = np.array([1.5, -2.0, 0.5])
sigma = np.linspace(0.5, 3.0, n)    # 이분산
eps = rng.normal(0, sigma)
y = X @ w_true + eps

# 가중치: w_i = 1/Var(eps_i) ~ 1/sigma_i^2
W = np.diag(1.0/(sigma**2))
Xtil = np.c_[np.ones(n), X]
w_wls = np.linalg.solve(Xtil.T @ W @ Xtil, Xtil.T @ W @ y)
print("WLS w:", w_wls[1:], "b:", w_wls[0])
```

---

## 11. 체크리스트 & 요약

### 체크리스트
- [ ] **절편**: 스케일링 시 절편 제외 또는 별도 처리(표준화는 보통 특징만).  
- [ ] **스케일/조건수**: 표준화/화이트닝, OLS는 QR/SVD 사용.  
- [ ] **CV와 누수 방지**: 스케일/다항/인코딩은 **Fold 내부**에서 fit/transform.  
- [ ] **정규화 선택**: 다중공선성/과적합 → 릿지/라쏘/엘라스틱넷.  
- [ ] **진단**: 잔차 플롯, 레버리지 \(h_{ii}\), Cook’s D, VIF.  
- [ ] **비선형성**: 다항/상호작용/커널(커널 릿지) 고려.  
- [ ] **이분산/상관오차**: WLS/GLS.  
- [ ] **이상치**: 허버/RANSAC/데이터 점검.  
- [ ] **대규모**: GD/SGD/미니배치 + 적절한 스케줄러.

### 요약
- 선형 회귀는 **정사영**과 **가우시안 MLE**로 이해된다. OLS는 \( \hat y = P y \)에서 \(P\)가 대칭·멱등인 프로젝터임을 기억하라.  
- 수치적으론 **정규방정식보다 QR/SVD**가 안정적이다.  
- **릿지/라쏘/엘라스틱넷**은 과적합과 공선성 해결의 핵심 도구이며, 베이지안 관점에서 자연스러운 **사전(prior)** 로 해석된다.  
- 진단(잔차·레버리지·VIF)과 **데이터 생성 가정** 점검 없이는 **좋은 R²**도 허상일 수 있다.  
- 실전에서는 **파이프라인 + CV + 누수 방지**를 기본값으로, 필요 시 **WLS/GLS/강건/커널**로 확장하라.

---
**부록: 핵심 수식 모음**

- OLS:  
$$
\hat{\tilde w}=(\tilde X^\top \tilde X)^{-1}\tilde X^\top y,\quad
\hat y=\tilde X\hat{\tilde w}=P y,\ P=\tilde X(\tilde X^\top \tilde X)^{-1}\tilde X^\top.
$$

- 잔차 직교:  
$$
\tilde X^\top (y-\hat y)=0,\quad P^2=P,\ P^\top=P.
$$

- Ridge:  
$$
\hat{\tilde w}=(\tilde X^\top \tilde X+\lambda\tilde R)^{-1}\tilde X^\top y.
$$

- Lasso(직교 가정 근사):  
$$
\hat w_j=\mathrm{sign}(z_j)\max(|z_j|-\tfrac{\lambda}{2},0).
$$

- LOOCV:  
$$
\hat y_{(i)}=\hat y_i-\frac{r_i}{1-h_{ii}},\ 
\text{PRESS}=\sum_i\left(\frac{r_i}{1-h_{ii}}\right)^2.
$$