---
layout: post
title: 기계학습 - 선형 회귀
date: 2025-08-14 23:20:23 +0900
category: 기계학습
---
# 선형 회귀(Linear Regression)

**선형 회귀(Linear Regression)**는 머신 러닝에서 가장 기본적이면서도 널리 사용되는 **회귀 분석(regression analysis)** 기법입니다.  
입력 변수(Feature)와 출력 변수(Target) 사이의 관계를 **직선(또는 초평면)**으로 모델링하여, 주어진 입력에 대한 연속적인 출력 값을 예측합니다.

---

## 1. 선형 회귀의 개념

### (1) 정의
- 입력 \(X\)와 출력 \(y\) 사이의 선형 관계를 추정
- 모델 식:
$$
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
$$
또는 벡터/행렬 형태로:
$$
\hat{y} = \mathbf{X} \mathbf{w} + b
$$
- 여기서:
  - \(\mathbf{w}\): 가중치(Weight)
  - \(b\): 절편(Bias, Intercept)
  - \(\hat{y}\): 예측값

---

### (2) 가정(Assumptions)
선형 회귀는 다음과 같은 통계적 가정을 전제로 합니다.
1. **선형성(Linearity)**: 입력과 출력 사이의 관계가 선형
2. **독립성(Independence)**: 데이터 샘플이 서로 독립
3. **등분산성(Homoscedasticity)**: 오차의 분산이 일정
4. **정규성(Normality)**: 오차가 정규분포를 따름

---

## 2. 단순 선형 회귀(Simple Linear Regression)

- 독립변수 1개, 종속변수 1개
- 모델:
$$
\hat{y} = w x + b
$$
- 목표: 오차 제곱합(SSE, Sum of Squared Errors) 최소화
$$
SSE = \sum_{i=1}^n (y_i - (w x_i + b))^2
$$

---

## 3. 다중 선형 회귀(Multiple Linear Regression)

- 독립변수 여러 개
- 모델:
$$
\hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
$$
- 벡터 표현:
$$
\hat{y} = \mathbf{X} \mathbf{w} + b
$$

---

## 4. 손실 함수(Loss Function)

### 평균 제곱 오차(MSE)
선형 회귀의 학습은 **MSE 최소화**로 표현됩니다.
$$
MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

---

## 5. 학습 방법

### (1) 정규방정식(Normal Equation)
- 해석적(Analytical) 해:
$$
\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$
- 장점: 반복 학습 불필요, 해를 바로 구함
- 단점: 특성 수가 많으면 \((\mathbf{X}^T \mathbf{X})^{-1}\) 계산 비용이 큼

---

### (2) 경사하강법(Gradient Descent)
- MSE의 기울기 계산:
$$
\frac{\partial MSE}{\partial w_j} = -\frac{2}{n} \sum_{i=1}^n x_{ij} (y_i - \hat{y}_i)
$$
- 업데이트 식:
$$
w_j := w_j - \alpha \cdot \frac{\partial MSE}{\partial w_j}
$$
- 장점: 대규모 데이터에 효율적
- 단점: 학습률(α) 선택이 중요

---

## 6. 정규화(Regularization)를 포함한 선형 회귀

- **릿지 회귀(Ridge Regression)**:
$$
\min_w \left[ MSE + \lambda \sum_{j=1}^n w_j^2 \right]
$$
- **라쏘 회귀(Lasso Regression)**:
$$
\min_w \left[ MSE + \lambda \sum_{j=1}^n |w_j| \right]
$$
- **엘라스틱넷(Elastic Net)**:
  - L1, L2 패널티 혼합

정규화는 과적합을 방지하고, 라쏘는 변수 선택 기능까지 제공

---

## 7. 성능 평가 지표

| 지표 | 수식 | 특징 |
|------|------|------|
| MSE | \(\frac{1}{n} \sum (y - \hat{y})^2\) | 큰 오차에 민감 |
| RMSE | \(\sqrt{MSE}\) | 원 단위 |
| MAE | \(\frac{1}{n} \sum |y - \hat{y}|\) | 이상치에 덜 민감 |
| R² | \(1 - \frac{\sum (y - \hat{y})^2}{\sum (y - \bar{y})^2}\) | 1에 가까울수록 좋음 |

---

## 8. 파이썬 예제
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# 데이터 생성
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([2.2, 2.8, 4.5, 3.9, 5.1])

# 모델 학습
model = LinearRegression()
model.fit(X, y)

# 예측
y_pred = model.predict(X)

# 결과 출력
print("기울기 w:", model.coef_)
print("절편 b:", model.intercept_)
print("R² score:", model.score(X, y))

# 시각화
plt.scatter(X, y, color="blue", label="Actual")
plt.plot(X, y_pred, color="red", label="Predicted")
plt.legend()
plt.show()
```

---

## 9. 선형 회귀의 장단점

### 장점
- 구현 간단, 해석 용이
- 계산 효율적 (특성 수 적을 때)
- 통계적 해석 가능 (가중치 → 변수 중요도)

### 단점
- 선형 관계만 모델링 가능
- 이상치에 민감
- 다중공선성(Multicollinearity) 문제 발생 가능

---

## 📌 정리
- 선형 회귀는 입력과 출력 간 선형 관계를 모델링
- MSE 최소화로 파라미터 추정
- 정규방정식 또는 경사하강법으로 학습
- 릿지, 라쏘 등 정규화 기법으로 과적합 방지
- 간단하지만 데이터와 문제의 특성에 따라 한계 존재