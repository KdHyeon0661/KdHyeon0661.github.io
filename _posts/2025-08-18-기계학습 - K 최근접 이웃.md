---
layout: post
title: 기계학습 - K 최근접 이웃
date: 2025-08-18 20:30:23 +0900
category: 기계학습
---
# K-최근접 이웃(K-Nearest Neighbors, KNN)

## 개념과 절차

- **아이디어:** “**가까우면 비슷하다**”를 공식화. 학습(모델 적합)이 거의 없고(**lazy**), 예측 시 이웃 탐색을 수행.
- **분류:** K개 이웃의 **다수결**(혹은 거리 가중 다수결).
- **회귀:** K개 이웃의 **평균**(혹은 가중 평균).

### 절차

1. **K 결정**(교차검증 권장).
2. **거리 정의**(유클리드/맨해튼/민코프스키/코사인/해밍/사전계산 등).
3. **이웃 탐색**(브루트/트리/근사).
4. **의사결정**(다수결·평균·가중).
5. 필요 시 **타이브레이크**(동률 처리), **확률** 산출(비율).

---

## 수학적 표현

### (1) 분류

샘플 \(\mathbf{x}\)의 K-이웃 집합 \(N_k(\mathbf{x})\)에 대해
$$
\hat{y}(\mathbf{x})=\arg\max_{c\in\mathcal{C}}\ \sum_{i\in N_k(\mathbf{x})}\mathbf{1}(y_i=c)
$$

**가중(거리 역수) 버전**
$$
\hat{y}(\mathbf{x})=\arg\max_{c\in\mathcal{C}}\ \sum_{i\in N_k(\mathbf{x})}\frac{w(d(\mathbf{x},\mathbf{x}_i))}{\sum_{j\in N_k(\mathbf{x})}w(d(\mathbf{x},\mathbf{x}_j))}\ \mathbf{1}(y_i=c)
$$

- 보편적 선택: \( w(r)=1/(r+\varepsilon) \) 또는 \( w(r)=\exp(-\gamma r^2) \)

**확률 추정(빈도 기반)**
$$
\hat{P}(Y=c\mid \mathbf{x})=\frac{1}{K}\sum_{i\in N_k(\mathbf{x})}\mathbf{1}(y_i=c)
$$

### (2) 회귀

$$
\hat{y}(\mathbf{x})=\frac{1}{K}\sum_{i\in N_k(\mathbf{x})}y_i
$$

**거리 가중 회귀**
$$
\hat{y}(\mathbf{x})=\frac{\sum_{i\in N_k(\mathbf{x})} w(d(\mathbf{x},\mathbf{x}_i))\,y_i}{\sum_{i\in N_k(\mathbf{x})} w(d(\mathbf{x},\mathbf{x}_i))}
$$

---

## 거리 척도·스케일링·타이브레이크

### (1) 거리

- **유클리드**:
  $$
  d_2(\mathbf{x},\mathbf{z})=\sqrt{\sum_j (x_j-z_j)^2}
  $$
- **맨해튼**: \( d_1(\mathbf{x},\mathbf{z})=\sum_j |x_j-z_j| \)
- **민코프스키**: \( d_p=(\sum |x_j-z_j|^p)^{1/p} \)
- **코사인**(방향 유사): \( 1-\frac{\mathbf{x}\cdot\mathbf{z}}{\|\mathbf{x}\|\,\|\mathbf{z}\|} \)
- **해밍**(0/1·범주형), **고워(Gower)**(혼합형; 라이브러리 필요), **하버사인**(지리좌표; 라디안).

> **주의:** KNN은 **스케일에 매우 민감**. 반드시 **표준화/정규화**(또는 **whitening**)를 고려.

### (2) 스케일링

- **표준화**: \( z=(x-\mu)/\sigma \)
- **Robust**: 중앙값·IQR 사용(이상치에 강함).
- **파이프라인**으로 **훈련셋에만 fit**, 검증/테스트엔 transform만(누수 방지).

### (3) 타이브레이크

- 동률(분류) 시 **가중 KNN** 사용, 혹은 **우선순위 규칙**(사전확률/소수 클래스/라벨 순) 명시.
- 거리 가중 시 **0 거리**(완전 동일 샘플) 발생하면 **직접 그 라벨 반환** 또는 \(\varepsilon\) 안정화.

---

## KNN 이론 스냅샷

### (1) 일관성(Consistency, Stone 1977)

- 조건: \(n\to\infty\), \(K\to\infty\), **하지만** \(K/n\to 0\).
  그러면 KNN 분류기의 위험이 **베이즈 위험**에 수렴:
$$
\lim_{n\to\infty} R_{KNN}=R_{Bayes}
$$

**아이디어:** K-이웃 반경 \(r_k(\mathbf{x})\to 0\) 이고, 이웃 평균이 지역적 \(P(Y\mid X=\mathbf{x})\)를 근사.

### (2) Cover–Hart 경계(1967, 1-NN)

- **1-NN**의 오류율 \(R_{1NN}\)은 베이즈 오류 \(R^\*\)에 대해
$$
R_{1NN}\le 2R^\*(1-R^\*) \le 2R^\*
$$
(직관: 라벨 노이즈가 없을수록 1-NN도 준수.)

### (3) 결정경계/밀도 관점

- KNN 분류는 **지역적 다수 클래스**를 따른다 → **결정경계**는 데이터 밀도에 적응.
- KNN 회귀는 **국소 평균(커널 회귀)**과 유사한 스무딩.

---

## 복잡도와 이웃 탐색

| 방법 | 빌드 | 쿼리(평균) | 장점 | 단점 |
|---|---|---|---|---|
| Brute-force | – | \(O(nd)\) | 정확, 단순 | 느림(대규모) |
| KD-Tree | \(O(n\log n)\) | 저차원 \(O(\log n)\) | 10~20차원 내 강력 | 고차원 성능 붕괴 |
| Ball-Tree | \(O(n\log n)\) | 분포/메트릭에 견고 | 다양한 메트릭 | 고차원 한계 |
| 근사 NN(HNSW/FAISS/Annoy) | 인덱스 비용 | 매우 빠름(서브선형) | 대규모 실전 | 근사(정확도-속도 트레이드오프) |

- \(n\): 학습샘플 수, \(d\): 차원.
- 고차원에서는 **사실상 브루트 ≈ 트리**(분할 무력화).

---

## 차원의 저주 — 정량과 완화

### (1) 현상

- 차원 증가 시 “가까움/멀어짐”의 대비가 감소 → **최근접과 최원 거리 비율이 1에 수렴**.

#### 시뮬레이션(정규화된 범위 내)

```python
import numpy as np
rng = np.random.default_rng(0)
for d in [2,5,10,50,100,500]:
    X = rng.random((2000, d))
    q = rng.random((1, d))
    dists = np.linalg.norm(X - q, axis=1)
    ratio = dists.min() / dists.max()
    print(f"d={d:3d}  min/max≈ {ratio:.4f}")
```

### (2) 완화 전략

- **스케일링**(표준화/whitening).
- **차원축소**(PCA/UMAP).
- **특징선택**(상관/MI/모델 기반).
- **근사 NN**로 실용적 속도 확보.
- **K 증가**·가중 사용으로 노이즈 완화(과적합 ↘).

---

## 실전 이슈

- **클래스 불균형:** 단순 다수결은 취약 → `weights='distance'`, **샘플링**(언더/오버, SMOTE), **의사결정 임계 조정**, 비용 민감 평가.
- **결측치:** KNN 자체는 NaN 미지원 → **Imputer**(평균/중앙/MICE/`KNNImputer`).
- **범주형/혼합형:** 원-핫 후 유클리드(차원↑)보다 **해밍/Gower**·embedding 활용 고려.
- **중복/제로거리:** 동일 샘플 존재 시 **그 라벨 직접 사용**(분류) 또는 평균(회귀).
- **메모리:** 학습 샘플 전부 저장(비모수) → 데이터/인덱스 메모리 관리 필요.
- **설명가능성:** 예측 근거로 **이웃 목록**을 제시(실무 보고서에 유용).

---

## 평가·튜닝

- **하이퍼파라미터:** \(K\), 거리(및 `p`), `weights`(uniform/distance), 탐색기(‘auto’/‘kd_tree’/‘ball_tree’/‘brute’).
- **검증:** Stratified K-Fold(분류), GroupKFold(그룹 누수 방지), 시간의존 데이터는 시계열 CV.
- **지표:**
  - 분류: Accuracy/F1(불균형)/ROC-AUC/PR-AUC(희소양성).
  - 회귀: RMSE/MAE/R².
- **파이프라인:** `Scaler → KNN`(훈련셋만 fit).
- **모형선택:** `GridSearchCV`/`RandomizedSearchCV`·다중지표·최종선택 기준 명시.

---

## 확장 기능

- **KNN 회귀:** 국소 평균(또는 가중 평균). 이상치 많으면 **가중** 권장.
- **밀도 추정(kNN density):**
  $$
  \hat{f}(\mathbf{x})=\frac{k}{n\,V_d\,r_k(\mathbf{x})^d}
  $$
  (\(r_k\): k번째 이웃 거리, \(V_d\): 단위 \(d\)-볼륨)
- **이상치 탐지:** LOF(지역 밀도 대비), kNN distance 기반 스코어.
- **결측 대치:** `KNNImputer`(비슷한 샘플 평균/가중 평균).

---

## 파이썬 실전

### (A) 분류 파이프라인 + 그리드 서치

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, roc_auc_score

X, y = load_iris(return_X_y=True)
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier())
])

param = {
    "knn__n_neighbors": [3,5,7,9,15,21],
    "knn__weights": ["uniform", "distance"],
    "knn__p": [1,2],                 # 1:맨해튼, 2:유클리드
    "knn__algorithm": ["auto"],      # kd_tree/ball_tree/brute/auto
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
gs = GridSearchCV(pipe, param, scoring="f1_macro", cv=cv, n_jobs=-1)
gs.fit(Xtr, ytr)

print("Best params:", gs.best_params_)
print("CV score:", gs.best_score_)
yhat = gs.predict(Xte)
print(classification_report(yte, yhat, digits=3))
```

### (B) 회귀(가중 KNN) + 튜닝

```python
import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import KFold, GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error

X, y = make_regression(n_samples=3000, n_features=10, noise=15.0, random_state=0)
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)

pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsRegressor())
])

param = {
    "knn__n_neighbors": [3,5,7,11,21,31],
    "knn__weights": ["uniform", "distance"],
    "knn__p": [1,2]
}
cv = KFold(n_splits=5, shuffle=True, random_state=42)
gs = GridSearchCV(pipe, param, scoring="neg_root_mean_squared_error", cv=cv, n_jobs=-1)
gs.fit(Xtr, ytr)

best = gs.best_estimator_
pred = best.predict(Xte)
rmse = mean_squared_error(yte, pred, squared=False)
mae = mean_absolute_error(yte, pred)
print("Best params:", gs.best_params_)
print(f"RMSE={rmse:.3f}  MAE={mae:.3f}")
```

### (C) 이웃과 거리 확인(설명가능성)

```python
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler

sc = StandardScaler().fit(Xtr)
Xtr_s = sc.transform(Xtr)
Xte_s = sc.transform(Xte)

nn = NearestNeighbors(n_neighbors=5, algorithm="auto", metric="minkowski", p=2)
nn.fit(Xtr_s)

q = Xte_s[0:1]
d, idx = nn.kneighbors(q, return_distance=True)
print("Query true y:", yte[0])
print("Neighbor indices:", idx[0])
print("Neighbor distances:", d[0])
print("Neighbor labels:", ytr[idx[0]])
```

### (D) 근사 NN(HNSW 예: hnswlib) — 선택적

> 대규모 벡터(수십만~수천만)에서는 근사 NN가 필수적이다. (설치 환경에서 지원될 때 활용)
```python
# pip install hnswlib

import hnswlib, numpy as np
dim = X.shape[1]
p = hnswlib.Index(space='l2', dim=dim)
p.init_index(max_elements=len(Xtr), ef_construction=200, M=16)
p.add_items(Xtr_s, np.arange(len(Xtr)))
p.set_ef(100)  # 검색 정확도-속도 균형
labels, dists = p.knn_query(q, k=5)
```

---

## 체크리스트 & 요약

**체크리스트**
- [ ] **스케일링**: 훈련셋 기준 `StandardScaler/RobustScaler` 후 KNN.
- [ ] **거리**: 데이터 타입·목표에 맞는 메트릭(연속=유클리드, 희소/텍스트=코사인/해밍/사전계산).
- [ ] **K/가중**: CV로 최적화(동률/노이즈 완화엔 `weights='distance'`).
- [ ] **차원**: PCA/UMAP/특징선택 또는 **근사 NN** 고려.
- [ ] **불균형**: 샘플링/임계조정/가중 투표.
- [ ] **결측/범주형**: Imputer·적절한 거리·임베딩.
- [ ] **해석**: 이웃 인덱스·거리·라벨을 근거로 제공.
- [ ] **복잡도**: 대규모는 트리/근사 NN·배치 검색·캐싱.

**요약**
- KNN은 **간단하지만 강력한 지역적 추론기**로, **스케일·메트릭·K**가 성능을 좌우한다.
- 이론적으로 **일관성**을 가지며, 1-NN은 베이즈 오류의 상한을 가진다.
- 차원이 높아질수록 거리 대비가 약화되므로 **정규화·차원축소·근사 NN**가 실전 핵심이다.
- 파이프라인·교차검증·이웃 근거 제시를 습관화하면 **신뢰가능한 KNN**을 운영할 수 있다.

---

### 부록: 표 — 거리/적용 가이드

| 데이터/목표 | 권장 거리 | 코멘트 |
|---|---|---|
| 연속형(스케일 유사) | 유클리드(p=2) | 표준화 후 적용 |
| 연속형(희소·L1 친화) | 맨해튼(p=1) | 이상치 견고성 ↑ |
| 텍스트/임베딩 | 코사인 | 방향 유사성 초점 |
| 이진/원-핫 | 해밍 | 일치 비율 |
| 혼합형 | Gower | 사전계산/맞춤 |
| 지리좌표 | 하버사인 | 라디안 변환 |

### 부록: 표 — 복잡도 개요

| 단계 | 비용(대략) |
|---|---|
| Brute 이웃탐색 | \(O(nd)\) / 쿼리 |
| KD/Ball-Tree 빌드 | \(O(n\log n)\) |
| KD/Ball-Tree 쿼리 | 저차원 \(O(\log n)\), 고차원 퇴화 |
| 근사 NN | 인덱스 비용 + 빠른 쿼리(근사) |
