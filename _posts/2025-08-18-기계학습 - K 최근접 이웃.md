---
layout: post
title: 기계학습 - K 최근접 이웃
date: 2025-08-18 20:25:23 +0900
category: 기계학습
---
# K-최근접 이웃(K-Nearest Neighbors, KNN)

**K-최근접 이웃(KNN)** 알고리즘은 가장 직관적이고 간단한 지도학습(Supervised Learning) 방법 중 하나입니다.  
입력 데이터가 주어졌을 때, **가장 가까운 K개의 이웃 데이터**를 찾아 다수결(분류) 또는 평균(회귀)으로 예측을 수행합니다.

---

## 1. 개념

### (1) 정의
- 새로운 샘플이 들어오면, 학습 데이터에서 **가장 가까운 K개의 이웃**을 찾음
- **분류(Classification)**: K개의 이웃 중 가장 많이 등장하는 클래스로 예측
- **회귀(Regression)**: K개의 이웃 값의 평균으로 예측

---

### (2) 특징
- 비모수(Non-parametric) 학습: 데이터 분포에 대한 가정이 없음
- 훈련(Training) 과정이 없음 → **Lazy Learning**
- 예측 시점에서 계산량이 많음

---

## 2. 알고리즘 절차

1. **K 값 결정**  
   - K가 너무 작으면 과적합(Overfitting)
   - K가 너무 크면 과소적합(Underfitting)
2. **거리 계산** (유사도 측정)  
   - 일반적으로 **유클리드 거리(Euclidean Distance)** 사용:
$$
d(\mathbf{x}, \mathbf{x}_i) = \sqrt{\sum_{j=1}^n (x_j - x_{ij})^2}
$$
   - 다른 거리 척도:
     - 맨해튼 거리(Manhattan Distance)
     - 민코프스키 거리(Minkowski Distance)
     - 코사인 유사도(Cosine Similarity)
3. **가장 가까운 K개의 이웃 선택**
4. **다수결 또는 평균으로 결과 결정**

---

## 3. 수학적 표현

### (1) 분류(Classification)
주어진 샘플 \(\mathbf{x}\)에 대해:
$$
\hat{y} = \arg\max_{c \in C} \sum_{i \in N_k(\mathbf{x})} \mathbf{1}(y_i = c)
$$
- \(N_k(\mathbf{x})\): \(\mathbf{x}\)의 K개의 최근접 이웃 집합
- \(\mathbf{1}(\cdot)\): indicator 함수 (조건 만족 시 1, 아니면 0)

---

### (2) 회귀(Regression)
$$
\hat{y} = \frac{1}{K} \sum_{i \in N_k(\mathbf{x})} y_i
$$

---

### (3) 가중치 부여(Weighted KNN)
- 거리가 가까운 이웃일수록 더 큰 가중치 부여:
$$
\hat{y} = \frac{\sum_{i \in N_k(\mathbf{x})} \frac{1}{d(\mathbf{x}, \mathbf{x}_i)} y_i}{\sum_{i \in N_k(\mathbf{x})} \frac{1}{d(\mathbf{x}, \mathbf{x}_i)}}
$$

---

## 4. 수학적 증명: KNN의 일관성(Consistency)

**정리**:  
KNN은 다음 조건을 만족하면 확률적으로 최적(Bayes optimal)에 수렴한다.
- 데이터 개수 \(n \to \infty\)
- \(K \to \infty\) 이지만 \(K/n \to 0\)

**아이디어**:
1. **Bayes 분류기**:
$$
f^*(\mathbf{x}) = \arg\max_y P(Y = y \mid X = \mathbf{x})
$$
2. KNN은 국소적으로 \(P(Y = y \mid X = \mathbf{x})\)를 근사
3. \(n \to \infty\)이고 \(K\)가 충분히 크면, K개의 이웃은 \(\mathbf{x}\) 주변의 데이터 분포를 대표
4. 따라서 KNN은 Bayes 분류기와 동일한 성능에 수렴

**참고**: Stone's Theorem (1977)  
$$
\lim_{n \to \infty} R_{KNN} = R_{Bayes}
$$
- \(R_{KNN}\): KNN의 오분류율
- \(R_{Bayes}\): Bayes 최적 분류기의 오분류율

---

## 5. K 값 선택과 거리 척도

### (1) K 값 선택
- 홀수 사용(이진 분류에서 동률 방지)
- 교차 검증(Cross-Validation)으로 최적 K 결정

### (2) 거리 척도 선택
- 유클리드 거리: 연속형 데이터
- 맨해튼 거리: 고차원 희소 데이터
- 코사인 유사도: 방향성 비교 (텍스트, 문서)

---

## 6. 장단점

### 장점
- 구현 간단, 직관적
- 데이터 분포 가정 없음
- 분류/회귀 모두 사용 가능

### 단점
- 예측 시 계산 비용 큼 (모든 거리 계산 필요)
- 고차원 데이터에서 성능 저하 (차원의 저주)
- 불필요한 특징(Noise Feature)에 민감

---

## 7. 파이썬 예제
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 데이터 로드
iris = load_iris()
X, y = iris.data, iris.target

# 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# KNN 분류기
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# 예측
y_pred = knn.predict(X_test)

# 정확도
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

## 📌 정리
- KNN은 **가장 가까운 K개의 데이터**를 사용하여 예측
- 거리 기반 알고리즘으로, 비모수적이고 단순함
- K와 거리 척도 선택이 성능에 큰 영향
- 충분한 데이터와 적절한 K 설정 시 **Bayes 최적 분류기**에 수렴
- 고차원 데이터에서는 차원의 저주로 성능 저하 가능