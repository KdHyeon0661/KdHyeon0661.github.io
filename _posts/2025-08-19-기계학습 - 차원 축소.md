---
layout: post
title: 기계학습 - 차원 축소
date: 2025-08-19 16:25:23 +0900
category: 기계학습
---
# 차원 축소(Dimensionality Reduction)

## 왜 차원 축소인가 — 동기와 현상

### 목적 요약

- **시각화**: 2D/3D로 투영해 군집/경계/이상치 구조 관찰
- **잡음 제거·압축**: 신호만 남기고 노이즈 제거(denoising)
- **계산 효율**: 학습/추론 가속, 메모리 절감
- **특징 추출**: 잠재 구조/다양성 축(“주성분”) 발견

### 차원의 저주 & 거리 집중

- 차원이 커질수록 **데이터 희소화**, 최근접/최원점 거리 **수렴**(distance concentration) → 최근접 기반 알고리즘(KNN 등) 성능 저하.
- 해결: **저차원 다양체(manifold)** 가정 → “본질적 차원”에 맞춰 축소.

---

## PCA (Principal Component Analysis, 주성분 분석)

### 개념과 두 가지 관점

1) **분산 최대화**: 분산을 최대화하는 직교축(주성분) 찾기
2) **재구성 오차 최소화**: 선형 부분공간으로 투영 후 MSE 최소화

### 수학(중심화·공분산·고유분해)

데이터 \(X\in\mathbb{R}^{n\times d}\), 열평균 \(\mu\), 중심화 \(X_c=X-\mathbf{1}\mu^\top\).
공분산:
$$
\Sigma=\frac{1}{n}X_c^\top X_c.
$$
고유분해:
$$
\Sigma v_i=\lambda_i v_i,\quad \lambda_1\ge\cdots\ge \lambda_d.
$$
상위 \(k\)개 주성분 \(V_k=[v_1,\dots,v_k]\)에 투영:
$$
Z = X_c V_k\in\mathbb{R}^{n\times k}.
$$

### SVD와의 동치

$$
X_c = U\Sigma_s V^\top,\quad \Sigma = V\Sigma_s^2 V^\top.
$$
따라서 **오른쪽 특이벡터 \(V\)** 가 주성분.
**누적 설명 분산 비율**:
$$
\mathrm{Explained}(k)=\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^d \lambda_i}.
$$

### 최적성 스케치(재구성 오류)

랭크-\(k\) 투영 \(X_c\approx X_c V_k V_k^\top\)의 SSE는
$$
\|X_c - X_c V_k V_k^\top\|_F^2 = \sum_{i=k+1}^d \sigma_i^2
$$
로 최소(에크하르트–영 정리).

### 변형: Whitening

- **PCA Whitening**: \(Z_w = Z \Lambda_k^{-1/2}\) → 축별 분산 1
- **ZCA Whitening**: 원공간 정렬 보존 \(X_{\text{zca}}=X_c V \Lambda^{-1/2} V^\top\)

### 실전 팁

- **표준화** 필수(특징 규모 상이할 때): `StandardScaler` + `PCA`
- **희소/대규모**: `TruncatedSVD`(LSA), `IncrementalPCA`(배치) 또는 **Randomized SVD**
- **해석**: 주성분 로딩(특징 기여도)로 의미 파악(단, 회전 자유도 주의)

---

## t-SNE (t-Distributed Stochastic Neighbor Embedding)

### 핵심 아이디어

- 고차원 근접성 → 확률 \(p_{ij}\), 저차원 근접성 → 확률 \(q_{ij}\).
- **KL divergence** \( \sum p_{ij}\log\frac{p_{ij}}{q_{ij}} \) 최소화.
- 저차원에서 **스튜던트 t(자유도 1)** 의 heavy-tailed 커널로 **crowding 문제 해결**.

고차원 조건부 확률(시그마는 퍼플렉시티에서 유도):
$$
p_{j|i} = \frac{\exp(-\|x_i-x_j\|^2/2\sigma_i^2)}{\sum_{k\ne i}\exp(-\|x_i-x_k\|^2/2\sigma_i^2)},\quad
p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n}.
$$
저차원:
$$
q_{ij}=\frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k\ne l}(1+\|y_k-y_l\|^2)^{-1}}.
$$
목적:
$$
C=\sum_{i\ne j} p_{ij}\log\frac{p_{ij}}{q_{ij}}.
$$

### 하이퍼파라미터 가이드

- **perplexity**(~kNN 크기): 5–50 추천, 데이터 크기 ↑ → perplexity ↑
- **learning_rate**: 너무 작으면 수렴 지연, 너무 크면 뭉침/발산
- **early_exaggeration**: 초기에 \(p_{ij}\) 확대 → 큰 구조 분리
- **init='pca'** 권장, **random_state** 고정
- **주의**: 축 방향/스케일 의미 없음, 서로 다른 런 비교시 정렬 문제

### 한계·운영

- **Out-of-sample**: 일반 t-SNE는 새로운 점 변환 곤란(별도 최적화 필요).
  - 대안: **parametric t-SNE**, **UMAP**(transform 지원)
- **구조 왜곡**: 전역 거리 보존 아님 → **근접성** 해석 중심

---

## 커널 PCA (Kernel PCA)

### 동기

비선형 다양체를 선형 PCA로는 평탄화 어려움 → 고차원 \(\phi(x)\) 공간에서 PCA 수행.
내적은 커널 \(K(x,x')=\langle\phi(x),\phi(x')\rangle\)로 대체.

### 절차(요지)

- 그램 행렬 \(K_{ij}=K(x_i,x_j)\) **중심화**:
  $$
  \tilde{K}=K - \mathbf{1}K/n - K\mathbf{1}/n + \mathbf{1}K\mathbf{1}/n^2.
  $$
- 고유분해 \(\tilde{K} \alpha = \lambda \alpha\).
- 샘플 \(x\)의 \(k\)번째 좌표:
  $$
  z_k(x) = \sum_{i=1}^n \alpha_{i}^{(k)} K(x_i, x)/\sqrt{\lambda_k}.
  $$
- 커널: RBF, Poly 등(PSD 커널 권장, Mercer 조건)

### 이슈

- **Pre-image 문제**: 임베딩을 원공간으로 되돌리기 어려움(근사 필요).
- **스케일**: \(O(n^2)\) 메모리/시간 → 대규모에 부적합(랜덤 특성/Nyström)

---

## 거리/이웃 보존형 기법

### Isomap (지오데식 거리 보존)

- 그래프 kNN 구성 → 엣지 가중치=유클리드 거리
- **지오데식 거리**(최단경로) 계산 후 **클래식 MDS**로 내림차원
- **비선형 “곡면”** 전역 보존에 유리, 노이즈/샘플링 밀도 민감

### LLE (Locally Linear Embedding)

- 각 점을 이웃들의 선형결합으로 재구성하는 가중치 \(W\) 추정
- 저차원에서 **같은 가중치**로 재구성 오차 최소 → 고유문제 풀기
- 매니폴드 국소 선형성 가정, **구멍/노이즈**에 민감

### UMAP (Uniform Manifold Approximation and Projection)

- 이론: 측도 공간/심플리셜 복합체 근사 → **퍼지 단체 그래프** 구축
- 목적: 고차원/저차원 퍼지 집합의 **교차 엔트로피** 최소화
- 장점: **빠름**, 전역/국소 균형 좋음, **`transform` 지원**
- 주요 파라미터: `n_neighbors`(국소성), `min_dist`(클러스터 응집도), `metric`

> 실무 권장: **시각화는 UMAP 또는 t-SNE**, 전처리는 **PCA→UMAP/t-SNE** 파이프라인.

---

## 랜덤 투영 & JL 보장

### Johnson–Lindenstrauss 정리(요지)

임의의 \(n\)개 점 집합은
$$
k = O\!\left(\frac{\log n}{\varepsilon^2}\right)
$$
차원으로 선형 사영해도 모든 쌍 거리 \((1\pm\varepsilon)\) 근사 보존 가능.

### 구현

- **Gaussian / Sparse random projection**: 빠르고 메모리 효율적, 약한 해석력
- 대규모 전처리·최근접 탐색 가속에 적합

---

## 오토인코더(딥러닝 기반)

- 인코더 \(f_\theta: \mathbb{R}^d\to \mathbb{R}^k\), 디코더 \(g_\phi\)로 **재구성 손실** 최소화:
$$
\min_{\theta,\phi}\sum_i \|x_i - g_\phi(f_\theta(x_i))\|^2.
$$
- **Denoising AE**: 노이즈 입력→클린 출력
- **Variational AE**: 확률 잠재공간(정규화/생성 가능)
- 장점: **비선형 복잡 패턴**, 단점: **학습 비용/설계** 필요, 해석 난이도

---

## “특징 선택” vs “특징 추출”

- **선택(Selection)**: 원특징 중 일부만(필터/래퍼/임베디드—L1, 트리 중요도)
- **추출(Extraction)**: 새 축 생성(PCA, KPCA, UMAP, AE 등)
→ 둘을 병행: **선택으로 차수↓ → 추출로 구조화**

---

## 평가 지표(재구성·이웃 보존·신뢰도)

### 재구성 오류(선형 기법)

$$
\mathrm{MSE}=\frac{1}{n}\sum_i \|x_i - \hat{x}_i\|^2.
$$

### 이웃 보존

- **Trustworthiness** \(T\in[0,1]\): 고차원에서 가깝지 않던 점이 저차원에서 가까워지는 “위반”을 벌점
- **Continuity**: 반대 방향 위반 측정
- **MRRE / Rank-based** 지표도 활용

### 다운스트림 성능

- 축소 후 **클래스 분리도**, **클러스터링 지표**(실루엣), **분류/회귀 성능** 변화

---

## 파이프라인·운영 체크리스트

- [ ] **훈련 데이터로만** `fit`(데이터 누수 방지), 검증/테스트는 `transform`
- [ ] **표준화** → PCA/t-SNE/UMAP
- [ ] 대용량: **Randomized/Incremental**, 미니배치
- [ ] 시각화는 **라벨 색상**·마커·투명도 조절, **축 해석 금지**(특히 t-SNE/UMAP)
- [ ] **랜덤시드 고정**, 결과 재현
- [ ] **Out-of-sample** 필요? → PCA/KPCA/UMAP/AE 선호, t-SNE는 주의
- [ ] 민감 파라미터(t-SNE perplexity, UMAP n_neighbors/min_dist) 스윕

---

## Python 실전 레시피

### Scree plot & 누적 설명분산 (PCA)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.datasets import load_wine

X, y = load_wine(return_X_y=True)
Xz = StandardScaler().fit_transform(X)

pca = PCA(n_components=min(10, X.shape[1]), svd_solver="full", random_state=42)
Z = pca.fit_transform(Xz)

evr = pca.explained_variance_ratio_
plt.figure(figsize=(8,4))
plt.subplot(1,2,1)
plt.plot(np.arange(1, len(evr)+1), evr, marker='o')
plt.title("Scree: Explained Variance Ratio")
plt.xlabel("#PC"); plt.ylabel("EVR")

plt.subplot(1,2,2)
plt.plot(np.arange(1, len(evr)+1), np.cumsum(evr), marker='o')
plt.title("Cumulative EVR")
plt.ylim(0,1.05); plt.axhline(0.9, ls='--')
plt.xlabel("#PC"); plt.ylabel("Cum. EVR")
plt.tight_layout(); plt.show()
```

### PCA 2D 시각화 + 신뢰도(Trustworthiness)

```python
from sklearn.manifold import trustworthiness

Z2 = PCA(n_components=2, random_state=42).fit_transform(Xz)
tw = trustworthiness(Xz, Z2, n_neighbors=10)
print("Trustworthiness (k=10):", round(tw, 4))

plt.figure(figsize=(5,4))
plt.scatter(Z2[:,0], Z2[:,1], c=y, cmap='tab10', s=25)
plt.title("PCA (2D)")
plt.show()
```

### t-SNE (PCA→t-SNE 파이프라인 권장)

```python
from sklearn.manifold import TSNE

Xp = PCA(n_components=min(50, Xz.shape[1]), random_state=42).fit_transform(Xz)
tsne = TSNE(n_components=2, perplexity=30, learning_rate='auto',
            init='pca', n_iter=1000, random_state=42)
Y = tsne.fit_transform(Xp)

plt.figure(figsize=(5,4))
plt.scatter(Y[:,0], Y[:,1], c=y, cmap='tab10', s=25)
plt.title("t-SNE (perplexity=30)")
plt.show()
```

### UMAP (가능 시)

```python
# 선택적으로 설치되어 있을 때만 실행

try:
    import umap
    um = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
    Yu = um.fit_transform(Xz)
    plt.figure(figsize=(5,4))
    plt.scatter(Yu[:,0], Yu[:,1], c=y, cmap='tab10', s=25)
    plt.title("UMAP (n_neighbors=15, min_dist=0.1)")
    plt.show()
except Exception as e:
    print("UMAP 사용 불가:", e)
```

### 커널 PCA (RBF) + 새 샘플 변환

```python
from sklearn.decomposition import KernelPCA
from sklearn.model_selection import train_test_split

Xtr, Xte, ytr, yte = train_test_split(Xz, y, test_size=0.3, stratify=y, random_state=42)

kpca = KernelPCA(n_components=10, kernel='rbf', gamma=0.3, fit_inverse_transform=False,
                 eigen_solver='auto', random_state=42)
Ztr = kpca.fit_transform(Xtr)
Zte = kpca.transform(Xte)  # out-of-sample 지원

plt.figure(figsize=(5,4))
plt.scatter(Ztr[:,0], Ztr[:,1], c=ytr, cmap='tab10', s=15, label='train')
plt.scatter(Zte[:,0], Zte[:,1], c=yte, cmap='tab10', s=40, marker='x', label='test')
plt.title("Kernel PCA (RBF) 2D projection")
plt.legend(); plt.show()
```

### Isomap / LLE 빠른 체험

```python
from sklearn.manifold import Isomap, LocallyLinearEmbedding

iso = Isomap(n_neighbors=10, n_components=2)
Zi = iso.fit_transform(Xz)

lle = LocallyLinearEmbedding(n_neighbors=12, n_components=2, method='standard', random_state=42)
Zl = lle.fit_transform(Xz)

fig, ax = plt.subplots(1,2, figsize=(10,4))
ax[0].scatter(Zi[:,0], Zi[:,1], c=y, cmap='tab10', s=20); ax[0].set_title("Isomap")
ax[1].scatter(Zl[:,0], Zl[:,1], c=y, cmap='tab10', s=20); ax[1].set_title("LLE")
plt.show()
```

### 랜덤 투영(JL) — 대규모 가속

```python
from sklearn.random_projection import SparseRandomProjection
from sklearn.metrics import pairwise_distances

rp = SparseRandomProjection(n_components=50, random_state=42)
Xrp = rp.fit_transform(Xz)

# 거리 왜곡 대략 점검(샘플 일부)

idx = np.random.RandomState(0).choice(len(Xz), size=500, replace=False)
D0 = pairwise_distances(Xz[idx], metric='euclidean')
D1 = pairwise_distances(Xrp[idx], metric='euclidean')
rel_err = np.median(np.abs(D1 - D0) / (D0 + 1e-9))
print("Median relative distance error:", round(float(rel_err), 3))
```

### Denoising PCA

```python
rng = np.random.RandomState(0)
X_noisy = Xz + 0.2 * rng.randn(*Xz.shape)

pca_d = PCA(n_components=10, random_state=0)
Z_dn = pca_d.fit_transform(X_noisy)
X_rec = pca_d.inverse_transform(Z_dn)

mse = np.mean((Xz - X_rec)**2)
print("Denoising MSE:", round(float(mse), 4))
```

---

## 복잡도·스케일·메모리

| 방법 | 시간/메모리(대략) | 강점 | 주의 |
|---|---|---|---|
| PCA/SVD | \(O(nd\min\{n,d\})\), randomized: 선형 | 빠름·해석 | 스케일/중심화 필수 |
| TruncatedSVD | 희소/대규모 | IRL1/Randomized SVD | 해석(로딩) 주의 |
| t-SNE | \(O(n^2)\) (BH/FFT로 가속) | 시각화 우수 | 전역거리·OOS 어려움 |
| UMAP | \(O(n\log n)\) 근사 | 속도·transform | 파라민감 |
| KPCA | \(O(n^2)\) | 비선형 | 메모리 큼 |
| Isomap/LLE | 그래프 + 고유 | 다양체 | 노이즈 민감 |
| RP(JL) | 선형·희소 | 대규모 | 해석 약함 |

---

## 흔한 함정과 대응

- **데이터 누수**: 전체 데이터로 차원축소 `fit` 금지 → **Train**만
- **t-SNE 남용**: 클러스터 간 거리/면적 해석 금지, **다중 런 일관성** 체크
- **고카디널 범주**: PCA 전 원-핫 폭발 → **해시/임베딩/빈도 기준 축소**
- **희소 행렬**: `TruncatedSVD` 우선, 밀집 변환은 메모리 폭발
- **대규모**: **Randomized/Incremental** + 샘플링 기반 시각화

---

## 요약

- **PCA**: 선형 구조·속도·해석(주성분) 탁월 → 전처리/잡음제거/시각화 1차 선택
- **t-SNE/UMAP**: **국소 구조 시각화** 최강(UMAP은 transform도 지원)
- **커널/거리 기반**: 복잡 다양체에 효과적이나 스케일/메모리 주의
- **랜덤 투영/JL**: 대규모 근사·가속용
- **오토인코더**: 비선형 강력하지만 학습비용/해석 난이도
- 평가 시 **Trustworthiness/Continuity/다운스트림 성능**을 함께 본다.

---

## 부록 A) 수식 모음(발췌)

- PCA 분산 최대화:
$$
\max_{\|w\|=1} \mathrm{Var}(X_c w)=w^\top \Sigma w \Rightarrow w=v_1.
$$
- t-SNE 목적:
$$
C = \sum_{i\ne j} p_{ij}\log\frac{p_{ij}}{q_{ij}},\;\; q_{ij}=\frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k\ne l}(1+\|y_k-y_l\|^2)^{-1}}.
$$
- JL 차원:
$$
k \ge \frac{8\ln n}{\varepsilon^2}.
$$

---

## 부록 B) 실무 선택 가이드(퀵)

1) **전처리/모델 가속**: `StandardScaler → PCA/TruncatedSVD → (모델)`
2) **시각화(탐색)**: `PCA(50) → t-SNE/UMAP(2)`
3) **비선형 구조**: `UMAP` 또는 `KernelPCA(RBF)`
4) **대규모/희소**: `TruncatedSVD / Random Projection`
5) **OOS 필요**: `PCA/KPCA/UMAP/AE`(t-SNE는 복잡)
