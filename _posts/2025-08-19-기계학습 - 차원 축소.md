---
layout: post
title: 기계학습 - 차원 축소
date: 2025-08-19 16:25:23 +0900
category: 기계학습
---
# 차원 축소(Dimensionality Reduction)

## 1. 개요
**차원 축소(Dimensionality Reduction)**는 고차원 데이터(예: 1000차원 텍스트 임베딩, 수십만 픽셀의 이미지)를 보다 작은 차원으로 줄여 표현하는 방법입니다.  
주요 목적은 다음과 같습니다:

- **시각화**: 2D, 3D 공간으로 변환하여 데이터 구조를 직관적으로 관찰.
- **잡음 제거(Denoising)**: 데이터의 본질적인 패턴만 남기고 불필요한 잡음을 제거.
- **계산 효율성 향상**: 머신러닝 모델의 학습 속도 및 성능 개선.
- **특징 추출**: 데이터 내 잠재적 구조/패턴을 찾는 과정.

대표적인 방법은 **PCA(주성분 분석)**와 **t-SNE(확률적 이웃 임베딩)**입니다.

---

## 2. PCA (Principal Component Analysis, 주성분 분석)

### 2.1 개념
PCA는 데이터의 분산이 가장 큰 방향(Principal Component)을 찾아 차원을 축소합니다.  
즉, 고차원 데이터에서 **정보 손실을 최소화**하면서 더 낮은 차원으로 투영합니다.

### 2.2 수학적 절차
1. 데이터 \(X \in \mathbb{R}^{n \times d}\) (n개 샘플, d차원).
2. 평균을 빼서 데이터 중앙화:
   $$
   X_c = X - \mu
   $$
   (\(\mu\)는 각 차원의 평균).
3. 공분산 행렬 계산:
   $$
   \Sigma = \frac{1}{n} X_c^T X_c
   $$
4. 고유값 분해(Eigendecomposition) 수행:
   $$
   \Sigma v_i = \lambda_i v_i
   $$
   - \(\lambda_i\): 분산 크기(고유값).
   - \(v_i\): 주성분 방향(고유벡터).
5. 가장 큰 고유값을 가진 k개의 고유벡터를 선택.
6. 데이터 투영:
   $$
   Z = X_c V_k
   $$
   → \(Z\): 축소된 k차원 데이터.

### 2.3 해석
- **첫 번째 주성분**: 데이터 분산이 가장 큰 방향.
- **두 번째 주성분**: 첫 번째 주성분에 직교하면서 분산이 큰 방향.
- 고차원 데이터가 실제로는 저차원 구조(예: 선형 평면)에 놓여 있음을 찾을 수 있음.

### 2.4 장단점
✅ 장점
- 계산이 간단하고 빠름.
- 노이즈 제거, 압축 효과.
- 선형 구조를 잘 설명.

❌ 단점
- 선형 변환만 가능 → 비선형 패턴에는 한계.
- 분산이 크다고 반드시 중요한 특징은 아닐 수 있음.

---

## 3. t-SNE (t-Distributed Stochastic Neighbor Embedding)

### 3.1 개념
t-SNE는 데이터의 **지역적 구조(local structure)**를 보존하면서 저차원에 임베딩하는 **비선형 차원 축소 기법**입니다.  
주로 **시각화(2D, 3D)**에 사용됩니다.

### 3.2 아이디어
- 고차원 공간에서 점들 간의 거리를 **확률적 유사도**로 변환.
  - 가까운 점일수록 높은 확률.
  - 멀리 떨어진 점일수록 낮은 확률.
- 저차원에서도 같은 확률 분포가 유지되도록 좌표를 배치.
- 즉, **고차원 → 저차원 확률 분포의 KL Divergence 최소화**.

### 3.3 수학적 절차
1. 고차원에서 데이터 \(x_i, x_j\) 간 유사도 정의:
   $$
   p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i}\exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}
   $$
   (가우시안 기반 조건부 확률).

   대칭화:
   $$
   p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}
   $$

2. 저차원에서 유사도 정의(점 \(y_i, y_j\)):
   $$
   q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l}(1 + \|y_k - y_l\|^2)^{-1}}
   $$
   (t-분포 기반 → heavy-tailed 분포 사용으로 crowding 문제 해결).

3. 목적 함수: KL Divergence 최소화
   $$
   C = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
   $$

4. 경사하강법(Gradient Descent)으로 \(C\) 최소화 → 최적의 \(y_i\) 찾기.

---

## 4. PCA vs t-SNE 비교

| 항목 | PCA | t-SNE |
|------|-----|-------|
| 변환 방식 | 선형(직교변환) | 비선형(확률적 매핑) |
| 목적 | 최대 분산 유지 | 지역 구조 보존 |
| 차원 수 | 원하는 k차원 | 주로 2D/3D 시각화 |
| 계산 속도 | 빠름 | 상대적으로 느림 |
| 해석력 | 비교적 명확 (주성분=특징 방향) | 해석 어려움 (주로 시각화용) |

---

## 5. 파이썬 예제

```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

# 데이터 불러오기
digits = load_digits()
X, y = digits.data, digits.target

# PCA (2D)
X_pca = PCA(n_components=2).fit_transform(X)

plt.figure(figsize=(8,4))
plt.subplot(1,2,1)
plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='tab10')
plt.title("PCA (2D)")

# t-SNE (2D)
X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)

plt.subplot(1,2,2)
plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y, cmap='tab10')
plt.title("t-SNE (2D)")

plt.show()
```

---

## 📌 요약
- **PCA**: 선형 차원 축소, 최대 분산을 보존. 빠르고 해석 가능.
- **t-SNE**: 비선형 차원 축소, 지역 구조 보존. 주로 시각화용.
- 두 방법은 보완적: **PCA는 데이터 전처리 및 특징 추출**, **t-SNE는 고차원 데이터 시각화**에 주로 사용.
