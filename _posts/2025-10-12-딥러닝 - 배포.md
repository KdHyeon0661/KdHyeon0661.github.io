---
layout: post
title: 딥러닝 - 배포
date: 2025-10-12 15:25:23 +0900
category: 딥러닝
---
# 배포(ONNX/TensorRT) 체크리스트

> 목표
> - **PyTorch 모델을 ONNX로 내보내고 TensorRT 엔진으로 배포**하는 과정을 **체크리스트** 중심으로 정리합니다.
> - **동적/정적 입력**, **FP32/FP16/INT8(PTQ/QAT)**, **플러그인/미지원 연산** 처리, **성능/정확도 검증**을 예제 코드와 함께 제공합니다.
> - **런타임 통합(순수 PyTorch + TensorRT 파이썬 API)**, **동적 마이크로배칭**/스트림, **가드레일·알람·롤백** 운영 패턴을 포함합니다.
> - 프레임워크는 **PyTorch를 기준**으로 하며, **ONNX/TensorRT**는 배포 목적상 필요한 범위에서만 사용합니다. (ONNX Runtime 등 기타 추론 프레임워크 코드는 사용하지 않습니다.)

---

## 0. 큰 그림(결정표)

| 결정 포인트 | 선택지 | 가이드 |
|---|---|---|
| 입력 형태 | **정적** vs **동적** | 동적이면 **ONNX dynamic axes + TRT 최적화 프로파일** 필요 |
| 정밀도 | FP32 / **FP16** / **INT8** | GPU(Ampere+)면 FP16 기본. INT8은 **캘리브레이션/검증** 필수 |
| 양자화 | **PTQ** / **QAT** | 정확도 민감 시 **QAT 우선**(PyTorch QAT → ONNX/INT8) |
| 연산 지원 | 네이티브 / **플러그인** | NMS/grid_sample/mish 등은 **TRT plugin** 또는 대체 구현 |
| 엔진 제작 | **trtexec(CLI)** / torch-tensorrt / Python API | 배포 자동화는 `trtexec`가 단순·안정 |
| 검증 | **수치 오차(참조 PyTorch vs TRT)**, 성능 | FP16: `max_abs≤1e-3` 정도, INT8: 작업별 허용오차 정의 |

---

## 1. 사전 체크리스트(환경·모델·코드)

### 1.1 환경 호환성
- [ ] **CUDA 드라이버/GPU 아키텍처**(sm\_xx) 확인(Ampere/Hopper 등)
- [ ] **TensorRT 버전 ↔ CUDA ↔ GPU** 매트릭스 호환
- [ ] **ONNX opset**: 권장 **opset\_17+** (가능하면 최신 TRT가 지원하는 범위)

### 1.2 모델/코드 준비(PyTorch)
- [ ] `model.eval()` / 드롭아웃·배치정규화 **추론 모드 고정**
- [ ] **결정적 경로**: 데이터 의존적 제어 흐름/랜덤 제거
- [ ] **지원 연산으로 리팩토링**: `F.interpolate`는 모드/align\_corners 명시, `grid_sample`/NMS 등은 **대체 경로** 마련
- [ ] 메모리 포맷 고정: CNN은 `channels_last`(선택), 텐서는 `contiguous()`
- [ ] 입력 스펙 문서화: 배치/채널/해상도/토큰길이, **min/opt/max**(동적일 때)

---

## 2. PyTorch → ONNX 내보내기(동적/정적 축)

### 2.1 예시 모델(간단 CNN)
```python
# model_def.py (예시)
import torch, torch.nn as nn, torch.nn.functional as F

class SmallCNN(nn.Module):
    def __init__(self, in_ch=3, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool  = nn.AdaptiveAvgPool2d(1)
        self.fc    = nn.Linear(64, num_classes)
    def forward(self, x):
        x = F.silu(self.conv1(x))     # SiLU: TRT 지원(opset/버전에 따라 swish로 변환)
        x = F.relu(self.conv2(x))
        x = self.pool(x).flatten(1)
        return self.fc(x)
```

### 2.2 ONNX 내보내기 스크립트
```python
# export_onnx.py
import torch, os
from model_def import SmallCNN

def export(ckpt="ckpt/model_best.pt", onnx_path="export/cnn.onnx",
           dynamic=True, opset=17, half=False):
    os.makedirs(os.path.dirname(onnx_path) or ".", exist_ok=True)
    model = SmallCNN()
    if os.path.exists(ckpt):
        state = torch.load(ckpt, map_location="cpu")
        model.load_state_dict(state["model"] if isinstance(state, dict) and "model" in state else state)
    model.eval()

    # 더미 입력(동적이면 범위 설명용)
    B, C, H, W = 1, 3, 224, 224
    x = torch.randn(B, C, H, W, dtype=torch.float16 if half else torch.float32)
    if half: model.half()

    dynamic_axes = {"input":{0:"batch", 2:"height", 3:"width"},
                    "logits":{0:"batch"}} if dynamic else None

    torch.onnx.export(
        model, (x,),
        onnx_path,
        input_names=["input"], output_names=["logits"],
        opset_version=opset,
        do_constant_folding=True,
        dynamic_axes=dynamic_axes,
        training=torch.onnx.TrainingMode.EVAL
    )
    print("saved:", onnx_path)

if __name__=="__main__":
    export(dynamic=True, half=False, opset=17)
```

> 메모
> - **동적 축**을 지정하지 않으면 TRT에서 **동적 입력 지원 불가**.
> - **Half(AMP)** 내보내기는 일부 연산 호환성 이슈가 있을 수 있습니다. 일반적으로 **FP32로 내보내고 엔진에서 FP16 최적화**가 안전합니다.

---

## 3. 수치 검증(참조 PyTorch vs TRT 엔진)

> **원칙**: 배포 전 **같은 입력 배치**에 대해 **PyTorch 출력**과 **TRT 출력**을 비교해 오차 허용 범위를 문서화합니다. (FP16은 `max_abs≈1e-3` 수준, INT8은 모델·데이터 의존)

### 3.1 TensorRT 엔진 로드·실행(Python, PyTorch 텐서 포인터 사용)
```python
# trt_runner.py
import tensorrt as trt
import torch, ctypes, os

class TrtRunner:
    def __init__(self, engine_path: str, device="cuda"):
        logger = trt.Logger(trt.Logger.WARNING)
        with open(engine_path, "rb") as f, trt.Runtime(logger) as rt:
            self.engine = rt.deserialize_cuda_engine(f.read())
        self.context = self.engine.create_execution_context()
        self.device = device

    def infer(self, x: torch.Tensor):
        """x: [B,C,H,W] float32/float16 (GPU/CPU 상관없음)"""
        x = x.contiguous()
        if x.device.type != "cuda":
            x = x.to("cuda")
        # 동적 shape 바인딩
        bi = self.engine.get_binding_index("input")
        bo = self.engine.get_binding_index("logits")
        self.context.set_binding_shape(bi, tuple(x.shape))
        # 출력 shape 질의
        out_shape = tuple(self.context.get_binding_shape(bo))
        dtype = torch.float16 if self.engine.get_binding_dtype(bo) == trt.DataType.HALF else torch.float32

        y = torch.empty(out_shape, device="cuda", dtype=dtype)
        bindings = [None]*self.engine.num_bindings
        bindings[bi] = ctypes.c_void_p(int(x.data_ptr()))
        bindings[bo] = ctypes.c_void_p(int(y.data_ptr()))

        # PyTorch CUDA 스트림 활용
        stream = torch.cuda.current_stream().cuda_stream
        self.context.execute_async_v3(stream_handle=stream, bindings=bindings)
        # 동기화 (필요 시)
        torch.cuda.current_stream().synchronize()
        return y

def max_abs_diff(a: torch.Tensor, b: torch.Tensor) -> float:
    return float((a - b).abs().max().item())

if __name__=="__main__":
    # 예시: PyTorch 출력과 비교
    from model_def import SmallCNN
    model = SmallCNN().eval().cuda()
    x = torch.randn(8,3,224,224, device="cuda")
    y_ref = model(x).float().detach()

    runner = TrtRunner("export/cnn_fp16.engine")
    y_trt = runner.infer(x)

    print("max_abs:", max_abs_diff(y_ref, y_trt.float()))
```

> 포인트
> - **pycuda 없이** `torch.Tensor.data_ptr()`를 바인딩에 사용합니다.
> - 동적 입력은 `set_binding_shape`와 **최적화 프로파일** 범위 안에서만 실행됩니다(아래 4장).

---

## 4. TensorRT 엔진 생성(trtexec 중심)

### 4.1 동적 입력 & 프로파일
- TensorRT는 동적 입력을 위해 **최적화 프로파일(Optimization Profile)**에 **min/opt/max** shape가 필요합니다.
- 실서비스에서 쓰일 입력 범위를 실측하여 **너무 넓지도, 너무 좁지도 않게** 설정합니다.

### 4.2 FP32/FP16/INT8별 `trtexec` 예시

```bash
# FP16 엔진(동적 HxW, 배치 1~16)
trtexec --onnx=export/cnn.onnx --saveEngine=export/cnn_fp16.engine \
  --explicitBatch --fp16 \
  --minShapes=input:1x3x160x160 \
  --optShapes=input:8x3x224x224 \
  --maxShapes=input:16x3x320x320 \
  --workspace=4096 --buildOnly

# FP32 엔진(기본)
trtexec --onnx=export/cnn.onnx --saveEngine=export/cnn_fp32.engine \
  --explicitBatch --workspace=4096 --buildOnly

# INT8(PTQ) 엔진: 캘리브레이터 제공(Entropy, 평활화 등)
trtexec --onnx=export/cnn.onnx --saveEngine=export/cnn_int8.engine \
  --explicitBatch --int8 --calib=<cache_or_calib_file> \
  --minShapes=input:1x3x160x160 --optShapes=input:8x3x224x224 --maxShapes=input:16x3x320x320 \
  --workspace=4096 --buildOnly
```

> 체크
> - `--explicitBatch`는 동적 배치 필수.
> - FP16은 대부분의 현대 GPU에서 안전. **INT8은 캘리브레이션/검증**을 반드시 수행하세요.
> - `--workspace`(MiB)는 빌드(전략 탐색) 여유 메모리. 실행 시 메모리와 다릅니다.

### 4.3 INT8 캘리브레이션(PTQ) 개념
- 입력 텐서 \(x\)의 레인지 \([a,b]\)를 정하고 **스케일 \(s\)** 로 정량화:
  \[
  q=\mathrm{clip}\left(\left\lfloor \frac{x}{s}\right\rceil, q_{\min}, q_{\max}\right),\quad s \approx \frac{b-a}{2^{k}-1}
  \]
  (k=8)
- **대표 배치**(수백~수천 샘플)로 레인지 추정(최대/평균/Entropy).
- 정확도 민감 시 **QAT**로 이동(학습 시 `fake-quant`) 후 내보내기.

---

## 5. 정확도·성능 검증 체크리스트

### 5.1 정확도(Functional parity)
- [ ] 고정 시드/데이터로 **PyTorch vs TRT** 출력 비교(샘플 N=1000 이상)
- [ ] 메트릭: `max_abs`, `mean_abs`, 분류는 `top-1/5`, 회귀는 `RMSE/MAE`
- [ ] FP16 오차 한계 문서화(레이어별 민감도 점검: softmax/logit 영역 주의)
- [ ] INT8: 캘리브레이터 세트 다양성 확보, **계층별 clipping 로그** 검토

### 5.2 성능(Throughput/Latency/GPU Mem)
- [ ] 마이크로배칭 폭/윈도 튜닝(실시간)
- [ ] 최적화 프로파일 분할(해상도가 크게 다른 그룹)
- [ ] **엔진 warmup**(3~10회) 후 측정, **p50/p95/p99** 기록
- [ ] GPU 메모리 피크/프래그먼테이션 관찰(멀티 엔진 공존 시)

---

## 6. 런타임 통합(동적 배칭·스트림·폴백)

### 6.1 간단 러너(동적 배칭 포함)
```python
# trt_runtime_batcher.py
import threading, queue, time
import torch
from trt_runner import TrtRunner

class DynamicBatcher:
    def __init__(self, runner: TrtRunner, max_batch=32, window_ms=5):
        self.r = runner
        self.maxb = max_batch
        self.win = window_ms/1000.0
        self.q = queue.Queue(maxsize=2048)
        threading.Thread(target=self._worker, daemon=True).start()

    def submit(self, x: torch.Tensor):
        ev = threading.Event(); slot = {}
        self.q.put((x, ev, slot))
        ok = ev.wait(timeout=1.0)
        if not ok: raise TimeoutError("TRT timeout")
        return slot["y"]

    def _worker(self):
        while True:
            items=[self.q.get()]; t0=time.time()
            while len(items)<self.maxb and (time.time()-t0) < self.win:
                try: items.append(self.q.get_nowait())
                except queue.Empty: time.sleep(0.0005)
            xs = torch.cat([it[0] for it in items], dim=0).to("cuda", non_blocking=True)
            ys = self.r.infer(xs)
            # fan-out
            off=0
            for x, ev, slot in items:
                n = x.shape[0]
                slot["y"] = ys[off:off+n].contiguous()
                off += n; ev.set()
```

### 6.2 폴백(엔진 미존재/에러 시 PyTorch)
```python
# fallback.py
import torch
from model_def import SmallCNN
from trt_runner import TrtRunner

class Predictor:
    def __init__(self, engine_path=None):
        self.use_trt=False
        if engine_path and os.path.exists(engine_path):
            try:
                self.trt = TrtRunner(engine_path)
                self.use_trt=True
            except Exception:
                self.use_trt=False
        if not self.use_trt:
            self.model = SmallCNN().eval().cuda()
    @torch.no_grad()
    def __call__(self, x):
        if self.use_trt: return self.trt.infer(x)
        return self.model(x.cuda())
```

---

## 7. 미지원/주의 연산 대체 가이드

| 연산/패턴 | 상태/이슈 | 대체 전략 |
|---|---|---|
| `grid_sample` | TRT 일부 버전 미지원/제약 | 사전 샘플링/왜곡 예열 또는 커스텀 플러그인 |
| NMS | ONNXOpset에 다양한 변형 | TRT **BatchedNMS** 플러그인 또는 후처리 CPU |
| `F.interpolate` | align\_corners/모드에 민감 | `mode, align_corners` 명시, 가능하면 `nearest/bilinear` |
| `SiLU/Mish` | 구버전 변환 이슈 | 명시적 활성화 모듈 사용(새 TRT는 OK) |
| LayerNorm | 오퍼레이터 매핑 이슈 | opset/torch 버전 상향, 필요 시 플러그인 |
| 동적 슬라이스/분기 | Shape 연산 복잡 | 정형화(패딩/마스크), TorchScript로 경로 고정 |

> **원칙**: **학습·추론 코드 공유**. 배포 전 **“ONNX-friendly” 모델**로 리팩토링하여 재학습/재저장하는 것이 장기적으로 안정합니다.

---

## 8. INT8: PTQ vs QAT 실무 메모

- **PTQ**: 학습된 FP32 → INT8. 빠르지만 **정확도 리스크**. 대표 샘플 다양성/분포 매칭이 관건.
- **QAT**: 학습 중 **fake-quant** 삽입. 배포 시 INT8로 자연스럽게 전환. 정확도 유지가 우수.
- **검증**: 클래스별 성능/오류 케이스, **캘리브레이션 로그**(range, outlier) 확인.
- **가드레일**: INT8 전환은 반드시 **카나리 1% → 5% → 25%** 단계 롤아웃 + 자동 롤백.

---

## 9. 아티팩트/메타데이터·호환성

- **엔진은 보통 이식성 제한**: TRT 버전/앱티마이저/디바이스에 종속. **타겟 환경에서 빌드** 권장.
- 엔진 옆에 `metadata.json` 저장:
  ```json
  {
    "model_id":"cnn@abc123", "onnx_opset":17, "precision":"fp16",
    "profile":{"min":[1,3,160,160],"opt":[8,3,224,224],"max":[16,3,320,320]},
    "trt_version":"8.x.y", "cuda":"12.x", "git":"abcd1234", "created":"2025-09-19T12:00:00Z"
  }
  ```
- **검증 로그**(오차/레이턴시 분포)와 함께 **영구 보관**. 롤백 시 참조.

---

## 10. CI/CD 파이프라인(요약 시나리오)

1) **학습 산출**: `model_best.pt` + `metrics.json`
2) **내보내기**: `export_onnx.py`로 `cnn.onnx` 생성
3) **엔진 빌드**: `trtexec`(FP16/INT8) — 프로파일/워크스페이스 명시
4) **수치 검증**: 스크립트로 PyTorch vs TRT 오차 산출(샘플 ≥1000)
5) **성능 검증**: 워밍업 후 p50/p95/p99 측정(입력 스펙별)
6) **패키징**: `*.engine + metadata.json` 아티팩트 업로드
7) **릴리즈**: 섀도우 → 카나리 단계 + **알람 규칙** 적용
8) **운영 모니터링**: SLO/오차율/드리프트, 임계 초과 시 **자동 롤백**

---

## 11. 문제해결(트러블슈팅)

| 증상 | 원인 | 해결 |
|---|---|---|
| ONNX 내보내기 실패 | 미지원 연산/동적 제어 | 모듈화/대체 연산, opset 상향, 스크립트화 |
| TRT 빌드 실패 | 쉐이프/프로파일 불일치 | ONNX 동적축 vs `--min/opt/max` 일치 확인 |
| FP16 NaN | 감쇠 손실/정규화 | 민감 레이어 FP32 유지(혼합정밀), 스케일 안정화 |
| INT8 정확도 급락 | 캘리브레이션 부족 | 대표세트 확대, 아웃라이어 처리, QAT 전환 |
| 레이턴시 변동↑ | 배치/프로파일 미스매치 | 마이크로배칭 윈도 조정, 프로파일 분리 |
| 메모리 OOM | 동시 엔진 과다 | 엔진 공유/비동기 스트림 제한, 배치 상한 |

---

## 12. “지금 바로” 최소 실행 순서

1) `model_def.py` 완성, `model_best.pt` 준비
2) `export_onnx.py` 실행(opset=17, 동적축 지정)
3) `trtexec --fp16`으로 엔진 생성(프로파일 min/opt/max 합리화)
4) `trt_runner.py`로 **PyTorch vs TRT** 오차 확인
5) 서비스에 `trt_runtime_batcher.py`로 배치 통합 → 카나리 롤아웃

---

## 13. 부록 — 참고 명령 모음

```bash
# 엔진 검증(내장 벤치)
trtexec --loadEngine=export/cnn_fp16.engine --shapes=input:8x3x224x224 --warmUp=100 --duration=30 --streams=1

# 여러 입력 스펙 벤치(프로파일 범위 내)
trtexec --loadEngine=export/cnn_fp16.engine --shapes=input:1x3x160x160
trtexec --loadEngine=export/cnn_fp16.engine --shapes=input:16x3x320x320

# 메모리 프로파일(로그 레벨↑)
trtexec --onnx=export/cnn.onnx --fp16 --verbose --profilingVerbosity=detailed
```

---

## 14. 요약

- **PyTorch 기준 코드**를 **ONNX-friendly**로 리팩토링한 뒤, **ONNX → TensorRT 엔진**으로 전개하세요.
- **동적축·프로파일**, **FP16/INT8 전략**, **미지원 연산 대체**가 성공의 핵심입니다.
- 배포는 **정확도·성능 검증 + 메타데이터 봉인 + 카나리/롤백**이 표준 절차입니다.
