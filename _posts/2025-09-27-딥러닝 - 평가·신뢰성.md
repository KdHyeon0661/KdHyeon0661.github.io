---
layout: post
title: 딥러닝 - 평가·신뢰성
date: 2025-09-27 23:25:23 +0900
category: 딥러닝
---
# 1.12 평가·신뢰성(Evaluation & Reliability)
**메트릭(정확도·F1·mAP·IoU·BLEU 등) · 캘리브레이션 · OOD/분포 드리프트 · 에러 분석**

## A. 평가의 큰 그림: 프로토콜과 함정

### A-1. 데이터 분할 & 프로토콜
- **Train / Val / Test**: 학습·튜닝·최종평가 분리. 데이터 누수 방지.
- **K-겹 교차검증**: 표본 수가 적거나 변동성 추정이 필요할 때.
- **시간의존 데이터**: 시계열/로그 → **시간 순서 유지**(rolling window, walk-forward).
- **클래스 불균형**: 정확도(Accuracy)만으로 과대평가 금지 → **F1/PR-AUC** 중심.

### A-2. 보고의 원칙
- **포인트 추정치 + 불확실성**: 평균 ± 신뢰구간(부트스트랩/교차검증 표준오차).
- **여러 메트릭 병행**: 예) 이진분류: ROC-AUC, PR-AUC, F1@t*, Calibration(ECE) 동시 보고.
- **공정성/서브그룹**: 성별/연령/도메인 별 성능 편차 분석.

---

## B. 분류(Classification) 메트릭 정리

### B-1. 혼동행렬 & 기본 지표
이진분류의 혼동행렬:
- TP, FP, FN, TN

기본 지표:
$$
\text{Accuracy}=\frac{TP+TN}{TP+FP+FN+TN}
$$
$$
\text{Precision}=\frac{TP}{TP+FP},\qquad
\text{Recall}=\frac{TP}{TP+FN}
$$
$$
\text{F1}=\frac{2\cdot \text{Precision}\cdot \text{Recall}}{\text{Precision}+\text{Recall}}
$$

멀티클래스 평균:
- **Macro**: 클래스별 지표 평균(클래스 균등 가중).
- **Weighted**: 클래스 지원도(표본 수)로 가중.
- **Micro**: 전체 TP/FP/FN 합산 후 계산(불균형에 덜 민감).

Top-K 정확도:
$$
\text{Top-}k = \frac{1}{N}\sum_{i=1}^N \mathbb{1}\{y_i \in \text{top-}k(\hat{\mathbf{p}}_i)\}.
$$

### B-2. ROC-AUC & PR-AUC
- **ROC 곡선**: TPR vs FPR. **AUC-ROC**: 임의의 양성/음성 쌍 순위 정확도.
- **PR 곡선**: Precision vs Recall. **AUC-PR**: **희귀 양성** 문제에서 더 유용.

### B-3. 임계값 튜닝(Thresholding)
- 점수 $$s\in[0,1]$$ 에서 **F1** 최대 임계값:
$$
t^\*=\arg\max_t \ \text{F1}(t)
$$
- 비용 민감 최적 임계값:
$$
t^\*=\arg\min_t \ \mathbb{E}[\text{Cost}(t)].
$$

#### 코드: 기본 분류 메트릭
```python
import numpy as np
from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, roc_auc_score, average_precision_score

y_true = np.array([0,1,1,0,1,0,0,1])
y_score = np.array([0.1,0.8,0.6,0.3,0.9,0.2,0.4,0.7])  # 예측 확률
y_pred = (y_score >= 0.5).astype(int)

cm = confusion_matrix(y_true, y_pred)
acc = accuracy_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

rocauc = roc_auc_score(y_true, y_score)
prauc = average_precision_score(y_true, y_score)

print("CM=\n", cm)
print(f"ACC={acc:.3f} F1={f1:.3f} ROC-AUC={rocauc:.3f} PR-AUC={prauc:.3f}")
```

---

## C. 검출(Detection) & 세그멘테이션(Segmentation)

### C-1. IoU(Intersection over Union)
$$
\mathrm{IoU}=\frac{|A\cap B|}{|A\cup B|}.
$$
- **세그멘테이션**: 픽셀 단위 집합.
- **박스 검출**: 바운딩 박스 간 면적 비.

**Dice (F1 of sets)**:
$$
\mathrm{Dice}=\frac{2|A\cap B|}{|A|+|B|}.
$$

#### 세그멘테이션: mIoU/mDice 코드
```python
import numpy as np

def iou_per_class(y_true, y_pred, num_classes, ignore_index=None):
    ious=[]
    for c in range(num_classes):
        if c==ignore_index: continue
        t = (y_true==c); p = (y_pred==c)
        inter = np.logical_and(t,p).sum()
        union = np.logical_or(t,p).sum()
        if union==0: continue
        ious.append(inter/union)
    return ious, np.mean(ious) if ious else float('nan')

def dice_per_class(y_true, y_pred, num_classes, ignore_index=None):
    dices=[]
    for c in range(num_classes):
        if c==ignore_index: continue
        t = (y_true==c); p = (y_pred==c)
        inter = np.logical_and(t,p).sum()
        s = t.sum()+p.sum()
        if s==0: continue
        dices.append(2*inter/s)
    return dices, np.mean(dices) if dices else float('nan')
```

### C-2. 검출: AP/mAP(평균정밀도)
- **정확 매칭**: IoU ≥ 임계값(예: 0.5)
- **AP**: PR곡선 아래 면적
  - PASCAL VOC: **11-point** 근사
  - COCO: **AP@[.50:.95]** (0.5~0.95 step 0.05 평균)
- **mAP**: 클래스별 AP 평균.

#### 간단 AP 계산(11-point 근사)
```python
def average_precision_11pt(prec, rec):
    # prec, rec: recall 증가 순 정렬
    ap = 0.0
    for t in np.linspace(0,1,11):
        p = (prec[rec>=t].max() if np.any(rec>=t) else 0)
        ap += p
    return ap/11.0
```

#### 검출 매칭 스케치
```python
# gts: list of [image_id, class, xmin,ymin,xmax,ymax]
# dets: list of [image_id, class, score, xmin,ymin,xmax,ymax]
# 1) dets를 score 내림차순 정렬  2) 각 det를 해당 이미지의 GT와 IoU 최대 매칭
# 3) 매칭이 IoU>=thr이고 그 GT가 아직 미매칭이면 TP, 아니면 FP
# 4) 누적 TP/FP로 prec/rec 곡선 → AP
```

---

## D. NLP 메트릭: BLEU(및 간단 대안)

### D-1. BLEU
- **n-그램 정밀도** 기반(보통 n=1..4), **클리핑**으로 중복 보정
- **Brevity Penalty (BP)**: 짧게 생성하면 가산점 제한
$$
\text{BLEU} = \text{BP} \cdot \exp\Big(\sum_{n=1}^N w_n \log p_n\Big),\quad \sum w_n=1
$$
$$
\text{BP}=\begin{cases}
1 & c>r\\
\exp(1-r/c) & c\le r
\end{cases}
$$
- \(c\): 후보 길이, \(r\): 레퍼런스 길이(가까운 참조 길이 선택), \(p_n\): n-그램 정밀도.

#### 간단 BLEU 구현(단일 참조)
```python
import math, collections

def bleu_score(candidate, reference, max_n=4, weights=None):
    if weights is None:
        weights = [1/max_n]*max_n
    def ngrams(tokens, n):
        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
    c = candidate.split(); r = reference.split()
    c_len, r_len = len(c), len(r)
    p_ns = []
    for n, w in enumerate(weights, start=1):
        cand_ngrams = collections.Counter(ngrams(c,n))
        ref_ngrams  = collections.Counter(ngrams(r,n))
        overlap = {k: min(v, ref_ngrams.get(k,0)) for k,v in cand_ngrams.items()}
        num = sum(overlap.values()); den = max(sum(cand_ngrams.values()), 1)
        p_ns.append(num/den if den>0 else 0.0)
    # BP
    bp = 1.0 if c_len>r_len else math.exp(1 - r_len/max(c_len,1))
    # geometric mean with logs
    s = sum(w*math.log(p+1e-12) for w,p in zip(weights, p_ns))
    return bp*math.exp(s)
```

> **팁**: 재현성 있는 비교를 위해 **sacreBLEU** 사용 권장(토크나이저·settings 고정).  
> 요약/QA 등에선 **ROUGE-L**, **BERTScore**(임베딩 기반)도 보조적으로 사용.

---

## E. 캘리브레이션(Calibration)

### E-1. 개념
- **정확도와 별개**로 “**확률의 믿을 만함**”:  
  $$\mathbb{P}(Y=\hat{y} \mid \hat{p}=0.8)\approx 0.8 ?$$
- **Reliability Diagram**: 예측 확률 구간(bin)별 **예측 평균** vs **실제 빈도**.

### E-2. ECE/MCE/Brier
- **ECE(Expected Calibration Error)**:  
  $$\mathrm{ECE}=\sum_{b=1}^B \frac{|S_b|}{n}\left|\operatorname{acc}(S_b)-\operatorname{conf}(S_b)\right|$$
- **MCE**: 위 차이의 최대값.
- **Brier score**(이진):
$$
\mathrm{Brier}=\frac{1}{n}\sum_{i=1}^n (\hat{p}_i - y_i)^2
$$

#### 코드: ECE 계산
```python
import numpy as np

def ece_score(y_true, y_prob, n_bins=15):
    # y_prob: 이진 양성 확률(또는 멀티클래스의 max prob)
    bins = np.linspace(0.0, 1.0, n_bins+1)
    ece = 0.0
    n = len(y_true)
    for i in range(n_bins):
        l, r = bins[i], bins[i+1]
        mask = (y_prob>=l) & (y_prob<r) if i<n_bins-1 else (y_prob>=l) & (y_prob<=r)
        if mask.sum()==0: continue
        acc = (y_true[mask] == (y_prob[mask]>=0.5)).mean()
        conf = y_prob[mask].mean()
        ece += (mask.sum()/n) * abs(acc - conf)
    return ece
```

### E-3. 확률 교정: Temperature Scaling(로짓 rescale)
로짓 $$\mathbf{z}$$ 에 대해 **스칼라 \(T>0\)** 로 나눔:
$$
\hat{\mathbf{p}}=\mathrm{softmax}\Big(\frac{\mathbf{z}}{T}\Big)
$$
- **Val 세트**에서 \(T\)를 최적화(CE 최소). 구조 유지, **정확도 불변**, 확률만 교정.

#### 코드: Temperature Scaling 학습
```python
import torch, torch.nn as nn
def optimize_temperature(logits_val, y_val):
    T = torch.nn.Parameter(torch.ones(1, device=logits_val.device))
    opt = torch.optim.LBFGS([T], lr=0.1, max_iter=50)
    ce = nn.CrossEntropyLoss()
    def closure():
        opt.zero_grad()
        loss = ce(logits_val / T.clamp_min(1e-3), y_val)
        loss.backward()
        return loss
    opt.step(closure)
    return T.detach()

# 사용 예
# logits_val: (N,C), y_val: (N,)
# T = optimize_temperature(logits_val, y_val)
# probs_test = softmax(logits_test / T)
```

> 기타: **Platt scaling**(이진 로지스틱), **Isotonic regression**(비모수) 등.

---

## F. OOD(Out-of-Distribution) / 분포 드리프트

### F-1. 개념
- **Covariate drift**: 입력 분포 \(p(\mathbf{x})\) 변동(라벨 조건부 불변 가정).  
- **Label shift**: \(p(y)\) 변동.  
- **Concept drift**: \(p(y|\mathbf{x})\) 자체 변동(난이도↑).

### F-2. OOD 탐지 지표
- **MSP(Max Softmax Probability)**: \(\max_c \hat{p}_c\) 낮으면 OOD 신호.
- **Entropy**: \(-\sum \hat{p}_c\log \hat{p}_c\) 높으면 OOD 가능.
- **AUROC(FPR95)**: ID vs OOD 점수로 분리 성능 측정.
- **피처 거리**: penultimate feature를 가우시안 가정 → **마할라노비스** 거리.
- **에너지 기반**: \(E(\mathbf{x})=-T\log\sum_c e^{z_c/T}\).

#### 코드: MSP/Entropy 기반 OOD 점수
```python
import numpy as np

def msp_scores(logits):   # 높을수록 ID(=확신)
    exps = np.exp(logits - logits.max(axis=1, keepdims=True))
    probs = exps / exps.sum(axis=1, keepdims=True)
    return probs.max(axis=1)

def entropy_scores(logits):  # 높을수록 OOD
    exps = np.exp(logits - logits.max(axis=1, keepdims=True))
    probs = exps / exps.sum(axis=1, keepdims=True)
    ent = -(probs * np.log(probs+1e-12)).sum(axis=1)
    return ent
```

### F-3. 분포 드리프트 감지(탭/로그)
- **PSI(Population Stability Index)**: 구간화 후 분포 차이
$$
\mathrm{PSI}=\sum_i (\hat{p}_i - p_i)\ln\frac{\hat{p}_i}{p_i}
$$
- **KS검정/χ²검정**, **KL/JS Divergence**, 피처별 모니터링.
- **라벨 쉬프트 추정**: 혼동행렬/우도 기반 reweighting.

#### 코드: 간단 PSI
```python
def psi_score(ref, cur, bins=10):
    q = np.linspace(0,1,bins+1)
    cuts = np.quantile(ref, q)
    ref_hist, _ = np.histogram(ref, bins=cuts)
    cur_hist, _ = np.histogram(cur, bins=cuts)
    ref_p = ref_hist / max(ref_hist.sum(), 1)
    cur_p = cur_hist / max(cur_hist.sum(), 1)
    cur_p = np.where(cur_p==0, 1e-6, cur_p)
    ref_p = np.where(ref_p==0, 1e-6, ref_p)
    return ((cur_p - ref_p) * np.log(cur_p/ref_p)).sum()
```

---

## G. 에러 분석(Error Analysis)

### G-1. 어디서 틀리는가?
- **혼동행렬 히트맵**: 클래스 간 오인 패턴 파악.
- **서브그룹 분석**: 메타데이터(조명/날씨/디바이스/길이/언어)별 성능을 표/그래프로 비교.
- **Hard example mining**: 확신 높았던 오답, 불확실 오답 리스트업.

### G-2. 슬라이스 분석 코드(예: 길이/밝기/언어코드)
```python
import pandas as pd
def slice_metrics(df, slice_col, y_col='y_true', p_col='y_prob', t=0.5):
    rows=[]
    for k, sub in df.groupby(slice_col):
        y = sub[y_col].values
        p = sub[p_col].values
        y_pred = (p>=t).astype(int)
        acc = (y==y_pred).mean()
        prec = ( (y_pred & y).sum() / max(y_pred.sum(),1) )
        rec = ( (y_pred & y).sum() / max(y.sum(),1) )
        f1 = 2*prec*rec/(prec+rec+1e-12)
        rows.append((k, len(sub), acc, f1))
    return pd.DataFrame(rows, columns=[slice_col,'n','acc','f1'])
```

### G-3. 임계값 재튜닝(서브그룹별)
- 전역 vs 서브그룹별 임계값 비교(운영 정책·공정성 고려 필요).

---

## H. 불확실성(불확도) 추정

### H-1. 종류
- **Aleatoric**(데이터 고유 노이즈): 관측 잡음.  
- **Epistemic**(모델 불확실성): 데이터 부족/파라미터 불확실.

### H-2. 방법
- **MC Dropout**: 추론 시 드롭아웃 켜고 여러 번 샘플 → 평균/분산.
- **Deep Ensembles**: 초기화/데이터 순서 달리한 여러 모델 평균.
- **로그-분산 출력**(회귀):  
  $$\mathcal{L}=\frac{1}{2\sigma^2}\|y-\hat{y}\|^2 + \frac{1}{2}\log\sigma^2$$

#### 코드: MC Dropout
```python
def mc_dropout_predict(model, x, T=30):
    model.train()  # 드롭아웃 ON
    probs=[]
    with torch.no_grad():
        for _ in range(T):
            logits = model(x)
            p = torch.softmax(logits, dim=-1)
            probs.append(p)
    P = torch.stack(probs, dim=0)  # (T,B,C)
    mean = P.mean(0)
    var  = P.var(0)  # 에피스테믹 추정치 근사
    return mean, var
```

---

## I. 임계값·코스트·경계조건

### I-1. 비용행렬 기반 F1/정확도 대체
- 비용: \(C_{FP}, C_{FN}\) → 기대비용 최소화 임계값:
$$
t^\*=\arg\min_t \ C_{FP}\cdot FP(t) + C_{FN}\cdot FN(t)
$$

#### 코드: 비용기반 최적 t
```python
def best_threshold_cost(y_true, y_prob, c_fp=1.0, c_fn=1.0):
    ts = np.linspace(0,1,101)
    best=(None, float('inf'))
    y = y_true.astype(int)
    for t in ts:
        yp = (y_prob>=t).astype(int)
        fp = ((yp==1)&(y==0)).sum()
        fn = ((yp==0)&(y==1)).sum()
        cost = c_fp*fp + c_fn*fn
        if cost<best[1]: best=(t, cost)
    return best
```

---

## J. 신뢰구간·유의성

### J-1. 부트스트랩 CI
- 샘플링으로 성능 지표의 분포 근사 → **95% CI** 보고.

#### 코드: 부트스트랩 CI (예: F1)
```python
def bootstrap_ci(y_true, y_pred, stat_fn, B=1000, alpha=0.05, seed=0):
    rng = np.random.default_rng(seed)
    n = len(y_true)
    stats=[]
    for _ in range(B):
        idx = rng.integers(0, n, n)
        stats.append(stat_fn(y_true[idx], y_pred[idx]))
    lo, hi = np.quantile(stats, [alpha/2, 1-alpha/2])
    return (float(lo), float(hi))
```

### J-2. NLP 비교 유의성
- **Approximate Randomization Test**, **Paired bootstrap** 등(문장 단위 스코어 분포 고려).

---

## K. 통합 평가 파이프라인(실전 템플릿)

아래는 **분류 모델**을 예로, 핵심 평가·캘리브레이션·에러분석을 한 번에 수행하는 스케치입니다.

```python
import numpy as np, pandas as pd
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, average_precision_score, confusion_matrix

def evaluate_classification(y_true, logits, groups=None, do_temp_scale=True):
    """
    y_true: (N,)
    logits: (N,C) or (N,) for binary logit; 여기선 (N,2) 가정
    groups: (N,) 서브그룹 라벨(예: 'male','female' ...)
    """
    # 확률
    exps = np.exp(logits - logits.max(axis=1, keepdims=True))
    probs = exps / exps.sum(axis=1, keepdims=True)
    y_prob = probs[:,1]
    y_pred = (y_prob>=0.5).astype(int)

    out={}
    out['acc']  = accuracy_score(y_true, y_pred)
    out['f1']   = f1_score(y_true, y_pred)
    out['roc']  = roc_auc_score(y_true, y_prob)
    out['pr']   = average_precision_score(y_true, y_prob)
    out['cm']   = confusion_matrix(y_true, y_pred)

    # ECE
    out['ece']  = ece_score(y_true, y_prob, n_bins=15)

    # 서브그룹
    if groups is not None:
        df = pd.DataFrame({'g':groups, 'y':y_true, 'p':y_prob})
        out['by_group'] = slice_metrics(df, 'g', 'y','p')

    # Temperature scaling (val 로짓 대신 here 가정)
    if do_temp_scale:
        import torch, torch.nn.functional as F
        with torch.no_grad():
            logits_t = torch.from_numpy(logits).float().cuda()
            y_t      = torch.from_numpy(y_true).long().cuda()
            T = optimize_temperature(logits_t, y_t)  # 위 섹션 함수
            probs_ts = F.softmax(logits_t/T, dim=-1).cpu().numpy()[:,1]
        out['ece_ts'] = ece_score(y_true, probs_ts, n_bins=15)

    return out
```

---

## L. 운영·모니터링: 배포 후 신뢰성

- **온라인 모니터링**: 입력분포(PSI/KL), 예측확률(평균/분산), 캘리브레이션(ECE proxy) 추적.
- **슬라이스 알람**: 특정 서브그룹에서 **FDR/TPR** 급변 시 경고.
- **데이터·모델 카드**: 메트릭·데이터 출처·한계·공정성 분석·의사결정 경로 투명화.
- **캐니어리/AB 테스트**: 오프라인 개선이 **온라인 목표(KPI)** 개선으로 이어지는지 확인.

---

## M. 자주 겪는 함정 & 체크리스트

1) **정확도만 높다**  
   - 불균형 데이터. **PR-AUC, F1, 리콜** 함께 보고.
2) **PR-AUC가 낮다**  
   - 임계값 최적화/데이터 증강/클래스 가중/샘플링.
3) **과도한 튜닝**  
   - test set 재사용은 금물. val 분리·nested CV 고려.
4) **캘리브레이션 불량(ECE↑)**  
   - Temperature scaling/Platt/Isotonic, 데이터 확대.  
5) **OOD 미탐**  
   - MSP/Entropy/Feature distance/Ensemble 등 복합 지표, 임계값 모니터링.
6) **서브그룹 편향**  
   - 그룹별 성능·캘리브레이션 점검, 재가중/데이터 보강/손실 가중.
7) **신뢰구간 미보고**  
   - 부트스트랩/교차검증으로 변동성 함께 보고.

---

## N. 연습 과제

1) COCO 스타일 AP@[.50:.95] 계산기를 직접 구현하고, NMS 임계값 변화가 AP에 미치는 영향 관찰.  
2) Temperature scaling vs Isotonic regression의 ECE 개선 비교(Val/Test 분리).  
3) MSP/Entropy/에너지 점수로 OOD AUROC 비교(여러 분포에서).  
4) 세그멘테이션에서 mIoU vs mDice 상관과 차이 사례 수집.  
5) 비용행렬이 주어졌을 때 최적 임계값과 F1@t*의 차이를 분석.

---

## O. 수식·정의 모음(퀵 레퍼런스)

- 정확도  
  $$\mathrm{ACC}=\frac{TP+TN}{TP+FP+FN+TN}$$
- 정밀/재현/F1  
  $$\mathrm{P}=\frac{TP}{TP+FP},\quad \mathrm{R}=\frac{TP}{TP+FN},\quad \mathrm{F1}=\frac{2PR}{P+R}$$
- ROC-AUC: TPR vs FPR 면적
- PR-AUC: Precision vs Recall 면적
- IoU / Dice  
  $$\mathrm{IoU}=\frac{|A\cap B|}{|A\cup B|},\quad \mathrm{Dice}=\frac{2|A\cap B|}{|A|+|B|}$$
- BLEU  
  $$\mathrm{BLEU}=\mathrm{BP}\cdot\exp\!\left(\sum_{n=1}^N w_n\log p_n\right)$$
- ECE  
  $$\mathrm{ECE}=\sum_b \frac{|S_b|}{n}\left|\mathrm{acc}(S_b)-\mathrm{conf}(S_b)\right|$$
- 에너지 점수  
  $$E(\mathbf{x})=-T\log\sum_c e^{z_c/T}$$
- PSI  
  $$\mathrm{PSI}=\sum_i (\hat{p}_i-p_i)\ln\frac{\hat{p}_i}{p_i}$$

---

### 마무리
평가·신뢰성은 **단일 숫자**가 아니라 **체계**입니다.  
**메트릭 조합 + 캘리브레이션 + OOD/드리프트 감시 + 에러분석 + 불확실성**이 한 세트로 돌아갈 때, 오프라인 성능과 실제 운영 신뢰성이 일치합니다.  
이 장의 수식과 템플릿을 프로젝트에 삽입해 **지속 가능한 평가 파이프라인**을 구축하세요.