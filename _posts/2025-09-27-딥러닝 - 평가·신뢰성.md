---
layout: post
title: 딥러닝 - 평가·신뢰성
date: 2025-09-27 23:25:23 +0900
category: 딥러닝
---
# 평가·신뢰성(Evaluation & Reliability)

## 1. 평가의 큰 그림: 프로토콜과 함정

### 1.1 데이터 분할 & 프로토콜

#### 1) 기본: Train / Validation / Test

- **Train**: 파라미터 학습.
- **Validation(Val)**:  
  - 하이퍼파라미터 튜닝  
  - Early stopping  
  - 임계값(Threshold) 결정
- **Test**:  
  - **단 한 번**, 최종 성능 확인용.  
  - 어떤 튜닝에도 사용하지 않는 “봉인된” 데이터.

**데이터 누수(Data leakage) 금지 예시**

- 잘못된 예:
  - 전체 데이터를 섞은 후,  
    - 여기서 교차검증으로 모델/튜닝 선택,  
    - 같은 데이터로 최종 테스트까지 반복.
- 문제:
  - 사실상 Test를 여러 번 본 효과 → **과적합된 Test 성능**.

#### 2) K-겹 교차검증(K-fold CV)

- 표본이 적거나 분산 추정이 필요할 때 사용.
- 절차:
  1. 데이터를 $$K$$개 Fold로 나눈다.
  2. $$K-1$$개를 Train, 나머지 1개를 Val로 사용.
  3. Fold를 바꿔가며 $$K$$번 반복 → $$K$$개의 점수.
  4. 평균과 표준편차로 성능과 변동성을 본다.

**사용 팁**

- **작은 탭 데이터**나 **의료·금융**처럼 라벨이 비싼 환경에서 특히 유용.
- 딥러닝에서는
  - 대형 Vision/NLP 모델은 CV 비용이 크므로
  - K=5 대신 K=3 혹은 단일 Val split + 부트스트랩을 쓰는 경우도 많다.

#### 3) 시간의존(시계열/로그) 데이터

- 시계열, 로그 데이터에서는
  - **미래 정보가 과거에 섞이는 것**이 치명적 누수.

**권장 프로토콜**

- **Hold-out by time**
  - 예: 2023-01~2023-10 Train, 2023-11 Val, 2023-12 Test
- **Rolling window / walk-forward**
  - Train range를 앞쪽으로 굴려가며 여러 시점 성능 측정.
  - 예: 6개월 Train → 1개월 Test, 다음 달로 한 달씩 이동.

#### 4) 클래스 불균형

- 예: 사기 거래(0.1%), 고장 감지(0.5%), 희귀 질환(1%) 등.
- 단순 Accuracy:
  - “모두 정상”이라고 예측해도 Accuracy 99% → **의미 없음**.
- 이럴 때는:
  - F1 / PR-AUC / Recall / Balanced Accuracy 등 **양성(희귀) 클래스 중심** 메트릭이 중요.

---

### 1.2 보고의 원칙: 숫자를 어떻게 보여줄 것인가

#### 1) 포인트 추정치 + 불확실성

- 단일 숫자:
  - “F1 = 0.87”  
  - 이 값 자체만으론 **불확실성**이 보이지 않는다.
- 권장:
  - $$\text{F1} = 0.87 \pm 0.02$$ (95% CI)  
  - CI는 부트스트랩/교차검증으로 계산.
- 해석:
  - 모델 A: $$0.87 \pm 0.02$$  
  - 모델 B: $$0.88 \pm 0.05$$  
  - 평균만 보면 B가 더 좋아 보이지만,  
    - 변동성을 감안하면 “A가 더 안정적”일 수 있다.

#### 2) 여러 메트릭 병행

- 이진 분류 예:
  - ROC-AUC
  - PR-AUC
  - F1@$$t^\*$$
  - Calibration (ECE/Brier)
- 왜 필요한가?
  - **ROC-AUC**는 클래스 불균형에 둔감.
  - **PR-AUC**는 희귀 양성에 민감.
  - **F1**은 임계값 지정 후 실제 운영 Trade-off를 반영.
  - **ECE/Brier**는 “확률이 믿을 만한지”를 측정.

#### 3) 공정성 / 서브그룹 분석

- 예: 성별, 연령, 지역, 디바이스 종류, 언어 등.
- 전체 F1이 높아도
  - 특정 서브그룹에서만 F1이 극도로 낮을 수 있다.
- 권장:
  - **By-group 테이블**,  
  - 서브그룹별 ROC/PR,  
  - 서브그룹별 캘리브레이션(ECE)까지 확인.

---

## 2. 메트릭 정리: 분류·회귀·랭킹

### 2.1 이진 분류: 혼동행렬과 기본 지표

이진 분류의 혼동행렬:

- TP (True Positive)
- FP (False Positive)
- FN (False Negative)
- TN (True Negative)

기본 지표:

$$
\mathrm{Accuracy}
= \frac{TP+TN}{TP+FP+FN+TN}
$$

$$
\mathrm{Precision}
= \frac{TP}{TP+FP}
$$

$$
\mathrm{Recall}
= \frac{TP}{TP+FN}
$$

$$
\mathrm{F1}
= \frac{2\cdot \mathrm{Precision}\cdot \mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}
$$

직관:

- Precision: “양성이라고 한 것 중에 진짜 양성 비율”
- Recall: “실제 양성 중에 얼마나 찾았는가”
- F1: 둘의 조화평균 → 둘 중 하나가 작으면 F1도 작다.

### 2.2 멀티클래스 평균 방법

멀티클래스에서 “F1”을 말할 때 평균 방법을 명시해야 한다.

- Macro:
  - 클래스별 F1을 구한 후 **그냥 평균**.
  - 클래스가 적은 클래스도 동등하게 반영.
- Weighted:
  - 클래스별 F1에 **표본 수로 가중**.
  - 전체 데이터 분포를 반영.
- Micro:
  - 모든 TP/FP/FN을 합산하고 나서 F1을 계산.
  - 사실상 **전체 이진 문제 하나**로 보는 것과 같다.

### 2.3 Top-K 정확도

이미지 분류 등에서 자주 쓰는 지표:

$$
\mathrm{Top}\text{-}k 
= \frac{1}{N}\sum_{i=1}^N \mathbf{1}\{y_i \in \mathrm{top}\text{-}k(\hat{\mathbf{p}}_i)\}
$$

- 사용자는 종종
  - 상위 5개 중 하나만 맞아도 괜찮은 경우(추천/검색).
- Top-1만 보면
  - “근접하게 맞힌” 경우를 모두 0으로 처리 → 정보 손실.

### 2.4 ROC-AUC & PR-AUC

- ROC 곡선:
  - x축: FPR (False Positive Rate)
  - y축: TPR (True Positive Rate, Recall)
- ROC-AUC:
  - 임의의 양성/음성 한 쌍을 뽑았을 때
  - 모델이 양성을 더 높게 점수 매길 확률.

- PR 곡선:
  - x축: Recall
  - y축: Precision
- PR-AUC:
  - 희귀 양성에서 **양성 예측의 질**을 보는 데 중요.

**실무 룰**

- 양성이 희귀(1% 이하)면
  - ROC-AUC는 “대부분 음성”을 잘 구분하는 것으로도 높게 나올 수 있다.
  - **PR-AUC**를 반드시 함께 본다.

### 2.5 임계값 튜닝(Thresholding)

점수 $$s \in [0,1]$$에서 임계값 $$t$$에 따라 이진 예측:

- $$\hat{y} = 1 \iff s \ge t$$

#### 1) F1 최대 임계값

$$
t^\* = \arg\max_t \ \mathrm{F1}(t)
$$

- Validation set에서 F1을 최대화하는 t 선택.
- 단점:
  - F1은 $$C_{FP}$$와 $$C_{FN}$$(FP/FN 비용)가 같다고 가정.

#### 2) 비용 기반 임계값

비용행렬:

- FP 비용: $$C_{FP}$$
- FN 비용: $$C_{FN}$$

기대 비용 최소화:

$$
t^\* = \arg\min_t \ \mathbb{E}[\mathrm{Cost}(t)]
$$

- 클래스 불균형, 도메인 요구사항에 따라
  - $$C_{FP}, C_{FN}$$을 다른 값으로 설계해야 한다.

---

### 2.6 코드: 기본 분류 메트릭 & 임계값 스캔

```python
import numpy as np
from sklearn.metrics import (
    confusion_matrix, f1_score, accuracy_score,
    roc_auc_score, average_precision_score
)

y_true = np.array([0,1,1,0,1,0,0,1])
y_score = np.array([0.1,0.8,0.6,0.3,0.9,0.2,0.4,0.7])  # 예측 확률(양성)

def basic_metrics(y_true, y_score, t=0.5):
    y_pred = (y_score >= t).astype(int)
    cm = confusion_matrix(y_true, y_pred)
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    rocauc = roc_auc_score(y_true, y_score)
    prauc = average_precision_score(y_true, y_score)
    return {
        "threshold": t,
        "cm": cm,
        "acc": acc,
        "f1": f1,
        "rocauc": rocauc,
        "prauc": prauc
    }

def scan_best_f1(y_true, y_score, n_steps=101):
    ts = np.linspace(0, 1, n_steps)
    best = None
    for t in ts:
        y_pred = (y_score >= t).astype(int)
        f1 = f1_score(y_true, y_pred)
        if best is None or f1 > best[1]:
            best = (t, f1)
    return best

m = basic_metrics(y_true, y_score, t=0.5)
best_t, best_f1 = scan_best_f1(y_true, y_score)

print("CM=\n", m["cm"])
print(f"ACC={m['acc']:.3f} F1={m['f1']:.3f} ROC-AUC={m['rocauc']:.3f} PR-AUC={m['prauc']:.3f}")
print(f"Best F1={best_f1:.3f} at t={best_t:.2f}")
```

---

### 2.7 회귀 메트릭

회귀 문제에서는

- MSE/RMSE, MAE, $$R^2$$가 핵심.

$$
\mathrm{MSE}
= \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

$$
\mathrm{RMSE} = \sqrt{\mathrm{MSE}}
$$

$$
\mathrm{MAE}
= \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|
$$

$$
R^2
= 1 - \frac{\sum_i (y_i-\hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2}
$$

- RMSE:
  - 큰 오차에 더 큰 페널티.
- MAE:
  - 극단값에 덜 민감.
- $$R^2$$:
  - 선형모델에서 많이 쓰지만,
  - 비선형 딥러닝에서도 대략적인 설명력 지표로 사용할 수 있다.

---

## 3. 세그멘테이션(Segmentation) 메트릭

### 3.1 IoU(Intersection over Union)와 Dice

픽셀 단위 집합 $$A$$, $$B$$에 대해:

$$
\mathrm{IoU}
= \frac{|A \cap B|}{|A \cup B|}
$$

- 픽셀 수 또는 면적을 기준으로 계산.

Dice 계수(F1 of sets):

$$
\mathrm{Dice}
= \frac{2|A \cap B|}{|A| + |B|}
$$

관계:

- $$\mathrm{Dice} = \frac{2\mathrm{IoU}}{1+\mathrm{IoU}}$$
- $$\mathrm{IoU} = \frac{\mathrm{Dice}}{2-\mathrm{Dice}}$$

세그멘테이션에서는

- **mIoU**: 클래스별 IoU 평균
- **mDice**: 클래스별 Dice 평균

### 3.2 코드: mIoU/mDice 계산

```python
import numpy as np

def iou_per_class(y_true, y_pred, num_classes, ignore_index=None):
    ious = []
    for c in range(num_classes):
        if c == ignore_index:
            continue
        t = (y_true == c)
        p = (y_pred == c)
        inter = np.logical_and(t, p).sum()
        union = np.logical_or(t, p).sum()
        if union == 0:
            continue
        ious.append(inter / union)
    return ious, np.mean(ious) if ious else float('nan')

def dice_per_class(y_true, y_pred, num_classes, ignore_index=None):
    dices = []
    for c in range(num_classes):
        if c == ignore_index:
            continue
        t = (y_true == c)
        p = (y_pred == c)
        inter = np.logical_and(t, p).sum()
        s = t.sum() + p.sum()
        if s == 0:
            continue
        dices.append(2 * inter / s)
    return dices, np.mean(dices) if dices else float('nan')
```

### 3.3 실무 팁

- 클래스가 매우 불균형(배경 vs 작은 객체)일 때
  - Pixel Accuracy는 거의 1에 가까울 수 있지만,
  - mIoU는 작은 객체 성능 저하를 잘 드러낸다.
- 세그멘테이션에서는
  - **Boundary IoU**나
  - 특정 구조(예: 건물/도로)의 **연결성 지표**를 추가로 보는 것도 중요하다.

---

## 4. 검출(Detection) 메트릭: AP / mAP

### 4.1 AP와 mAP 개념

객체 검출에서는

- 각 예측 박스에 대해:
  - 클래스,
  - 점수(score),
  - 바운딩 박스 좌표가 있다.

**매칭 기준**

- 예측 박스와 GT(정답 박스)의 IoU를 계산.
- IoU ≥ 임계값(예: 0.5)이면서 아직 매칭되지 않은 GT가 있으면 TP,  
  아니면 FP.

**AP(평균정밀도)**

- Recall 증가 순으로 정렬된(또는 score 내림차순) 점들을 따라가면서
  - Precision-Recall 곡선을 그리고
  - 그 아래 면적을 계산.

**mAP**

- 클래스별 AP를 구한 후 평균.

데이터셋별 관행:

- PASCAL VOC:
  - IoU≥0.5에서 AP 계산, 11-point 근사 등.
- COCO:
  - IoU=0.5~0.95까지 0.05 step으로 AP를 계산한 후 평균(AP@[.50:.95]).

### 4.2 간단 AP 계산(11-point 근사)

```python
import numpy as np

def average_precision_11pt(prec, rec):
    # prec, rec: recall 증가 순 정렬
    ap = 0.0
    for t in np.linspace(0, 1, 11):
        p = (prec[rec >= t].max() if np.any(rec >= t) else 0)
        ap += p
    return ap / 11.0
```

### 4.3 검출 매칭 스케치

```python
# gts: list of dict {image_id, class_id, bbox}
# dets: list of dict {image_id, class_id, score, bbox}
# bbox: (xmin, ymin, xmax, ymax)

def iou(box1, box2):
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    if x2 <= x1 or y2 <= y1:
        return 0.0
    inter = (x2-x1) * (y2-y1)
    area1 = (box1[2]-box1[0])*(box1[3]-box1[1])
    area2 = (box2[2]-box2[0])*(box2[3]-box2[1])
    union = area1 + area2 - inter
    return inter / union

def match_detections(gts, dets, iou_thr=0.5):
    # GT를 이미지+클래스별로 묶고, detected 플래그 관리
    # 이후 dets를 score 내림차순으로 정렬
    # 각 det에 대해 GT를 스캔하여 IoU 최대인 것 찾기
    # IoU>=thr & 아직 미매칭이면 TP, 아니면 FP
    pass  # 실제 구현은 연습 문제로 남김
```

---

## 5. NLP 메트릭: BLEU와 대안

### 5.1 BLEU: n-그램 정밀도 + Brevity Penalty

BLEU는 기계번역에서 전통적으로 많이 쓰이는 지표.

정의:

- n-그램(보통 n=1..4)에 대한 **클리핑된 정밀도** $$p_n$$
- 가중치 $$w_n$$ (합이 1)
- Brevity Penalty(BP)

$$
\mathrm{BLEU}
= \mathrm{BP} \cdot \exp\Big(\sum_{n=1}^N w_n \log p_n\Big)
$$

$$
\mathrm{BP} = 
\begin{cases}
1, & c > r \\
\exp(1-r/c), & c \le r
\end{cases}
$$

- $$c$$: 후보 문장 길이
- $$r$$: 참조 문장 길이(여럿이면 “가장 가까운 길이” 사용)

**한계**

- 의미(semantic)를 직접적으로 측정하지 않는다.
- 표현이 조금만 달라도 n-그램 정밀도가 크게 떨어질 수 있다.
- 요약/대화/질답처럼
  - 표현 자유도가 큰 태스크에서는 한계가 분명하다.

### 5.2 간단 BLEU 구현(단일 레퍼런스)

```python
import math, collections

def bleu_score(candidate, reference, max_n=4, weights=None):
    if weights is None:
        weights = [1/max_n] * max_n

    def ngrams(tokens, n):
        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

    c = candidate.split()
    r = reference.split()
    c_len, r_len = len(c), len(r)

    p_ns = []
    for n, w in enumerate(weights, start=1):
        cand_ngrams = collections.Counter(ngrams(c, n))
        ref_ngrams  = collections.Counter(ngrams(r, n))
        overlap = {k: min(v, ref_ngrams.get(k,0)) for k,v in cand_ngrams.items()}
        num = sum(overlap.values())
        den = max(sum(cand_ngrams.values()), 1)
        p_ns.append(num/den if den>0 else 0.0)

    # BP
    bp = 1.0 if c_len > r_len else math.exp(1 - r_len / max(c_len, 1))

    # geometric mean with logs
    s = sum(w * math.log(p + 1e-12) for w, p in zip(weights, p_ns))
    return bp * math.exp(s)
```

### 5.3 다른 NLP 메트릭 (간략)

- **ROUGE-L**:
  - 요약 평가에서 많이 쓰이는 **Longest Common Subsequence** 기반.
- **BERTScore**:
  - 문장을 임베딩 후 유사도를 측정하는 방식.
  - 표현 다양성에 강하지만, 계산 비용이 크다.

---

## 6. 캘리브레이션(Calibration): 확률의 믿을 만함

### 6.1 개념

완벽한 캘리브레이션을 가진 이진 분류 모델이라면:

$$
\mathbb{P}(Y=1 \mid \hat{p}=0.8) \approx 0.8
$$

- 예측 확률 0.8인 샘플을 모으면,
  - 실제 양성 비율이 80%에 가깝다.

정확도와 별개로:

- “얼마나 맞출 수 있는가?”(정확도)
- “맞다고 할 때 얼마만큼 믿어도 되는가?”(캘리브레이션)

은 서로 다른 문제.

### 6.2 ECE/MCE/Brier

ECE(Expected Calibration Error):

$$
\mathrm{ECE}
= \sum_{b=1}^B \frac{|S_b|}{n} \left| \mathrm{acc}(S_b) - \mathrm{conf}(S_b) \right|
$$

- $$S_b$$: 확률 구간(bin) $$b$$에 속한 샘플 집합
- $$\mathrm{acc}(S_b)$$: bin 내 실제 정확도
- $$\mathrm{conf}(S_b)$$: bin 내 평균 예측 확률

MCE(Max Calibration Error):

- 위 차이의 **최대값**.

Brier score(이진):

$$
\mathrm{Brier}
= \frac{1}{n}\sum_{i=1}^n (\hat{p}_i - y_i)^2
$$

- MSE와 유사하지만
  - Target이 0/1,
  - 예측이 확률(0~1).

### 6.3 코드: ECE 계산

```python
import numpy as np

def ece_score(y_true, y_prob, n_bins=15):
    # y_prob: 이진 양성 확률 또는 멀티클래스의 max prob
    bins = np.linspace(0.0, 1.0, n_bins+1)
    ece = 0.0
    n = len(y_true)

    for i in range(n_bins):
        l, r = bins[i], bins[i+1]
        if i < n_bins - 1:
            mask = (y_prob >= l) & (y_prob < r)
        else:
            mask = (y_prob >= l) & (y_prob <= r)
        if mask.sum() == 0:
            continue

        acc = (y_true[mask] == (y_prob[mask] >= 0.5)).mean()
        conf = y_prob[mask].mean()
        ece += (mask.sum() / n) * abs(acc - conf)

    return ece
```

### 6.4 Temperature Scaling(로짓 rescale)

로짓 $$\mathbf{z}$$에 대해 스칼라 $$T>0$$으로 나눔:

$$
\hat{\mathbf{p}}
= \mathrm{softmax}\Big(\frac{\mathbf{z}}{T}\Big)
$$

- Val set에서 $$T$$를 최적화(교차엔트로피 최소화).
- 구조/순위는 유지되며
  - 정확도는 거의 변하지 않고
  - 캘리브레이션(ECE)이 개선되는 경우가 많다.

#### 코드: Temperature Scaling 학습

```python
import torch, torch.nn as nn
import torch.nn.functional as F

def optimize_temperature(logits_val, y_val):
    T = torch.nn.Parameter(torch.ones(1, device=logits_val.device))
    opt = torch.optim.LBFGS([T], lr=0.1, max_iter=50)
    ce = nn.CrossEntropyLoss()

    def closure():
        opt.zero_grad()
        loss = ce(logits_val / T.clamp_min(1e-3), y_val)
        loss.backward()
        return loss

    opt.step(closure)
    return T.detach()

# 사용 예 (val 세트)
# logits_val: (N,C), y_val: (N,)
# T = optimize_temperature(logits_val, y_val)
# probs_test = F.softmax(logits_test / T, dim=-1)
```

---

## 7. 분포 드리프트 & OOD(Out-of-Distribution)

### 7.1 분포 드리프트 종류

- Covariate drift:
  - $$p(\mathbf{x})$$ 변동, $$p(y\mid \mathbf{x})$$는 고정 가정.
- Label shift:
  - $$p(y)$$ 변동.
- Concept drift:
  - $$p(y\mid \mathbf{x})$$ 자체 변동.
  - 가장 다루기 어렵고, 모델 재학습이 필요.

### 7.2 OOD 탐지 지표

- MSP(Max Softmax Probability):
  - $$\max_c \hat{p}_c$$
  - 값이 작으면 OOD 가능성이 높다는 신호.
- Entropy:
  - $$-\sum_c \hat{p}_c \log \hat{p}_c$$
  - 높으면 분포 외 표본일 가능성↑.
- 마할라노비스 거리:
  - Feature를 가우시안 가정 후 거리로 OOD 점수.
- 에너지 기반:
  - $$E(\mathbf{x}) = -T \log\sum_c e^{z_c/T}$$
  - 값이 클수록 OOD 가능성이 크다고 볼 수 있다.

#### 코드: MSP/Entropy 점수

```python
import numpy as np

def softmax(logits):
    logits = logits - logits.max(axis=1, keepdims=True)
    exps = np.exp(logits)
    return exps / exps.sum(axis=1, keepdims=True)

def msp_scores(logits):
    probs = softmax(logits)
    return probs.max(axis=1)  # 높을수록 ID(=확신)

def entropy_scores(logits):
    probs = softmax(logits)
    ent = -(probs * np.log(probs + 1e-12)).sum(axis=1)
    return ent  # 높을수록 OOD 가능성
```

### 7.3 분포 드리프트 감지: PSI

PSI(Population Stability Index):

- 참조 분포 $$p_i$$와 현재 분포 $$\hat{p}_i$$ 사이의 차이:

$$
\mathrm{PSI}
= \sum_i (\hat{p}_i - p_i)\ln\frac{\hat{p}_i}{p_i}
$$

- 값이 일정 threshold를 넘으면 “분포가 많이 바뀌었다”는 신호로 사용.

#### 코드: PSI 계산

```python
def psi_score(ref, cur, bins=10):
    q = np.linspace(0, 1, bins+1)
    cuts = np.quantile(ref, q)
    ref_hist, _ = np.histogram(ref, bins=cuts)
    cur_hist, _ = np.histogram(cur, bins=cuts)
    ref_p = ref_hist / max(ref_hist.sum(), 1)
    cur_p = cur_hist / max(cur_hist.sum(), 1)
    cur_p = np.where(cur_p == 0, 1e-6, cur_p)
    ref_p = np.where(ref_p == 0, 1e-6, ref_p)
    return ((cur_p - ref_p) * np.log(cur_p / ref_p)).sum()
```

---

## 8. 에러 분석(Error Analysis)

### 8.1 어디서 틀리는가?

단순 글로벌 메트릭만 보고는

- 어떤 입력에서,
- 어떤 서브그룹에서,
- 어떤 유형의 예측 오류가 나는지 알 수 없다.

필수 도구:

- 혼동행렬 히트맵
- 클래스별 Precision/Recall
- 서브그룹별 성능 테이블
- Hard Example 리스트업

### 8.2 슬라이스 분석 코드 예

```python
import pandas as pd
import numpy as np

def slice_metrics(df, slice_col, y_col='y_true', p_col='y_prob', t=0.5):
    rows = []
    for k, sub in df.groupby(slice_col):
        y = sub[y_col].values.astype(int)
        p = sub[p_col].values.astype(float)
        y_pred = (p >= t).astype(int)
        acc = (y == y_pred).mean()
        tp = ((y_pred == 1) & (y == 1)).sum()
        fp = ((y_pred == 1) & (y == 0)).sum()
        fn = ((y_pred == 0) & (y == 1)).sum()
        prec = tp / max(tp + fp, 1)
        rec  = tp / max(tp + fn, 1)
        f1   = 2 * prec * rec / (prec + rec + 1e-12)
        rows.append((k, len(sub), acc, prec, rec, f1))
    return pd.DataFrame(rows, columns=[slice_col, 'n', 'acc', 'prec', 'rec', 'f1'])
```

사용 예:

- df에
  - 열: y_true, y_prob, device_type, country 등.
- `slice_metrics(df, 'device_type')`로
  - 디바이스 종류별 성능 비교.

### 8.3 임계값 재튜닝(서브그룹별)

- 한 개의 전역 임계값만 쓸지,
- 서브그룹별로 다른 임계값을 쓸지 결정해야 한다.

주의:

- 서브그룹별 임계값은
  - 공정성 관점에서 논쟁적일 수 있다.
  - 정책/규제/비즈니스 요구를 고려하여 설계해야 한다.

---

## 9. 불확실성 추정(Uncertainty Estimation)

### 9.1 Aleatoric vs Epistemic

- Aleatoric 불확실성:
  - 데이터 자체의 노이즈.
  - 동일한 조건에서도 결과가 달라지는 “본질적” 불확실.
- Epistemic 불확실성:
  - 모델 파라미터에 대한 불확실성.
  - 데이터가 부족하거나 영역 밖이면 커진다.

### 9.2 MC Dropout

- Dropout을 학습뿐 아니라 추론 시에도 켜고
  - 여러 번 추론 → 예측 분포 관찰.

```python
def mc_dropout_predict(model, x, T=30):
    model.train()  # 드롭아웃 ON (BatchNorm 등 주의 필요)
    probs = []
    with torch.no_grad():
        for _ in range(T):
            logits = model(x)
            p = torch.softmax(logits, dim=-1)
            probs.append(p)
    P = torch.stack(probs, dim=0)  # (T,B,C)
    mean = P.mean(0)               # (B,C)
    var  = P.var(0)                # (B,C) 에피스테믹 근사
    return mean, var
```

### 9.3 Deep Ensembles

- 모델을 여러 개 학습
  - 초기화, Data order, Subset 등을 달리한다.
- 추론 시:
  - 각 모델의 예측을 평균 → 성능 및 안정성 증가.
  - 분산으로 Epistemic 불확실성 추정.

### 9.4 회귀에서 로그-분산 출력

모델이 $$\hat{y}$$와 $$\log\sigma^2$$를 함께 출력하도록 학습:

$$
\mathcal{L}
= \frac{1}{2\sigma^2} \|y - \hat{y}\|^2 + \frac{1}{2}\log\sigma^2
$$

- 관측 노이즈(aleatoric)를 직접 모델링.
- 큰 노이즈가 있는 구간에서는
  - 모델이 큰 $$\sigma^2$$를 예측해
  - 잔차에 대해 덜 민감해진다.

---

## 10. 임계값·코스트·경계조건

### 10.1 비용행렬 기반 최적 임계값

비용:

- $$C_{FP}$$: FP 하나의 비용(예: 광고비 낭비, 잘못된 수술)
- $$C_{FN}$$: FN 하나의 비용(예: 사기 미검출, 질병 미진단)

임계값 $$t$$에서:

- $$\mathrm{Cost}(t) = C_{FP}\cdot FP(t) + C_{FN}\cdot FN(t)$$

최적 임계값:

$$
t^\*
= \arg\min_t \ \mathrm{Cost}(t)
$$

#### 코드: 비용 기반 최적 t

```python
def best_threshold_cost(y_true, y_prob, c_fp=1.0, c_fn=1.0, n_steps=101):
    ts = np.linspace(0,1,n_steps)
    best = (None, float('inf'))
    y = y_true.astype(int)
    for t in ts:
        yp = (y_prob >= t).astype(int)
        fp = ((yp == 1) & (y == 0)).sum()
        fn = ((yp == 0) & (y == 1)).sum()
        cost = c_fp * fp + c_fn * fn
        if cost < best[1]:
            best = (t, cost)
    return best  # (t*, min cost)
```

---

## 11. 신뢰구간·유의성(Significance)

### 11.1 부트스트랩 CI

- 데이터에서 재표본화(복원 추출)로
  - 지표 분포를 근사.
- 예: F1에 대한 95% CI.

#### 코드: 부트스트랩 CI (예: F1)

```python
def bootstrap_ci(y_true, y_pred, stat_fn, B=1000, alpha=0.05, seed=0):
    rng = np.random.default_rng(seed)
    n = len(y_true)
    stats = []
    for _ in range(B):
        idx = rng.integers(0, n, n)
        stats.append(stat_fn(y_true[idx], y_pred[idx]))
    lo, hi = np.quantile(stats, [alpha/2, 1-alpha/2])
    return float(lo), float(hi)
```

### 11.2 모델 비교 유의성

- 두 모델 A/B의 차이가 **우연인지** 판단하려면:
  - paired bootstrap,
  - Approximate randomization test 등을 사용.

---

## 12. 통합 평가 파이프라인(실전 템플릿)

### 12.1 전체 흐름

1. **데이터 분할**
   - Train / Val / Test 또는 시간 기반 split.
2. **학습 & 튜닝**
   - Train에서 모델 학습.
   - Val에서 하이퍼파라미터, 임계값, Temperature scaling 파라미터 등 결정.
3. **오프라인 평가**
   - 분류: Accuracy, F1, ROC-AUC, PR-AUC, ECE, Brier.
   - 세그멘테이션: mIoU, mDice.
   - 검출: AP/mAP.
   - NLP: BLEU, ROUGE, BERTScore.
   - 서브그룹 분석, 캘리브레이션, 에러 분석.
4. **불확실성 & OOD**
   - MC Dropout/Ensemble로 예측 분산.
   - MSP/Entropy로 OOD 감지 가능성 점검.
5. **신뢰구간**
   - 부트스트랩/교차검증으로 변동성 보고.
6. **배포 & 모니터링**
   - 입력분포(PSI, KL), 성능 프록시, 캘리브레이션, OOD 점수 모니터링.
7. **AB 테스트**
   - 온라인 KPI 개선 여부 확인.

### 12.2 분류 모델을 예로 한 통합 함수

```python
import numpy as np
import pandas as pd
from sklearn.metrics import (
    f1_score, accuracy_score, roc_auc_score,
    average_precision_score, confusion_matrix
)

def evaluate_classification(y_true, logits, groups=None, do_temp_scale=True):
    """
    y_true: (N,)
    logits: (N,C)  # 이진이면 C=2
    groups: (N,) 서브그룹 라벨(예: 디바이스, 국가 등)
    """

    # 확률
    exps = np.exp(logits - logits.max(axis=1, keepdims=True))
    probs = exps / exps.sum(axis=1, keepdims=True)
    y_prob = probs[:, 1]
    y_pred = (y_prob >= 0.5).astype(int)

    out = {}
    out['acc']  = accuracy_score(y_true, y_pred)
    out['f1']   = f1_score(y_true, y_pred)
    out['roc']  = roc_auc_score(y_true, y_prob)
    out['pr']   = average_precision_score(y_true, y_prob)
    out['cm']   = confusion_matrix(y_true, y_pred)
    out['ece']  = ece_score(y_true, y_prob, n_bins=15)

    # 서브그룹 분석
    if groups is not None:
        df = pd.DataFrame({'g': groups, 'y': y_true, 'p': y_prob})
        out['by_group'] = slice_metrics(df, 'g', 'y', 'p')

    # Temperature scaling(간단 예시: 동일 데이터 사용, 실제는 Val 분리 권장)
    if do_temp_scale:
        import torch, torch.nn.functional as F
        logits_t = torch.from_numpy(logits).float()
        y_t      = torch.from_numpy(y_true).long()
        T = optimize_temperature(logits_t, y_t)  # 위 섹션 함수
        probs_ts = F.softmax(logits_t / T, dim=-1).numpy()[:,1]
        out['ece_ts'] = ece_score(y_true, probs_ts, n_bins=15)

    return out
```

---

## 13. 운영·모니터링: 배포 후 신뢰성

### 13.1 온라인 모니터링 항목

- 입력 피처별 분포(PSI, KS, KL)
- 예측 확률 분포(평균, 분산, 상위 분위)
- MSP/Entropy 분포
- 서브그룹별 FDR/TPR 추이
- 시간에 따른 ECE/Brier proxy

### 13.2 캐니어리 / AB 테스트

- 새 모델은
  - **소수 트래픽**에 먼저 배포(canary).
- 온라인 KPI:
  - 클릭률, 전환율, 수익, 리스크 감소 등.
- 오프라인 성능이 좋아도
  - 실제 사용자 행동과 제약(비용/규제)을 고려하면
  - 온라인에서 더 나쁜 결과가 나올 수 있다.

---

## 14. 자주 겪는 함정 & 체크리스트

1) 정확도만 높다  
   - 클래스 불균형. PR-AUC, F1, Recall 확인.

2) PR-AUC가 낮다  
   - 임계값 최적화, 클래스 가중, 오버샘플링/언더샘플링, 적절한 손실(예: Focal loss) 고려.

3) Test set 반복 사용  
   - 누수. Val set 분리, nested CV, 최종 Test는 한 번만.

4) 캘리브레이션 불량(ECE↑)  
   - Temperature scaling, Platt scaling, Isotonic regression.

5) OOD 미탐  
   - MSP/Entropy/Feature distance/Ensemble 등 복합 지표 모니터링.

6) 서브그룹 편향  
   - 그룹별 성능·ECE, 데이터 보강, 재가중/공정성 제약 고려.

7) 신뢰구간 미보고  
   - 부트스트랩/교차검증으로 변동성 함께 보고.

8) 단일 메트릭 집착  
   - 설계 목표(KPI)와 비즈니스 요구에 맞는 **메트릭 세트**를 정의.

---

## 15. 연습 과제

1) COCO 스타일 AP@[.50:.95] 계산기를 직접 구현하고,  
   - IoU 임계값과 NMS 임계값 변화가 AP에 미치는 영향 분석.

2) Temperature scaling vs Isotonic regression으로  
   - Validation/Test ECE 변화를 비교.

3) MSP/Entropy/에너지 점수로  
   - 여러 데이터 분포에서 OOD AUROC 비교.

4) 세그멘테이션에서  
   - mIoU와 mDice의 상관관계와 차이가 두드러지는 실패 사례 수집.

5) 비용행렬이 주어졌을 때  
   - 비용 기반 최적 임계값과 F1 기반 최적 임계값이 어떻게 다른지,  
   - 실제 운영에서 어떤 차이를 초래하는지 시뮬레이션.

---

## 16. 수식·정의 모음(퀵 레퍼런스)

- 정확도  
  $$\mathrm{ACC}=\frac{TP+TN}{TP+FP+FN+TN}$$

- 정밀도/재현율/F1  
  $$\mathrm{P}=\frac{TP}{TP+FP},\quad \mathrm{R}=\frac{TP}{TP+FN},\quad \mathrm{F1}=\frac{2PR}{P+R}$$

- IoU / Dice  
  $$\mathrm{IoU}=\frac{|A\cap B|}{|A\cup B|},\quad \mathrm{Dice}=\frac{2|A\cap B|}{|A|+|B|}$$

- BLEU  
  $$\mathrm{BLEU}=\mathrm{BP}\cdot\exp\!\left(\sum_{n=1}^N w_n\log p_n\right)$$

- ECE  
  $$\mathrm{ECE}=\sum_b \frac{|S_b|}{n}\left|\mathrm{acc}(S_b)-\mathrm{conf}(S_b)\right|$$

- 에너지 점수(분류)  
  $$E(\mathbf{x})=-T\log\sum_c e^{z_c/T}$$

- PSI  
  $$\mathrm{PSI}=\sum_i (\hat{p}_i-p_i)\ln\frac{\hat{p}_i}{p_i}$$

---

## 17. 마무리

평가·신뢰성은 **“정답을 하나 뽑는 문제”가 아니라, “시스템을 설계하는 문제”**다.

- 올바른 데이터 프로토콜
- 다양한 메트릭과 서브그룹 분석
- 캘리브레이션·불확실성 추정
- 분포 드리프트·OOD 감지
- 신뢰구간·유의성 검정
- 운영·모니터링 파이프라인

이것들이 **함께 돌아갈 때**,  
오프라인 점수와 실제 운영 품질이 일치하는 **신뢰할 수 있는 딥러닝 시스템**을 구축할 수 있다.