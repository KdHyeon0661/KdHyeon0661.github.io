---
layout: post
title: DB - 분산 데이터베이스와 성능
date: 2025-02-12 21:20:23 +0900
category: DB
---
# 분산 데이터베이스와 성능

## 0. 분산 DB 성능을 구성하는 4항

분산 DB에서 관측 지연(응답시간)의 1차 근사:
$$
\text{Latency} \approx \underbrace{\text{Local CPU}}_{파싱/플랜/실행}
+ \underbrace{\text{Remote IO}}_{다른 샤드/리플리카 접근}
+ \underbrace{\text{Network}}_{왕복지연(RTT)\,\times\,\#차수}
+ \underbrace{\text{Sync Cost}}_{합의/복제(Quorum/2PC)}
$$

처리량(Throughput)은 보통 다음에 가깝다:
$$
\text{Throughput} \approx \sum_{i=1}^{S} \frac{1}{\text{ServiceTime}_i}
$$
여기서 \(S\)는 **병렬 샤드/노드 수**. 샤딩은 \(S\)를 키워 처리량을 올리지만, **네트워크/합의 비용**이 증가하면 이득이 둔화된다.

---

## 1. 샤딩(Sharding)과 쿼럼(Quorum)의 성능 수학

### 1.1 샤딩 키의 균등성·지역성·지정타격성

- **균등성(Uniformity)**: 해시 기반 키가 선호되는 이유. 샤드별 데이터/부하가 균등하면 **핫스팟**을 줄인다.
- **지역성(Locality)**: 범위 질의(기간/ID Range)에선 범위 샤딩이 유리(Scan 적음)하나 **특정 샤드 과열** 위험.
- **지정타격성(Directness)**: 키→샤드 매핑이 명확하면 라우팅이 상수 시간.

샤드 매핑의 간단 근사:
$$
\text{shard}(k) = \text{hash}(k) \bmod S
$$
균등 해시를 쓰면 기대 샤드 로드 분산 오차는 중심극한정리에 의해 \(O(\sqrt{N/S})\)로 감소(샤드가 많고 샘플이 클수록 균등해짐).

### 1.2 Quorum의 가용성과 충돌 회피

복제계수 \(N\), 읽기 쿼럼 \(R\), 쓰기 쿼럼 \(W\).
- **강한 일관성의 충분조건**(단순 모델):
$$
R + W > N \quad \land \quad W > \frac{N}{2}
$$
- 예: \(N=3, W=2, R=2\) → 읽기/쓰기 모두 과반 겹침 → 최신성 보장(네트워크 분할 시 가용성 손실 가능).

지연 예산:
- 합의 또는 쿼럼 수집은 최소 \(1 \sim 2\) RTT를 추가.
- 리전 간 왕복 \(RTT_{geo}\)가 80ms라면, 동기 합의 2 RTT ≈ 160ms **추가**.

---

## 2. CAP/ILC 직관과 일관성 레벨

- **CAP**: 네트워크 분할(Partition) 상황에서 **Consistency**와 **Availability** 중 택일(분산 시스템은 항상 P를 가정).
- **ILC(일관성–지연–비용) 트릴레마**: 낮은 지연과 일관성, 낮은 비용을 동시에 만족시키기 어려움.

실무 일관성 옵션:
- **Strong**(선형화) : 단일 복제 로그 헤드, 동기 확인 필요 → 지연↑, 의미 단순.
- **Bounded Staleness** : 지정 Staleness(시간/버전) 내에서 읽기.
- **Causal** : 인과 순서만 보장.
- **Eventual** : 결국 수렴, 최저 지연/최고 가용.

---

## 3. 아키텍처별 성능 특성(확장)

| 유형 | 쓰기 | 읽기 | 지연 | 운영 난이도 | 용례 |
|---|---|---|---|---|---|
| Master–Follower 복제 | Master 병목(수직확장 의존) | Follower 다중 확장 | 지역 Follower로 읽기 지연↓ | Failover·일관성 관리 | 읽기 편향 워크로드 |
| Peer-to-Peer(링/가십) | 샤드별 병렬, 충돌 합의 필요 | 지역 읽기 빠름 | 네트워크/합의 비용 | 충돌해결/수정 복잡 | 로그/센서/타임시리즈 |
| 샤딩 + 리플리카 | 샤드 병렬, 리플리카로 읽기 분산 | 샤드 로컬 읽기 | Cross-shard 시 늦음 | 라우팅/리밸런싱 | 커머스/대용량 OLTP |
| 글로벌 동기 DB(Spanner류) | 전역 트랜잭션·강일관성 | 지역 캐시로 보완 가능 | TrueTime·동기 합의 비용 | 고난도/고비용 | 글로벌 결제/재고 |

---

## 4. 실전 설계 절차 — “샤딩 키 → 라우팅 → 일관성 → 실패 모델 → 운영”

### 4.1 샤딩 키 선택 체크리스트
- **카디널리티**: 고유도 충분? (충분히 많은 버킷으로 분산되는가)
- **쿼리 패턴**: 조회가 키로 정확히 라우팅 가능한가? 범위 질의 비율?
- **핫스팟 회피**: 단조 증가 키(시간/번호) 단일 샤드로 몰리지 않는가 → 프리픽스 해시/버킷키 채택
- **조인 전략**: 로컬 조인이 핵심이면 **코로케이션 키** 통일

### 4.2 핫스팟 회피 키 예시
```js
// 예: userId 단조 증가 → 프리픽스 해시를 추가하여 균등화
function makeShardKey(userId, createdAt){
  const prefix = murmur3_32(userId) % 128; // 128 버킷
  return { sk_prefix: prefix, user_id: userId, created_at: createdAt };
}
// MongoDB 예: { sk_prefix: 0..127, user_id: asc, created_at: ts }
```

### 4.3 코로케이션(같은 샤드에 묶기)
- 주문/장바구니/결제 이벤트를 `customer_id`로 코로케이션 → **Cross-shard JOIN/트랜잭션** 최소화.

---

## 5. 시스템별 구체 예제

### 5.1 MongoDB 샤딩 — 키 설계·스플릿·밸런서

```js
// 1) 샤딩 활성화
sh.enableSharding("shop");

// 2) 샤딩 키 설계: 핫스팟 방지(해시) + 범위 질의(기간) 혼합은 다음과 같이 타협
//   - 실제 MongoDB는 복합 샤딩키에 대해 "Hashed + Range" 동시 사용 불가
//   - 대안: 해시 프리픽스 sk_prefix + 보조 인덱스(created_at)로 쿼리 효율 확보
db.orders.createIndex({ sk_prefix: 1, customer_id: 1, created_at: -1 });
sh.shardCollection("shop.orders", { sk_prefix: 1, customer_id: 1 });

// 3) 청크 스플릿(옵션): 특정 범위 쏠림 완화
sh.splitAt("shop.orders", { sk_prefix: 42, customer_id: 500000 });

// 4) 밸런서: 자동 균형(필요시 스케줄/스로틀 설정)
sh.setBalancerState(true)
```

**쿼리 라우팅**
```js
// 고객별 최근 주문 50건: 정밀 라우팅, 단일 샤드 히트
db.orders.find({ sk_prefix: hash(customerId)%128, customer_id: customerId })
         .sort({ created_at: -1 })
         .limit(50);
```

**주의**: 시간 범위 분석(월간 리포트)은 Cross-shard Aggregation이 되기 쉽다 → Spark/Presto·파이프라인으로 오프로드, 혹은 **Citus/ClickHouse** 등 별도 웨어하우스.

---

### 5.2 Cassandra — 일관성/파티션 키/TTL

Cassandra는 **파티션 키 설계와 일관성 레벨 선택**이 성능을 좌우한다.

```sql
-- 파티션 키: (tenant_id, user_bucket)로 균등화, 클러스터링: created_at desc
CREATE TABLE activity_log (
  tenant_id    text,
  user_bucket  int,
  user_id      text,
  created_at   timestamp,
  action       text,
  PRIMARY KEY ((tenant_id, user_bucket), user_id, created_at)
) WITH CLUSTERING ORDER BY (user_id ASC, created_at DESC);
```

- **버킷팅**: `user_bucket = murmur3(user_id) % 64` 로 파티션 키를 분산.
- **일관성 수준**:
  - 쓰기: `QUORUM` / 읽기: `LOCAL_QUORUM` (리전 내 강한 최신성에 준함)
  - 지연 민감 서비스: `LOCAL_ONE` 읽기(낮은 지연·낮은 일관)

```sql
-- 예: 지역(DC) 내 낮은 지연 읽기
CONSISTENCY LOCAL_ONE;
SELECT * FROM activity_log
WHERE tenant_id='acme' AND user_bucket=13 AND user_id='u42'
LIMIT 100;
```

TTL 기반 저비용 데이터만 Cassandra에, 강한 트랜잭션은 외부(OLTP)로.

---

### 5.3 CockroachDB/YugabyteDB — 글로벌 트랜잭션과 파티셔닝

CockroachDB(YugabyteDB YSQL)는 **분산 트랜잭션**과 자동 리밸런싱을 제공한다.

```sql
-- 지역 기반 파티셔닝: 고객 레코드를 리전에 고정(지연↓)
ALTER TABLE customer
PARTITION BY LIST (region) (
  PARTITION apac VALUES IN ('KR','JP','SG'),
  PARTITION emea VALUES IN ('DE','FR','UK'),
  PARTITION amer VALUES IN ('US','CA')
);

-- 파티션을 지역 노드에 핀(PIN)하여 로컬 쓰기·읽기 최소지연
ALTER PARTITION apac OF TABLE customer CONFIGURE ZONE USING num_replicas=5, constraints='[+region=apac]';
```

로컬 트랜잭션은 빠르고, **Cross-region 트랜잭션**은 TrueTime/Hybrid Logical Clock 계열 시간/합의 비용으로 지연↑.

---

### 5.4 Google Spanner 모델 직관

- **TrueTime(\(\epsilon\))**: 전역 시간 오차 상한. 커밋 시 오차 구간이 겹치지 않도록 **대기(wound-wait)** → 선형화 보장.
- 추가 지연 근사:
$$
\text{TT-Wait} \approx \epsilon + \text{RTT}_{quorum}
$$
- 장점: **강일관성 + 글로벌 동시성**. 단점: \(\epsilon\) 축소 위해 **하드웨어·GPS·원자시계·망 품질** 비용↑.

---

## 6. 분산 트랜잭션: 2PC vs SAGA

### 6.1 2PC(Two-Phase Commit)
- **강 일관성**, 지연↑, 코디네이터 장애 시 복구 복잡.
- 지연 근사: 준비(1 RTT) + 커밋(1 RTT) + 스토리지 동기 = 최소 2 RTT+

```sql
-- 의사 흐름
BEGIN;
-- 샤드 A: 준비(Prepare)
-- 샤드 B: 준비(Prepare)
-- 코디네이터: 모두 OK → 커밋 브로드캐스트
COMMIT;
```

### 6.2 SAGA(보상 트랜잭션)
- 로컬 트랜잭션의 **오케스트레이션**. 지연↓, **보상 로직** 복잡.
- 실패 시 보상 단계 호출(예: 결제 취소, 재고 원복).

```yaml
# 오케스트레이터(간략)
steps:
  - reserve_inventory(order_id)         # 실패시 compensate: release_inventory(order_id)
  - authorize_payment(order_id)         # 실패시 compensate: refund_payment(order_id)
  - create_shipment(order_id)           # 실패시 compensate: cancel_shipment(order_id)
```

**선정 기준**
- **강한 원자성** 필요/데이터 작은 범위 → 2PC(또는 단일 샤드-키)
- 글로벌·고부하·장애허용 요구 → SAGA + 멱등/재시도/보상

---

## 7. 지연 예산과 라우팅

지연 예산 예:
- **SLA p95 = 120ms** (KR 사용자)
  - 앱/네트워크(클라이언트↔엣지): 15ms
  - 엣지↔리전 게이트웨이: 10ms
  - **DB 왕복**: 70ms (내부 2호출 기준 35ms/호출)
  - 여유/큐/GC: 25ms

Cross-region 합의가 2 RTT(80ms×2=160ms)면 **SLA 불가** → 전략:
- 데이터 **지역 고정** + 리전 간 **비동기 복제**
- 강일관성 필요한 리소스만 **협소 범위 샤드**로 운용

---

## 8. 장애·리밸런싱 시 성능

### 8.1 재밸런싱(Rebalancing)
- 새 노드 추가 → **데이터 이동** 비용(네트워크/컴팩션/인덱스 재빌드)로 **일시적인 지연 상승**.
- 대응: **스루틀**(bandwidth cap), 야간 윈도우, Read Replica로 읽기 분산.

### 8.2 장애·분할 시나리오
- 네트워크 분할 → CAP 선택 강제. `R+W>N` 구조는 **일부 영역 Read/Write 불가**가 될 수 있음.
- 재시도 폭증(Thundering Herd) → **Exponential Backoff + Jitter** 필수.

---

## 9. 성능 안티패턴과 개선

| 안티패턴 | 증상 | 교정 |
|---|---|---|
| 단조 증가 키 샤딩 | 단일 샤드 핫스팟, 쓰기 큐 길어짐 | 해시 프리픽스/버킷팅, 시간-랜덤키 |
| Cross-shard JOIN 남발 | 네트워크 폭증, 지연↑ | 코로케이션 키 정규화, 사전 집계/서빙 |
| 글로벌 동기 트랜잭션 남용 | p95/99 지연 초과 | 지역화 설계, SAGA, 읽기 캐시 |
| 무한 확장 전제 | 노드 늘려도 지연↓ 안됨 | 합의/RTT 비용 모델링, 샤드-사이즈 상한 |
| 밸런서 과신 | 잦은 스플릿/마이그레이션 | 버킷/프리픽스 설계로 균등성 확보 |

---

## 10. 벤치마크·모니터링·회귀 방지

### 10.1 워크로드 모델
- 80/20 읽기/쓰기 · Hot/Cold 키 분포 · 단일샤드/크로스샤드 비율 · 지역/글로벌 비율

### 10.2 지표
- p50/p95/p99 지연, 타임아웃률, 샤드별 QPS/큐 길이
- 합의 RTT, 리더 변화율, 밸런싱 이동량, 압축/컴팩션 대기
- 충돌/보상 빈도(SAGA), 재시도 수·실패율

### 10.3 재현 스크립트(의사 코드)
```python
import random, time
from collections import defaultdict

S = 64 # 샤드
def shard(k): return murmur3_32(k) % S

qps = 2000
hot_ratio = 0.1
hot_keys = [f"u{i}" for i in range(1000)] # 10% 트래픽이 이 범위

def pick_key():
    if random.random() < hot_ratio:
        return random.choice(hot_keys)
    return f"u{random.randint(1,10_000_000)}"

def run_once():
    k = pick_key()
    s = shard(k)
    # simulate local vs cross-shard
    # record latency histogram, shard QPS
    return s

dist = defaultdict(int)
for _ in range(100_000):
    dist[run_once()] += 1

# dist를 샤드별로 확인하여 균등성/핫스팟 점검
```

---

## 11. 실무 시나리오 — 설계→쿼리→운영까지

### 11.1 사용자 타임라인(읽기 많음, 지역 분산)
- 샤딩: `user_id` 해시 버킷 → 코로케이션.
- 읽기: **지역 리플리카** `LOCAL_QUORUM`/`NEAREST` 라우팅.
- 집계(추천/랭킹): 비동기 파이프라인(Spark/Flink)→ 별도 카운터/머티뷰.

```sql
-- CockroachDB: 지역 파티션 + 인덱스
CREATE INDEX ix_timeline_user ON timeline (user_id, created_at DESC) STORING (payload);
ALTER PARTITION apac OF INDEX timeline@ix_timeline_user CONFIGURE ZONE USING constraints='[+region=apac]';
```

### 11.2 주문/결제(강한 일관성·원자성 일부)
- 원장 계정(강일관성·작은 범위)만 글로벌 동기 트랜잭션(2PC/TrueTime).
- 나머지 이벤트는 SAGA로 비동기 처리(출고/알림/포인트 적립).

```yaml
# SAGA 보상 설계 핵심: 멱등키, 재시도 안전, DLQ
```

---

## 12. 운영 팁 모음

- **읽기 전용 분석**은 가급적 **웨어하우스/레이크하우스**(BigQuery/Snowflake/ClickHouse)로 오프로드.
- 리밸런싱/스플릿/컴팩션은 **윈도우 + 스루틀**, 앱 타임아웃은 **p95×2** 이상.
- **키 규칙 변경**은 새 스키마를 **양면 쓰기(dual write)** → 백필(Backfill) → 컷오버.
- 장애 훈련(분할·리더 손실·코디네이터 다운) **게임데이**를 정례화.

---

## 13. 간단 레시피(요약 체크리스트)

1) **샤딩 키**: 균등·코로케이션·범위 질의 타협(해시 프리픽스 + 보조 인덱스).
2) **일관성 레벨**: 서비스마다 Strong/Bounded/Local/Eventually를 명시.
3) **라우팅**: 단일 샤드 히트가 대부분이게 쿼리를 설계.
4) **트랜잭션**: 강원자 구간만 2PC, 나머지는 SAGA/멱등/보상.
5) **지연 예산**: RTT×합의차수 + 로컬 처리 시간을 숫자로 관리.
6) **장애/리밸런싱**: 스루틀·윈도우·알람·회귀 테스트.
7) **오프로드**: 분석/집계는 별도 경로(물질화뷰/웨어하우스).

---

## 14. 부록 — 자주 쓰는 설정/쿼리 스니펫

### 14.1 MongoDB 핫스팟 완화 키 생성
```js
function shardKey(userId){
  const prefix = murmur3_32(userId) % 256;
  return { sk_prefix: prefix, user_id: userId };
}
db.timeline.createIndex({ sk_prefix: 1, user_id: 1, created_at: -1 });
sh.shardCollection("app.timeline", { sk_prefix: 1, user_id: 1 });
```

### 14.2 Cassandra 일관성
```sql
CONSISTENCY LOCAL_QUORUM; -- 리전 내 최신성 확보, 지연 균형
```

### 14.3 CockroachDB 트랜잭션 재시도(낙관적 충돌 대응)
```sql
BEGIN;
  -- 작업...
COMMIT; -- 충돌 시 자동 RetryableError → 클라이언트 재시도 루프 필요
```

---

## 결론

- 분산 DB 성능은 **샤딩 키·일관성·네트워크·합의** 네 축의 계산으로 설명된다.
- “강한 일관성·글로벌”의 대가는 **RTT × 합의 차수**이며, 이를 줄일 설계(지역화·코로케이션·비동기화)가 핵심이다.
- 구조적 선택(샤딩/파티셔닝/일관성/트랜잭션 모델) → **지연 예산**에 수치로 반영하고,
  **벤치마크–모니터링–회귀 방지** 루프를 돌리면, 규모가 커질수록 **지속적으로 빠르게** 운영할 수 있다.

> 성능을 얻기 위해 무엇을 분산하고, 일관성을 어디까지 유지하며, 네트워크 비용을 어떻게 상쇄할 것인가 —
> 이 세 질문에 대한 수치적 답이 곧 분산 데이터베이스 설계의 완성이다.
