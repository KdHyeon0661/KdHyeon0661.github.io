---
layout: post
title: 기계학습 - 손실 함수
date: 2025-08-14 21:20:23 +0900
category: 기계학습
---
# 손실 함수(Loss Function)

머신 러닝에서 **손실 함수(Loss Function)**는 **모델의 예측값과 실제 정답 값 사이의 차이를 수치로 표현한 함수**입니다.  
모델 학습의 목표는 **손실 함수를 최소화하는 파라미터를 찾는 것**이며, 경사하강법(Gradient Descent)과 같은 최적화 알고리즘은 이 손실 함수를 기준으로 학습 방향을 결정합니다.

---

## 1. 손실 함수의 역할

1. **성능 측정**
   - 예측값과 실제값이 얼마나 다른지를 수치로 나타냄
   - 값이 작을수록 모델의 예측이 정확

2. **최적화 기준 제공**
   - 경사하강법은 손실 함수의 기울기를 이용해 파라미터를 업데이트
   - 손실 함수의 형태가 학습 과정의 경로를 결정

3. **모델 특성 반영**
   - 회귀, 분류, 랭킹 등 문제 유형에 따라 손실 함수가 달라짐

---

## 2. 손실 함수와 비용 함수의 차이

- **손실 함수(Loss Function)**: **한 개의 데이터 샘플**에 대한 예측 오차
- **비용 함수(Cost Function)**: 전체 데이터셋 또는 배치에 대한 손실의 평균  
  예:
  $$
  J(\theta) = \frac{1}{m} \sum_{i=1}^m L(y_i, \hat{y}_i)
  $$

---

## 3. 주요 손실 함수 종류

### (1) 회귀(Regression)에서의 손실 함수

#### ① 평균 제곱 오차(Mean Squared Error, MSE)
- 정의:
  $$
  L_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
  $$
- 특징:
  - 큰 오차에 더 큰 패널티 부여(제곱 때문)
  - 이상치(Outlier)에 민감
- 활용: 선형 회귀(Linear Regression)

---

#### ② 평균 절대 오차(Mean Absolute Error, MAE)
- 정의:
  $$
  L_{\text{MAE}} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
  $$
- 특징:
  - 이상치에 덜 민감
  - 미분 불가능한 지점(0) 존재 → 최적화 시 주의
- 활용: 이상치가 많은 데이터 회귀 문제

---

#### ③ Huber Loss
- 정의:
  $$
  L_\delta =
  \begin{cases}
  \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \le \delta \\
  \delta |y - \hat{y}| - \frac{1}{2} \delta^2 & \text{otherwise}
  \end{cases}
  $$
- 특징:
  - 작은 오차 → MSE처럼 작용
  - 큰 오차 → MAE처럼 작용
- 활용: 이상치가 일부 있는 회귀 문제

---

### (2) 분류(Classification)에서의 손실 함수

#### ① 로지스틱 손실(Log Loss) / 이진 크로스 엔트로피(Binary Cross-Entropy)
- 정의:
  $$
  L_{\text{BCE}} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
  $$
- 특징:
  - 확률 예측을 정확히 반영
  - 예측 확률이 정답에서 멀어질수록 큰 페널티
- 활용: 이진 분류(Logistic Regression, Binary Classification)

---

#### ② 다중 클래스 크로스 엔트로피(Multi-class Cross-Entropy)
- 정의:
  $$
  L_{\text{CE}} = - \frac{1}{n} \sum_{i=1}^n \sum_{c=1}^C y_{i,c} \log(\hat{y}_{i,c})
  $$
  - \(C\): 클래스 개수
  - \(y_{i,c}\): 원-핫 인코딩된 실제 값
  - \(\hat{y}_{i,c}\): 예측 확률
- 활용: 소프트맥스 회귀(Softmax Regression), 신경망 다중 클래스 분류

---

#### ③ 힌지 손실(Hinge Loss)
- 정의:
  $$
  L_{\text{hinge}} = \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i \hat{y}_i)
  $$
- 특징:
  - SVM(Support Vector Machine)에서 사용
  - 마진 기반 분류

---

### (3) 기타 특수 목적 손실 함수
- **KL 발산(Kullback–Leibler Divergence)**:
  - 확률분포 \(P\)와 \(Q\)의 차이를 측정
  - 강화학습, 확률 모델 학습에 활용
- **코사인 유사도 손실(Cosine Similarity Loss)**:
  - 벡터 방향의 유사성을 기반으로 하는 손실
  - 추천 시스템, 문서 임베딩

---

## 4. 손실 함수 선택 가이드

| 문제 유형 | 권장 손실 함수 | 특징 |
|-----------|---------------|------|
| 회귀(오차 작음) | MSE | 큰 오차에 민감 |
| 회귀(이상치 존재) | MAE / Huber | 이상치 영향 완화 |
| 이진 분류 | BCE | 확률 예측 정확 |
| 다중 분류 | Cross-Entropy | 클래스 확률 기반 |
| 마진 기반 분류 | Hinge | SVM 최적화 |

---

## 5. 파이썬 예제
```python
import numpy as np

# 예시 데이터
y_true = np.array([1, 0, 1])
y_pred_prob = np.array([0.9, 0.2, 0.8])  # 예측 확률

# Binary Cross-Entropy
bce_loss = -np.mean(y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1 - y_pred_prob))
print("Binary Cross-Entropy Loss:", bce_loss)

# MSE
mse_loss = np.mean((y_true - y_pred_prob) ** 2)
print("Mean Squared Error:", mse_loss)
```

---

## 📌 정리
- 손실 함수는 **모델 학습의 나침반**
- 회귀, 분류, 특수 목적에 따라 다양한 형태 존재
- 올바른 손실 함수 선택은 모델 성능에 직접적인 영향을 줌
- 경사하강법은 손실 함수의 기울기를 이용해 파라미터를 최적화

손실 함수를 잘 이해하면 모델 학습 과정을 **"어디로 가야 하는지 알려주는 지도"**로 볼 수 있습니다.