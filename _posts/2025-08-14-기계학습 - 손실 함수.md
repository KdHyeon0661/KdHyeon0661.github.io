---
layout: post
title: 기계학습 - 손실 함수
date: 2025-08-14 21:20:23 +0900
category: 기계학습
---
# 손실 함수(Loss Function)

## 손실·비용·위험: 개념과 관계

- **손실 함수 \(L(y,\hat y)\)**: 단일 샘플의 예측 오차를 수치화.
- **비용 함수 \(J(\theta)\)**: 데이터셋(또는 배치) 평균 손실.
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^m L\big(y_i,\ \hat y_\theta(x_i)\big)
$$
- **(경험)위험 \(R(\theta)\)**: 모집단 기대 손실(관측 불가), ERM은 \(J\) 최소화로 근사.
- **정규화 목적**: \(J(\theta)+\lambda\,\Omega(\theta)\) (예: \(\Omega=\|w\|_2^2\) 등).

---

## 손실

### 평균제곱오차(MSE)

$$
L_{\text{MSE}}=\frac{1}{n}\sum_{i=1}^n(y_i-\hat y_i)^2,\quad
\nabla_{\hat y_i} L_{\text{MSE}}=\frac{2}{n}(\hat y_i-y_i)
$$
- **특징**: 이상치에 민감, 매끄럽고(1-스무스) 2차 최적화 친화.
- **확률 해석**: \(y\mid x\sim\mathcal{N}(\hat y,\sigma^2)\)의 음의 로그우도.

### 평균절대오차(MAE)

$$
L_{\text{MAE}}=\frac{1}{n}\sum_{i=1}^n|y_i-\hat y_i|,\quad
\partial_{\hat y_i}=\frac{1}{n}\,\mathrm{sign}(\hat y_i-y_i)\ (\text{0에서 서브그래디언트})
$$
- **특징**: 이상치 견고, 비매끄러움(0에서 미분 불능).
- **확률 해석**: 라플라스 노이즈.

### Huber / Pseudo-Huber

$$
L_\delta(e)=
\begin{cases}
\frac{1}{2}e^2,& |e|\le\delta\\
\delta|e|-\frac{1}{2}\delta^2,& |e|>\delta
\end{cases},\quad e=y-\hat y
$$
- 작은 오차엔 MSE, 큰 오차엔 MAE처럼 → **로버스트** + **스무스**.

### 분위수(Quantile/Pinball)

$$
L_\tau(e)=\max(\tau e,\ (\tau-1)e)=
\begin{cases}
\tau e,& e\ge 0\\
(\tau-1)e,& e<0
\end{cases}
$$
- **\(\tau\)**-분위수 회귀(예: 0.5는 중위수). 불확실성 대역 예측에 유용.

### 포아송/로그-링크(카운트)

- 포아송 음의 로그우도(정규화 상수 제외):
$$
L=\hat\mu - y\log\hat\mu,\ \hat\mu=\exp(\eta),\ \eta=w^\top x
$$
- **Tweedie**(포아송-감마 혼합) 등도 실무 보험/카운트에서 사용.

---

## 손실

### 이진 BCE (확률/로짓 별표기 주의)

- **확률 기반**(불안정할 수 있음):
$$
L_{\text{BCE}}=-\frac{1}{n}\sum_i\big[y_i\log p_i+(1-y_i)\log(1-p_i)\big],\quad p_i\in(0,1)
$$
- **로짓 \(z\) 기반(권장, 수치 안정)**:
$$
L=-\frac{1}{n}\sum_i \big[y_i\log\sigma(z_i)+(1-y_i)\log(1-\sigma(z_i))\big],\
\sigma(z)=\frac{1}{1+e^{-z}}
$$
- **그라디언트**:
$$
\nabla_z L=\frac{1}{n}\big(\sigma(z)-y\big)
$$

### 다중 클래스 Softmax-CE

$$
p_{i,c}=\frac{e^{z_{i,c}}}{\sum_{k}e^{z_{i,k}}},\
L_{\text{CE}}=-\frac{1}{n}\sum_{i}\sum_{c}y_{i,c}\log p_{i,c}
$$
- **원-핫 \(y\)**이면 \(\nabla_{z_{i}} L = p_i - y_i\).
- **안정성**: log-sum-exp로 구현(아래 §9).

### 힌지/소프트마진(SVM)

$$
L_{\text{hinge}}=\frac{1}{n}\sum_i\max(0,1-y_i f(x_i)),\quad y_i\in\{-1,+1\}
$$
- 마진 최적화. 비매끄러움 → 서브그래디언트로 학습.

### Focal Loss(클래스 불균형/하드 예제 강조)

- 이진:
$$
\text{FL}(p_t)=-(1-p_t)^\gamma \log(p_t),\quad
p_t=\begin{cases}p,&y=1\\1-p,&y=0\end{cases}
$$
- \(\gamma>0\)가 쉬운 샘플의 기여를 억제, \(\alpha\)로 클래스 가중 추가 가능.

### 라벨 스무딩(Label Smoothing)

- 원-핫 \(y\)를
$$
y^{(\epsilon)}=(1-\epsilon)\,y\ +\ \frac{\epsilon}{C}\,\mathbf{1}
$$
로 바꿔 CE 최적화 → 과신(calibration) 완화·일반화 개선.

---

## 다중라벨·멀티클래스·서열형(Ordinal)

- **멀티클래스**: softmax-CE(클래스 하나).
- **다중라벨**: 클래스별 독립 **BCE(With-Logits)**.
- **서열형**: CORN/Ordinal-CE 등 **순서 제약** 반영 손실 사용(개념만).

---

## 랭킹·추천·임베딩 손실(요약)

- **Pairwise hinge/logistic**: \((u,v)\) 선호쌍에 대해 \(s_u>s_v\) 유도
  - **BPR**(Bayesian Personalized Ranking):
  $$L=-\sum\log\sigma\big(s(u,i)-s(u,j)\big)$$
- **Triplet**: \(\max(0,\ d(a,p)-d(a,n)+m)\)
- **Contrastive/InfoNCE**(자기지도·멀티모달):
$$
L=-\log\frac{\exp(\mathrm{sim}(q,k^+)/\tau)}{\sum_j \exp(\mathrm{sim}(q,k_j)/\tau)}
$$

---

## 세그멘테이션·검출

- **Dice(Jaccard) 손실**(집합 유사도):
$$
\text{Dice}=\frac{2\sum p y + \epsilon}{\sum p + \sum y + \epsilon},\quad
L=1-\text{Dice}
$$
- **IoU(Jaccard)**:
$$
\text{IoU}=\frac{\sum p y + \epsilon}{\sum p+\sum y-\sum p y + \epsilon},\quad L=1-\text{IoU}
$$
- **실무**: **BCE + Dice** 조합이 자주 쓰임(확률 학습 + 경계 민감).

---

## 손실 ↔ 확률모형 ↔ Proper Scoring

- **MSE ↔ Gaussian**, **MAE ↔ Laplace**, **BCE/CE ↔ Bernoulli/Categorical** 음의 로그우도.
- **Proper Scoring Rule**: 진짜 분포 \(P\)에 대해 \(Q=P\)에서 기대 손실이 최소(CE/LogLoss는 proper).
- **베이지안 시각**: 정규화는 사전분포(MAP)로 자연스럽게 해석 가능.

---

## 그라디언트·헤시안 (자주 쓰는 모음)

- **MSE**:
$$
\nabla_{\hat y} \tfrac{1}{2}\|y-\hat y\|_2^2 = \hat y - y
$$
- **로지스틱(BCE-Logits)**:
$$
\nabla_z L = \sigma(z)-y,\quad \nabla_w L = X^\top(\sigma(Xw)-y)/n
$$
- **Softmax-CE**:
$$
\nabla_{Z} L = (P-Y)/n
$$
- **Huber**(스칼라 \(e\)):
$$
\partial_{\hat y} L_\delta(e)=
\begin{cases}
-(y-\hat y),& |e|\le\delta\\
-\delta\,\mathrm{sign}(y-\hat y),&|e|>\delta
\end{cases}
$$

---

## 수치안정성 & 구현 팁

### log-sum-exp

$$
\log\sum_k e^{z_k} = z_{\max} + \log\sum_k e^{z_k-z_{\max}}
$$

### BCE with logits(권장 구현)

- 확률로 변환 후 \(\log\) 금지(언더플로).
- **안정형**:
$$
\ell(z,y)=\max(z,0)-z\,y+\log\big(1+e^{-|z|}\big)
$$

### 안정형

$$
L=-\sum_c y_c\big(z_c - \log\sum_k e^{z_k}\big)
$$
를 log-sum-exp로 구현.

---

## 클래스 불균형·가중치·비용민감·리덕션

- **가중 BCE/CE**: 클래스별 \(w_c\) 또는 샘플별 \(w_i\) 적용.
$$
J=\frac{1}{\sum_i w_i}\sum_i w_i\,L_i
$$
- **Focal**: 쉬운 샘플 억제.
- **리덕션**: `"mean"|"sum"| "none"` — 배치 크기 변화에 일관성 유지.

---

## 라벨 노이즈·과신·정규화

- **라벨 스무딩**: \( \epsilon \)만큼 균등 분산 → 과신 감소·Calibration 개선.
- **Mixup/CutMix**: 입력/라벨 선형 혼합 → 손실은 혼합 가중 CE.
- **정규화 연결**: 스무딩은 implicit L2와 유사한 효과로 해석되기도 함.

---

## vs 손실(loss)

- **지표**(정확도, F1, AUC, IoU)는 **비미분/비선형**인 경우가 많아 **직접 최적화 어려움**.
- 대개 **미분 가능한 손실**(CE, Dice 등)로 **지표 상관** 높은 대리 목적을 최적화.
- 예: **AUC**는 pairwise surrogate(랭킹 손실)로 근사.

---

## 파이썬 코드 모음

### NumPy — 안정 BCEWithLogits / Softmax-CE / Huber / Quantile

```python
import numpy as np

def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))

def bce_with_logits(z, y, weights=None, reduction="mean"):
    # z: (n,), logits; y: (n,), {0,1}
    # stable: max(z,0) - z*y + log(1+exp(-|z|))
    loss = np.maximum(z, 0) - z*y + np.log1p(np.exp(-np.abs(z)))
    if weights is not None: loss = loss * weights
    if reduction == "mean": return loss.mean()
    if reduction == "sum":  return loss.sum()
    return loss  # none

def softmax_cross_entropy(Z, Y_onehot, reduction="mean"):
    # Z: (n,C) logits, Y_onehot: (n,C)
    Zmax = Z.max(axis=1, keepdims=True)
    logsumexp = Zmax + np.log(np.exp(Z - Zmax).sum(axis=1, keepdims=True))
    ce = -(Y_onehot * (Z - logsumexp)).sum(axis=1)
    if reduction == "mean": return ce.mean()
    if reduction == "sum":  return ce.sum()
    return ce

def huber_loss(y, yhat, delta=1.0, reduction="mean"):
    e = y - yhat
    mask = np.abs(e) <= delta
    loss = np.where(mask, 0.5*e**2, delta*np.abs(e) - 0.5*delta**2)
    return loss.mean() if reduction=="mean" else (loss.sum() if reduction=="sum" else loss)

def quantile_loss(y, yhat, tau=0.5, reduction="mean"):
    e = y - yhat
    loss = np.maximum(tau*e, (tau-1)*e)
    return loss.mean() if reduction=="mean" else (loss.sum() if reduction=="sum" else loss)
```

### / Focal / Dice

```python
import torch
import torch.nn.functional as F

def bce_with_logits_torch(z, y, weight=None, reduction="mean"):
    return F.binary_cross_entropy_with_logits(z, y, weight=weight, reduction=reduction)

def ce_softmax_torch(Z, y_long, reduction="mean"):
    return F.cross_entropy(Z, y_long, reduction=reduction)  # 내부적으로 log-sum-exp 안정화

def focal_loss_binary(z, y, gamma=2.0, alpha=None, reduction="mean"):
    # z: logits, y: {0,1}
    p = torch.sigmoid(z)
    pt = y*p + (1-y)*(1-p)
    w = (1-pt).pow(gamma)
    if alpha is not None:
        w = w * (alpha*y + (1-alpha)*(1-y))
    loss = F.binary_cross_entropy_with_logits(z, y, reduction="none") * w
    if reduction=="mean": return loss.mean()
    if reduction=="sum": return loss.sum()
    return loss

def dice_loss_binary(p, y, eps=1e-7, reduction="mean"):
    # p: probs in [0,1] (apply sigmoid before); y: {0,1}
    intersection = (p*y).sum(dim=(1,2,3))
    union = p.sum(dim=(1,2,3)) + y.sum(dim=(1,2,3))
    dice = (2*intersection + eps) / (union + eps)
    loss = 1 - dice
    return loss.mean() if reduction=="mean" else (loss.sum() if reduction=="sum" else loss)
```

### 비용민감/가중 샘플의 평균 정의

```python
import numpy as np

def weighted_mean(loss, weights=None):
    if weights is None: return loss.mean()
    w = np.asarray(weights)
    return (loss * w).sum() / (w.sum() + 1e-12)
```

### CE + 라벨 스무딩

```python
def ce_with_label_smoothing(Z, y_long, epsilon=0.1):
    # Z: (n,C), y_long: (n,)
    n, C = Z.shape
    with np.errstate(over='ignore'):
        Zmax = Z.max(axis=1, keepdims=True)
        logsumexp = Zmax + np.log(np.exp(Z - Zmax).sum(axis=1, keepdims=True))
        logp = Z - logsumexp
    Yls = np.full((n, C), epsilon / C)
    Yls[np.arange(n), y_long] += (1 - epsilon)
    ce = -(Yls * logp).sum(axis=1).mean()
    return ce
```

---

## 체크리스트와 요약

### 체크리스트

- [ ] **문제 유형**: 회귀/이진/다중/다중라벨/랭킹/세그멘테이션 구분
- [ ] **손실–분포 정합**: MSE–Gaussian, BCE/CE–Bernoulli/Categorical, MAE–Laplace, Poisson–카운트
- [ ] **수치안정성**: BCEWithLogits, log-sum-exp, 언더/오버플로 방지
- [ ] **그라디언트**: \(\nabla\) 형태 숙지(CE: \(p-y\), BCE-Logits: \(\sigma(z)-y\))
- [ ] **불균형/가중치**: 클래스/샘플 가중, Focal, 리덕션 일관성
- [ ] **노이즈/과신**: 라벨 스무딩, Mixup/CutMix, 정규화 병행
- [ ] **지표와의 연결**: F1/AUC/IoU 등은 대리 손실로 근사
- [ ] **검증**: 학습/검증 손실, Calibration(리라이어빙), 오버핏 모니터링

### 요약

- 손실 함수는 모델 학습의 **목적**이자 **나침반**이다.
- 문제와 데이터의 통계적 특성, 클래스 불균형, 노이즈 수준을 반영해 **적절한 손실**을 고르고, **수치안정성**을 갖춘 구현(BCEWithLogits, log-sum-exp)으로 최적화하라.
- 지표는 설명용/평가지표일 뿐, **학습은 미분 가능한 대리 손실**로 수행하는 것이 일반적이다.
- 라벨 스무딩·Focal·Dice·Contrastive 등 **현업 손실 레시피**를 숙지하면, 다양한 도메인(회귀·분류·랭킹·세그멘테이션·추천·자기지도)에서 **일반화 성능**을 크게 끌어올릴 수 있다.
