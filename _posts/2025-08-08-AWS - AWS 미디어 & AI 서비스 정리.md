---
layout: post
title: AWS - AWS 미디어 & AI 서비스 정리
date: 2025-08-08 23:30:23 +0900
category: AWS
---
# AWS 미디어 & AI 서비스 심층 정리

## 0. 한 장 요약 (무엇을 언제 쓰나)

| 분류 | 서비스 | 핵심 역할 | 언제 적합한가 |
|---|---|---|---|
| 파일 트랜스코딩(기본) | **Elastic Transcoder** | S3 파일을 간단히 다른 포맷/해상도로 변환 | **소규모**/간단 파이프라인, 정형 프리셋 |
| 파일 트랜스코딩(방송급) | **AWS Elemental MediaConvert** | 4K/HDR/HEVC, 다중 오디오, 자막, ABR 패키징 | **OTT/방송** 품질·정책·DRM이 필요한 경우 |
| 비전 AI | **Amazon Rekognition** | 객체·얼굴·부적절 콘텐츠·텍스트(OCR) | UGC 검수, Moderation, 검색/탐색, 분석 |
| NLP | **Amazon Comprehend** | 감정/키워드/개체/언어/주제 모델링 | 리뷰 분석, 분류·요약 파이프라인, 태깅 |

---

## 1. 공통 준비: 보안·권한·아키텍처

### 1.1 IAM 최소 권한(샘플 정책)
```json
{
  "Version": "2012-10-17",
  "Statement": [
    { "Effect": "Allow", "Action": ["s3:GetObject","s3:PutObject","s3:ListBucket"], "Resource": [
      "arn:aws:s3:::media-input-bucket",
      "arn:aws:s3:::media-input-bucket/*",
      "arn:aws:s3:::media-output-bucket",
      "arn:aws:s3:::media-output-bucket/*"
    ]},
    { "Effect": "Allow", "Action": [
      "elastictranscoder:*", "mediaconvert:*", "rekognition:*", "comprehend:*"
    ], "Resource": "*" },
    { "Effect": "Allow", "Action": ["iam:PassRole"], "Resource": "arn:aws:iam::123456789012:role/MediaServiceRole" }
  ]
}
```
- 실무에선 `*` 대신 **작업 단위별 API**를 좁히고, 서비스별 역할을 **분리**한다.
- S3는 **버킷 정책**으로 업로드 위치/접근 소스 제한, **KMS(SSE-KMS)** 사용 권장.

### 1.2 이벤트 기반 아키텍처(샘플)
```mermaid
flowchart LR
  U[Uploader] -->|S3 Put| S3[(S3 media-input)]
  S3 -->|Event| L1[Lambda Orchestrator]
  L1 -->|decide| ET[Elastic Transcoder]
  L1 -->|or| MC[MediaConvert Job]
  MC --> S3out[(S3 media-output)]
  S3out --> CF[CloudFront Distribution]
  S3out --> L2[Lambda Moderation]
  L2 --> RK[Rekognition (Labels/Moderation/OCR)]
  L2 --> DB[(DynamoDB/ES)]
  TXT[(Text Docs)] --> CP[Comprehend]
  CP --> DB
```

---

## 2. Elastic Transcoder — “간단 변환” 파이프라인

### 2.1 개념
- **파일 기반 트랜스코딩** 서비스. 입력/출력은 S3.  
- **프리셋**(디바이스별 권장 설정)이 많아 **빠르게 시작**하기 좋다.

### 2.2 파이프라인 생성(최소)
1) S3 입력/출력 버킷 준비 (`media-input-bucket`, `media-output-bucket`).  
2) 콘솔 또는 CLI로 **Pipeline** 생성 → **Job** 제출.

#### 파이프라인 생성 (CLI)
```bash
aws elastictranscoder create-pipeline \
  --name "basic-pipeline" \
  --input-bucket media-input-bucket \
  --output-bucket media-output-bucket \
  --role arn:aws:iam::123456789012:role/MediaServiceRole
```

#### Job 제출 (CLI)
```bash
aws elastictranscoder create-job \
  --pipeline-id 1234567890abcdef \
  --input Key="input/video.mp4" \
  --output Key="output/video-720p.mp4",PresetId="1351620000001-000010"
```
- `PresetId`는 720p/1080p, 모바일, Web 등 **AWS 제공 프리셋**을 사용하거나 커스텀을 지정.

### 2.3 Lambda로 자동화 (S3 업로드 트리거)
```python
# lambda_function.py
import json, os, boto3
ets = boto3.client("elastictranscoder")
PIPELINE_ID = os.environ["PIPELINE_ID"]

def handler(event, context):
    for rec in event["Records"]:
        key = rec["s3"]["object"]["key"]
        if not key.lower().endswith((".mp4",".mov",".mkv",".webm",".m4v")):
            continue
        out = key.rsplit(".",1)[0] + "-720p.mp4"
        ets.create_job(
          PipelineId=PIPELINE_ID,
          Input={"Key": key},
          Outputs=[{"Key": out, "PresetId":"1351620000001-000010"}]
        )
    return {"ok": True}
```

### 2.4 운영 팁
- **간단/저비용** 파일 변환에 적합. 광고 삽입, DRM, 다중 오디오 등 **방송급 요구**는 MediaConvert를 고려.  
- 출력 버킷은 **버전 관리·Lifecycle**로 비용 절감.

---

## 3. AWS Elemental MediaConvert — 방송/OTT급 트랜스코딩

### 3.1 강점
- **HEVC/H.265**, 4K/HDR(10-bit), 다중 오디오/자막, **ABR 패키징(HLS/DASH/CMAF)**, **DRM(Widevine/PlayReady/FairPlay)**, 필터·노이즈 리덕션 등.  
- 파이프라인을 **정책**처럼 JSON으로 관리(IaC 친화).

### 3.2 IAM 역할 (필수)
MediaConvert가 S3에 접근·CloudWatch에 로깅하려면 **서비스 역할** 필요.

```json
{
  "Version":"2012-10-17",
  "Statement":[
    {"Effect":"Allow","Action":["s3:*"],"Resource":[
      "arn:aws:s3:::media-input-bucket","arn:aws:s3:::media-input-bucket/*",
      "arn:aws:s3:::media-output-bucket","arn:aws:s3:::media-output-bucket/*"
    ]},
    {"Effect":"Allow","Action":["logs:CreateLogGroup","logs:CreateLogStream","logs:PutLogEvents"],"Resource":"*"}
  ]
}
```

### 3.3 Job 설정(JSON) 예시
`job-settings.json` (H.264 1080p MP4 + 썸네일 + HLS ABR)
```json
{
  "Inputs": [
    {
      "FileInput": "s3://media-input-bucket/input/video.mp4",
      "TimecodeSource": "EMBEDDED"
    }
  ],
  "OutputGroups": [
    {
      "Name": "File Group",
      "OutputGroupSettings": {
        "Type": "FILE_GROUP_SETTINGS",
        "FileGroupSettings": { "Destination": "s3://media-output-bucket/file/" }
      },
      "Outputs": [
        {
          "ContainerSettings": { "Container": "MP4" },
          "VideoDescription": {
            "CodecSettings": { "Codec": "H_264",
              "H264Settings": { "Bitrate": 4500000, "RateControlMode": "QVBR", "GopSize": 2, "GopSizeUnits": "SECONDS" }
            },
            "Height": 1080, "Width": 1920
          },
          "AudioDescriptions": [
            {
              "CodecSettings": { "Codec": "AAC", "AacSettings": { "Bitrate": 128000, "CodingMode": "CODING_MODE_2_0", "SampleRate": 48000 } }
            }
          ]
        }
      ]
    },
    {
      "Name": "HLS ABR",
      "OutputGroupSettings": {
        "Type": "HLS_GROUP_SETTINGS",
        "HlsGroupSettings": {
          "Destination": "s3://media-output-bucket/hls/",
          "SegmentLength": 6,
          "MinSegmentLength": 0
        }
      },
      "Outputs": [
        { "NameModifier": "_360p", "VideoDescription": { "Height": 360, "Width": 640,
            "CodecSettings": { "Codec": "H_264", "H264Settings": { "Bitrate": 800000 } } },
          "AudioDescriptions": [{ "CodecSettings": { "Codec": "AAC", "AacSettings": { "Bitrate": 96000 } } }]
        },
        { "NameModifier": "_720p", "VideoDescription": { "Height": 720, "Width": 1280,
            "CodecSettings": { "Codec": "H_264", "H264Settings": { "Bitrate": 2500000 } } },
          "AudioDescriptions": [{ "CodecSettings": { "Codec": "AAC", "AacSettings": { "Bitrate": 128000 } } }]
        }
      ]
    }
  ],
  "TimecodeConfig": { "Source": "EMBEDDED" },
  "Settings": {}
}
```

### 3.4 작업 실행 (CLI)
```bash
aws mediaconvert create-job \
  --role arn:aws:iam::123456789012:role/MediaServiceRole \
  --settings file://job-settings.json
```

### 3.5 S3 이벤트로 자동 생성(파이썬)
```python
import boto3, os, json
mc = boto3.client("mediaconvert", endpoint_url=os.environ["MC_ENDPOINT"])

def handler(event, context):
    for rec in event["Records"]:
        key = rec["s3"]["object"]["key"]
        if not key.lower().endswith((".mp4",".mov",".mxf",".mkv")): continue
        job = {
          "Role": os.environ["MC_ROLE_ARN"],
          "Settings": json.load(open("/var/task/job-settings.json"))
        }
        # 동적으로 입력/출력 경로 교체
        job["Settings"]["Inputs"][0]["FileInput"] = f"s3://{rec['s3']['bucket']['name']}/{key}"
        mc.create_job(**job)
    return {"ok": True}
```

### 3.6 품질/비용 팁
- **QVBR**(Quality-defined Variable Bitrate)로 화질 대비 비트레이트 최적화.  
- ABR Ladder(360p/480p/720p/1080p) **비트레이트 계단**을 사용자 디바이스/네트워크 통계로 조정.  
- DRM/자막/다중 오디오가 필요하면 MediaConvert 선택.

#### 용량 대략치
$$
\text{Size (bytes)} \approx \frac{\text{VideoBitrate}+ \text{AudioBitrate}}{8} \times \text{Duration (sec)}
$$

---

## 4. Amazon Rekognition — 이미지/영상 인식

### 4.1 주요 기능별 API
- **DetectLabels**: 객체/장면 라벨  
- **DetectModerationLabels**: 부적절 콘텐츠 필터  
- **DetectText**: 이미지 OCR (인쇄체 중심)  
- **Face APIs**: 얼굴 감지/분석/비교/컬렉션 인덱싱  
- **Video**: S3 기반 비동기 영상 분석(Job 기반)

### 4.2 이미지 라벨 감지 (CLI/Python)
```bash
aws rekognition detect-labels \
  --image "S3Object={Bucket=media-input-bucket,Name=frames/img001.jpg}" \
  --max-labels 10 --min-confidence 80
```

```python
import boto3
rk = boto3.client("rekognition")
resp = rk.detect_labels(
  Image={"S3Object":{"Bucket":"media-input-bucket","Name":"frames/img001.jpg"}},
  MaxLabels=10, MinConfidence=80.0
)
for lbl in resp["Labels"]:
    print(lbl["Name"], lbl["Confidence"])
```

### 4.3 Moderation 파이프라인(UGC 검수)
```python
def moderate_image(bucket, key, threshold=80.0):
    resp = rk.detect_moderation_labels(Image={"S3Object":{"Bucket":bucket,"Name":key}})
    flags = [x for x in resp["ModerationLabels"] if x["Confidence"] >= threshold]
    return {"flagged": bool(flags), "labels": flags}
```
- 결과를 DynamoDB/ES에 저장하여 **검수 대시보드** 구성.  
- **거짓 양성/음성** 대응을 위한 **수동 리뷰 큐**(Step Functions + SNS) 권장.

### 4.4 영상 분석(예: 라벨)
```bash
# Start
aws rekognition start-label-detection \
  --video "S3Object={Bucket=media-input-bucket,Name=video.mp4}" \
  --notification-channel "SNSTopicArn=arn:aws:sns:...,RoleArn=arn:aws:iam::...:role/rek-sns-role"
# Get
aws rekognition get-label-detection --job-id <job-id> --max-results 1000 --sort-by TIMESTAMP
```

### 4.5 OCR (DetectText)
```python
resp = rk.detect_text(Image={"S3Object":{"Bucket":"media-input-bucket","Name":"poster.png"}})
texts = [d["DetectedText"] for d in resp["TextDetections"] if d["Type"]=="LINE"]
print(texts)
```

### 4.6 운영 팁
- **비용 최적화**: 프레임 샘플링(영상), 해상도 축소, confidence 임계값 조정.  
- 개인 정보(얼굴) 사용은 **법적/윤리** 기준 준수, 키 관리(KMS)·로그 감사(CloudTrail) 필수.

---

## 5. Amazon Comprehend — NLP 파이프라인

### 5.1 단건 분석(감정/키워드/개체/언어)
```bash
aws comprehend detect-sentiment --language-code "ko" --text "서비스 품질이 매우 만족스럽다."
aws comprehend detect-entities --language-code "ko" --text "도시는 서울이고, 회사는 아마존입니다."
```

```python
import boto3
cp = boto3.client("comprehend")
text = "이 서비스는 정말 훌륭하지만 응답 속도는 개선이 필요해요."
print(cp.detect_sentiment(Text=text, LanguageCode="ko"))
print(cp.detect_key_phrases(Text=text, LanguageCode="ko"))
```

### 5.2 배치/비동기 작업 (S3 I/O)
```bash
aws comprehend start-key-phrases-detection-job \
  --language-code ko \
  --input-data-config S3Uri=s3://text-input-bucket/docs/,InputFormat=ONE_DOC_PER_FILE \
  --output-data-config S3Uri=s3://text-output-bucket/keyphrases/ \
  --data-access-role-arn arn:aws:iam::123456789012:role/ComprehendAccessRole
```

### 5.3 사용자 정의 분류/개체 인식(Custom)
- **Custom Classification/Entity Recognition**로 도메인 특화 레이블 학습.  
- 학습/검증 데이터 S3 제공 → 모델 배포 → **실시간/배치**로 사용.

### 5.4 파이프라인 설계 팁
- 전처리(언어 감지 → 문장 분리 → 토큰화), **PII Redaction** 옵션 검토.  
- 분석 결과를 DynamoDB/ES/Redshift에 적재, QuickSight 대시보드로 시각화.

---

## 6. 서버리스 “UGC 업로드 → 트랜스코딩 → 검수 → 게시” 레퍼런스

### 6.1 흐름
1) 사용자 업로드(S3 presigned URL).  
2) **Lambda Orchestrator**가 확장 규칙에 따라 **MediaConvert/Elastic Transcoder**에 작업 의뢰.  
3) 완료 시 S3 출력 → 썸네일/ABR HLS 준비.  
4) **Rekognition Moderation**으로 유해물 검수.  
5) 합격 건은 **CloudFront(OAC)**로 공개, 실패 건은 관리자 큐로.

### 6.2 IaC(요약; CDK/TF 권장)
- S3 입력/출력 버킷(+KMS), CloudFront 배포, WAF(필터), Lambda 2종(오케스트레이터·모더레이터), MediaConvert Role/Endpoint, DynamoDB(메타데이터), SNS/SQS(알림/리뷰).

---

## 7. 비용/성능 계산 감 잡기

### 7.1 트랜스코딩 시간·비용 근사
$$
\text{Cost} \approx \sum_i \text{Rate}_i \times \text{Duration}_i
$$
- 프로파일(i)별 단가 × 입력 길이(또는 출력 길이).  
- 해상도/코덱/옵션에 따라 단가가 달라지므로 **가격표와 실제 사용량**으로 보정.

### 7.2 대역폭/스토리지
$$
\text{Egress(GB)} \approx \frac{\text{Avg Bitrate (bps)}}{8} \times \text{Watch Time (sec)} \div 10^9
$$
- CloudFront 캐시 히트율 개선은 **이 값**을 직접 줄인다.

---

## 8. 운영·보안·품질 모범 사례

1) **KMS(버킷·로그)**: 입력/출력/로그에 KMS 적용, 키 정책 분리.  
2) **태깅/수명주기**: `Project/Owner/TTL` 태그와 S3 Lifecycle로 비용 통제.  
3) **CloudWatch/CloudTrail**: 작업 실패율·큐 적체·오류 원인 추적.  
4) **재시도/백오프**: 미디어 길이/해상도에 따라 처리 시간이 길 수 있으니 Job 상태를 폴링/이벤트 기반으로 견고화.  
5) **ABR Ladder 최적화**: 사용자 디바이스/네트워크 통계를 반영해 프로파일/비트레이트/세그먼트 길이 조정.  
6) **Moderation 워크플로**: 자동 판정 + 휴먼 리뷰. 정책/증거(썸네일/타임코드) 저장.  
7) **NLP 품질 관리**: 표본 셋을 정기 검증, 커스텀 모델로 도메인 적합도 향상.  
8) **네트워크 가속**: 글로벌 업로드면 **S3 Transfer Acceleration** 고려.  
9) **배포 경로 보호**: CloudFront **OAC**로 오리진 직공개 금지, 서명 URL/쿠키로 접근 제어.  
10) **DRM/암호화**: OTT면 MediaConvert + **DRM 키 공급 체인** 준비.

---

## 9. 빠른 스타트 패키지 (복붙용)

### 9.1 Python: 업로드 → MediaConvert → Rekognition Moderation
```python
import boto3, json, os
s3 = boto3.client("s3")
mc = boto3.client("mediaconvert", endpoint_url=os.environ["MC_ENDPOINT"])
rk = boto3.client("rekognition")
ROLE = os.environ["MC_ROLE_ARN"]
IN_BUCKET = "media-input-bucket"
OUT_PREFIX = "hls/"
JOB_TEMPLATE = json.load(open("job-settings.json"))

def start_job(key:str):
    job = {"Role": ROLE, "Settings": JOB_TEMPLATE}
    job["Settings"]["Inputs"][0]["FileInput"] = f"s3://{IN_BUCKET}/{key}"
    return mc.create_job(**job)["Job"]["Id"]

def moderate_thumbnail(bucket:str, key:str, threshold=80):
    resp = rk.detect_moderation_labels(Image={"S3Object":{"Bucket":bucket,"Name":key}})
    return any(x["Confidence"]>=threshold for x in resp["ModerationLabels"])

def handler(event, context):
    for rec in event["Records"]:
        key = rec["s3"]["object"]["key"]
        if key.endswith(".mp4"):
            jid = start_job(key)
            print("MediaConvert Job:", jid)
        elif key.endswith(".jpg") and key.startswith("thumbnails/"):
            if moderate_thumbnail(rec["s3"]["bucket"]["name"], key):
                print("Flagged content:", key)
                # TODO: quarantine / alert / block publish
            else:
                print("Approved:", key)
```

### 9.2 Bash: Rekognition OCR → Comprehend 감정
```bash
IMG_KEY="frames/shot01.png"
aws rekognition detect-text \
  --image "S3Object={Bucket=media-input-bucket,Name=$IMG_KEY}" \
  --query "TextDetections[?Type=='LINE'].DetectedText" \
  --output text > lines.txt

TXT=$(tr '\n' ' ' < lines.txt)
aws comprehend detect-sentiment --language-code ko --text "$TXT"
```

---

## 10. 결론

- **Elastic Transcoder**: 빠른 입문/간단 변환.  
- **MediaConvert**: **방송급 규격/DRM/ABR**까지 포괄하는 프로급 파일 트랜스코딩.  
- **Rekognition**: **UGC/검수/검색/인덱싱**을 위한 실전 비전 AI.  
- **Comprehend**: 리뷰/코멘트/자막·메타데이터에 대한 **감정/키워드/개체/주제 분석**.

모든 파이프라인은 **S3·KMS·CloudFront·Lambda**와 유기적으로 결합해 **보안·비용·성능**을 균형 있게 설계할 수 있다.  
본 글의 **JSON·코드 스니펫**을 그대로 가져가 **IaC/서버리스** 방식으로 자동화하고, 사용량과 품질 메트릭을 꾸준히 관찰·튜닝하라.