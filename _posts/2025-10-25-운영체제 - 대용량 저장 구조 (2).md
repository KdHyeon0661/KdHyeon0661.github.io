---
layout: post
title: 운영체제 - 대용량 저장 구조 (2)
date: 2025-10-25 20:30:23 +0900
category: 운영체제
---
# Error Detection & Correction • Storage Device Management • Swap-Space Management

## 11.4 Error Detection and Correction (EDAC)

### 11.4.1 오류 모델과 용어
- **BER (Bit Error Rate)**: 비트 오류 확률 \(p\). 저장·전송 중 한 비트가 뒤집힐 확률.
- **URE (Unrecoverable Read Error)**: 장치에서 **내부 ECC로도 복구 불가한** 읽기 오류. 제조사 스펙은 보통 “10\^{-15} ~ 10\^{-16} per bit read” 등으로 표기.
- **Soft error**: 일시적(코스믹 레이, 전기적 노이즈) → 재시도/리맵으로 치유 가능.
- **Hard error**: 영구적 미디어 결함(배드 블록).
- **Silent data corruption (SDC)**: 검출 실패한 오류. 시스템 전체에서 가장 위험.

### 11.4.2 오류 검출/정정 체계 개관
- **Parity**: 1비트 검출, **정정 불가**.
- **Checksum**(1’s complement, Fletcher, Adler32): 속도↑, 검출력은 CRC < RS/LDPC.
- **CRC**(Cyclic Redundancy Check): 다항식 나눗셈 기반 **강력 검출**.
- **Hamming / SECDED**: 1비트 정정, 2비트 검출(추가 패리티 포함).
- **Reed–Solomon (RS)**: 바이트/기호 단위 다중 비트 **정정**, RAID-6, 테이프·광디스크에 널리 사용.
- **LDPC/BCH**: 대용량 NVM, 통신, 현대 스토리지 컨트롤러 내부에서 사용.

### 11.4.3 패리티와 체크섬

#### 1. 짝수 패리티
- \(k\)비트 데이터 \(d\)에 대해 패리티 \(p\)를 추가하여 전체 1의 개수가 **짝수**가 되게 함.
- 검출 능력: **홀수 개** 비트 오류만 검출.

#### 2. 1’s complement 체크섬(IPv4 유사)
- 16비트 워드 합을 **1의 보수**로 저장. 전송/저장 시 합치면 **0xFFFF**가 되어야 함(캐리 재접기).

```python
# ones_complement_checksum.py
def checksum(data: bytes) -> int:
    if len(data) % 2: data += b'\x00'
    s = 0
    for i in range(0, len(data), 2):
        s += (data[i] << 8) + data[i+1]
        s = (s & 0xFFFF) + (s >> 16)
    return (~s) & 0xFFFF

def verify(data: bytes, cs: int) -> bool:
    return checksum(data) == cs
```

### 11.4.4 CRC (Cyclic Redundancy Check)

- **아이디어**: 데이터 비트열을 **GF(2)** 다항식으로 보고 **생성다항식 \(G(x)\)** 로 나눴을 때의 **나머지**를 부착.
- 전형적 **CRC-32 (Ethernet/ZIP)** 생성다항식:
  $$G(x)=x^{32}+x^{26}+x^{23}+x^{22}+x^{16}+x^{12}+x^{11}+x^{10}+x^8+x^7+x^5+x^4+x^2+x+1$$
- **검출력**: 모든 1비트/2비트 오류, 대부분의 짧은 burst 오류 검출.

```python
# crc32_table.py — 테이블 기반 CRC-32 (IEEE 802.3)
POLY = 0xEDB88320
def table_make():
    tbl = []
    for i in range(256):
        crc = i
        for _ in range(8):
            crc = (crc>>1) ^ POLY if (crc & 1) else (crc>>1)
        tbl.append(crc)
    return tbl

CRC_TBL = table_make()

def crc32(data: bytes) -> int:
    crc = 0xFFFFFFFF
    for b in data:
        crc = CRC_TBL[(crc ^ b) & 0xFF] ^ (crc >> 8)
    return crc ^ 0xFFFFFFFF

# quick test
if __name__ == "__main__":
    assert crc32(b"123456789") == 0xCBF43926
```

### 11.4.5 Hamming (SECDED) — 1비트 정정/2비트 검출

- **코드 길이**: \((7,4)\) 예 — 4비트 데이터에 3비트 패리티로 7비트 코드워드.
- 일반화: 데이터 비트 \(m\), 패리티 비트 \(r\)일 때
  $$2^r \ge m + r + 1$$
- **SECDED**: Hamming + 전체 패리티 비트(더블 오류 검출).

```python
# hamming_secdec.py — (7,4) Hamming + SECDED
HAM_P = [1,2,4]      # parity bit positions (1-based)
HAM_M = [3,5,6,7]    # data bit positions

def _bitset(x,pos,val):
    return x | (1<<(pos-1)) if val else x & ~(1<<(pos-1))

def encode_7_4(data4: int) -> int:
    # place data
    cw = 0
    for i, pos in enumerate(HAM_M):
        cw = _bitset(cw, pos, (data4>>i)&1)
    # parity
    for p in HAM_P:
        s = 0
        for pos in range(1,8):
            if pos & p:
                s ^= (cw>>(pos-1)) & 1
        cw = _bitset(cw, p, s)
    return cw

def decode_7_4(cw: int):
    syndrome = 0
    for p in HAM_P:
        s = 0
        for pos in range(1,8):
            if pos & p:
                s ^= (cw>>(pos-1)) & 1
        if s: syndrome |= p
    corrected = cw
    if syndrome:  # 1-bit error
        corrected ^= 1<<(syndrome-1)
    # extract data
    data=0
    for i,pos in enumerate(HAM_M):
        data |= (((corrected>>(pos-1))&1)<<i)
    return data, (syndrome!=0)

if __name__ == "__main__":
    for d in range(16):
        cw = encode_7_4(d)
        for flip in range(0,8):
            c = cw ^ (0 if flip==0 else 1<<(flip-1))
            out, fixed = decode_7_4(c)
            assert out == (d & 0xF)
```

### 11.4.6 Reed–Solomon(개념)
- **GF(\(2^m\))** 위 다항식 코드. **바이트 단위** 오류에 강함.
- \(n,k\) 코드에서 최대 \(t = \frac{n-k}{2}\) 개의 **기호** 오류 정정.
- RAID-6의 **P/Q 패리티**는 RS의 특수화(또는 이와 동등한 Galois 연산).

#### (실습) RAID-5/6 패리티 개념 실험

```python
# raid_parity.py — RAID-5/6 개념 스케치 (바이트 단위 XOR / Galois multiply)
def raid5_parity(stripes):
    # stripes: list of equal-length bytearrays (data disks)
    import operator
    from functools import reduce
    return bytearray(reduce(lambda a,b: bytes(x^y for x,y in zip(a,b)), stripes))

# Galois field multiply for RAID-6 (GF(2^8) with x^8+x^4+x^3+x^2+1)
GF_POLY = 0x11D
def gmul(a,b):
    p=0
    for _ in range(8):
        if b&1: p^=a
        hi=a&0x80
        a=(a<<1)&0xFF
        if hi: a^=GF_POLY&0xFF
        b>>=1
    return p

def raid6_pq(stripes):
    length = len(stripes[0])
    P = bytearray(length)
    Q = bytearray(length)
    for sidx, blk in enumerate(stripes):
        coef = pow(2, sidx, 256)  # α^i 근사(데모)
        for i,b in enumerate(blk):
            P[i] ^= b
            Q[i] ^= gmul(b, coef)
    return P, Q
```

---

## 11.5 Storage Device Management

### 11.5.1 계층과 역할
- **디바이스 → OS 블록 계층 → 파일시스템 → 볼륨 관리자**(LVM/DM) → 상위 애플리케이션.
- **OS 책임**: 파티셔닝, 배드블록 관리, 스케줄링, 캐싱, 디바이스 매핑(암호화/미러/RAID), 모니터링(S.M.A.R.T/Log).

### 11.5.2 파티셔닝과 포맷
- **GPT**: 64-bit LBA, 백업 헤더, CRC로 메타 보호.
- **MBR**: 2TiB 제한, 호환성 목적.
- 파일시스템 포맷 시 **블록 크기/저널 모드**/옵션(TRIM, atime 등) 결정.

### 11.5.3 배드 블록 관리
- **HDD**: **Reallocation** — 내부 스페어에 자동 리맵.
- **SSD**: **FTL**이 불량 페이지/블록을 회피, **웨어 레벨링**으로 수명 균등화.
- **OS 레벨**: `badblocks`, 파일시스템 레벨 remap(Ext4 `-E lazy_journal_init`, Btrfs/ZFS는 자체 무결성).

```python
# badblock_bitmap.py — 배드 블록 비트맵 + 연속 영역 할당기 스케치
class BlockBitmap:
    def __init__(self, n):
        self.n = n
        self.bad = set()
        self.used = set()
    def mark_bad(self, idxs):
        self.bad.update(idxs)
    def alloc_run(self, length):
        run=0; start=0
        for i in range(self.n):
            if i in self.bad or i in self.used:
                run=0; start=i+1; continue
            run+=1
            if run==length:
                for j in range(start, start+length): self.used.add(j)
                return start
        return None
```

### 11.5.4 RAID/LVM/Device-Mapper
- **RAID-0**: 스트라이핑(성능↑, 내고장성X)
- **RAID-1**: 미러(가용성↑, 쓰기=최저 디스크 속도)
- **RAID-5**: 분산 패리티(가용성+공간 효율), 쓰기 **Read-Modify-Write** 오버헤드.
- **RAID-6**: 이중 패리티(2개 장애 허용).
- **RAID-10**: 미러+스트라이프(성능/가용성 균형 좋음).
- **LVM**: PV→VG→LV. 온라인 크기 조절, 스냅샷(COW), 레이아웃 유연.
- **DM-crypt**: 장치 단 **암호화**(XTS-AES), TRIM 전파 옵션 주의.

### 11.5.5 ZNS/SMR와 구역화(Zoned) 장치
- **SMR HDD**: 트랙 겹침. **순차 쓰기** 요구 — 랜덤 쓰기엔 **로그 구조** 필요.
- **NVMe ZNS**: zone 단위 **reset/append** — 호스트가 쓰기 정렬 관리 → **쓰기 증폭↓**.

### 11.5.6 TRIM/Discard & 공간 회수
- 파일 삭제/여유 공간을 장치에 알려 **GC 효율↑**.
- 실시간 `discard`는 컨트롤러에 따라 비용↑ → **주기적 `fstrim`** 권장(서버).

### 11.5.7 S.M.A.R.T / NVMe Log 모니터링
- **예측 지표**: 재배치 섹터 수, 프로그램/소거 카운트, 미디어 오류, 온도, 경고 플래그.
- **스크러빙**: 주기적 **백그라운드 검증**으로 잠재 오류를 조기 발견.

```python
# scrub_sim.py — 주기 스크럽으로 SDC 확률을 낮추는 간단 모델
import random
def simulate(files=10000, years=3, p_latent=1e-9, scrub_interval_days=7):
    silent=0
    for f in range(files):
        latent = random.random() < (p_latent * 365*24*3600 * years)
        if not latent: continue
        # 스크럽 주기 이전에만 발견 못하면 SDC로 가정
        # (아주 조악한 근사)
        if scrub_interval_days>0 and random.random() < (365*years)/scrub_interval_days:
            continue
        silent += 1
    return silent
print("approx silent:", simulate())
```

### 11.5.8 쓰기 경계와 저널/배리어
- **FUA/FLUSH**는 순서/내구 보장. 너무 잦으면 p99↑ → **그룹 커밋/배치**.
- 파일시스템 **저널 모드**(data=ordered/journal/writeback), `barrier`/`nobarrier` 옵션의 의미 파악(장치 캐시/전원보호와 맞물림).

---

## 11.6 Swap-Space Management

### 11.6.1 목적
- 물리 메모리가 부족할 때 **비활성 페이지**를 **스왑 공간**으로 내보내 **RAM 확보**.
- **스왑 파티션** 또는 **스왑 파일**을 백업 저장소로 사용.

### 11.6.2 스왑 설계 요소
- **단위**: 페이지(4KiB 등).
- **스왑 맵**: 사용/미사용 슬롯 추적(비트맵/프리리스트).
- **클러스터링**: 인접 페이지를 묶어 I/O 단위를 키움(순차 I/O 효율↑).
- **우선순위**: 여러 스왑 영역 간 **priority**로 분산.
- **압축과의 협업**: zswap(zram) → **메모리 내 압축 캐시** 후 **실스왑** fallback.

### 11.6.3 스왑 슬롯 할당/해제(비트맵 예)

```python
# swap_bitmap.py — 스왑 비트맵 + 클러스터 할당기
class SwapMap:
    def __init__(self, slots, cluster=16):
        self.slots = slots
        self.cluster = cluster
        self.used = set()
    def alloc_cluster(self):
        run=0; start=0
        for i in range(self.slots):
            if i in self.used:
                run=0; start=i+1; continue
            run += 1
            if run == self.cluster:
                for j in range(start, start+self.cluster): self.used.add(j)
                return start
        return None
    def free_cluster(self, start):
        for j in range(start, start+self.cluster):
            self.used.discard(j)
```

**설명**
- 페이지 아웃 시 **연속 슬롯**에 기록하여 **병합/순차성**을 높임.
- 회수 시 다시 클러스터 단위로 **빈 공간**을 복원(외부 단편화 완화).

### 11.6.4 페이지 아웃 정책과 프리페치
- 교체 알고리즘(Clock/NRU/LRU 근사)로 희생 페이지 선정.
- **Swap prefetch**: 과거 스왑된 페이지를 유휴 I/O 시간에 **미리 가져오기**(워크로드 의존).

### 11.6.5 스왑 I/O 경로 최적화
- **큰 I/O**(연속 페이지 묶음) → HDD에서는 특히 이득, SSD에서도 FTL 내부 병합에 유리.
- **멀티 스왑 디바이스**: 우선순위/라운드로빈/가중 분배로 **대역폭↑**.
- **zram**: RAM 내 압축 스왑 장치(디스크 I/O 회피).
- **zswap**: 메모리 압축 캐시 → 미스 시 백엔드 스왑으로 배출.

```bash
# (Linux 예) 스왑 파일 만들기 & 활성화 & 우선순위
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon --priority 10 /swapfile
swapon --show
```

### 11.6.6 정책 튜닝(리눅스 관점)
- **`vm.swappiness`**: 페이지캐시 vs 프로세스 메모리의 상대적 회수 성향(0~100).
- **`vm.page-cluster`**: 스왑 I/O 묶음 크기(아키/커널 버전에 따라 의미 차이).
- **cgroup v2**: `memory.swap.max`/`memory.low`로 컨테이너별 스왑 사용 통제.
- **NUMA**: 스왑 인/아웃 시 노드 간 이동 비용 고려. **퍼스트 터치**/바인딩과 일관성을 유지.

### 11.6.7 관측/운영 지표
- **`vmstat`**: `si/so`(swap in/out), `pgfault/majflt`.
- **`sar -B`**, `/proc/vmstat`: `pswpin/out`, `pgscan`, `kswapd`.
- **p95/p99 latency**: 스왑 이벤트와의 상관관계(특히 SSD GC 타이밍).

### 11.6.8 실습: 워크셋 초과에 따른 스왑 동작 체감

```python
# swap_pressure_demo.py — 큰 배열을 점증적으로 사용하여 스왑 압력 유도(시뮬 개념)
import mmap, os
MB = 1024*1024
N  = 1024  # 1GiB
with mmap.mmap(-1, N*MB, access=mmap.ACCESS_WRITE) as mm:
    for i in range(0, N*MB, 4096):
        mm[i:i+1] = b'\x01'   # first-touch (NUMA 로컬 할당)
    # 이후 랜덤 접근/휴면을 섞어 major fault/p99 변화를 관찰(실시스템에서 vmstat/perf 동시 관측)
```

---

## 통합 시나리오: 신뢰·성능·복구 가능성의 균형

**문제**: 4개의 데이터 디스크에 128KiB 스트라이프 RAID-5를 구성, 주기적 스크러빙과 S.M.A.R.T 모니터링, 스왑 파일 8GiB(zswap on)를 운영. OLTP 워크로드(읽기 70%)에서 p99 5ms 목표.

**설계 체크리스트**
1) **EDAC**: 파일시스템/전송 경계마다 **CRC**(ZFS/Btrfs/obj checksum) 켜기. RAID-5는 **중복 패리티 검증**과 **scrub** 주기를 설정.
2) **스케줄러**: NVMe면 `mq-deadline` 또는 `kyber`, 큐딥=32부터 p99 프로파일.
3) **쓰기 배치**: DB WAL을 **별도 네임스페이스**로 격리, **그룹 커밋**으로 FLUSH 빈도 제어.
4) **TRIM**: 주기적 `fstrim`(야간).
5) **스왑**: `zswap` 활성, `swappiness`를 워크로드에 맞게(예: 20) — 페이지캐시 보존 측면 최적화.
6) **스크럽/SMART**: 주 1회 스크럽, 임계치 상승 시 **예비 디스크에 프리-코피**.
7) **백업**: **오프라인 무결성 검사**(해시)를 포함한 주기 백업.

---

## 핵심 요약

- **11.4 EDAC**: CRC/SECDED/RS/LDPC로 **검출·정정**을 다층 배치. SDC를 최소화하려면 **엔드-투-엔드 체크섬**과 **주기 스크럽**이 필수.
- **11.5 장치 관리**: 파티셔닝·배드블록·RAID/LVM·TRIM·스케줄링·모니터링을 일관된 정책으로 묶어 **성능과 내구성을 동시에** 확보. ZNS/SMR 등 **매체 특성**을 알맞게 반영.
- **11.6 스왑 관리**: 비활성 페이지를 효율적으로 배출·회수. **클러스터링·다중 스왑·압축(zswap/zram)** 과 **튜닝(swappiness/cgroup)** 로 **지연의 롱테일**을 눌러 예측 가능성을 높인다.
