---
layout: post
title: 선형대수 - 텐서와 다차원 선형대수
date: 2025-07-01 22:20:23 +0900
category: 선형대수
---
# 텐서와 다차원 선형대수 (Tensor & Multilinear Algebra)

> **핵심 한 줄**
> 텐서는 “여러 축을 가진 선형 구조”이며, **텐서곱·축소(수축)·전치(축 재배열)** 같은 연산으로 다룬다.
> **CP/Tucker(HOSVD)/TT** 같은 분해는 고차 데이터를 **저차 핵심 구조**로 바꾸어 압축·해석·학습을 돕는다.

---

## 로드맵

1. 텐서의 수학적/프로그래밍적 정의 (배열 vs 다중선형함수)
2. 형(차수·형식), 좌표 표현과 기저 변경, 공변/반변
3. 핵심 연산: 텐서곱, 외적/내적(수축), 모드-곱, 전개(unfold), 전치(permute)
4. 행렬/특수 곱: 크로네커 ⊗, 카트리-라오 ⊙, 아다마르 ∘, vec-정체성
5. 노름·내적·랭크(행렬 vs 텐서), 다중선형 랭크
6. 분해: **CP**, **Tucker/HOSVD**, **TT(MPS)** 기본/알고리즘/오류
7. 응용: 물리(응력/관성), CV(이미지/비디오), **딥러닝(컨볼루션/어텐션)**
8. PyTorch 실전 코드(✅ 사용자 선호 반영): 연산·HOSVD·CP-ALS·어텐션·⊗ 확인
9. 수치/실무 팁: 메모리/stride, 초기화, 정규화, 수렴과 지표

모든 수식은 **$$ · $$** 로 표기합니다. 코드 블록은 ``` 로 감쌉니다.

---

## 텐서란?

### 배열로서의 정의

- **0차 텐서**: 스칼라, $$\alpha\in\mathbb{F}$$
- **1차 텐서**: 벡터, $$v\in\mathbb{F}^I$$
- **2차 텐서**: 행렬, $$A\in\mathbb{F}^{I\times J}$$
- **3차 이상**: $$\mathcal{X}\in\mathbb{F}^{I_1\times I_2\times\cdots\times I_N}$$

### 다중선형함수로서의 정의

- $$V_1,\ldots,V_k$$ 위의 **다중선형함수**:
$$
T:V_1\times\cdots\times V_k\to\mathbb{F},\quad T(\cdot)\ \text{가 각 인자에 선형}.
$$
- 기저를 정하면 배열 성분 $$T_{i_1\cdots i_k}$$ 로 동일시 가능:
$$
T=\sum_{i_1,\ldots,i_k} T_{i_1\cdots i_k}\,e^{(1)}_{i_1}\otimes\cdots\otimes e^{(k)}_{i_k}.
$$

### (p, q)-형식 / 공변·반변

- 좌표 변환 아래 **반변(contravariant)** 지수는 역행렬로, **공변(covariant)** 지수는 전치로 변환.
- 실무에서는 주로 “축의 개수(차수)” 관점으로 이해하고, 기하가 필요한 분야(미분기하/물리)에서 (p,q) 표기를 사용.

---

## 텐서곱과 좌표 표현

### 텐서곱의 생성

- 두 벡터공간 $$V,W$$:
$$
V\otimes W=\operatorname{Span}\{\,v_i\otimes w_j\,\}.
$$
- 차원: $$\dim(V\otimes W)=\dim V\cdot\dim W$$.

### 외적(순위-1 텐서)

- 벡터 $$a\in\mathbb{F}^{I},\ b\in\mathbb{F}^{J},\ c\in\mathbb{F}^{K}$$:
$$
(a\otimes b\otimes c)_{ijk}=a_i b_j c_k.
$$
- CP 분해의 기본 벽돌(순위-1 텐서).

### 좌표 표현 & 기저 변경

- 기저 $$\{e_i\}$$ 가 $$\tilde e=P e$$ 으로 바뀌면 성분은 반대로 변환:
$$
\tilde T = (P^{-1})^{\otimes N} \cdot T.
$$

---

## 텐서 핵심 연산 (프레임워크 관점 포함)

### 수축(내적, contraction) — 축 줄이기

- 두 텐서의 일부 축을 합산하여 제거:
$$
(\mathcal{X}\times_{(i=j)}\mathcal{Y})_{\text{remaining indices}}=\sum_{k} X_{\cdots k\cdots}\,Y_{\cdots k\cdots}.
$$
- 아인슈타인 합 표기:
$$
Z_{ab}=\sum_{k} X_{ak}Y_{kb}\quad\leftrightarrow\quad Z=\mathrm{einsum}('ak,kb->ab',X,Y).
$$

### 모드-n 전개(unfold, matricization)

- $$\mathcal{X}\in\mathbb{F}^{I_1\times\cdots\times I_N}$$ 의 **모드-n 전개**:
$$
X_{(n)}\in\mathbb{F}^{I_n\times \prod_{m\neq n} I_m},
$$
- 예: $$N=3$$ 이면 $$X_{(1)}\in\mathbb{F}^{I_1\times (I_2 I_3)}$$.

### 모드-n 곱 (mode-n product)

- 행렬 $$U\in\mathbb{F}^{J\times I_n}$$ 로 모드-n 축을 선형 변환:
$$
(\mathcal{Y}=\mathcal{X}\times_n U)\ \Rightarrow\ Y_{i_1\cdots i_{n-1} j i_{n+1}\cdots i_N}
=\sum_{i_n} X_{i_1\cdots i_n\cdots i_N} U_{j i_n}.
$$

### 전치/permute

- 축의 순서만 변경(데이터 복사 없이 stride 변경 가능):
$$
\mathcal{X}^{\pi}(i_{\pi(1)},\ldots,i_{\pi(N)})=\mathcal{X}(i_1,\ldots,i_N).
$$

---

## 행렬적 연산과의 연결

### 크로네커 곱 ⊗

- 행렬 $$A\in\mathbb{F}^{m\times n}, B\in\mathbb{F}^{p\times q}$$:
$$
A\otimes B\in\mathbb{F}^{mp\times nq},\quad
\mathrm{vec}(AXB^\top)=(B\otimes A)\,\mathrm{vec}(X).
$$

### 카트리-라오 곱 ⊙ (열별 크로네커)

- $$A\in\mathbb{F}^{I\times R}, B\in\mathbb{F}^{J\times R}$$:
$$
A\odot B=[\,a_1\otimes b_1\ \cdots\ a_R\otimes b_R\,]\in\mathbb{F}^{(IJ)\times R}.
$$
- CP-ALS에서 **MTTKRP** 계산 핵심.

### 아다마르 곱 ∘ (원소별)

- 동형 행렬/텐서의 원소별 곱:
$$
(C=A\circ B)_{ij}=A_{ij}B_{ij}.
$$

---

## 노름·내적·랭크

### 내적·프로베니우스 노름

- 텐서 내적:
$$
\langle \mathcal{X},\mathcal{Y}\rangle=\sum_{i_1,\ldots,i_N} X_{i_1\cdots i_N} Y_{i_1\cdots i_N}.
$$
- $$\|\mathcal{X}\|_F=\sqrt{\langle\mathcal{X},\mathcal{X}\rangle}$$.

### 텐서 랭크(=CP 랭크)와 다중선형 랭크

- **CP 랭크**: 순위-1 텐서 합의 최소 개수.
- **다중선형 랭크**: $$\operatorname{rank}(X_{(n)}) = r_n$$ 의 벡터 $$\mathbf{r}=(r_1,\ldots,r_N)$$ (Tucker/HOSVD에서 등장).
- 고차 텐서의 랭크 결정은 일반적으로 **NP-난해**.

---

## 고차 분해

### CP 분해 (CANDECOMP/PARAFAC)

- 목적:
$$
\mathcal{X}\approx \sum_{r=1}^{R} a_r\otimes b_r\otimes c_r
\quad(\text{3차 기준})
$$
또는
$$
\mathcal{X}\approx [[A,B,C]]\quad\text{(인자행렬 표기)}.
$$
- 식별성(유일성): **Kruskal 조건** 등으로 보장.
- 알고리즘: **ALS(Alternating Least Squares)** — 한 인자행렬씩 고정하고 나머지 선형회귀로 갱신.

### Tucker / HOSVD

- Tucker:
$$
\mathcal{X}\approx \mathcal{G}\times_1 U^{(1)}\times_2 U^{(2)}\times_3 U^{(3)},
$$
여기서 코어 $$\mathcal{G}\in\mathbb{F}^{r_1\times r_2\times r_3}$$, 각 $$U^{(n)}\in\mathbb{F}^{I_n\times r_n}$$ 직교(보통).
- **HOSVD**: 각 모드 전개 $$X_{(n)}$$ 에 SVD→ 좌특이벡터 상위 $$r_n$$ 열을 $$U^{(n)}$$ 로 채택 → 코어는
$$
\mathcal{G}=\mathcal{X}\times_1 {U^{(1)}}^\top\times_2 {U^{(2)}}^\top\times_3 {U^{(3)}}^\top.
$$
- 절단(HOT-SVD) 오차 경계(정규화 가정하) 존재.

### Tensor Train (TT, MPS)

- 고차 $$N$$ 텐서를 3차 코어들의 사슬로 표현:
$$
\mathcal{X}(i_1,\ldots,i_N)=G_1[i_1]\,G_2[i_2]\cdots G_N[i_N],
$$
메모리/계산을 선형급으로 제어.

---

## 응용 스냅샷

### 물리

- **응력/변형률**: 2차 텐서, 좌표계에 따라 성분 변환.
- **관성 텐서**: 강체 회전에 대한 질량 분포 요약(대칭, 고유벡터=주축).

### CV/시계열

- 이미지: $$\mathbb{R}^{H\times W\times C}$$, 비디오: $$\mathbb{R}^{T\times H\times W\times C}$$.
- **Tucker**로 압축/노이즈 제거, **CP**로 주 구조 뽑기.

### 딥러닝

- **컨볼루션 커널**: $$\mathbb{R}^{O\times I\times K_h\times K_w}$$, 입력과의 **수축**으로 출력 생성.
- **어텐션**: $$Q,K,V$$ 의 **텐서 수축**:
$$
\mathrm{Attn}(Q,K,V)=\operatorname{softmax}\!\big(\tfrac{QK^\top}{\sqrt{d}}\big)\,V.
$$

---

## PyTorch: 핵심 연산 & 분해 코드

> 모든 코드는 **PyTorch** 기준(사용자 선호). GPU 사용 시 `to('cuda')`.

### 기본 연산: 외적·수축·모드-곱·전개

```python
import torch

# 예시 텐서 (3차)

X = torch.arange(2*3*4, dtype=torch.float64).reshape(2,3,4)

# (1) 외적: rank-1 3차 텐서

a, b, c = torch.randn(2), torch.randn(3), torch.randn(4)
rank1 = torch.einsum('i,j,k->ijk', a, b, c)  # a⊗b⊗c

# (2) 수축(contraction): X와 rank1를 마지막 축으로 내적
# Z_{ij} = sum_k X_{ijk} * R_{ijk}

Z = (X * rank1).sum(dim=2)  # == einsum('ijk,ijk->ij', X, rank1)

# (3) 모드-n 곱: Y = X ×_2 U  (여기서 n=2는 두 번째 축, 0-index면 축=1)

U = torch.randn(5, 3)  # (J x I_n) = (5 x 3)
# Y shape: (2, 5, 4)

Y = torch.einsum('ijk,aj->iak', X, U)  # i=0:2, j=0:3 -> a=0:5, k=0:4

# (4) 모드-1 전개(unfold)

X_1 = X.permute(0,1,2).reshape(2, 12)  # I1 x (I2*I3) = 2 x 12
# 모드-2 전개

X_2 = X.permute(1,0,2).reshape(3, 8)
# 모드-3 전개

X_3 = X.permute(2,0,1).reshape(4, 6)
```

**상황 예시**: 커널 크기별(축=Kh, Kw) SVD로 필터 축소하거나, 모드-채널 곱으로 색공간 회전(Whitening).

---

### HOSVD (3차, 직교 축 선택 및 재구성)

```python
def hosvd(X, ranks):
    # X: (I, J, K), ranks: (r1, r2, r3)
    I, J, K = X.shape
    # unfold
    X1 = X.permute(0,1,2).reshape(I, J*K)
    X2 = X.permute(1,0,2).reshape(J, I*K)
    X3 = X.permute(2,0,1).reshape(K, I*J)

    # SVD 각 모드
    U1 = torch.linalg.svd(X1, full_matrices=False).U[:, :ranks[0]]
    U2 = torch.linalg.svd(X2, full_matrices=False).U[:, :ranks[1]]
    U3 = torch.linalg.svd(X3, full_matrices=False).U[:, :ranks[2]]

    # core = X ×_1 U1^T ×_2 U2^T ×_3 U3^T
    G = torch.einsum('ijk,ia->ajk', X, U1.T)
    G = torch.einsum('ajk,jb->abk', G, U2.T)
    G = torch.einsum('abk,kc->abc', G, U3.T)

    # 재구성: X_hat = G ×_1 U1 ×_2 U2 ×_3 U3
    Xh = torch.einsum('abc,ia->ibc', G, U1)
    Xh = torch.einsum('ibc,jb->ijc', Xh, U2)
    Xh = torch.einsum('ijc,kc->ijk', Xh, U3)
    return (U1, U2, U3), G, Xh

# 데모

torch.manual_seed(0)
X = torch.randn(20, 15, 10)
(U1,U2,U3), G, Xh = hosvd(X, ranks=(8, 6, 5))
rel_err = (X - Xh).norm() / X.norm()
print("HOSVD 상대오차:", float(rel_err))
```

**상황**: 비디오 $$\mathbb{R}^{T\times H\times W}$$ 에서 $(r_T,r_H,r_W)$ 로 압축·노이즈 제거.

---

### CP-ALS (3차, 간단 참조 구현 — 소규모/연구용)

```python
def cp_als_3d(X, R, iters=50, lam=1e-6):
    # X: (I,J,K), R: rank
    I,J,K = X.shape
    # 초기화(정규화 포함)
    A = torch.randn(I, R); A = A / A.norm(dim=0, keepdim=True).clamp_min(1e-12)
    B = torch.randn(J, R); B = B / B.norm(dim=0, keepdim=True).clamp_min(1e-12)
    C = torch.randn(K, R); C = C / C.norm(dim=0, keepdim=True).clamp_min(1e-12)

    def mttkrp_mode1(X, B, C):
        # MTTKRP: X_(1) @ (C ⊙ B)
        # einsum: X_{i j k} * B_{j r} * C_{k r} -> M_{i r}
        return torch.einsum('ijk,jr,kr->ir', X, B, C)
    def mttkrp_mode2(X, A, C):
        return torch.einsum('ijk,ir,kr->jr', X, A, C)
    def mttkrp_mode3(X, A, B):
        return torch.einsum('ijk,ir,jr->kr', X, A, B)

    for t in range(iters):
        # 갱신식: A <- MTTKRP * (C^T C ∘ B^T B)^{-1}
        # 그램 행렬
        BtB = (B.T @ B)
        CtC = (C.T @ C)
        G = BtB * CtC + lam*torch.eye(R)  # 아다마르 + ridge
        A = mttkrp_mode1(X, B, C) @ torch.linalg.pinv(G)
        A = A / A.norm(dim=0, keepdim=True).clamp_min(1e-12)

        AtA = (A.T @ A)
        G = AtA * CtC + lam*torch.eye(R)
        B = mttkrp_mode2(X, A, C) @ torch.linalg.pinv(G)
        B = B / B.norm(dim=0, keepdim=True).clamp_min(1e-12)

        AtA = (A.T @ A)
        BtB = (B.T @ B)
        G = AtA * BtB + lam*torch.eye(R)
        C = mttkrp_mode3(X, A, B) @ torch.linalg.pinv(G)
        C = C / C.norm(dim=0, keepdim=True).clamp_min(1e-12)

    return A, B, C

# 데모

torch.manual_seed(1)
I,J,K,R = 30, 20, 10, 3
# 합성 데이터(정답 인자)

A0 = torch.randn(I,R); B0 = torch.randn(J,R); C0 = torch.randn(K,R)
X = torch.einsum('ir,jr,kr->ijk', A0, B0, C0) + 0.05*torch.randn(I,J,K)

A,B,C = cp_als_3d(X, R=R, iters=80)
Xhat = torch.einsum('ir,jr,kr->ijk', A,B,C)
print("CP 상대오차:", float((X-Xhat).norm()/X.norm()))
```

**상황**: 사용자-아이템-시간 텐서에서 $$R$$ 개의 잠재 요인으로 요약(권장: 정규화/정렬/스케일 고정).

---

### 어텐션을 텐서 수축으로 (einsum)

```python
# Q: (B, H, L, d), K: (B, H, L, d), V: (B, H, L, d)

B,H,L,d = 2, 4, 128, 64
Q = torch.randn(B,H,L,d)
K = torch.randn(B,H,L,d)
V = torch.randn(B,H,L,d)

# 점수 S_{b,h,i,j} = <Q_{b,h,i,:}, K_{b,h,j,:}>

S = torch.einsum('bhid,bhjd->bhij', Q, K) / (d**0.5)
P = torch.softmax(S, dim=-1)
O = torch.einsum('bhij,bhjd->bhid', P, V)
print(O.shape)  # (B,H,L,d)
```

**상황**: 다중 헤드에서 축을 분리해 메모리 지역성을 높이고, einsum 최소화로 커널 호출 수를 제어.

---

### ⊗–vec 정체성 검증

```python
# vec(AXB^T) = (B ⊗ A) vec(X)

m,n,p = 3,4,5
A = torch.randn(m,n)
B = torch.randn(p,n)  # (주의) B^T의 n 일치
X = torch.randn(n,n)  # 정체성 검증 위해 정방 사용

lhs = (A @ X @ B.T).reshape(-1,1)
Kron = torch.kron(B, A)            # (mp) x (n n)
rhs = Kron @ X.reshape(-1,1)
print("OK?", torch.allclose(lhs, rhs, atol=1e-6))
```

---

## 수치/실무 팁

1) **stride/메모리**: `permute` 후 `contiguous()` 필요 여부 파악. 불필요한 `clone` 지양.
2) **정규화**: CP-ALS에서 인자 행렬의 열 정규화·스케일 공유(한 축으로 몰기)로 발산 방지.
3) **초기화**: HOSVD로 초기화 후 CP 미세조정(“HOSVD→ALS”).
4) **정칙화**: Ridge/라쏘/스무딩을 인자에 추가(시계열/공간적 인접성 반영).
5) **수렴지표**: 상대재구성오차, 코어/인자 변화량, 최대 반복·시간 제한.
6) **동일성(유일성)**: CP는 회전·스케일 불일치의 **불변성** 존재 → 후처리로 정렬/부호 고정.
7) **대규모**: 랜덤라이즈드 SVD, 블록 ALS, 분산(샤딩), 혼합정밀(FP16) 고려.
8) **미분**: CP-ALS는 비미분적(내부 선형해); end-to-end 학습은 **미분가능한 목적** 설계(예: 재구성 손실에서 인자 직접 최적화).

---

## 요약 표

| 주제 | 핵심 식/연산 | 코멘트 |
|---|---|---|
| 모드-n 곱 | $$\mathcal{Y}=\mathcal{X}\times_n U$$ | 축별 선형변환 |
| 전개 | $$X_{(n)}\in\mathbb{F}^{I_n\times\prod_{m\ne n}I_m}$$ | HOSVD 전처리 |
| 수축 | $$Z_{ab}=\sum_k X_{ak}Y_{kb}$$ | einsum으로 일반화 |
| 크로네커 | $$\mathrm{vec}(AXB^\top)=(B\otimes A)\mathrm{vec}(X)$$ | 전개-선형화 연결 |
| CP | $$\sum_{r=1}^R a_r\otimes b_r\otimes c_r$$ | 랭크-1 합 |
| Tucker/HOSVD | $$\mathcal{G}\times_1 U^{(1)}\times_2 U^{(2)}\times_3 U^{(3)}$$ | 다중선형 랭크 |
| TT | 코어 사슬 | 초고차에 효율적 |
| DL 응용 | $$\mathrm{Attn}=\mathrm{softmax}(QK^\top/\sqrt d)V$$ | 수축 관점 |

---

### 마무리

텐서는 “축을 쌓은 선형대수”다. **전개/모드-곱/수축**을 정확히 이해하면,
**분해(CP/Tucker/TT)** 로 고차 데이터를 압축·해석·가속할 수 있다.
딥러닝의 컨볼루션과 어텐션 역시 **텐서 수축**으로 통일적으로 기술된다.
