---
layout: post
title: 정보통신기사 - 부하분산 · 이중화 · 모니터링
date: 2025-08-22 21:25:23 +0900
category: 정보통신기사
---
# 부하분산 · 이중화 · 모니터링 총정리 (tx-lb-ha-observability)

> **목표**: 정보통신기사 · 실무 엔지니어 관점에서 **부하분산(load balancing)**, **이중화(HA)**, **모니터링/관측성**을  
> “원리 → 설계 → 구성 예제 → 용량/가용성 계산 → 운영/트러블슈팅 → 체크리스트 → 연습문제” 순으로 **생략 없이** 정리합니다.  
> 수식은 **MathJax**, 설정/스크립트는 **코드블록**으로 제공합니다.

---

## 0) 큰 그림: “안정성 = (분산) + (중복) + (가시성)”

- **부하분산**: 트래픽을 여러 인스턴스에 공평·현명하게 분배해 **처리량↑**·**응답시간↓**·**결함허용성↑**.  
- **이중화**: 단일 실패지점(SPOF)을 제거하고 **N+1 / Active-Active / Active-Standby** 등으로 **가용성↑**.  
- **모니터링/관측성**: **메트릭/로그/트레이스/상태검사**로 **문제 조기 탐지**·**자동/수동 대응**.  
- **핵심 직감**  
  1) **분산**이 성능을, **중복**이 가용성을 만든다.  
  2) **모니터링이 없는 중복은 미신**이다(작동을 검증하지 않으면 실패 시 놀란다).  
  3) 설계는 **수식**으로, 운영은 **런북**으로 관리한다.

---

## 1) 부하분산 기초 (L2/3/4/7)

### 1.1 레이어별
- **L2/3**: **ECMP**(Equal-Cost Multi-Path), **LAG/LACP**(링크 집합). 해시 기반 분산(5튜플 등).  
- **L4(TCP/UDP)**: 커넥션 단위 분배(커널/IPVS, NLB류). **초고성능**, **콘텐츠 인지 X**.  
- **L7(HTTP/gRPC/WS)**: 요청/URI/헤더/쿠키 단위 분배, **라우팅/재시도/회로차단/WAF** 가능.

### 1.2 분산 알고리즘
- **Round Robin/Weighted RR**: 단순·균등.  
- **Least Connections / Weighted LC**: 장기 커넥션 많은 환경에 유리.  
- **Least Response Time / EWMA**: 지연 기반 동적 분배.  
- **Hash(IP/URI)**: 세션 고정, 캐시 적합.  
- **Consistent Hashing**: 노드 증감 시 **재배치 최소**(CDN/캐시/샤딩).  
- **Power of Two Choices**: 임의 2개 후보 중 부하 적은 곳 선택(간단+효율).

### 1.3 세션 고정(Sticky)
- **IP 해시**(NAT/프록시 뒤면 불안정), **쿠키**(LB가 주입), **헤더 기반**(JWT/세션ID), **일관 해시**.  
- **권장**: *가능하면 애플리케이션을 **무상태(stateless)**로 설계*하고, 상태는 **외부 저장소**(Redis/DB)에 둔다.

### 1.4 상태검사(Health Check)
- **Active**: TCP/HTTP/gRPC 헬스 엔드포인트 주기 검사(200/OK, 응답시간 임계).  
- **Passive**: 실패율/타임아웃 감지 시 **Outlier Ejection**(일시 격리).  
- **히스테리시스**: **N번 연속 실패 시 다운**, **M번 성공 시 복귀** → 플랩 방지.  
- **멀티 시점**: LB→백엔드 검사 + 외부 **Synthetic**(사용자 관점) 병행.

### 1.5 TLS/HTTP 최적화
- **종단/재암호화**: LB에서 TLS 종료(TLS Termination), 백엔드와 **mTLS** 가능.  
- **Keep-Alive/Connection Pool**로 백엔드 커넥션 재사용.  
- **ALPN/HTTP/2·3**, **압축**, **캐시/정적 오프로드**.

### 1.6 용량 직감(대기행렬)
트래픽 도착률 \(\lambda\), 처리율 \(\mu\) (초당 요청)인 **M/M/1** 근사:
\[
\rho=\frac{\lambda}{\mu},\quad \text{Avg Wait}\approx \frac{\rho}{\mu(1-\rho)},\quad \text{Avg System}=\frac{1}{\mu-\lambda}
\]
- **교훈**: **여유율(1−ρ)**이 20% 미만이면 지연이 급증한다. **평상시 ρ≤0.6~0.7** 권장.

---

## 2) 이중화(HA) 전략

### 2.1 N+1 / Active-Active / Active-Standby
- **N+1**: 정상 N대 + 예비 1대(한 대 장애까지 무중단).  
- **Active-Active**: 모든 노드가 동시에 처리(ECMP/LB). 성능↑, 상태 공유 설계 필요.  
- **Active-Standby**: 주-대기(Preempt/Delay). 단순하지만 **페일오버 순간**이 존재.

### 2.2 계층별 HA
- **전원/하드웨어**: PSU **A/B**, 이중 UPS/피더, 팬/NIC/디스크 중복.  
- **링크**: **LACP(LAG)**, **ECMP**, **MC-LAG/MLAG/스택**.  
- **게이트웨이**: **VRRP/HSRP** 가상 IP.
```bash
# VRRP(keepalived) 예시
vrrp_instance VI_10 {
  interface bond0
  virtual_router_id 10
  priority 120
  advert_int 1
  virtual_ipaddress { 10.10.10.1/24 }
  authentication { auth_type PASS auth_pass vrrpKey }
  preempt_delay 10
}
```
- **데이터/세션**: 세션 외부화(쿠키/토큰+Redis), DB 복제/합의(아래).  
- **사이트**: **멀티 AZ/리전**, **GSLB(DNS/Anycast)**, **활·대기** 혹은 **액티브-액티브**.

### 2.3 가용성 산술(직렬/병렬)
- 단일 구성요소 가용성 \(A_i\). **직렬(의존)**:
\[
A_{\text{series}}=\prod_i A_i
\]
- **병렬(이중)**, 독립 가정:
\[
A_{\text{parallel}}=1-\prod_i (1-A_i)
\]
**예)** 단일 서버 \(A=99.0\%\) → 이중화(독립) 시 \(1-(0.01)^2=99.99\%\).

> 현실은 **상관 실패**(전원/냉각/버그/인증서 만료)로 **독립성↓**. 물리적/논리적 분리까지 고려.

### 2.4 페일오버 용량(여유율)
- 장애 시 남은 노드에 부하가 몰림 → **수용 가능한 ρ를 장애 후에도 유지**해야 한다.  
- **N개 액티브**에서 **1개 장애** 가정, 각 노드 최대 처리율 \(\mu\), 총 도착률 \(\lambda\):
\[
\text{장애 후 } \rho'=\frac{\lambda/(N-1)}{\mu}\le \rho_{\max}
\]
→ **사전 기준**: 평시 \(\rho \le \rho_{\max}\cdot \frac{N-1}{N}\).  
(예: \(\rho_{\max}=0.7, N=4 \Rightarrow \rho\le0.525\))

### 2.5 DB/스토리지 HA 한눈에
- **복제 방식**: **동기(RPO=0)/비동기(RPO>0)**.  
- **합의(Quorum)**: **Raft/Paxos**, **3/5/7 노드** 홀수 구성, **펜싱(STONITH)**로 스플릿브레인 방지.  
- **스토리지**: RAID(디스크 단일실패), **Erasure Coding**, **복제(3-way)**.  
- **재해복구**: RTO/RPO 목표 → **Warm/Hot Standby**, 스냅샷/로그 재생.

---

## 3) 글로벌 분산: DNS/GSLB/Anycast

- **DNS 기반(GSLB)**: 헬스/지연/지역/가중치로 **최적 엔드포인트** 응답. **TTL/캐시**가 전환 속도에 영향.  
- **Anycast**: 동일 IP를 여러 PoP에서 BGP로 광고 → **가까운 경로**로 도달, **DDoS 분산**.  
- **하이브리드**: Anycast 프론트 + 지역별 L7 LB.

---

## 4) 모니터링/관측성(Observability)

### 4.1 골든 시그널 4가지
1) **지연(Latency)** – p50/p95/p99, 백엔드/외부 구분  
2) **트래픽(Traffic)** – RPS, QPS, bps, CPS  
3) **오류(Errors)** – 5xx/업스트림 실패/타임아웃  
4) **포화(Saturation)** – CPU, 메모리, 큐 길이, 연결 수

### 4.2 SLI/SLO/에러버짓
- **SLI**: 측정치(예: 성공 비율, p95<300ms).  
- **SLO**: 목표(예: 월 성공 ≥ 99.9%).  
- **에러버짓** \(E=1-\text{SLO}\). 월 99.9% → 허용 실패 0.1%.  
- **정책**: 버짓 소진 시 배포 동결/안정화 우선.

### 4.3 계측 지점
- **LB 앞/뒤**(프론트/백), **백엔드**(애플리케이션), **DB/캐시**, **의존 API**.  
- **블랙박스**: 외부 synthetic(HTTP/TCP/DNS).  
- **트레이싱**: **OpenTelemetry**로 **TraceID** 전파, 구간별 지연/오류 가시화.

### 4.4 알림 설계
- 임계 + **지속시간**(Noise 억제), **중복 억제**, **상관관계**(부모 다운 시 자식 억제).  
- **예시**: “5분 평균 5xx ≥ 1% & p95 ≥ 800ms & 인입 RPS≥100 → P1”.

---

## 5) 구성 예제 모음

### 5.1 HAProxy(L7) – 가중치, 헬스체크, 스티키, 회로차단
```cfg
global
  maxconn 50000
  log /dev/log local0
  tune.ssl.default-dh-param 2048

defaults
  mode http
  option httplog
  timeout client  30s
  timeout server  30s
  timeout connect 5s
  retries 2

frontend fe_web
  bind :443 ssl crt /etc/ssl/web.pem alpn h2,http/1.1
  http-request set-header X-Request-ID %[unique-id]
  default_backend be_web

backend be_web
  balance leastconn
  cookie SRV insert indirect nocache
  option httpchk GET /healthz
  http-check expect status 200
  default-server inter 2s fall 3 rise 2 slowstart 10s maxconn 2000
  # Outlier Ejection 유사 - observe layer7
  server app1 10.0.1.11:8080 check cookie a1
  server app2 10.0.1.12:8080 check cookie a2
  server app3 10.0.1.13:8080 check cookie a3
  # 서킷브레이크 느낌: 연속 5xx시 일시 weight 0 (agent-check로도 구현 가능)
```

### 5.2 NGINX – 라운드로빈 + 최소연결 + Keep-Alive
```nginx
upstream app {
  least_conn;
  server 10.0.1.11:8080 max_fails=3 fail_timeout=5s;
  server 10.0.1.12:8080 max_fails=3 fail_timeout=5s;
  keepalive 200;
}
server {
  listen 443 ssl http2;
  ssl_certificate /etc/ssl/full.pem;
  ssl_certificate_key /etc/ssl/key.pem;
  location / {
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto https;
    proxy_set_header X-Request-ID $request_id;
    proxy_http_version 1.1;
    proxy_set_header Connection "";
    proxy_pass http://app;
  }
  location = /healthz { return 200; }
}
```

### 5.3 Envoy – Outlier Ejection / 분산정책
```yaml
static_resources:
  listeners:
  - name: https
    address: { socket_address: { address: 0.0.0.0, port_value: 443 } }
    filter_chains:
    - filters:
      - name: envoy.filters.network.http_connection_manager
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
          route_config:
            virtual_hosts:
            - name: vh
              domains: ["*"]
              routes: [{ match: { prefix: "/" }, route: { cluster: app } }]
          http_filters: [{ name: envoy.filters.http.router }]
  clusters:
  - name: app
    connect_timeout: 0.25s
    type: STRICT_DNS
    lb_policy: LEAST_REQUEST
    outlier_detection:
      consecutive_5xx: 5
      interval: 2s
      base_ejection_time: 30s
      max_ejection_percent: 50
    load_assignment:
      cluster_name: app
      endpoints:
      - lb_endpoints:
        - endpoint: { address: { socket_address: { address: 10.0.1.11, port_value: 8080 } } }
        - endpoint: { address: { socket_address: { address: 10.0.1.12, port_value: 8080 } } }
```

### 5.4 Kubernetes – Ingress + HPA
```yaml
apiVersion: apps/v1
kind: Deployment
metadata: { name: web }
spec:
  replicas: 4
  selector: { matchLabels: { app: web } }
  template:
    metadata: { labels: { app: web } }
    spec:
      containers:
      - name: web
        image: registry/web:1.0
        ports: [{containerPort: 8080}]
        readinessProbe: { httpGet: { path: /healthz, port: 8080 }, periodSeconds: 5, failureThreshold: 3 }
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata: { name: web-hpa }
spec:
  scaleTargetRef: { apiVersion: apps/v1, kind: Deployment, name: web }
  minReplicas: 4
  maxReplicas: 20
  metrics:
  - type: Resource
    resource: { name: cpu, target: { type: Utilization, averageUtilization: 60 } }
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata: { name: web-ing }
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend: { service: { name: web-svc, port: { number: 80 } } }
```

---

## 6) 모니터링/알림 예제 (Prometheus/Alertmanager)

### 6.1 레코딩 규칙
```yaml
groups:
- name: lb.rules
  rules:
  - record: job:http_request_rate:sum
    expr: sum(rate(http_requests_total[1m])) by (job)
  - record: job:http_5xx_rate:ratio
    expr: sum(rate(http_requests_total{code=~"5.."}[5m])) by (job) /
          sum(rate(http_requests_total[5m])) by (job)
  - record: job:http_latency_p95
    expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le,job))
```

### 6.2 알림 규칙
```yaml
groups:
- name: alerts.rules
  rules:
  - alert: High5xxAndLatency
    expr: job:http_5xx_rate:ratio > 0.01 and job:http_latency_p95 > 0.8
    for: 10m
    labels: { severity: P1 }
    annotations:
      summary: "고오류/고지연 ({{ $labels.job }})"
      runbook: "https://runbooks.example/lb-high-5xx"
  - alert: BackendUnhealthy
    expr: avg_over_time(lb_backend_healthy[5m]) < 0.7
    for: 5m
    labels: { severity: P1 }
```

---

## 7) 성능/용량/가용성 계산

### 7.1 LB 뒤 서버 수 산정(단순 근사)
- 요청 평균 서비스시간 \(S\) (초), 한 서버 처리율 \(\mu=1/S\). 목표 여유율 \(\rho_{\max}\)일 때 필요한 서버 수 \(N\):
\[
N \ge \left\lceil \frac{\lambda}{\rho_{\max}\mu} \right\rceil
\]
**예)** 2000 RPS, \(S=10\ \text{ms}\Rightarrow \mu=100\), \(\rho_{\max}=0.6\) → \(N\ge\lceil 2000/(0.6\cdot 100)\rceil=34\).

### 7.2 N+1 고려
- 장애 후에도 \(\rho'\le 0.7\) 유지하려면 **평시** \(\rho\le 0.7\cdot (N-1)/N\).  
- 위 예에서 \(N=35\)로 잡으면 한 대 장애 시에도 감당.

### 7.3 TLS 핸드셰이크 비용(개념)
- **핸드셰이크/초** 당 CPU 비용 \(C_h\), **데이터 처리** 비용 \(C_d\). Keep-Alive 비율 \(k\) (핸드셰이크 비중↓).  
- **튜닝**: **세션 재사용/0-RTT**(주의), **대규모 커넥션 풀**.

### 7.4 SLA 결합
- 두 AZ 각각 99.95%, 독립 가정 A-A:
\[
A_{pair}=1-(1-0.9995)^2\approx 99.999975\%
\]
- **현실 보정**: 공통 원인(인증서/배포/설정)을 **분리**하고, **혼합 배포 윈도우**로 상관↓.

---

## 8) 운영/트러블슈팅 런북

### 8.1 “5xx 증가 + 지연↑” 공통 플로우
1) **범위 확인**: 모든 경로? 특정 엔드포인트/테넌트?  
2) **용량/포화**: LB/백엔드 **활성연결/큐** 확인, 새 배포 여부.  
3) **의존성**: DB/캐시/외부 API 지연? 회로차단/리트라이 폭주?  
4) **조치**:  
   - 스케일아웃(+노드), **재시도/타임아웃 상향 금지**(폭주 위험),  
   - **캐시 TTL 단축/무효화** 조절,  
   - **트래픽 셰이핑/율제한** 임시 적용,  
   - 최근 배포 **롤백**.  
5) **사후**: 원인(RCA), 재발 방지(레이트리미트/큐/백프레셔, 캐시 키 설계, 힙/FD 상한).

### 8.2 “백엔드 다수 Unhealthy”
- **헬스엔드포인트 자체 의존**(DB down이면 /healthz도 실패) → **Liveness/Readiness 분리**.  
- **히스테리시스** 재검토(너무 민감해 플랩).  
- **네임서비스/DNS**: 백엔드 IP 변경 반영? TTL?

### 8.3 “페일오버는 됐는데 과부하”
- **N+M 설계 미흡**: 여유 노드 부족 → 임시로 **율제한**/**지역 분리**.  
- **콜드 캐시**: Warm-up, 프리페치, **트래픽 슬로스타트**(HAProxy slowstart, Envoy warmup).

---

## 9) 보안/안정성 강화

- **WAF/봇 방어/레이트리미팅**(L7) + **SYN 쿠키/Conntrack 튜닝**(L4).  
- **mTLS**(동서 트래픽), **HSTS**, **TLS 최소버전**.  
- **증분 배포**(Canary/Blue-Green), **서킷브레이커/재시도 백오프/타임아웃**.  
- **증명서 만료 모니터링**, 비밀(키) 롤테이션 자동화.

---

## 10) 체크리스트 (현장용)

### 설계
- [ ] L4/L7 선택, 알고리즘(LC/EWMA/Consistent Hash)  
- [ ] **헬스체크** 방식, rise/fall, 타임아웃, 경로 분리  
- [ ] **N+1/N+2** 용량, 장애 후 \(\rho'\) 기준  
- [ ] 상태 관리: **무상태 우선**, 세션 외부화  
- [ ] 글로벌: **GSLB/Anycast**, TTL/캐시전략  
- [ ] 보안: WAF, mTLS, 레이트리미팅

### 구축/운영
- [ ] **대시보드**(골든 시그널, 백엔드/의존성)  
- [ ] **알림**(임계+지속, 상관 억제), 런북 링크  
- [ ] 배포 전략(Canary), 자동 롤백  
- [ ] **장애훈련**(페일오버/캡시티 테스트), 혼잡제어 리허설  
- [ ] 인증서/키 만료 알림, 시계(NTP) 일치

---

## 11) 자주 틀리는 포인트(정오표)

1) **LB가 무조건 고르게 분배**한다고 믿음 → 커넥션 길이/응답시간 편차를 고려해 **LeastConn/EWMA**.  
2) **Sticky**로 문제 회피 → **무상태화가 정답**(필요 시 일관 해시).  
3) **독립성 과신** → 같은 랙/전원/스위치면 **공통 실패**. **물리 분리**.  
4) **페일오버 테스트 부재** → 실제론 ARP/DNS/캐시/콜드스타트로 **장애 악화**.  
5) **무제한 재시도** → 장애 시 **폭주 증폭**. 백오프·최대재시도·써킷브레이커 필수.  
6) **헬스체크 = 애플리케이션 상태** 착각 → **의존성 불통**을 분리 반영(Ready/Live).

---

## 12) 연습문제(풀이 포함)

**Q1.** 1500 RPS, 평균 서비스시간 8ms(한 서버 \(\mu=125\) RPS). 평시 \(\rho\le0.6\) 목표. 필요한 서버 수?  
**A.** \(N\ge\lceil1500/(0.6\cdot125)\rceil=\lceil20\rceil=20\).

**Q2.** 위 구성을 **N+1**로 하고 한 대 장애 시 \(\rho'\le0.7\) 만족하는가?  
**A.** 장애 후 \(1500/19/125≈0.63\) → **OK**.

**Q3.** 단일 서버 99.5%를 이중화(독립)하면 가용성은?  
**A.** \(1-(0.005)^2=99.9975\%\).

**Q4.** GSLB 전환이 느리다. 원인 2가지와 개선?  
**A.** **TTL/캐시**와 **리졸버 보수적 캐싱**. TTL 단축/서브도메인 분리, **헬스체크→Failover 레코드**.

**Q5.** 5xx↑·지연↑ 시 재시도를 늘리면 안 되는 이유?  
**A.** **증폭 효과**로 백엔드 더 과부하. 백오프/써킷브레이커·율제한 필요.

**Q6.** Sticky를 꼭 써야 하는 경우 2가지?  
**A.** **상태ful 세션 유지**(일시), **캐시 일관성/샤딩**(일관 해시).

**Q7.** 헬스체크 rise/fall를 각각 높게 잡을 때/낮게 잡을 때의 장단점?  
**A.** 높게: 플랩↓·전환 느림 / 낮게: 전환 빠름·거짓양성↑.

---

## 13) 압축 암기표(시험 직전 18줄)

1) **L4=고성능**, **L7=지능/정책**  
2) 알고리즘: **RR/LC/EWMA/Hash/Consistent Hash/Two Choices**  
3) **헬스체크**: Active+Passive, rise/fall, 타임아웃  
4) **무상태 우선**, Sticky는 예외적 사용  
5) **N+1/N+2** 용량, 장애 후 \(\rho'\) 기준  
6) 가용성: 직렬 **곱**, 병렬 **1-곱(1-A)**  
7) **VRRP/ECMP/LACP**로 네트워크 HA  
8) **GSLB/DNS/Anycast** 글로벌 분산  
9) TLS 종료 + Keep-Alive + 커넥션 풀  
10) **골든 시그널** 4개 / **SLI·SLO·버짓**  
11) **알림=임계+지속+상관 억제**  
12) **Outlier Ejection**/회로차단/재시도 백오프  
13) **배포**: Canary/Blue-Green, 자동 롤백  
14) **페일오버 테스트/드릴** 정기화  
15) 콜드 캐시 → **슬로스타트/프리워밍**  
16) **공통원인 실패 제거**(전원/랙/스위치/인증서)  
17) **OpenTelemetry**로 트레이싱 일원화  
18) **증명서 만료/키** 모니터링 필수

---

## 14) 마무리

- **부하분산**은 “잘 나누기”, **이중화**는 “여분 갖추기”, **모니터링**은 “보이게 하기”입니다.  
- **수식 기반 용량**과 **히스테리시스가 있는 전환**, **표준화된 계측/알림/런북**으로  
  고부하/장애 상황에서도 **짧은 RTO**와 **낮은 사용자 체감**을 달성하세요.
