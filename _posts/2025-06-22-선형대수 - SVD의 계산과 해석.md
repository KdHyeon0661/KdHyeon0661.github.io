---
layout: post
title: 선형대수 - SVD의 계산과 해석
date: 2025-06-22 20:20:23 +0900
category: 선형대수
---
# SVD의 계산과 해석

> **핵심 요약**
> 임의의 행렬 $$A\in\mathbb{R}^{m\times n}$$에 대해 항상
> $$
> \boxed{A=U\,\Sigma\,V^{\mathsf T}}
> $$
> 로 분해된다. 여기서 $$U\in\mathbb{R}^{m\times m},\ V\in\mathbb{R}^{n\times n}$$ 는 정규직교,
> $$\Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_r)\ (r=\operatorname{rank}A)$$ 이고
> $$\sigma_1\ge\cdots\ge\sigma_r>0$$, 나머지는 0.
> **기하학적 해석**: “회전(정규직교) → 축방향 스케일 → 회전”.
> **수치 해석**: **조건수** $$\kappa_2(A)=\sigma_{\max}/\sigma_{\min}$$, **의사역행렬** $$A^+=V\Sigma^+U^{\mathsf T}$$,
> **저랭크 근사** $$\hat A_k=U_k\Sigma_kV_k^{\mathsf T}$$(Eckart–Young–Mirsky 최적).

---

## 형식 복습과 두 가지 SVD 형태

- **Full SVD**
  $$
  A=U\,\Sigma\,V^{\mathsf T},\quad
  U\in\mathbb{R}^{m\times m},\
  \Sigma\in\mathbb{R}^{m\times n},\
  V\in\mathbb{R}^{n\times n}.
  $$
- **Economy(Thin) SVD** — 계산·메모리 절약
  $$
  A=U_r\,\Sigma_r\,V_r^{\mathsf T},\quad
  U_r\in\mathbb{R}^{m\times r},\
  \Sigma_r\in\mathbb{R}^{r\times r},\
  V_r\in\mathbb{R}^{n\times r},
  $$
  여기서 $$r=\operatorname{rank}(A)$$.

**성질 요약**
- $$A^{\mathsf T}A=V\,\Sigma^{\mathsf T}\Sigma\,V^{\mathsf T}$$, $$AA^{\mathsf T}=U\,\Sigma\Sigma^{\mathsf T}\,U^{\mathsf T}$$
  → **특이값**: $$\sigma_i=\sqrt{\lambda_i(A^{\mathsf T}A)}=\sqrt{\lambda_i(AA^{\mathsf T})}$$
- (정방행렬) 부피/면적 스케일: $$|\det A|=\prod_i \sigma_i$$

---

## 계산 절차 — 이론부터 실무 알고리즘까지

### 이론적(교육용) 절차

1) **우측 특이벡터**
$$
A^{\mathsf T}A\,\vec v_i=\lambda_i\,\vec v_i,\quad \sigma_i=\sqrt{\lambda_i}.
$$
정규화한 고유벡터들 $$\{\vec v_i\}$$ 로 $$V$$ 구성.

2) **좌측 특이벡터**
$$
\vec u_i=\frac{1}{\sigma_i}A\,\vec v_i
$$
로 $$U$$ 구성(영공간에 해당하는 경우는 적절히 보충).

> ⚠️ 실무에서는 **직접 $$A^{\mathsf T}A$$ 고유분해를 권장하지 않음**:
> 조건수가 **제곱**으로 악화되고(불안정), 불필요한 제곱 연산으로 **정밀도 손실**.

### 실무 표준: Golub–Kahan **바이대각화 → QR/분할정복**

1) **Householder** 변환으로 $$A$$ 를 **바이대각행렬** $$B$$ 로 축소:
   $$A=Q_L\,B\,Q_R^{\mathsf T}$$ (여기서 $$Q_L,Q_R$$ 는 정규직교).
2) 작은 $$B$$ 의 **대각화**(QR 스텝, 분할정복, MRRR 등)로 특이값·벡터 획득.
3) 누적 변환으로 $$U,Q_L$$, $$V,Q_R$$ 복원.
- 복잡도(대략): $$m\ge n$$ 일 때 **Thin SVD**가 $$\mathcal O(mn^2)$$.

### 대규모 데이터용

- **Truncated/Partial SVD**: 상위 $$k$$ 개만 필요할 때 IRLBA/Lanczos.
- **Randomized SVD**(근사): 랜덤 사영 + 파워이테레이션으로 $$\mathcal O(ndk)$$ 근사.
- **Incremental/Streaming SVD**: 배치로 갱신.

---

## 2×2 예제로 손계산 감각 익히기

$$
A=\begin{bmatrix}3&1\\[2pt]0&2\end{bmatrix}
$$

### $$A^{\mathsf T}A$$

$$
A^{\mathsf T}A
=\begin{bmatrix}3&0\\1&2\end{bmatrix}\begin{bmatrix}3&1\\0&2\end{bmatrix}
=\begin{bmatrix}9&3\\3&5\end{bmatrix}.
$$

특성다항식
$$
\det(A^{\mathsf T}A-\lambda I)=
\lambda^2-14\lambda+36=0
$$
$$
\Rightarrow\ \lambda_{1,2}=7\pm\sqrt{13}.
$$
특이값
$$
\boxed{\sigma_1=\sqrt{7+\sqrt{13}},\quad \sigma_2=\sqrt{7-\sqrt{13}}}.
$$

각 고유값에 대해 $$\vec v_i$$ 를 구해 정규화 → $$V=[\vec v_1,\vec v_2]$$,
$$\vec u_i=\tfrac1{\sigma_i}A\vec v_i$$ 로 $$U$$ 구함.
(수작업은 길어지므로 실제 수치는 코드에서 확인 권장)

---

## 기하학적 해석

임의의 벡터 $$\vec x$$ 에 대해
$$
A\vec x=U\,\Sigma\,V^{\mathsf T}\vec x
$$
1) **$$V^{\mathsf T}$$**: 좌표축을 특이벡터 기저로 **회전**
2) **$$\Sigma$$**: 축방향 **스케일링**(반지름을 $$\sigma_i$$ 로)
3) **$$U$$**: 다시 **회전**하여 최종 위치로

- 단위 원(구) → **타원(타원체)**
- 주축 방향 = **특이벡터**, 주축 길이 = **특이값**
- 면적/부피 스케일(정방행렬) = $$\prod_i \sigma_i=|\det A|$$

---

## 수치적 해석과 진단 지표

### 조건수와 민감도

- **2-노름 조건수**:
  $$
  \boxed{\kappa_2(A)=\frac{\sigma_{\max}}{\sigma_{\min}}}
  $$
  클수록 **불안정(ill-conditioned)** — 작은 교란이 해를 크게 흔듦.

### 수치 랭크와 임계값

- 머신 엡실론 $$\varepsilon$$ 에 대해 자주 쓰는 기준:
  $$
  \sigma_i\le \sigma_{\max}\cdot \max(m,n)\,\varepsilon \ \Rightarrow\ \text{유효 0으로 간주.}
  $$

### 의사역행렬(무어–펜로즈)

$$
\boxed{A^+=V\,\Sigma^+\,U^{\mathsf T}},\quad
\Sigma^+=\operatorname{diag}\Big(\tfrac1{\sigma_i}\Big)\ \text{(0은 그대로 0)}.
$$
- 최소제곱해: $$\vec x^\star=A^+\vec b$$
- **Truncated SVD** 또는 **Tikhonov**로 잡음에 강인하게.

---

## 저랭크 근사와 Eckart–Young–Mirsky 정리

랭크-$$k$$ 최적 근사:
$$
\boxed{\hat A_k=U_k\,\Sigma_k\,V_k^{\mathsf T}}
$$
오차:
$$
\|A-\hat A_k\|_F=\sqrt{\sum_{i>k}\sigma_i^2},\qquad
\|A-\hat A_k\|_2=\sigma_{k+1}.
$$
→ **설명력(누적에너지)**
$$
\text{ExplainedRatio}(k)=\dfrac{\sum_{i\le k}\sigma_i^2}{\sum_{j}\sigma_j^2}.
$$

---

## 파이썬 구현 — PyTorch(기본) & NumPy(비교)

### PyTorch: SVD, 저랭크 근사, 의사역행렬, 조건수

```python
import torch

def thin_svd(A, k=None):
    # 경제형 SVD
    U, S, Vh = torch.linalg.svd(A, full_matrices=False)
    if k is not None and k < S.numel():
        U, S, Vh = U[:, :k], S[:k], Vh[:k, :]
    return U, S, Vh

def best_rank_k(A, k):
    U, S, Vh = torch.linalg.svd(A, full_matrices=False)
    Uk, Sk, Vhk = U[:, :k], S[:k], Vh[:k, :]
    Ahat = Uk @ torch.diag(Sk) @ Vhk
    fro_err = torch.linalg.matrix_norm(A - Ahat, ord='fro').item()
    spec_err = S[k].item() if k < S.numel() else 0.0
    expl = (Sk**2).sum().item() / (S**2).sum().item()
    return Ahat, fro_err, spec_err, expl

def pinv_via_svd(A, rcond=None):
    U, S, Vh = torch.linalg.svd(A, full_matrices=False)
    if rcond is None:
        rcond = max(A.shape)*torch.finfo(A.dtype).eps
    cutoff = rcond * S.max()
    Sinv = torch.where(S > cutoff, 1.0/S, torch.zeros_like(S))
    return Vh.mT @ torch.diag(Sinv) @ U.mT  # A^+ = V Σ^+ U^T

def cond2(A):
    S = torch.linalg.svdvals(A)
    return (S.max()/S.min()).item()

# Demo

torch.manual_seed(0)
A = torch.tensor([[3.,1.],[0.,2.]], dtype=torch.float64)
U,S,Vh = thin_svd(A)
Ahat1, fro_err, spec_err, expl = best_rank_k(A, k=1)
Aplus = pinv_via_svd(A)
kappa = cond2(A)

print("U=\n",U); print("S=",S); print("V^T=\n",Vh)
print("Rank-1 근사 설명률=", expl, " Fro-err=", fro_err, " Spec-err≈σ2=", spec_err)
print("의사역행렬 A^+=\n", Aplus)
print("조건수 κ2(A)=", kappa)
```

### PyTorch: Randomized SVD(간단 구현)

```python
def randomized_svd(A, k, p=5, q=1):
    """
    A: (m,n), k: target rank, p: oversampling, q: power iters
    반환: U_k, S_k, Vh_k  (근사)
    """
    m, n = A.shape
    rng = torch.Generator(device=A.device)
    Omega = torch.randn(n, k+p, dtype=A.dtype, device=A.device, generator=rng)  # (n, k+p)
    Y = A @ Omega                                     # (m, k+p)
    for _ in range(q):
        Y = A @ (A.mT @ Y)                            # power iteration
    Q, _ = torch.linalg.qr(Y, mode='reduced')         # (m, k+p)
    B = Q.mT @ A                                      # (k+p, n)
    Ub, S, Vh = torch.linalg.svd(B, full_matrices=False)
    U = Q @ Ub                                        # (m, k+p)
    return U[:, :k], S[:k], Vh[:k, :]                 # truncate to k

# Demo

Uk, Sk, Vhk = randomized_svd(A, k=1, p=2, q=1)
Ahat = Uk @ torch.diag(Sk) @ Vhk
print("Randomized rank-1 근사:\n", Ahat)
```

### NumPy: 동일 연산(간단 비교)

```python
import numpy as np

A = np.array([[3.,1.],[0.,2.]])
U, S, VT = np.linalg.svd(A, full_matrices=False)
Ahat1 = (U[:, :1] * S[:1]) @ VT[:1, :]
Aplus = (VT.T * (1/S)) @ U.T  # full rank인 경우 간단식
kappa = S.max()/S.min()
print("U=\n",U, "\nS=",S, "\nV^T=\n",VT)
print("rank-1 근사=\n", Ahat1)
print("A^+=\n", Aplus)
print("κ2=", kappa)
```

---

## 최소제곱·정칙화 관점의 해석

- **최소제곱** $$\min_{\vec x}\|A\vec x-\vec b\|_2$$ 의 해:
  $$
  \vec x^\star = A^+\vec b
  = \sum_{i=1}^r \frac{\vec u_i^{\mathsf T}\vec b}{\sigma_i}\,\vec v_i.
  $$
  작은 $$\sigma_i$$ 가 **잡음 증폭**을 유발.

- **Truncated SVD**: 작은 $$\sigma_i$$ 성분 제거 → 잡음 억제.
- **Tikhonov(릿지)**:
  $$
  \vec x_\lambda=\sum_{i=1}^r \frac{\sigma_i}{\sigma_i^2+\lambda}\ (\vec u_i^{\mathsf T}\vec b)\ \vec v_i
  $$
  로 안정화(고주파 억제).

---

## SVD로 보는 PCA(초간단 연결)

평균 제거된 $$X\in\mathbb{R}^{n\times d}$$ 에서
$$
X=U\,\Sigma\,V^{\mathsf T}
$$
- **주성분 축**: $$V$$ 의 열벡터,
- **분산**: $$\sigma_i^2/(n-1)$$,
- **점수(투영)**: $$Z=XV=U\Sigma$$,
- **재구성**: $$\hat X_k=U_k\Sigma_kV_k^{\mathsf T}$$.

(자세한 PCA 연결은 이전 SVD↔PCA 정리 참조)

---

## 실전 체크리스트 & 흔한 함정

1) **$$A^{\mathsf T}A$$ 직접 고유분해 지양** — 조건수 제곱, 수치오차 증폭.
2) **Thin SVD** 사용 — 불필요한 full 축소 회피(속도·메모리).
3) **수치 랭크 판정** — tol 설정(예: `rcond = max(m,n)*eps*σ_max`).
4) **부호 비유일성** — $$\vec u_i,\vec v_i$$ 의 부호는 임의(결과 해석 시 일관만 유지).
5) **중복 특이값** — 서브스페이스는 동일하나 기저는 비유일.
6) **결측치/이상치** — 로버스트 변형(Trimmed SVD, RPCA) 고려.
7) **대규모** — Truncated/Randomized/Incremental로 스케일.
8) **GPU/정밀도** — FP32로 충분하나 악조건에서는 FP64·혼합정밀도 점검.

---

## 응용 미니 시나리오

- **이미지 압축**: 채널별로 SVD, 상위 $$k$$ 만 유지 → 저장·전송량 절감.
- **추천 시스템(잠재요인)**: 사용자–아이템 행렬에 Truncated SVD → 잠재공간에서 근접 이웃.
- **신호잡음분리**: 작은 특이값 성분 제거 → 노이즈 억제.
- **선형 역문제**: $$A^+$$ 또는 Tikhonov/Truncated로 안정적 복원.

---

## 추가: 시각화/설명률 코드 스니펫(PyTorch)

```python
import torch
import matplotlib.pyplot as plt

def svd_explained_variance(X, center=True, ddof=1):
    X = torch.as_tensor(X, dtype=torch.float64)
    if center:
        X = X - X.mean(0, keepdim=True)
    S = torch.linalg.svdvals(X)
    variances = (S**2) / (X.shape[0] - ddof)
    ratio = variances / variances.sum()
    cum = torch.cumsum(ratio, dim=0)
    return S, variances, ratio, cum

# Demo (임의 데이터)

torch.manual_seed(0)
X = torch.randn(500, 50, dtype=torch.float64)
S, var, ratio, cum = svd_explained_variance(X)

plt.plot(cum.numpy())
plt.xlabel("k"); plt.ylabel("Cumulative explained ratio")
plt.title("Scree(누적 설명률)"); plt.grid(True); plt.show()
```

---

## 요약 표

| 항목 | 핵심 식/포인트 |
|---|---|
| 정의 | $$A=U\Sigma V^{\mathsf T}$$, 모든 행렬에서 존재 |
| 특이값 | $$\sigma_i=\sqrt{\lambda_i(A^{\mathsf T}A)}$$ |
| 기하 | 회전 → 스케일 → 회전 |
| 조건수 | $$\kappa_2(A)=\sigma_{\max}/\sigma_{\min}$$(클수록 불안정) |
| 의사역행렬 | $$A^+=V\Sigma^+U^{\mathsf T}$$ |
| 저랭크 근사 | $$\hat A_k=U_k\Sigma_kV_k^{\mathsf T}$$(EYM 최적) |
| 실무 알고리즘 | Householder 바이대각화 → QR/분할정복 |
| 대규모 | Truncated/Randomized/Incremental SVD |

---

### 덧붙임: 빠른 자체 점검 질문

- 평균 제거가 필요한가? (PCA/데이터행렬)
- full vs thin 어느 쪽이 적절한가?
- tol/rcond 설정은 합리적인가?
- 희망하는 **설명률** 또는 **목표 랭크**는?
- 결과 해석에서 **부호/기저 비유일성**을 고려했는가?
