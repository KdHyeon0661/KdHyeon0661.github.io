---
layout: post
title: 선형대수 - SVD의 응용
date: 2025-06-22 22:20:23 +0900
category: 선형대수
---
# 🧩 SVD의 실제 응용: 이미지 압축, 추천 시스템

**특이값 분해(SVD)**는 단순한 행렬 분해 도구를 넘어  
실제 산업, 기술 분야에서 널리 사용되는 강력한 기법입니다.

이 글에서는 대표적인 두 가지 응용 사례인  
**이미지 압축**과 **추천 시스템**에서 SVD가 어떻게 사용되는지 설명합니다.

---

## 🖼️ 1. 이미지 압축 (Image Compression)

### 📌 원리

이미지를 흑백으로 표현하면 픽셀 값은 행렬로 나타낼 수 있습니다.

- \( m \times n \) 행렬 \( A \): 밝기 정보
- SVD 분해: \( A = U \Sigma V^T \)
- 앞의 \( k \)개의 특이값만 사용하여 근사한 행렬 \( A_k \) 생성

\[
A_k = U_k \Sigma_k V_k^T
\]

- \( k \)가 작을수록 압축률 ↑, 품질 ↓
- 대부분의 정보는 **상위 몇 개 특이값**에 집중되어 있음

---

### 🧪 예제 (Python 코드)

```python
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# 이미지 로딩 및 흑백 변환
img = Image.open("image.jpg").convert("L")  # L: grayscale
A = np.array(img)

# SVD
U, S, VT = np.linalg.svd(A, full_matrices=False)

# 압축 수준 선택 (예: 상위 50개 특이값)
k = 50
A_k = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

# 결과 시각화
plt.subplot(1, 2, 1)
plt.title("원본")
plt.imshow(A, cmap='gray')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"SVD 압축 (k={k})")
plt.imshow(A_k, cmap='gray')
plt.axis('off')

plt.show()
```

### 📉 압축률 계산

원래의 저장 용량: \( m \times n \)  
압축 후 용량: \( k(m + n + 1) \)  
→ \( k \ll \min(m, n) \) 일수록 압축률 상승

---

### ✅ 요약

| 항목 | 내용 |
|------|------|
| 원리 | SVD로 이미지 행렬을 분해 |
| 주요 정보 | 상위 특이값에 집중 |
| 장점 | 손쉬운 손실 압축, 계산 단순 |
| 단점 | 컬러 이미지의 경우 3채널 처리 필요 |

---

## 🎯 2. 추천 시스템 (Recommendation System)

### 📌 사용자-아이템 행렬

- 행: 사용자
- 열: 영화, 상품 등
- 값: 평점 (또는 0은 미평가)

\[
R = 
\begin{bmatrix}
5 & 4 & 0 & 1 \\
3 & 0 & 0 & 5 \\
4 & 2 & 3 & 0 \\
0 & 0 & 5 & 4 \\
\end{bmatrix}
\]

### SVD 적용

\[
R \approx U_k \Sigma_k V_k^T
\]

- \( U_k \): 사용자 잠재 특성
- \( V_k \): 아이템 잠재 특성
- \( \Sigma_k \): 중요도 조절

→ 내적 계산을 통해 **사용자-아이템 평점 예측**

---

### 🧪 예제 (Python 코드)

```python
import numpy as np

# 사용자-아이템 평점 행렬 (0 = 미입력)
R = np.array([
    [5, 4, 0, 1],
    [3, 0, 0, 5],
    [4, 2, 3, 0],
    [0, 0, 5, 4]
], dtype=float)

# 평균 중심화 (사용자 기준)
user_means = np.true_divide(R.sum(1), (R != 0).sum(1))
R_centered = R - user_means[:, np.newaxis]
R_centered[R == 0] = 0

# SVD
U, S, VT = np.linalg.svd(R_centered, full_matrices=False)

# 상위 k차원만 사용
k = 2
R_approx = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

# 다시 원래 스케일로
R_predicted = R_approx + user_means[:, np.newaxis]

print("예측된 평점:\n", np.round(R_predicted, 2))
```

---

### ✅ 추천 시스템에서의 장점

| 장점 | 설명 |
|------|------|
| 잠재 요인 추출 | 사용자/아이템의 숨겨진 특성 파악 |
| 예측 가능 | 비어 있는 평점 예측 |
| 차원 축소 | 잡음 제거, 연산 효율 개선 |

### ⚠️ 주의점

- 실제 구현에서는 **결측치 보간/최적화 기반 SVD** 사용
- SciPy, Surprise, TensorFlow Recommenders 등에서 활용

---

## 📌 SVD 기반 추천의 수학적 모델

SVD는 사용자 \( u_i \)와 아이템 \( v_j \)의 내적을 통해  
다음과 같은 예측 평점을 계산합니다:

\[
\hat{r}_{ij} = \vec{u}_i^T \Sigma_k \vec{v}_j
\]

→ 이는 사용자와 아이템 간의 **유사도 기반 예측 모델**입니다.

---

## ✅ 정리 요약

| 응용 분야 | 핵심 역할 | 장점 |
|-----------|-----------|------|
| 이미지 압축 | 데이터 근사 | 계산 단순, 시각화 용이 |
| 추천 시스템 | 평점 예측 | 숨겨진 정보 추출, 확장성 |