---
layout: post
title: 기계학습 - 계층적 군집화
date: 2025-08-19 15:25:23 +0900
category: 기계학습
---
# 계층적 군집화(Hierarchical Clustering)

## 개요: 왜 “계층적”인가?

- **계층적 군집화(Hierarchical Clustering)** 는 데이터의 **군집 구조를 여러 스케일에서** 탐색한다.
- **사전 K 지정 불필요**: 덴드로그램에서 적절한 높이를 잘라 군집 수를 결정.
- 두 가지 큰 흐름:
  - **병합적(Agglomerative)**: 각 점 → 군집으로 시작, **가까운 군집부터 합침**(bottom-up).
  - **분할적(Divisive)**: 전체 집합 → 점차 **쪼갬**(top-down). (고전 알고리즘 DIANA 등, 실무에선 병합이 보편)

---

## 거리와 링크 함수(Linkage): “군집 간 거리”를 어떻게 정의할까?

군집 간 거리를 정하는 방식(= **링크 함수**)이 결과에 **지배적인 영향**을 준다.

| 이름 | 수식(군집 \(A,B\)) | 성향/특징 |
|---|---|---|
| **단일 연결 (Single)** | $$d(A,B)=\min_{x\in A,y\in B}\|x-y\|$$ | **사슬(Chaining)** 효과, 복잡한 모양 보전, 노이즈에 민감 |
| **완전 연결 (Complete)** | $$d(A,B)=\max_{x\in A,y\in B}\|x-y\|$$ | 조밀·둥근 군집 선호, 노이즈·이상치 강건 |
| **평균 연결 (UPGMA)** | $$d(A,B)=\frac{1}{|A||B|}\sum_{x\in A}\sum_{y\in B}\|x-y\|$$ | 균형적, 단일/완전의 절충 |
| **중심 연결 (Centroid)** | $$d(A,B)=\|\mu_A-\mu_B\|$$ | 빠름, **역전(inversion)** 발생 가능(덴드로그램 높이가 내려감) |
| **Ward** | $$\Delta SSE=\frac{|A||B|}{|A|+|B|}\|\mu_A-\mu_B\|^2$$ | 클러스터 내 제곱합(SSE) 증가 최소 → **K-평균 유사** (유클리드 전용) |

> **주의**: Ward는 본질적으로 **유클리드 공간에서 SSE 최소화**와 동치. 코사인/맨해튼 등과 임의 결합은 이론적 일관성이 약해짐.

---

## 수학적 정당화: 덴드로그램·초메트릭·MST

### 덴드로그램(Dendrogram)과 초메트릭(ultrametric)

- 병합 과정에서 각 병합의 높이 \(h\)는 해당 **두 군집 간 거리**(혹은 비용).
- 덴드로그램으로 유도되는 거리 \(d_U\) (두 점이 **처음 합쳐지는 높이**로 정의)는 **초메트릭**:
  $$
  d_U(x,z)\le \max\{d_U(x,y), d_U(y,z)\}.
  $$
  덕분에 **계층 구조**가 수학적으로 일관된다.

### MST(최소신장트리)와 단일 연결의 동일성

- **단일 연결** 병합 순서 ↔ **Kruskal 알고리즘**이 만든 **MST의 간선 가중치 순서**와 동일.
- 증개요: 단일 연결은 항상 **가장 가까운 두 컴포넌트**를 잇는 간선을 택하며, 이는 Kruskal의 선택 규칙과 일치.

### Ward의 목적: SSE 증가 최소화

- 군집 \(C\)의 SSE:
  $$
  SSE(C)=\sum_{x\in C}\|x-\mu_C\|^2.
  $$
- 두 군집 \(A,B\) 병합 시 SSE 증가:
  $$
  \Delta SSE= \frac{|A||B|}{|A|+|B|}\|\mu_A-\mu_B\|^2.
  $$
- 따라서 Ward는 각 단계에서 **\(\Delta SSE\) 최소** 쌍을 합친다.

---

## 병합적 알고리즘: Lance–Williams 업데이트(일반형)

병합 후 새 군집 \(C=A\cup B\)와 다른 군집 \(D\) 사이 거리 갱신:
$$
d(C,D) = \alpha_A d(A,D)+\alpha_B d(B,D) + \beta d(A,B) + \gamma |d(A,D)-d(B,D)|.
$$

표준 링크들의 매개변수:

| 링크 | \(\alpha_A\) | \(\alpha_B\) | \(\beta\) | \(\gamma\) |
|---|---:|---:|---:|---:|
| 단일 | 1/2 | 1/2 | 0 | \(-1/2\) |
| 완전 | 1/2 | 1/2 | 0 | \(+1/2\) |
| 평균(UPGMA) | \(\frac{|A|}{|A|+|B|}\) | \(\frac{|B|}{|A|+|B|}\) | 0 | 0 |
| 중심 | \(\frac{|A|}{|A|+|B|}\) | \(\frac{|B|}{|A|+|B|}\) | \(-\frac{|A||B|}{(|A|+|B|)^2}\) | 0 |
| Ward | \(\frac{|A|+|D|}{|A|+|B|+|D|}\) | \(\frac{|B|+|D|}{|A|+|B|+|D|}\) | \(-\frac{|D|}{|A|+|B|+|D|}\) | 0 |

> 이 갱신식 덕분에 **O(n^2)** 메모리로 반복 갱신이 가능하다(나이브는 \(O(n^3)\) 시간, 고급 구현은 \(O(n^2 \log n)\) 근처).

---

## — 의사코드 & 복잡도

### 의사코드

```
입력: 표본 {x1,...,xn}, 거리함수 d, 링크 L
1) 초기 군집 = { {x1}, {x2}, ..., {xn} }
2) 모든 쌍의 군집 거리 계산 -> 우선순위큐(최소힙)
3) while 군집 수 > 1:
      (A,B) = 가장 가까운 군집 쌍 pop
      C = A ∪ B
      A,B 제거; C 추가
      모든 D에 대해 d(C,D) Lance–Williams로 갱신, 힙 갱신
      병합 높이 기록(덴드로그램)
4) 반환: 덴드로그램/병합 순서/높이
```

### 복잡도

- **시간**: 나이브 \(O(n^3)\), 힙·효율적 갱신으로 \(O(n^2 \log n)\) 수준.
- **공간**: 거리행렬 \(O(n^2)\). 대규모 데이터에선 **샘플링/요약(BIRCH)** 필요.

---

## 자르기(Cutting)와 군집 수 결정

### 고정 높이/고정 군집 수

- **높이 기준**: 덴드로그램 **세로축(거리)** 에서 천이점(gap)이 큰 곳에서 컷.
- **maxclust**: 원하는 **최대 군집 수 \(K\)** 로 컷.

### Inconsistency 계수

- 특정 링크가 **주변 높이 대비 비정상적으로 큼**을 지표화, 임계로 컷.

### 내부 검증지표와 결합

- **실루엣(Silhouette)**: 각 \(K\) 컷에서 평균 실루엣을 계산, 최대 \(K\) 선택.
- Dunn, Davies–Bouldin, Calinski–Harabasz 등도 활용.

---

## 검증·해석

### 상관계수

- 원거리 \(d(x_i,x_j)\)와 덴드로그램 **코페네틱 거리** \(d_c(x_i,x_j)\)의 상관:
  $$
  \text{coph}=\text{corr}\big(\{d_{ij}\},\{d_{c,ij}\}\big).
  $$
- 1에 가까울수록 덴드로그램이 원래 거리 구조를 잘 보존.

### 안정성(bootstrap)

- 리샘플/잡음 주입 후 덴드로그램/컷 결과의 **일관성** 측정(“pvclust”류 아이디어).

### 역전(Inversions)

- **중심 링크** 등은 희귀하게 병합 높이가 **감소**할 수 있음(덴드로그램 역전). 해석 시 주의.

---

## 전처리·거리 선택 체크리스트

1) **스케일링**: 거리 기반 → **표준화(z-score)** 권장. 단위가 다른 피처 혼재 시 필수.
2) **거리**:
   - 수치형: 유클리드(워드 전용), 맨해튼(이상치 강건), 코사인(방향 위주).
   - 범주형: 해밍/고워(Gower) 등 혼합거리.
3) **결측**: 삭제/임퓨테이션 후 거리 계산.
4) **이상치**: 완전/평균/워드가 단일보다 강건.
5) **피처 상관** 많으면 PCA로 차원축소 후 군집(워드+PCA 조합이 자주 쓰임).

---

## 대규모 데이터 대응

- **BIRCH**: CF-Tree로 점진 요약 → 말단 클러스터를 다시 계층/평면 군집.
- **Truncated/Approx**: 랜덤 서브샘플 덴드로그램 → 나머지를 최근접 할당.
- **HDBSCAN**: 밀도 기반 **계층적** 군집(노이즈 견고, 대규모 적합) — 성격은 DBSCAN 계열.

---

## 파이썬 실전 예제

### SciPy 덴드로그램 + 다양한 링크 비교

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs, make_moons
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet
from scipy.spatial.distance import pdist

# 데이터

X, _ = make_blobs(n_samples=120, centers=3, cluster_std=0.70, random_state=7)

# 링크 별 덴드로그램

fig, axes = plt.subplots(2, 2, figsize=(12, 8))
for ax, method in zip(axes.ravel(), ["single", "complete", "average", "ward"]):
    Z = linkage(X, method=method)  # Ward는 유클리드 전용
    dendrogram(Z, ax=ax, no_labels=True, color_threshold=None)
    ax.set_title(f"Linkage: {method}")
    ax.set_xlabel("Samples")
    ax.set_ylabel("Distance")
plt.tight_layout()
plt.show()

# 코페네틱 상관으로 링크 품질 비교

D = pdist(X)  # 원거리
for method in ["single", "complete", "average", "ward"]:
    Z = linkage(X, method=method)
    coph_corr, _ = cophenet(Z, D)
    print(f"{method:8s} coph_corr = {coph_corr:.3f}")

# 컷팅: 군집수 3

Z = linkage(X, method="ward")
labels_k3 = fcluster(Z, t=3, criterion="maxclust")
print("labels (k=3) head:", labels_k3[:10])
```

### “사슬” 사례: 단일 vs 완전/워드

```python
from sklearn.datasets import make_moons

X, _ = make_moons(n_samples=200, noise=0.07, random_state=0)

for method in ["single", "complete", "average", "ward"]:
    Z = linkage(X, method=method)
    plt.figure(figsize=(6,4))
    dendrogram(Z, no_labels=True)
    plt.title(f"Moons - {method}")
    plt.show()
```
- **single**은 반달의 경계를 따라 **사슬처럼 연결** → 컷 위치에 따라 군집이 쉽게 끊김.
- **complete/ward**는 보다 **조밀한 군집**을 형성.

### scikit-learn API: AgglomerativeClustering (거리 임계 컷팅)

```python
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler

X, _ = make_blobs(n_samples=400, centers=4, cluster_std=1.0, random_state=42)
X = StandardScaler().fit_transform(X)

# 군집수 지정

model_k = AgglomerativeClustering(n_clusters=4, linkage="ward")   # ward는 affinity='euclidean' 고정
labels_k = model_k.fit_predict(X)

# 거리 임계로 컷 (연결 그래프가 여러 컴포넌트가 되면 군집수 결정)

model_d = AgglomerativeClustering(distance_threshold=10.0, n_clusters=None, linkage="average", metric="euclidean")
labels_d = model_d.fit_predict(X)

print("k-cut unique:", len(np.unique(labels_k)))
print("dist-cut unique:", len(np.unique(labels_d)))
```

### 혼합거리(고워 유사)로 계층 군집 (간단 구현)

```python
import numpy as np
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import linkage, fcluster

def gower_distance(X_num, X_cat):
    # X_num: [0,1]로 스케일된 수치, X_cat: 같은 카테고리는 0, 다르면 1
    N = X_num.shape[0]
    D = np.zeros((N, N))
    # 수치 평균 L1
    D_num = squareform(pdist(X_num, metric="cityblock")) / X_num.shape[1]
    # 범주 해밍
    D_cat = squareform(pdist(X_cat, metric="hamming")) if X_cat.size else 0.0
    # 가중 평균(동일 가중)
    if X_cat.size:
        D = 0.5 * D_num + 0.5 * D_cat
    else:
        D = D_num
    return D

# 예시 데이터(숫자+범주)

np.random.seed(0)
Xn = np.random.rand(60, 3)         # 수치 -> [0,1] 가정
Xc = (np.random.rand(60, 2) > 0.5).astype(int)

D = gower_distance(Xn, Xc)
Z = linkage(squareform(D), method="average")   # 거리행렬 기반
labels = fcluster(Z, t=3, criterion="maxclust")
print(np.bincount(labels))
```

### 코페네틱 상관 & 실루엣 기반 K 선택

```python
from sklearn.metrics import silhouette_score

X, _ = make_blobs(n_samples=300, centers=5, cluster_std=0.9, random_state=3)
D = pdist(X)
best = (-1, None)  # (silhouette, k)

Z = linkage(X, method="ward")
for k in range(2, 9):
    labels = fcluster(Z, t=k, criterion="maxclust")
    sil = silhouette_score(X, labels, metric="euclidean")
    if sil > best[0]:
        best = (sil, k)
    print(f"k={k} silhouette={sil:.3f}")

print("Best k:", best[1])
```

---

## Ward vs K-평균: 언제 누가 유리한가?

- **Ward**: 병합적, **전역적 SSE 감소**를 단계적 탐욕으로 최적화 → **덜 민감**(초기화 없음), 계산은 더 무거움.
- **K-평균**: 반복적 할당-중심 업데이트, 매우 빠름(대용량), **초기화 민감**(K-means++ 권장).
- **실무 팁**: Ward로 덴드로그램 → 적절한 K 추정 → 그 K로 K-평균 재학습(속도/안정 절충).

---

## 흔한 함정과 회피 전략

1) **스케일 미스매치** → 표준화/정규화 필수.
2) **링크 선택 오류** → 데이터 지형: 사슬(단일) vs 조밀/둥근(완전/워드) 고려.
3) **노이즈/이상치** → 완전/평균/워드, 혹은 전처리(로버스트스케일, 이상치 제거).
4) **대규모 \(n\)** → BIRCH 요약 → 계층 군집.
5) **중심/평균 링크 역전** → 덴드로그램 해석 주의, 필요 시 링크 변경.
6) **임의 컷팅** → 실루엣/코페네틱/업무 지표로 보조.
7) **혼합 데이터형** → 고워/해밍+맨해튼 등 맞춤 거리.

---

## 요약·결론

- 계층적 군집화는 **멀티스케일 구조**를 파악하고 **덴드로그램**으로 **설명 가능성**이 높다.
- **링크 함수**가 핵심(단일/완전/평균/중심/Ward), **Ward=K-평균 유사**(SSE 감소).
- **MST 동일성(단일)**, **초메트릭** 성질로 이론적 일관성 확보.
- 컷팅은 **높이/군집수/인컨시스턴시** + **실루엣/코페네틱**으로 결정.
- 대규모/혼합형 데이터에 대한 **현실적 확장(BIRCH/고워)** 과 **전처리** 전략이 실무 성패를 좌우한다.

---

## 미니 레퍼런스(개념 정리용 수식)

- 단일/완전/평균/중심 링크 정의: 위 표 참조.
- **Ward 증분 SSE**:
  $$
  \Delta SSE=\frac{|A||B|}{|A|+|B|}\|\mu_A-\mu_B\|^2.
  $$
- **Lance–Williams** 갱신식:
  $$
  d(C,D)=\alpha_A d(A,D)+\alpha_B d(B,D)+\beta d(A,B)+\gamma|d(A,D)-d(B,D)|.
  $$
- **코페네틱 상관**:
  $$
  \text{corr}\big(\{d_{ij}\},\{d_{c,ij}\}\big).
  $$

---

## 실전 체크리스트

- [ ] 표준화/결측/이상치 처리 완료
- [ ] 링크 함수 목적에 맞게 선택(단일/완전/평균/Ward)
- [ ] 덴드로그램에서 천이점 확인 + 내부지표(실루엣/코페네틱) 병행
- [ ] 대규모면 BIRCH/샘플링/근사 방법 적용
- [ ] 혼합형 데이터는 고워/해밍 등 맞춤 거리
- [ ] 결과 **업무 의미/설명가능성** 검증(클러스터 프로파일링, 피처 분포 비교)
