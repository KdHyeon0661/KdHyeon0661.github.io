---
layout: post
title: 기계학습 - 벡터, 행렬 연산
date: 2025-08-14 19:20:23 +0900
category: 기계학습
---
# 선형대수(벡터, 행렬 연산)

## 벡터(Vector)

### 정의·표기

- \( \mathbf{x}\in\mathbb{R}^n \) 은 \(n\)-차원 **열 벡터**.
- 머신 러닝에서 보통 **특징 벡터**(feature vector)로 사용.

예:
$$
\mathbf{x}=\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}
$$

### 기본 연산

- 덧셈: \( \mathbf{a}+\mathbf{b}=(a_i+b_i)_i \)
- 스칼라 곱: \( c\mathbf{a}=(ca_i)_i \)

### 내적·각도·코사인 유사도

- **내적**:
$$
\mathbf{a}\cdot\mathbf{b}=\sum_{i=1}^n a_i b_i=\mathbf{a}^\top \mathbf{b}
$$
- **L2 노름**:
$$
\|\mathbf{a}\|_2=\sqrt{\sum_i a_i^2}
$$
- **각도**:
$$
\cos\theta=\frac{\mathbf{a}^\top \mathbf{b}}{\|\mathbf{a}\|_2\ \|\mathbf{b}\|_2}
$$

### 다른 노름

- \(L_1\): \( \|\mathbf{a}\|_1=\sum_i|a_i| \)
- \(L_\infty\): \( \|\mathbf{a}\|_\infty=\max_i |a_i| \)

### 정규화(단위 벡터)

$$
\hat{\mathbf{a}}=\frac{\mathbf{a}}{\|\mathbf{a}\|_2},\quad \|\hat{\mathbf{a}}\|_2=1
$$

```python
import numpy as np

a = np.array([3., 4.])
b = np.array([1., 0.])

dot = a @ b
cos = dot / (np.linalg.norm(a) * np.linalg.norm(b))
a_hat = a / np.linalg.norm(a)
print("dot:", dot, "cos:", cos, "unit a:", a_hat)
```

---

## 행렬(Matrix)

### 정의·데이터 표기

- \( X\in\mathbb{R}^{m\times n} \) : **m개 샘플 × n개 특징** (행=샘플, 열=특징).

### 전치·덧셈·스칼라

- 전치: \( (A^\top)_{ij}=A_{ji} \)

### 행렬 곱

- \( A\in\mathbb{R}^{m\times n},\ B\in\mathbb{R}^{n\times p}\Rightarrow AB\in\mathbb{R}^{m\times p} \)
- 원소:
$$
(AB)_{ij}=\sum_{k=1}^n a_{ik} b_{kj}
$$

**성질**
- 결합법칙: \( (AB)C=A(BC) \)
- 분배법칙: \( A(B+C)=AB+AC \)
- **교환법칙 성립X**: 일반적으로 \(AB\ne BA\)

### 단위·역행렬·행렬식

- 단위행렬 \(I\): \( AI=IA=A \)
- **역행렬** \(A^{-1}\): \( AA^{-1}=A^{-1}A=I \) (정방·가역일 때만)
- 행렬식 \( \det(A)\ne 0 \) ↔ 가역

```python
import numpy as np

A = np.array([[1.,2.],[3.,4.]])
B = np.array([[5.,6.],[7.,8.]])
print("A@B=\n", A@B)
print("det(A)=", np.linalg.det(A))
print("A^{-1}=\n", np.linalg.inv(A))
```

---

## 부분공간·랭크·정사영

### 랭크와 부분공간

- **열공간** \( \mathcal{R}(A)=\{A\mathbf{z}\} \), **영공간** \( \mathcal{N}(A)=\{\mathbf{x}:A\mathbf{x}=0\} \)
- **랭크** \( \mathrm{rank}(A)=\dim \mathcal{R}(A) \)

### 선형독립·Span·기저

- 독립: \( \sum_i c_i \mathbf{v}_i=0 \Rightarrow c_i=0 \)
- Span: 벡터들의 모든 선형결합 집합
- 기저: 독립 + Span 전체 생성

### 직교 정사영

- \(X\in\mathbb{R}^{m\times n}\) (열 독립)일 때, \( \mathbf{y} \)를 \( \mathcal{R}(X) \)로 정사영:
$$
P = X(X^\top X)^{-1}X^\top,\quad \hat{\mathbf{y}}=P\mathbf{y}
$$
- 성질: \( P^2=P,\ P^\top=P \)

```python
import numpy as np

X = np.array([[1.,0.],[1.,1.],[1.,2.]])  # m=3, n=2 (열 독립)
y = np.array([1.,2.,2.5])
P = X @ np.linalg.inv(X.T @ X) @ (X.T)
y_hat = P @ y
print("idempotent error:", np.linalg.norm(P@P - P))
print("symmetry error:", np.linalg.norm(P.T - P))
print("projection:", y_hat)
```

---

## 특수 행렬

- **대칭**: \(A^\top = A\)
- **직교**: \(Q^\top Q=QQ^\top=I\) → 길이/각도 보존, \(Q^{-1}=Q^\top\)
- **대각**: \(D=\mathrm{diag}(d_1,\dots,d_n)\)
- **양의 준정/정(PSD/PD)**:
$$
\mathbf{z}^\top A \mathbf{z}\ge 0\ (\text{PSD}),\quad >0\ (\text{PD},\ \mathbf{z}\ne 0)
$$
- **센터링 행렬** \( H=I-\frac{1}{m}\mathbf{1}\mathbf{1}^\top \): 평균 제거, \(H^2=H,\ H^\top=H\)

---

## 행렬 분해

### QR 분해 (직교 × 상삼각)

- \( A=QR \), \(Q^\top Q=I\), \(R\) 상삼각
- **최소제곱**에 안정적: \( \min_w \|Aw-b\|_2 \Rightarrow Rw=Q^\top b \)

### 고유분해

- \( A=A^\top \Rightarrow A=Q\Lambda Q^\top \), \( \Lambda=\mathrm{diag}(\lambda_i) \)
- \( \lambda_i \)가 모두 \( \ge 0 \) 이면 PSD

### SVD (핵심)

- 항상 존재: \( A=U\Sigma V^\top \)
- \(U\in\mathbb{R}^{m\times m},\ V\in\mathbb{R}^{n\times n}\) 직교, \( \Sigma=\mathrm{diag}(\sigma_1\ge \cdots\ge 0) \)
- **랭크-r 근사**: \( A_r = \sum_{i=1}^r \sigma_i \mathbf{u}_i \mathbf{v}_i^\top \) (최적)
- **PCA**와 직결: \(X\) 중심화 후 \(X=U\Sigma V^\top\Rightarrow\) 주성분= \(V\), 분산= \(\Sigma^2/(m-1)\)

```python
import numpy as np

X = np.array([[2.,0.],[0.,1.],[1.,1.],[3.,2.]])
Xc = X - X.mean(axis=0, keepdims=True)
U,S,Vt = np.linalg.svd(Xc, full_matrices=False)
explained_var = (S**2)/(len(X)-1)
print("PC directions (rows of Vt):\n", Vt)
print("explained variance:", explained_var)
Z = Xc @ Vt[:,:2]  # 2D 투영
```

---

## 노름과 거리

### 벡터 노름

- \( \|\cdot\|_1,\ \|\cdot\|_2,\ \|\cdot\|_\infty \), 삼각부등식 및 Cauchy–Schwarz:
$$
|\mathbf{a}^\top \mathbf{b}|\le \|\mathbf{a}\|_2\,\|\mathbf{b}\|_2
$$

### 행렬 노름

- **Frobenius**:
$$
\|A\|_F=\sqrt{\sum_{ij} a_{ij}^2}=\sqrt{\mathrm{tr}(A^\top A)}
$$
- **스펙트럴**(연산자 2-노름): \( \|A\|_2=\sigma_{\max}(A) \)

---

## 선형시스템·최소제곱

### 선형시스템

- \(Ax=b\) 해: 가역이면 \(x=A^{-1}b\), 일반적으론 **lstsq**/SVD 권장.

### 최소제곱(OLS)

$$
\min_w \|Xw-y\|_2^2
$$
- 정상방정식(수치 취약):
$$
(X^\top X)w=X^\top y,\quad w=(X^\top X)^{-1}X^\top y
$$
- **QR/SVD 권장**: 조건수 나쁠 때 안정적.

### 릿지(정규화)

$$
\min_w \|Xw-y\|_2^2+\lambda\|w\|_2^2
\Rightarrow
w=(X^\top X+\lambda I)^{-1}X^\top y
$$

```python
import numpy as np

rng = np.random.default_rng(0)
X = rng.normal(size=(200,5))
w_true = np.array([2.,-1.,0.,0.5,3.])
y = X@w_true + rng.normal(0,0.2,size=200)

# OLS: lstsq (SVD 기반)

w_lstsq, *_ = np.linalg.lstsq(X, y, rcond=None)

# Ridge (closed form)

lam = 1e-1
w_ridge = np.linalg.solve(X.T@X + lam*np.eye(X.shape[1]), X.T@y)
print("||w_lstsq-w_true||:", np.linalg.norm(w_lstsq - w_true))
print("||w_ridge-w_true||:", np.linalg.norm(w_ridge - w_true))
```

---

## 행렬 미분·트레이스 트릭

### 자주 쓰는 항등식

- \( \mathrm{tr}(AB)=\mathrm{tr}(BA) \)
- \( \|A\|_F^2=\mathrm{tr}(A^\top A) \)
- \( \nabla_X \mathrm{tr}(A^\top X)=A \)
- \( \nabla_x (a^\top x)=a \)
- \( \nabla_x \tfrac{1}{2}\|Ax-b\|_2^2 = A^\top(Ax-b) \)
- \( \nabla_x (x^\top A x)=(A+A^\top)x \) (대칭 \(A\)면 \(2Ax\))

### 선형회귀 손실의 기울기(벡터화)

$$
L(w)=\frac{1}{2}\|Xw-y\|_2^2
\Rightarrow
\nabla_w L = X^\top(Xw-y)
$$

```python
import numpy as np

def grad_quad(X, y, w):
    return X.T @ (X@w - y)

# 검증: 수치미분과 비교

def num_grad(f, w, eps=1e-6):
    g = np.zeros_like(w)
    for i in range(len(w)):
        e = np.zeros_like(w); e[i]=1.0
        g[i] = (f(w+eps*e) - f(w-eps*e)) / (2*eps)
    return g

X = np.random.randn(50, 3)
w = np.random.randn(3)
y = X@w + 0.1*np.random.randn(50)
f = lambda u: 0.5*np.linalg.norm(X@u - y)**2
print("||analytic - numeric||:", np.linalg.norm(grad_quad(X,y,w) - num_grad(f,w)))
```

---

## 수치 안정성·조건수·스케일링·화이트닝

### 조건수

- \( \kappa_2(A)=\|A\|_2\|A^{-1}\|_2=\sigma_{\max}/\sigma_{\min} \) (클수록 불안정)

### 스케일링/표준화

- 특징 스케일 차이는 \(X^\top X\)의 조건수 악화 → **표준화** 권장.

### 화이트닝(공분산 정규화)

- \(X_c\) (열 평균 0), SVD: \( X_c=U\Sigma V^\top \)
- 공분산: \( \Sigma_X=\frac{1}{m-1}X_c^\top X_c = V \frac{\Sigma^2}{m-1} V^\top \)
- **PCA 화이트닝**:
$$
X_{\text{white}}=X_c\,V\,\mathrm{diag}\!\left(\frac{1}{\sqrt{\Sigma^2/(m-1)+\epsilon}}\right)
$$

```python
import numpy as np

X = np.random.randn(500, 4) @ np.array([[2,0,0,0],[0,1,0,0],[0,0,0.1,0],[0,0,0,0.05]])
Xc = X - X.mean(axis=0, keepdims=True)
U,S,Vt = np.linalg.svd(Xc, full_matrices=False)
Xwhite = Xc @ Vt.T @ np.diag(1.0/np.sqrt(S**2/(len(X)-1) + 1e-8))
cov_white = (Xwhite.T @ Xwhite) / (len(X)-1)
print("cov approx I error:", np.linalg.norm(cov_white - np.eye(4)))
```

---

## 희소 행렬과 대규모 연산 팁

- 텍스트/원-핫은 **희소**. 메모리·시간을 위해 **CSR/CSC** 사용.
- 거리/유사도 계산은 **벡터화**(브로드캐스팅·einsum)로 처리.
- 대규모 SVD/PCA는 **랜덤화 SVD**(scikit-learn `randomized_svd`) 고려.

```python
# 예시 아이디어 (SciPy 사용 가정)
# from scipy.sparse import csr_matrix
# # 희소로 변환 후 선형모델/최근접검색 등에 사용

```

---

## 머신 러닝 실전 예제 모음

### 선형 회귀(벡터화 학습 루프)

```python
import numpy as np

rng = np.random.default_rng(42)
n,d = 1000, 50
X = rng.normal(size=(n,d))
w_true = rng.normal(size=d)
y = X@w_true + 0.1*rng.normal(size=n)

w = np.zeros(d)
lr = 1e-2
for t in range(500):
    grad = X.T @ (X@w - y) / n     # ∇ = X^T(Xw - y)/n
    w -= lr * grad
print("train MSE:", np.mean((X@w - y)**2))
```

### PCA = SVD (주성분 투영/복원)

```python
import numpy as np

X = rng.normal(size=(500, 5))
Xc = X - X.mean(axis=0, keepdims=True)
U,S,Vt = np.linalg.svd(Xc, full_matrices=False)
k = 2
Z = Xc @ Vt[:k].T        # k차원 주성분 점수
X_rec = Z @ Vt[:k]       # 복원(중심화 좌표)
rec_err = np.linalg.norm(Xc - X_rec) / np.linalg.norm(Xc)
print("relative recon error:", rec_err)
```

### 정사영 행렬과 잔차 직교성

- 최소제곱 해 \( \hat{y}=P y \), 잔차 \( r=y-\hat{y} \) 는 \( \mathcal{R}(X) \) 에 **직교**:
$$
X^\top (y - \hat{y}) = 0
$$

```python
X = np.c_[np.ones(n), rng.normal(size=(n,2))]
y = X@np.array([1., 2., -3.]) + rng.normal(0,0.1,size=n)
P = X @ np.linalg.inv(X.T@X) @ X.T
yhat = P @ y
r = y - yhat
print("orthogonality error:", np.linalg.norm(X.T@r))
```

### K-Means 거리의 벡터화(자주 쓰는 항등식)

- 항등식:
$$
\|\mathbf{x}-\mathbf{c}\|_2^2
= \|\mathbf{x}\|_2^2 + \|\mathbf{c}\|_2^2 - 2\mathbf{x}^\top \mathbf{c}
$$

```python
# → 거리 제곱 (n,k)

X = rng.normal(size=(1000, 32))
C = rng.normal(size=(10, 32))
X2 = np.sum(X**2, axis=1, keepdims=True)        # (n,1)
C2 = np.sum(C**2, axis=1)[None, :]              # (1,k)
d2 = X2 + C2 - 2*(X @ C.T)                      # (n,k)
assign = np.argmin(d2, axis=1)
print("shape d2:", d2.shape, "clusters:", np.bincount(assign, minlength=10))
```

---

## 체크리스트·요약

### 체크리스트

- [ ] **형상(Shape)**: 행=샘플, 열=특징 일관 유지
- [ ] **스케일링**: 표준화/화이트닝으로 조건수 개선
- [ ] **분해 선택**: 역/정상방정식 대신 **QR/SVD** 우선
- [ ] **프로젝터**: \(P=X(X^\top X)^{-1}X^\top\) 의 \(P^2=P,\ P^\top=P\) 확인
- [ ] **PCA/SVD**: 중심화 필수, 분산= \(\Sigma^2/(m-1)\)
- [ ] **미분 항등식**: \(\nabla \tfrac{1}{2}\|Xw-y\|^2=X^\top(Xw-y)\) 숙지
- [ ] **벡터화**: 브로드캐스팅·einsum으로 루프 제거
- [ ] **희소성**: 고차 희소는 CSR로 메모리/시간 절약

### 요약

- **벡터/행렬**은 데이터와 모델을 한 번에 표현하고 계산하는 **기본 문법**이다.
- **QR/SVD/고유분해**는 해석·최적화·차원축소의 **핵심 도구**다.
- **정사영/프로젝터/센터링**은 최소제곱·PCA 등 **핵심 알고리즘의 기하**를 설명한다.
- 수치적 관점에서 **조건수/스케일링/정규화**가 **성능과 안정성**을 좌우한다.
- 실전에서는 **벡터화·희소연산**·적절한 **분해**를 통해 **빠르고 견고한** 파이프라인을 만든다.
