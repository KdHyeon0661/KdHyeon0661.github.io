---
layout: post
title: 미적분 - 연쇄법칙
date: 2025-07-27 23:20:23 +0900
category: 미적분
---
# 연쇄법칙(Chain Rule)

연쇄법칙은 **합성함수의 미분법**입니다. “겉함수의 도함수(안은 그대로) × 속함수의 도함수”라는 **지역 선형 근사**의 연쇄가 본질이며, 단변수 → 다변수 → **야코비안/그래디언트/헤시안**까지 일관되게 확장됩니다.

---

## 0. 표기 및 전제

- 스칼라: \(x\in\mathbb R\). 벡터: \(\mathbf x\in\mathbb R^n\).  
- 스칼라함수 \(f:\mathbb R\to\mathbb R\), 다변수 \(f:\mathbb R^n\to\mathbb R\).  
- 벡터함수 \(\mathbf f:\mathbb R^n\to\mathbb R^m\).  
- 그래디언트 \(\nabla f(\mathbf x)\in\mathbb R^n\), 야코비안 \(D\mathbf f(\mathbf x)\in\mathbb R^{m\times n}\).

---

## 1. 단변수(일변수) 연쇄법칙

### 1.1 공식
$$
y=f(u),\quad u=g(x)\quad\Rightarrow\quad \frac{dy}{dx}=f'(u)\cdot g'(x)=f'\!\big(g(x)\big)\,g'(x).
$$

### 1.2 직관
- **작은 변화** \(dx\)가 먼저 \(du=g'(x)\,dx\)로 바뀌고, 이것이 다시 \(dy=f'(u)\,du\)로 바뀜.  
- 선형 근사들의 **곱**이 전체 변화율.

### 1.3 빠른 예제
- \(y=\sin(x^2)\)  
  $$
  \frac{dy}{dx}=\cos(x^2)\cdot 2x.
  $$
- \(y=\ln\!\big(1+e^{3x}\big)\)  
  $$
  \frac{dy}{dx}=\frac{1}{1+e^{3x}}\cdot e^{3x}\cdot 3=\frac{3}{1+e^{-3x}}.
  $$

---

## 2. 다변수 연쇄법칙

### 2.1 스칼라 출력, 매개변수 1개
\(z=f(x,y)\), \(x=x(t),y=y(t)\)이면
$$
\frac{dz}{dt} = f_x\,\frac{dx}{dt} + f_y\,\frac{dy}{dt}.
$$
모든 “경로”의 기여를 **합**합니다.

**예제**
- \(z=x^2+y^2\), \(x=t^2\), \(y=\sin t\).
$$
\frac{dz}{dt}=2x\cdot 2t + 2y\cdot \cos t = 4t^3 + 2\sin t \cos t.
$$

### 2.2 일반형 (여러 입력/여러 매개변수)
\(z=f(x_1,\dots,x_n)\), 각 \(x_i=x_i(t_1,\dots,t_m)\)이면
$$
\frac{\partial z}{\partial t_j}=\sum_{i=1}^n \frac{\partial f}{\partial x_i}\,\frac{\partial x_i}{\partial t_j}.
$$

### 2.3 벡터 출력(행렬형)
\(\mathbf y=\mathbf f(\mathbf u)\), \(\mathbf u=\mathbf g(\mathbf x)\)일 때
$$
D\mathbf y(\mathbf x) = D\mathbf f(\mathbf u)\big|_{\mathbf u=\mathbf g(\mathbf x)}\,\cdot\, D\mathbf g(\mathbf x).
$$
> 차원 일치: \((m\times p)\cdot(p\times n)=(m\times n)\).

---

## 3. 그래디언트/야코비안/헤시안 관점

### 3.1 스칼라 합성의 그래디언트
\(h(\mathbf x)=f\!\big(\mathbf g(\mathbf x)\big)\), \(\mathbf g:\mathbb R^n\to\mathbb R^p\), \(f:\mathbb R^p\to\mathbb R\).
$$
\nabla h(\mathbf x) = \big(D\mathbf g(\mathbf x)\big)^\top \,\nabla f\!\big(\mathbf g(\mathbf x)\big).
$$
> (VJP: Vector-Jacobian Product; 역모드 자동미분의 핵)

### 3.2 헤시안(스칼라→스칼라의 2계 미분)
\(h(x)=f(g(x))\):
$$
h''(x)=f''(g(x))\,[g'(x)]^2+f'(g(x))\,g''(x).
$$
다변수 버전은
$$
\nabla^2 h(\mathbf x) = J_g(\mathbf x)^\top \,\nabla^2 f(\mathbf g(\mathbf x))\, J_g(\mathbf x)\ +\ \sum_{k=1}^p \frac{\partial f}{\partial u_k}\,\nabla^2 g_k(\mathbf x),
$$
여기서 \(J_g=D\mathbf g\).

---

## 4. 실전 예제 (단·다변수)

### 4.1 다중 중첩 예제
$$
y=\exp\!\Big(\sin\!\big(x^3+1\big)\Big).
$$
- \(u=x^3+1\), \(v=\sin u\), \(y=e^v\).
$$
\frac{dy}{dx}=e^v\cdot \cos u \cdot 3x^2
=\exp\!\big(\sin(x^3+1)\big)\,\cos(x^3+1)\,3x^2.
$$

### 4.2 스칼라→스칼라 (로지스틱·로그-합-지수)
- \(y=\sigma(ax+b)=\dfrac{1}{1+e^{-(ax+b)}}\)  
  $$
  \frac{dy}{dx}=\sigma(z)\big(1-\sigma(z)\big)\cdot a,\quad z=ax+b.
  $$
- \(y=\log\!\sum_{i=1}^n e^{a_i x}\)  
  $$
  \frac{dy}{dx}=\sum_{i=1}^n \underbrace{\frac{e^{a_i x}}{\sum_j e^{a_j x}}}_{\text{softmax}_i}\,a_i.
  $$

### 4.3 다변수 스칼라: 합성 \(f(\mathbf g(\mathbf x))\)
- \(f(u,v)=u^2+v^2\), \(\mathbf g(x,y)=(x^2-y,\ \sin(xy))\).
  - \(\nabla f(u,v)=\langle 2u,\ 2v\rangle\).
  - \(J_g=\begin{bmatrix} 2x & -1 \\ y\cos(xy) & x\cos(xy) \end{bmatrix}\).
  - \(\nabla h(x,y)=J_g^\top \nabla f = 
  \begin{bmatrix} 2x & y\cos(xy)\\ -1 & x\cos(xy) \end{bmatrix}
  \begin{bmatrix} 2u\\ 2v\end{bmatrix}\) 에서 \(u=x^2-y,\ v=\sin(xy)\) 대입.

### 4.4 벡터 출력: 야코비안 곱
\(\mathbf f(u,v)=\begin{bmatrix}u v\\ u+v\end{bmatrix}\), \(\mathbf g(x,y)=\begin{bmatrix}x^2\\ \sin y\end{bmatrix}\).  
- \(D\mathbf f(u,v)=\begin{bmatrix} v & u \\ 1 & 1 \end{bmatrix}\),  
- \(D\mathbf g(x,y)=\begin{bmatrix} 2x & 0 \\ 0 & \cos y \end{bmatrix}\).  
- \(D(\mathbf f\circ\mathbf g)=D\mathbf f|_{(x^2,\sin y)}\cdot D\mathbf g
=\begin{bmatrix}\sin y & x^2\\ 1 & 1\end{bmatrix}
\begin{bmatrix} 2x & 0 \\ 0 & \cos y \end{bmatrix}
=\begin{bmatrix} 2x\sin y & x^2\cos y\\ 2x & \cos y\end{bmatrix}.\)

---

## 5. 특수 테크닉: 암시/로그/좌표변환

### 5.1 암시적 미분(Implicit differentiation)
관계식 \(F(x,y)=0\)에서 \(y=y(x)\)의 도함수:
$$
\frac{dy}{dx}=-\frac{F_x}{F_y}.
$$
**예** \(x^2+y^2=1\Rightarrow y'=-\dfrac{x}{y}\).

### 5.2 로그 미분(Log-differentiation)
곱/멱/지수의 복합형에 유리. \(y=x^{x}\):
$$
\ln y=x\ln x\quad\Rightarrow\quad \frac{y'}{y}=\ln x+1\ \Rightarrow\ y'=x^x(\ln x+1).
$$

### 5.3 좌표변환 (극좌표의 그래디언트)
\(x=r\cos\theta,\ y=r\sin\theta\), \(f(x,y)\Rightarrow g(r,\theta)=f(r\cos\theta,r\sin\theta)\):
$$
\frac{\partial g}{\partial r}=f_x\cos\theta+f_y\sin\theta,\qquad
\frac{\partial g}{\partial \theta}=-f_x r\sin\theta+f_y r\cos\theta.
$$
(여기서 \(f_x,f_y\)는 \((x,y)=(r\cos\theta,r\sin\theta)\)에서 평가)

---

## 6. 딥러닝 시각화: 역전파=연쇄법칙

- **순전파**: \(\mathbf x\overset{\mathbf g}{\longrightarrow}\mathbf u\overset{\mathbf f}{\longrightarrow}\mathbf y\).
- **역전파**: \(\displaystyle \frac{\partial \mathcal L}{\partial \mathbf x}
=\underbrace{D\mathbf g(\mathbf x)^\top}_{\text{J}^\top}\,
\underbrace{\frac{\partial \mathcal L}{\partial \mathbf u}}_{\text{upstream}}\),
\(\displaystyle \frac{\partial \mathcal L}{\partial \mathbf u}
=D\mathbf f(\mathbf u)^\top\,\frac{\partial \mathcal L}{\partial \mathbf y}\).
- 결국 **벡터-야코비안 곱(VJP)** 들의 **연쇄**가 역전파.

---

## 7. 흔한 실수 체크리스트

1. **안쪽 미분 누락**: \( (\sin x^2)'\stackrel{\color{red}{오류}}{=}\cos x^2\). 정답: \(\cos(x^2)\cdot 2x\).  
2. **평가점 착오**: \(f'(g(x))\)를 \(f'(x)\)로 착각.  
3. **다변수에서 방향 혼동**: \(f_x\), \(f_y\)는 **부분 미분**—대상 변수 외는 고정.  
4. **행렬 곱 순서**: \(Df|_{g(x)}\cdot Dg(x)\) (순서 바꾸면 차원 불일치).  
5. **헤시안 항 누락**: 2계 미분 시 \(f''\)-항과 \(g''\)-항 모두 필요.

---

## 8. 연습문제 (간단 해설 포함)

**(1)** \(y=\ln\!\big(\sin(2x+3)\big)\).  
- \(u=2x+3,\ v=\sin u\):  
  $$
  y'=\frac{1}{\sin u}\cdot \cos u \cdot 2
  =2\cot(2x+3).
  $$

**(2)** \(z=f(x,y)\), \(f_x=2x,\ f_y=3y^2\), \(x=t^2,\ y=e^{t}\).  
$$
\frac{dz}{dt}=f_x\cdot 2t+f_y\cdot e^{t}= (2x)\,2t+(3y^2)\,e^t=4t\,t^2+3e^{2t}\,e^t=4t^3+3e^{3t}.
$$

**(3)** \(h(\mathbf x)=f(\mathbf a^\top\mathbf x)\) (스칼라 합성).  
- \(\nabla h(\mathbf x)=f'(\mathbf a^\top\mathbf x)\,\mathbf a\).

**(4)** 극좌표: \(f(x,y)=x^2+y^2\), \(g(r,\theta)=f(r\cos\theta,r\sin\theta)\).  
- \(g(r,\theta)=r^2\), \(\partial g/\partial r=2r\), \(\partial g/\partial \theta=0\).

---

## 9. PyTorch로 연쇄법칙 검증 (수치/기호 일치)

> ✅ 사용자는 PyTorch를 선호하므로, **PyTorch 자동미분**으로 단·다변수 연쇄법칙을 검증합니다.

```python
import torch
torch.set_default_dtype(torch.float64)

# (A) 단변수: y = exp(sin(x^3 + 1))
def y_scalar(x):
    return torch.exp(torch.sin(x**3 + 1.0))

x = torch.tensor(0.7, requires_grad=True)
y = y_scalar(x)
y.backward()
autograd_grad = x.grad.item()

# 수식에 의한 해 (chain rule)
with torch.no_grad():
    u = x**3 + 1.0
    v = torch.sin(u)
    manual = torch.exp(v) * torch.cos(u) * (3.0 * x**2)

print("Scalar case:")
print(" autograd dy/dx =", autograd_grad)
print(" manual  dy/dx =", manual.item())

# (B) 다변수 스칼라: h(x,y)=f(g(x,y)), g=(x^2 - y, sin(xy)), f(u,v)=u^2 + v^2
def h_xy(xy):
    x, y = xy[0], xy[1]
    u = x**2 - y
    v = torch.sin(x*y)
    f = u**2 + v**2
    return f

xy = torch.tensor([0.8, -0.4], requires_grad=True)
h = h_xy(xy)
h.backward()
autograd_grad_xy = xy.grad.clone().detach()

# 수식에 의한 그래디언트
with torch.no_grad():
    x, y = xy
    u = x**2 - y
    v = torch.sin(x*y)
    fx = 2*u*(2*x) + 2*v*torch.cos(x*y)*y
    fy = 2*u*(-1)   + 2*v*torch.cos(x*y)*x
    manual_grad = torch.stack([fx, fy])

print("\nMultivariable scalar case (gradient):")
print(" autograd ∇h =", autograd_grad_xy.tolist())
print(" manual  ∇h =", manual_grad.tolist())

# (C) 야코비안 곱 검증: F(g(x)) with F: R^2->R^2, g: R^2->R^2
def g_xy(xy):
    x, y = xy[..., 0], xy[..., 1]
    return torch.stack([x**2, torch.sin(y)], dim=-1)  # (..,2)

def F_uv(uv):
    u, v = uv[..., 0], uv[..., 1]
    return torch.stack([u*v, u+v], dim=-1)            # (..,2)

xy = torch.tensor([0.5, 0.3], requires_grad=True)
u = g_xy(xy)            # (2,)
y = F_uv(u)             # (2,)
# Check VJP: v^T J equals grad of dot(y, v)
v = torch.tensor([1.2, -0.7])  # test row vector
s = (y * v).sum()
xy.grad = None
s.backward()
vjp_autograd = xy.grad.clone().detach()  # equals v^T * D(F∘g)

# Manual: v^T*DF|_{u} * Dg
with torch.no_grad():
    x, y_ = xy
    DF = torch.tensor([[u[1], u[0]], [1.0, 1.0]])  # [[v,u],[1,1]] at (u,v)
    Dg = torch.tensor([[2*x, 0.0], [0.0, torch.cos(y_)]])
    v_row = v.reshape(1,2)
    vjp_manual = (v_row @ DF @ Dg).reshape(-1)

print("\nVJP check (v^T J):")
print(" autograd vJP =", vjp_autograd.tolist())
print(" manual  vJP =", vjp_manual.tolist())
```

**설명**
- (A) 단변수 복합의 도함수: 자동미분 vs 수식(연쇄법칙) 일치.
- (B) 다변수 스칼라의 그래디언트: \(\nabla h = J_g^\top \nabla f\) 일치.
- (C) **벡터-야코비안 곱(VJP)** 검증: \(\mathbf v^\top D(\mathbf F\circ \mathbf g) = \mathbf v^\top D\mathbf F|_{\mathbf u}\, D\mathbf g\).

---

## 10. 요약 표

| 상황 | 연쇄법칙 형태 | 관점 |
|---|---|---|
| 단변수 | \(\dfrac{d}{dx}f(g(x))=f'(g(x))\,g'(x)\) | 지역 선형 근사의 곱 |
| 다변수(스칼라 출력) | \(\dfrac{dz}{dt}= \sum_i f_{x_i}\,x_i'(t)\) | 모든 경로의 합 |
| 벡터-행렬형 | \(D(f\circ g)=Df|_{g(x)}\cdot Dg\) | 야코비안 곱 |
| 그래디언트 | \(\nabla h=J_g^\top \nabla f\) | 역모드(VJP) |
| 헤시안 | \(J_g^\top \nabla^2 f\,J_g+ \sum_k f_{u_k}\nabla^2 g_k\) | 2계 연쇄 |

---

## 11. 마무리 & 확장 포인트

- 연쇄법칙은 **암시적 미분**, **로그 미분**, **좌표변환**, **최적화의 그래디언트 계산**, **딥러닝 역전파**의 공통 기저.
- 실전에서는 **야코비안-벡터/벡터-야코비안 곱**(JVP/VJP)을 효율적으로 구현하는 것이 핵심.
- 다음 확장 주제: **토션/곡률의 연쇄 해석**, **변분법(오일러-라그랑주)**, **확률변수 변환의 야코비안(정규화 유량)** 등.
