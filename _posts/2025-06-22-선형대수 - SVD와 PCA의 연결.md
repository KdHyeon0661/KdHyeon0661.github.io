---
layout: post
title: 선형대수 - SVD와 PCA의 연결
date: 2025-06-22 21:20:23 +0900
category: 선형대수
---
# SVD와 PCA의 연결

> **핵심 요약**  
> 평균 제거(centering)된 데이터 행렬 $$X\in\mathbb{R}^{n\times d}$$(행=샘플, 열=특징)에 **SVD**를 하면  
> $$
> X=U\,\Sigma\,V^\top
> $$
> **PCA의 주성분 방향**은 $$V$$의 열벡터(우측 특이벡터)이고,  
> 각 축의 **분산(설명력)**은 $$\sigma_i^2/n$$(또는 $$\sigma_i^2/(n-1)$$ 스케일)이다.  
> 저차원 투영은 $$Z_k=X\,V_k=U_k\Sigma_k$$, 재구성은 $$\hat X_k=Z_k V_k^\top=U_k\Sigma_kV_k^\top$$.

---

## 1. 표기와 전처리(아주 중요)

- 데이터 행렬: $$X\in\mathbb{R}^{n\times d}$$, **각 행이 하나의 샘플**, 각 열이 피처.  
- **평균 제거(centering)**:  
  $$
  \bar{\boldsymbol{\mu}}=\frac{1}{n}\sum_{i=1}^n X_{i,:},\qquad
  X_c = X - \mathbf{1}\,\bar{\boldsymbol{\mu}}^\top.
  $$
  PCA 전에는 반드시 $$X_c$$(평균 0)를 사용. 표준화(스케일링)는 **상황에 따라**(아래 §7.1).

---

## 2. SVD ↔ PCA의 수학적 연결

### 2.1 SVD
$$
X_c = U\,\Sigma\,V^\top,\quad
U^\top U=I_n,\ V^\top V=I_d,\ 
\Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_r),\ \sigma_1\ge\cdots\ge\sigma_r>0.
$$

### 2.2 공분산과 고유값
공분산(표본) $$C=\frac{1}{n}X_c^\top X_c$$(또는 $$\frac{1}{n-1}$$):
$$
\frac{1}{n}X_c^\top X_c
= \frac{1}{n}\big(V\Sigma^\top U^\top\big)\big(U\Sigma V^\top\big)
= V\,\frac{\Sigma^\top\Sigma}{n}\,V^\top.
$$
- **주성분 방향**: $$V$$의 열벡터 $$\vec v_i$$.  
- **분산(설명력)**: $$\lambda_i=\sigma_i^2/n$$(또는 $$/(n-1)$$).

### 2.3 투영(PC scores)와 재구성
- **주성분 점수(투영)**:
  $$
  Z_k = X_c\,V_k = U_k\,\Sigma_k\in\mathbb{R}^{n\times k}.
  $$
- **저랭크 재구성(랭크-$$k$$ 근사)**:
  $$
  \hat X_k = Z_k V_k^\top = U_k\,\Sigma_k\,V_k^\top.
  $$
- 누적 설명률:
  $$
  \textstyle \mathrm{ExplainedRatio}(k)
  = \frac{\sum_{i=1}^k \sigma_i^2}{\sum_{j=1}^r \sigma_j^2}.
  $$

---

## 3. 기하학적 직관

- $$V$$: **특징 공간의 정규직교 축**(주성분 축).  
- $$\Sigma$$: 각 축으로의 **늘림/축소 크기**(분산의 제곱근에 비례).  
- $$U$$: 샘플 공간에서의 **좌표 회전**.  
- 단위 원판을 $$X_c$$가 타원으로 보낸다고 볼 때, 타원의 주축이 $$V$$, 반축 길이가 $$\sigma_i$$.

---

## 4. 차원 축소의 최적성(Eckart–Young–Mirsky)

랭크-$$k$$로의 **최적(스펙트럴/프로베니우스 노름) 근사**는
$$
\boxed{\hat X_k=U_k\Sigma_kV_k^\top}
$$
이며 오차는
$$
\|X_c-\hat X_k\|_2=\sigma_{k+1},\quad
\|X_c-\hat X_k\|_F=\sqrt{\sum_{i>k}\sigma_i^2}.
$$
즉, SVD 상위 특이값/특이벡터만 쓰는 것이 **정보 손실 최소**.

---

## 5. 구현(코드) — PyTorch 중심

> 아래 코드는 **평균 제거 → SVD → 설명분산 → 투영/재구성/화이트닝**까지 한 번에.
> (PyTorch를 기본으로, 비교용 NumPy 스니펫도 덧붙임)

### 5.1 PyTorch: PCA by SVD (경제 SVD)
```python
import torch

def pca_svd(X, k=None, center=True, whiten=False, ddof=1, device=None, dtype=None):
    """
    X: (n, d) tensor [rows = samples]
    k: number of components (None => full rank)
    center: subtract mean
    whiten: return whitened scores Z_whiten = U_k * diag(σ_i / sqrt(σ_i^2/(n-ddof)))? -> see below
    ddof: 1 -> 1/(n-1) scaling; 0 -> 1/n
    """
    X = torch.as_tensor(X, device=device, dtype=dtype)  # (n, d)
    n, d = X.shape
    if center:
        mean = X.mean(dim=0, keepdim=True)
        Xc = X - mean
    else:
        mean = torch.zeros(1, d, device=X.device, dtype=X.dtype)
        Xc = X

    # economic SVD
    U, S, Vh = torch.linalg.svd(Xc, full_matrices=False)  # U:(n,r), S:(r,), Vh:(r,d)
    V = Vh.transpose(-2, -1)  # (d, r)
    r = S.numel()

    if k is None or k > r:
        k = r

    Uk = U[:, :k]           # (n, k)
    Sk = S[:k]              # (k,)
    Vk = V[:, :k]           # (d, k)

    # Explained variance & ratio
    # variance_i = σ_i^2 / (n - ddof)    (match sklearn’s default: ddof=1)
    denom = (n - ddof)
    variances = (Sk ** 2) / denom
    total_var = (S ** 2).sum() / denom
    explained_ratio = (Sk ** 2) / (S ** 2).sum()

    # Scores (projection) and reconstruction
    Zk = Uk * Sk  # (n, k); since Xc V_k = U_k Σ_k  => elementwise multiply columns by σ
    Xk = Zk @ Vk.T  # (n, d)

    # Optional whitening (PCA-whitening): Z_white = U_k (Σ_k / sqrt(σ_i^2/(n-ddof)))
    # => divide each component by sqrt(variance_i)
    Zk_white = None
    if whiten:
        stds = torch.sqrt(variances)  # (k,)
        Zk_white = Zk / stds  # each component scaled to unit variance

    out = {
        "mean": mean.squeeze(0),        # for centering new data
        "components": Vk,               # (d, k) principal axes (columns)
        "scores": Zk,                   # (n, k) projected coordinates
        "reconstruction": Xk,           # (n, d) centered reconstruction
        "singular_values": Sk,          # (k,)
        "explained_variance": variances,# (k,)
        "explained_variance_ratio": explained_ratio,  # (k,)
        "total_variance": total_var,
        "Z_white": Zk_white,
    }
    return out

# --- Example usage ---
torch.manual_seed(0)
X = torch.tensor([[2.5, 2.4],
                  [0.5, 0.7],
                  [2.2, 2.9],
                  [1.9, 2.2],
                  [3.1, 3.0],
                  [2.3, 2.7],
                  [2.0, 1.6],
                  [1.0, 1.1],
                  [1.5, 1.6],
                  [1.1, 0.9]], dtype=torch.float64)

res = pca_svd(X, k=1, center=True, whiten=False, ddof=1)
print("components (V_k) =\n", res["components"])        # (d, 1)
print("explained ratio =", res["explained_variance_ratio"].item())
print("scores shape =", res["scores"].shape)            # (n, 1)
print("reconstruction error (Fro) =",
      torch.linalg.matrix_norm((X - X.mean(0) - res["reconstruction"]), ord='fro').item())
```

### 5.2 새로운 데이터 변환/역변환 API (PyTorch)
```python
def pca_transform(X_new, mean, components):
    """
    Project centered X_new onto principal axes: Z = (X_new - mean) @ components
    Note: components shape (d, k)
    """
    Xc = X_new - mean
    return Xc @ components

def pca_inverse_transform(Z, components):
    """
    Reconstruct centered data: Xc_hat = Z @ components.T
    (원래 좌표로 돌아가려면 + mean 필요)
    """
    return Z @ components.T
```

### 5.3 NumPy 스니펫(비교용, 짧게)
```python
import numpy as np

X = np.array([[2.5,2.4],[0.5,0.7],[2.2,2.9],[1.9,2.2],[3.1,3.0],
              [2.3,2.7],[2.0,1.6],[1.0,1.1],[1.5,1.6],[1.1,0.9]], dtype=float)
Xc = X - X.mean(axis=0, keepdims=True)
U, S, VT = np.linalg.svd(Xc, full_matrices=False)
V = VT.T
k = 1
Z = U[:, :k] * S[:k]           # (n,k)
Xhat = Z @ V[:, :k].T          # centered reconstruction
expl_ratio = (S[:k]**2)/(S**2).sum()
```

---

## 6. 실전 시나리오와 선택 가이드

### 6.1 차원 축소(시각화/전처리)
- **2D/3D 투영**: $$V_k$$로 투영하여 산점도 시각화.  
- **노이즈 제거/壓縮**: 작은 특이값(작은 분산 축) 제거 → 잡음 감소.

### 6.2 고차원(텍스트, 이미지, 유전자)
- **LSA(텍스트)**: 단어–문서 행렬에 SVD → 상위 $$k$$만 보존.  
- **이미지 압축**: 채널별로 SVD, 누적 설명률로 $$k$$ 선택.  
- **유전자/센서**: 공분산이 심하게 편향되면 **상관행렬 기반 PCA**(표준화) 고려.

### 6.3 모델링 파이프라인
- **PCR(Principal Component Regression)**: $$Z_k$$로 회귀/분류.  
- **릿지 ↔ PCA 필터링**: 작은 $$\sigma_i$$ 억제는 릿지와 유사한 안정화 효과.

---

## 7. 전처리, 스케일링, 화이트닝

### 7.1 스케일링 선택
- **그대로(variance 본연 유지)**: 모든 피처가 같은 단위/스케일이면 OK.  
- **표준화(z-score)**: 단위/스케일 크게 다르면 **상관행렬 PCA** 권장(각 열 표준화 후 SVD).  
  이때 설명력은 “표준화된 분산”.

### 7.2 화이트닝(whitening)
- **PCA 화이트닝**:  
  $$
  Z_{\text{white}} = U_k\,\operatorname{diag}\Big(\frac{\sigma_i}{\sqrt{\sigma_i^2/(n-\mathrm{ddof})}}\Big)
  = U_k\,\operatorname{diag}\big(\sqrt{n-\mathrm{ddof}}\big).
  $$
  구현 관점에서는 보통 **투영 후 각 PC를 표준편차로 나눔**:
  $$
  Z_{\text{white}} = Z_k / \sqrt{\sigma_i^2/(n-\mathrm{ddof})}.
  $$
- 특징: 각 주성분의 분산을 1로 맞춰 **구면화**; 일부 모델에서 학습 안정성에 유리.

---

## 8. 계산/스케일 측면: 대규모 데이터

- 전형적 복잡도: SVD는 $$\mathcal{O}(nd\min\{n,d\})$$.  
- **부분(Truncated) SVD**: 상위 $$k$$만 원하면 Lanczos/IRLBA/PROPACK.  
- **Randomized SVD**: 대규모에서 근사하지만 빠르고 메모리 효율적.  
- **Incremental PCA**: 스트리밍/배치 처리.

> PyTorch에서는 `torch.pca_lowrank`(내부 randomized)로 대규모 PCA 근사 가능:
```python
import torch
# Xc: centered (n, d)
U, S, V = torch.pca_lowrank(Xc, q=50)  # q >= k
# 상위 k만 사용
k = 10
Vk = V[:, :k]
Zk = Xc @ Vk
```

---

## 9. 흔한 함정과 체크리스트

1. **평균 제거 누락** → 잘못된 방향/설명력.  
2. **스케일 불균형** → 큰 단위 피처가 PC를 지배(상관행렬 PCA 고려).  
3. **ddof(스케일링 상수)** 혼동: 보고할 때 $$n$$ vs $$n-1$$ 일관성 유지.  
4. **부호 불정**: 주성분 벡터의 부호는 의미 없음(필요하면 일관되게 정렬).  
5. **중복 특이값**: 같은 분산 축들 사이의 기저는 비유일.  
6. **누적 설명률 기준**: 95%/99% 등 임곗값을 맹신 말고 **과제 목적**과 **오류 비용** 고려.  
7. **누락값/이상치**: 평균/표준화, 로버스트 PCA(Outlier에 둔감) 고려.

---

## 10. 실험 가이드(작업 절차)

1. **데이터 분해**: train/val/test.  
2. **train에서만 평균/스케일** 산출 → transform 저장.  
3. **SVD(PCA) 적합**: train으로 $$V_k$$ 결정, k는 누적설명률/검증성능/정보이론 기준(AIC/BIC 유사)으로 선택.  
4. **val/test 투영**: 저장한 mean/scale과 $$V_k$$로 일관 투영.  
5. **모델링/시각화/재구성오차 분석**.

---

## 11. 미니 케이스 스터디(2D → 1D)

- 데이터: 위 예제(10개 점).  
- 평균 제거 후 경제 SVD: 가장 큰 특이값 방향이 **데이터의 길게 늘어진 방향**.  
- 1D 투영 $$Z_1$$로도 대부분의 분산을 설명(설명비율 출력 확인).  
- 재구성 $$\hat X_1$$는 원 데이터의 “실선”을 잘 따라가지만, 직교 방향 정보는 사라짐.

---

## 12. 수학 요약(공식 모음)

- **PCA 축**: $$V=[\vec v_1,\ldots,\vec v_d]$$, $$C=\frac{1}{n}X_c^\top X_c=V\frac{\Sigma^\top\Sigma}{n}V^\top$$.  
- **분산**: $$\lambda_i=\sigma_i^2/n,\quad \text{(또는) } \sigma_i^2/(n-1).$$  
- **점수**: $$Z_k=X_cV_k=U_k\Sigma_k.$$  
- **재구성**: $$\hat X_k=Z_kV_k^\top=U_k\Sigma_kV_k^\top.$$  
- **누적 설명률**: $$\sum_{i\le k}\sigma_i^2/\sum_j\sigma_j^2.$$  
- **최적 저랭크**: $$\|X_c-\hat X_k\|_F^2=\sum_{i>k}\sigma_i^2.$$

---

## 13. 빠른 Q&A

- **Q. SVD 없이 공분산 고유분해로 PCA 해도 되나요?**  
  A. 가능. 하지만 수치안정성과 속도 면에서 **직접 SVD**가 실무 표준.

- **Q. n≪d(특징이 매우 많은)일 때?**  
  A. 직접 $$X_c$$ SVD가 유리. 공분산은 $$d\times d$$라 너무 큼.

- **Q. 스케일 차이가 큰 피처**가 많다면?  
  A. **표준화** 후 SVD(=상관행렬 PCA) 권장.

- **Q. 화이트닝은 언제?**  
  A. 다운스트림 모델이 등방성 가정/최적화 안정성 필요 시 유용. 과도한 화이트닝은 의미 해석을 어렵게 할 수 있음.

---

## 14. 결론

- **PCA는 SVD의 직접 응용**: $$V$$가 주성분 축, $$\sigma_i^2$$가 분산.  
- 저차원 투영/압축/노이즈 제거/시각화/전처리에서 **안정성·최적성**을 동시에 제공.  
- 실무에서는 **경제 SVD + 누적설명률 기반의 k 선택 + 일관된 전처리 파이프라인**이 베스트 프랙티스.