---
layout: post
title: 선형대수 - SVD와 PCA의 연결
date: 2025-06-22 21:20:23 +0900
category: 선형대수
---
# 🔗 SVD와 PCA의 연결

**PCA (Principal Component Analysis)**는 고차원 데이터를 이해하고 시각화하기 위한 강력한 차원 축소 기법입니다.  
**SVD (Singular Value Decomposition)**는 모든 행렬에 적용 가능한 일반적인 분해 기법입니다.

이 두 개념은 서로 깊이 연결되어 있으며, PCA는 사실상 **SVD의 특수한 응용**이라고 볼 수 있습니다.

---

## 📘 1. PCA 요약

### 목적

- 고차원 데이터를 **저차원 공간으로 투영**해 핵심 정보만 보존
- 정보량(분산)이 가장 큰 방향을 찾는 것이 목표

### 방법 요약

1. 데이터 행렬 \( X \) (각 행이 데이터 포인트)
2. 평균 중심화: \( X' = X - \bar{X} \)
3. 공분산 행렬 계산: \( C = \frac{1}{n} X'^T X' \)
4. 고유값 분해: \( C \vec{v}_i = \lambda_i \vec{v}_i \)
5. 가장 큰 고유값에 대응하는 고유벡터가 **주성분 방향**

---

## 📌 2. SVD로 PCA를 하는 방법

### SVD 정의

\[
\boxed{
X = U \Sigma V^T
}
\]

- \( X \): 평균 중심화된 데이터 행렬 (\( m \times n \))
- \( U \): 좌측 특이벡터 (\( m \times m \)), 데이터 축
- \( \Sigma \): 특이값 대각행렬 (\( m \times n \))
- \( V \): 우측 특이벡터 (\( n \times n \)), **주성분 방향**

### 연결점

1. **PCA에서의 주성분 벡터** = **SVD에서의 \( V \)**
2. **분산(고유값)** = \( \frac{\sigma_i^2}{n} \)  
   → 특이값 \( \sigma_i \)는 주성분 방향의 분산 비율 크기를 뜻함
3. **데이터를 주성분에 투영한 값** = \( X V = U \Sigma \)

---

## 📐 3. 기하학적 의미 비교

| 단계 | PCA 해석 | SVD 해석 |
|------|-----------|------------|
| 기준축 설정 | 데이터 분산이 가장 큰 방향 | 우측 특이벡터 \( V \) |
| 투영 | \( X \vec{v}_i \) | \( X V = U \Sigma \) |
| 정보량 | 고유값 \( \lambda_i \) | \( \sigma_i^2 \) |

→ 결국 SVD의 \( V \)는 PCA의 **주성분 축**,  
\( \Sigma \)는 **각 축의 분산 크기**를 나타냅니다.

---

## 🔍 4. 차원 축소: SVD를 이용한 PCA

차원 축소란?  
상위 \( k \)개의 주성분만 남기고 나머지 제거

\[
X_k = U_k \Sigma_k V_k^T
\]

- \( k \): 원하는 차원 수
- \( U_k \): 앞 \( k \)개의 좌측 특이벡터
- \( \Sigma_k \): 앞 \( k \)개의 특이값
- \( V_k \): 앞 \( k \)개의 우측 특이벡터

이로써 \( X \)는 **저차원 근사 행렬** \( X_k \)로 표현됩니다.

---

## 🧮 5. 예제 (Python)

```python
import numpy as np
import matplotlib.pyplot as plt

# 예제 데이터
X = np.array([
    [2.5, 2.4],
    [0.5, 0.7],
    [2.2, 2.9],
    [1.9, 2.2],
    [3.1, 3.0],
    [2.3, 2.7],
    [2, 1.6],
    [1, 1.1],
    [1.5, 1.6],
    [1.1, 0.9]
])

# 평균 중심화
X_centered = X - np.mean(X, axis=0)

# SVD
U, S, VT = np.linalg.svd(X_centered)

# 주성분 (우측 특이벡터)
principal_components = VT[:2]

# 데이터 투영 (2D → 1D)
X_pca_1d = X_centered @ principal_components[0].reshape(-1, 1)

# 복원 (1D → 2D)
X_reconstructed = X_pca_1d @ principal_components[0].reshape(1, -1)

# 시각화
plt.scatter(X_centered[:, 0], X_centered[:, 1], label="원본", alpha=0.5)
plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], label="1D 복원", color='red')
plt.legend()
plt.axis('equal')
plt.title("SVD를 이용한 PCA")
plt.grid(True)
plt.show()
```

---

## ✅ 요약

| 항목 | PCA | SVD |
|------|-----|-----|
| 주성분 | 공분산행렬의 고유벡터 | \( V \) (우측 특이벡터) |
| 분산값 | 고유값 \( \lambda_i \) | \( \sigma_i^2 \) |
| 투영 | \( X \vec{v}_i \) | \( X V = U \Sigma \) |
| 차원 축소 | 상위 \( k \)개의 고유벡터 선택 | \( X_k = U_k \Sigma_k V_k^T \) |