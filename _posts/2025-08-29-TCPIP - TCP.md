---
layout: post
title: TCPIP - TCP
date: 2025-08-29 23:25:23 +0900
category: TCPIP
---
# TCP

## 1. TCP의 역할과 기본 특성

### 1.1 TCP가 하는 일

TCP는 **전송 계층(L4)** 프로토콜로, IP 위에서 다음을 보장한다.

- **신뢰성**: 손실/중복/순서 뒤바뀜이 있어도, 응용에는 **정확히 한 번, 순서대로** 전달.
- **바이트 스트림**: 메시지 경계가 없는 **연속적인 바이트 흐름**으로 제공.
- **흐름 제어**: 수신 애플리케이션·버퍼가 감당 가능한 속도로만 전송.
- **혼잡 제어**: 네트워크 혼잡 시 **스스로 전송률을 줄여** 전체 네트워크를 보호.

UDP/QUIC와 비교:

- **UDP**: 헤더만 얇고, 신뢰성·혼잡제어는 전혀 없음(앱이 떠맡음).
- **QUIC**: UDP 위에서 “TCP+TLS” 비슷한 기능을 제공하는 **새 프로토콜**. HTTP/3에 사용.

### 1.2 TCP 헤더 구조(요약)

```text
0      4       10             16               24              32
+------+--------+--------------+----------------+---------------+
|      Source Port             |  Destination Port              |
+------------------------------+----------------+---------------+
|                        Sequence Number                        |
+---------------------------------------------------------------+
|                    Acknowledgment Number                      |
+----------+-------+-----------+----------------+---------------+
| Data Off |  RSV  | Flags     |   Window Size  |   Checksum    |
+----------+-------+-----------+----------------+---------------+
| Urgent Ptr        |           Options (if any)                |
+-------------------+-------------------------------------------+
|                  Data (Payload, variable)                     |
+---------------------------------------------------------------+
```

- **Sequence Number**: 바이트 스트림 상의 “첫 바이트 번호”.
- **Acknowledgment Number**: “**다음에 받고 싶어하는 바이트 번호**”(누적 ACK).
- **Flags**: SYN, ACK, FIN, RST, PSH, URG 등.
- **Window Size**: 수신 측 남은 버퍼 용량(흐름제어).
- **Options**: MSS, SACK Permitted, Timestamps, Window Scale, Fast Open 등.

---

## 2. 3-way 핸드셰이크 & 종료, 상태 다이어그램

### 2.1 연결 성립 (3-way handshake)

```text
클라이언트                                      서버
-----------                                      -----------
(Closed)                                         (LISTEN)
SEQ = x, SYN  ------------------------------->   LISTEN → SYN_RCVD
                <-------------------------------  SEQ = y, ACK = x+1, SYN/ACK
ACK = y+1   --------------------------------->   ESTABLISHED
ESTABLISHED
```

- **초기 순번(ISN)**: 무작위/시간 기반으로 선택해  
  - 옛 패킷이 새 연결에 섞이는 것 방지  
  - 시퀀스 예측 공격(세션 하이재킹) 난이도 증가 :contentReference[oaicite:0]{index=0}
- **SYN 큐/백로그(backlog)**: 서버는 LISTEN 상태에서 들어오는 SYN을 **반개방(half-open)** 상태로 큐에 쌓는다.
  - 큐가 가득 차면 새 SYN을 드롭하거나 RST 응답.

#### SYN flood와 SYN cookies

- **SYN flood**: 공격자가 **SYN만 대량 전송**하고 ACK를 안 보내 반개방 상태를 채움 → 정상 클라이언트 수용 불가.
- **SYN cookies**:
  - 서버가 SYN/ACK를 보낼 때, **ISN에 인코딩된 쿠키**를 심고, 큐에 상태를 저장하지 않음.
  - 클라이언트 ACK에서 돌아온 시퀀스 번호를 보고 쿠키를 검증 → 합법 연결만 상태 할당.:contentReference[oaicite:1]{index=1}  

대부분의 현대 리눅스·BSD·Windows 스택은 SYN cookies를 기본 또는 옵션으로 지원한다.:contentReference[oaicite:2]{index=2}  

### 2.2 연결 종료 (4-way 기본 종료)

```text
(클라이언트가 먼저 종료하는 경우)

클라이언트                                   서버
-----------                                   -----------
ESTABLISHED                                   ESTABLISHED
FIN, SEQ=u ------------------------------->   CLOSE_WAIT
             <------------------------------   ACK = u+1
FIN, SEQ=v  <------------------------------   LAST_ACK로 이동 전에 전송
ACK = v+1  -------------------------------->  CLOSED
TIME_WAIT
```

- **Half-close**:
  - FIN을 보낸 쪽은 **더 이상 보낼 수는 없지만**, 받을 수는 있음.
  - 예: HTTP/1.1에서 서버가 응답을 다 보내고 FIN → 클라가 ACK 후 유휴.

- **TIME_WAIT(종료 대기)**:
  - 마지막 ACK를 보낸 쪽이 들어감.
  - **2×MSL(Maximum Segment Lifetime)** 동안 해당 4-튜플(IP/Port 조합)을 “보호”.
  - 이유:
    1. 마지막 ACK가 유실되었을 때, 상대의 재전송 FIN에 다시 ACK를 보낼 수 있도록.
    2. 네트워크 중에 떠돌던 예전 세그먼트가 **새 연결로 오인되는 것 방지**.

실무에서 TIME_WAIT 소켓이 많이 보여도, 커널은 이를 효율적으로 관리하므로 **대부분 정상 현상**이다.

### 2.3 상태 다이어그램(요약)

```text
CLOSED
  | active open (connect, SYN 전송)
SYN_SENT  <----(SYN)-----------------  LISTEN
  | (SYN/ACK)                             |
  |                                       | passive open (listen)
  v                                       v
ESTABLISHED  <------------------------  SYN_RCVD
  | (close)                                  ^
  | FIN                                      |
  v                                          |
FIN_WAIT_1  ---------------------------->  CLOSE_WAIT
  | (ACK)                                    | (close)
  v                                          v
FIN_WAIT_2  <----------------------------  LAST_ACK (FIN 전송)
  | (FIN)                                   | (ACK)
  v                                         v
TIME_WAIT  -----------------------------> CLOSED
```

- **CLOSING** 상태는 양쪽이 거의 동시에 FIN을 보낸 드문 경우에 등장.
- **RST**:
  - 처리할 수 없는 세그먼트(닫힌 포트, 잘못된 상태) → 즉시 연결 해제.
  - 애플리케이션 오류(잘못된 프로토콜)에도 사용.

---

## 3. TCP 신뢰성: 슬라이딩 윈도우, 재전송, 흐름제어

### 3.1 바이트 스트림 & 시퀀스/ACK 의미

TCP는 패킷이 아니라 **바이트 스트림**에 번호를 붙인다.

- 송신자가 보내는 각 세그먼트:
  - `(SEQ = N, Length = L)` → **N~N+L-1** 바이트를 실어 보냄.
- 수신자의 ACK:
  - `ACK = K` 이면 “**0~K-1까지는 모두 받았고, K부터 달라**”라는 의미(누적 ACK).

예:

```text
보낸 순서:
  SEQ=100, LEN=100   → 100~199
  SEQ=200, LEN=100   → 200~299
수신 상태:
  100~199 수신 OK, 200~299는 손실
ACK:
  ACK=200 (200부터 달라) → 송신자는 200 이후만 재전송(또는 재전송 후보)
```

### 3.2 송신/수신 윈도우

송신 가능한 범위는

$$
\text{snd\_una} \le \text{seq} < \text{snd\_una} + \min(\text{cwnd},\, \text{rwnd})
$$

- **snd\_una**: “아직 ACK 받지 못한 첫 바이트” 번호.
- **cwnd(congestion window)**: 네트워크 혼잡을 고려한 **송신자 자신의 제한**.
- **rwnd(receive window)**: 수신자 버퍼 여유 기반 **흐름제어 제한**.

“윈도우 바깥”의 바이트는 보내지 **않는다**.

수신 측:

- **수신 버퍼**가 있고, 앱이 read()를 늦게 하면 버퍼가 꽉 찰 수 있다.
- 그 경우 광고 윈도우 `rwnd=0`을 보내 **일시적으로 송신을 멈춤**(zero-window).
- 송신자는 **persist timer**로 주기적으로 **probe**를 보내 창이 열렸는지 확인.

### 3.3 RTT/RTO 추정과 재전송

손실은 다음 두 방식으로 감지된다.

1. **RTO(Retransmission Timeout) 만료**
2. **Fast Retransmit**(중복 ACK)

#### RTT/RTO 추정 (Jacobson/Karels 알고리즘)

실측 RTT 샘플을 이용해 **평균(SRTT)**와 **편차(RTTVAR)**를 추정하고, 이를 이용해 RTO를 계산한다.  

$$
\begin{aligned}
\text{SRTT} &\leftarrow (1-\alpha)\,\text{SRTT} + \alpha \cdot RTT_{\text{sample}} \\
\text{RTTVAR} &\leftarrow (1-\beta)\,\text{RTTVAR} + \beta \cdot \left|\text{SRTT}-RTT_{\text{sample}}\right| \\
\text{RTO} &= \text{SRTT} + \max\left(G,\, K\cdot\text{RTTVAR}\right)
\end{aligned}
$$

- 전통적인 값: \(\alpha = 1/8, \beta = 1/4, K = 4\).
- 예:
  - SRTT ≈ 100 ms, RTTVAR ≈ 20 ms → RTO ≈ 100 + 4×20 = 180 ms.

#### RTO 기반 재전송

- 세그먼트마다(또는 창 전체에 대해) RTO 타이머를 두고, 만료 시 **재전송**.
- RTO는 손실뿐 아니라 **혼잡·큐 지연**도 반영하므로 너무 짧으면 **불필요한 재전송**이 늘고, 너무 길면 **복구가 느려짐**.

#### Fast Retransmit / Fast Recovery

- 수신자는 손실의 “구멍” 이후 세그먼트를 받으면 **중복 ACK**를 보낸다.
- 송신자가 **동일 ACK를 3번 연속** 받으면(중복 ACK 3개) → 해당 세그먼트가 손실됐다고 판단하고 **RTO를 기다리지 않고 재전송**.
- Reno 계열에서는 이 때 **cwnd를 절반으로 줄이고(ssthresh 설정), Congestion Avoidance로 전환**.

### 3.4 SACK(Selective Acknowledgment)

누적 ACK만으로는 **여러 구간 손실**을 효율적으로 처리하기 어렵다.

- **SACK 옵션**을 사용하면 수신자가 **수신된 범위 블록**(예: [1000,2000), [3000,3500))을 알려줄 수 있다.
- 송신자는 “진짜로 빠진 구간”만 재전송 가능.
- 오늘날 리눅스/Windows/macOS를 포함한 주요 스택에서 기본적으로 SACK을 활성화하며, IETF는 SACK을 **사실상 필수 기능**으로 간주한다.  

---

## 4. TCP 혼잡제어: Reno / CUBIC / BBR

### 4.1 공통 원리: AIMD

고전적인 TCP는 **AIMD(Additive Increase / Multiplicative Decrease)**를 따른다.

- 혼잡이 감지되지 않으면:
  - RTT마다 cwnd를 **+1 MSS씩 선형 증가**(Additive Increase).
- 손실(혼잡)이 감지되면:
  - cwnd를 **β배로 감소(보통 1/2)** (Multiplicative Decrease).

수식으로 쓰면(각 ACK마다):

$$
\text{cwnd} \leftarrow
\begin{cases}
\text{cwnd} + \dfrac{\text{MSS}^2}{\text{cwnd}} & \text{(ACK 1개마다)} \\
\beta \cdot \text{cwnd} & \text{(손실 시, } \beta \approx 0.5\text{)}
\end{cases}
$$

이 구조 덕분에:

- 여러 플로우가 공유 링크에서 **대략 공평하게** 대역폭을 나눠 가지는 특성이 있다.

### 4.2 Reno / NewReno

- **Reno**:
  - Slow Start → 혼잡 발생 시 cwnd 절반으로 줄이고 **Congestion Avoidance**로.
  - Fast Retransmit/Recovery 구현.
- **NewReno**:
  - Fast Recovery 구간에서 **부분 ACK(Partial ACK)**를 이용해 **여러 손실 세그먼트**를 더 잘 복구.

오늘날 Reno는 주로 **참조 알고리즘**(baseline)으로 쓰이지만, 실무 환경에서는 CUBIC/BBR 등 **보다 현대적인 CC 알고리즘이 기본값**이다.  

### 4.3 CUBIC (현대 기본 혼잡제어)

리눅스 커널 2.6.19 이후로, CUBIC는 거의 모든 현대 리눅스 배포판에서 기본 TCP 혼잡 제어 알고리즘으로 사용된다. Windows 10 이후 및 일부 BSD에서도 CUBIC가 널리 채택되었다.  

핵심 아이디어:

- Reno는 RTT에 따라 cwnd 증가 속도가 달라지기 때문에, **RTT가 짧은 플로우가 유리**하다.
- CUBIC는 RTT가 아닌 **시간 t** 기반으로 cwnd를 갱신하는 **3차 함수**를 사용한다.

$$
W(t) = C\,(t-K)^3 + W_{\max}
$$

- \(W_{\max}\): 마지막 손실 직전 cwnd.
- \(K\): cwnd가 다시 \(W_{\max}\)에 도달하는 데 걸리는 시간.
- \(C\): 함수 모양을 조정하는 상수.

특징:

- 손실 이후 **큐를 줄이기 위해** 잠시 \(W_{\max}\) 아래에서 유지한 뒤,  
  다시 \(W_{\max}\)를 향해 증가하며, 그 이후에는 조심스럽게 더 크게 시도.
- RTT에 크게 의존하지 않아 **RTT가 다른 플로우 간 공정성**이 Reno보다 낫다.

### 4.4 BBR(Bottleneck Bandwidth and RTT)

Google이 제안하고 오픈소스로 공개한 혼잡제어 알고리즘으로, 이후 여러 클라우드/플랫폼에서 실험 및 부분 배포가 진행 중이다.  

기존 TCP(Reno/CUBIC)는 “손실=혼잡”으로 가정하지만, BBR은 다음 두 값을 추정한다.

- **BtlBw(Bottleneck Bandwidth)**: 병목 링크에서의 최대 전송 속도.
- **RTprop(Min RTT)**: 병목이 비어 있을 때의 RTT.

목표:

$$
\text{Rate} \approx \text{BtlBw}, \qquad
\text{cwnd} \approx \text{BtlBw} \times \text{RTprop} \times \text{gain}
$$

특징:

- 손실이 없더라도 **큐 지연이 커지면**, BtlBw·RTprop 추정값이 변해 전송률을 낮춰 **버퍼블로트**를 줄인다.
- BBR v2에서는 **공정성 문제(특히 Reno/CUBIC와의 상호작용)**와 **큐 공유** 이슈를 개선하기 위해 손실·ECN 신호를 혼합해 사용한다.  

---

## 5. 고속 네트워크 옵션: Window Scale, SACK, Timestamps

### 5.1 BDP와 윈도우 스케일

TCP 헤더의 윈도우 필드는 16비트라서 **최대 65,535 바이트**만 표현 가능하다.

하지만 고속·장거리 링크에서는 **BDP(Bandwidth–Delay Product)**가 훨씬 크다.

$$
\text{BDP} = \text{Bandwidth} \times \text{RTT}
$$

예:

- 1 Gbps 링크, RTT 100 ms:
  - \(1{\rm Gbps} = 125{\rm MB/s}\)
  - BDP ≈ \(125{\rm MB/s} \times 0.1{\rm s} = 12.5{\rm MB}\).

즉, **12.5 MB 이상**을 inflight로 보내야 링크를 꽉 채울 수 있는데, 기본 64 KB로는 턱없이 부족하다.

그래서 등장한 것이 **Window Scale 옵션**이다.

- SYN 교환 시 `WSopt = s`를 교환.
- 실제 윈도우는:
  $$ \text{rwnd}_{\text{effective}} = \text{win\_field} \times 2^s $$

### 5.2 Timestamp 옵션

- RTT 샘플을 더 정확하게 얻고, **PAWS(Protect Against Wrapped Sequence numbers)**를 구현하기 위해 사용.
- 헤더에 Timestamp 값(TSval)과 최근 수신한 TS(TSecr)를 실어 보낸다.
- SACK, RACK(TCP 빠른 손실 감지) 등과 함께, 현대 TCP 스택의 복구 메커니즘에서 핵심 역할.  

---

## 6. Nagle, Delayed ACK, Keepalive, TIME_WAIT, MSL

### 6.1 Nagle Algorithm

**목적**: 작은 세그먼트가 너무 많이 발생하는 것을 막아, 네트워크와 CPU 부하를 줄이기.

규칙(요약):

- 아직 **ACK 되지 않은 데이터가 있을 때**:
  - 새 데이터가 MSS보다 작다면 **버퍼에 모아 두었다가** ACK가 오거나 충분히 쌓이면 전송.

문제:

- **Nagle + Delayed ACK**가 같이 있을 때, 작은 write()이 잦은 상호작용 애플리케이션에서 **추가 RTT 지연**이 발생할 수 있다.

해결:

- 상호작용/저지연 애플리케이션(게임, RPC, 일부 웹소켓 등)은
  - `TCP_NODELAY` 소켓 옵션으로 Nagle 비활성화.
  - 대신 **애플리케이션에서 자체적으로 write 코알레싱**을 설계하는 편이 좋다.

### 6.2 Delayed ACK

- 수신자가 ACK를 **약간 지연(수 ms~수십 ms)**하여 여러 세그먼트에 대한 ACK를 한 번에 보내도록 함.
- CPU/네트워크 부담 감소.
- 반면, 작은 데이터 주고 받기에서는 **왕복 지연을 늘리는 요인**이 될 수 있다.

실전 팁:

- 매우 작은 요청/응답이 잦은 프로토콜이라면,  
  - 메시지를 조금 더 크게 묶어서 보내거나,  
  - TCP 대신 QUIC/UDP 기반 프로토콜을 검토하는 것이 나을 수도 있다.

### 6.3 Keepalive

- TCP 수준의 keepalive는 기본적으로 **수 시간 단위**로 동작하도록 설정되어 있는 경우가 많다.
- NAT/방화벽은 수 분~수십 분 이상의 유휴 TCP 연결을 폐기할 수 있다.
- 그래서 실무에서는 **애플리케이션 레벨 heartbeat**(예: 30초~5분 간격 ping)를 설계하는 것이 일반적이다.

### 6.4 TIME_WAIT & MSL(복습)

- TIME_WAIT 상태의 소켓 수가 많은 서버에서:
  - **포트 고갈 위험**이 있는 경우
    - SO\_REUSEADDR / SO\_REUSEPORT 사용 가능 (의미 잘 이해해야 함).
    - 리눅스의 `tcp_tw_reuse` 등 옵션은 **부작용**을 이해한 뒤 신중히 사용.
- 보통은 TIME\_WAIT를 **인위적으로 줄이기보다는**:
  - 포트 재사용 전략・로드밸런서・앱 레벨 연결 재사용(keep-alive)을 조정하는 편이 더 안전하다.

---

## 7. 현대 TCP 확장: ECN, RACK/TLP, BBR, MPTCP

### 7.1 ECN(Explicit Congestion Notification)

- 라우터가 패킷을 드롭하기 전에, IP 헤더의 **ECN 비트**를 설정해 “혼잡”을 알림.
- 수신자가 TCP 헤더의 **ECE 플래그**를 통해 송신자에게 혼잡을 전달,
- 송신자는 **CWR(콘제션 윈도우 감소) 플래그**를 세워 응답 후 cwnd 감소.

장점:

- 패킷을 버리지 않고도 혼잡 신호 전달 → **지연과 손실률을 감소**.  
- 특히 데이터센터 네트워크에서 **ECN + AQM(CoDel, RED 등)**을 결합하면 짧은 큐 유지에 크게 도움이 된다.  

### 7.2 RACK(TCP 빠른 손실 감지) / TLP(Tail Loss Probe)

전통적인 “중복 ACK 기반” 손실 감지는 **패킷 재정렬(reordering)**이 많은 환경에서 성능이 나빠질 수 있다. 이를 개선하기 위해 IETF에서 논의된 알고리즘이 **RACK**와 **TLP**이다.  

- **RACK (Recent ACKnowledgment)**:
  - 시퀀스 번호 순서가 아니라 **송신 시간**을 기준으로 손실을 판단.
  - “나보다 나중에 보낸 세그먼트가 먼저 ACK됐다면, 오래된 세그먼트는 손실/지연 의심”이라는 직관.
- **TLP (Tail Loss Probe)**:
  - 송신 끝부분(꼬리)의 손실을 빠르게 찾기 위해 **RTO 진입 전**에 “마지막 데이터” 또는 작은 probe 세그먼트를 재전송.

리눅스 최신 커널과 주요 서버 OS들은 RACK/TLP를 도입하여 **손실 복구 속도와 재전송 효율**을 개선하고 있다.  

### 7.3 BBR(요약 재정리)

앞서 설명했듯 BBR는 혼잡 신호를 손실 대신 **대역폭/RTT 추정**에서 얻는다.

운영 팁:

- BBR는 **pacing**(전송 간격 제어)이 필수이므로:
  - 커널의 pacing 지원, `fq` 또는 `fq_codel` 큐디스크와 함께 사용 시 성능이 좋다.  
- BBR v2는 ECN/손실을 혼합해 **공정성과 안정성**을 개선하고 있으며,  
  여러 대형 클라우드 서비스에서 순차적으로 배포 중이다.  

### 7.4 MPTCP(Multipath TCP)

- 하나의 TCP 연결을 **여러 하위 TCP 서브플로우로 분할**해, Wi-Fi+LTE, 여러 NIC, 다중 경로를 동시에 사용하는 기술.
- IETF RFC 8684에서 표준화.  

핵심 개념:

- **MP\_CAPABLE** 옵션: 첫 연결 수립 시 MPTCP 사용 가능 여부 협상.
- **MP\_JOIN** 옵션: 이미 있는 MPTCP 연결에 **새 서브플로우 추가**.
- **DSS(Data Sequence Signal)**: 각각의 서브플로우 시퀀스와 **논리적 데이터 시퀀스**를 매핑.

이점:

- **장애 허용성**: 하나의 경로가 끊겨도, 다른 서브플로우로 계속 전송.
- **대역폭 집계**: 여러 경로의 대역폭을 합산해 고속 전송.

주의:

- NAT/방화벽/중간 박스가 TCP 옵션을 삭제하거나 차단할 수 있어 호환성 이슈.
- 서버/클라이언트 양단이 모두 MPTCP를 지원해야 한다.

---

## 8. 고성능/저지연 튜닝 포인트

### 8.1 MTU/MSS/PMTUD

- **MTU(Maximum Transmission Unit)**: 링크 계층에서 한 프레임에 실을 수 있는 최대 IP 패킷 크기.
- TCP는 그 위에 동작하므로, TCP 페이로드 크기는 **MSS(Maximum Segment Size)**로 제한.

예:

- 일반 Ethernet: MTU 1500바이트.
- IPv4 헤더 20 + TCP 헤더 20 = 40바이트 → MSS ≈ 1460바이트.

**PMTUD(Path MTU Discovery)**:

- 경로 상에서 가장 작은 MTU를 찾기 위해, DF(Don’t Fragment) 비트를 세워 보내고,  
  너무 크면 라우터가 “Fragmentation Needed” ICMP를 보내는 메커니즘.
- 일부 네트워크에서 ICMP가 잘못 차단되면 **PMTUD 블랙홀** 문제가 발생.

실무 팁:

- VPN/터널/오버레이( VXLAN 등 ) 환경에서는 오버헤드를 고려해 **MSS를 1400~1452 정도로 클램핑**하는 것이 일반적이다.

### 8.2 윈도우/버퍼 오토튜닝

리눅스의 `tcp_rmem`, `tcp_wmem`, `rmem_max`, `wmem_max`는 **자동 튜닝 범위**를 제한한다.

```bash
sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728"
sysctl -w net.core.rmem_max=134217728
sysctl -w net.core.wmem_max=134217728
```

- 대역폭×RTT(=BDP)가 큰 환경(예: 10Gbps, RTT 50ms 이상)에서는 상한을 수십 MB 수준으로 높여야 링크를 제대로 활용할 수 있다.

### 8.3 큐/버퍼블로트 & 페이싱

- **버퍼블로트(bufferbloat)**: 스위치/라우터에 큐 버퍼가 과하게 쌓여 **RTT가 크게 증가**하는 현상.
- 대책:
  - AQM(Active Queue Management): **CoDel, PIE, RED** 등.
  - 리눅스에서는 **fq/fq_codel** 큐디스크 사용 권장.

```bash
tc qdisc replace dev eth0 root fq_codel
```

- **Pacing**: 패킷 전송을 일정한 속도로 “고르게” 퍼뜨리는 것.
  - CUBIC/BBR 모두 pacing을 사용하면 **burst에 의한 지연/손실**을 줄일 수 있다.  

### 8.4 Nagle/ACK 전략, Zero-copy

- 레이턴시 민감 앱:
  - `TCP_NODELAY` 사용.
  - 가능한 한 **몇 바이트씩이 아니라, 의미 있는 메시지 단위**로 모아 write().
- 대용량 파일 전송:
  - `sendfile()`, `splice()`, 또는 `MSG_ZEROCOPY`(리눅스) 사용으로 **user/kernel 카피 감소**.
  - 최근 리눅스와 주요 유닉스 계열 OS에서 적극 지원하며 대용량 전송에서 CPU 사용량 감소와 Throughput 향상이 입증되어 있다.  

---

## 9. 예제 코드: 간단한 TCP 에코 서버/클라이언트

### 9.1 C로 작성한 간단한 에코 서버

```c
// tcp_echo_server.c
// 단일 스레드, blocking I/O 예제 (에러 체크 단순화)
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>

int main(void) {
    int listen_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (listen_fd < 0) { perror("socket"); exit(1); }

    int on = 1;
    setsockopt(listen_fd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));

    struct sockaddr_in addr = {0};
    addr.sin_family = AF_INET;
    addr.sin_addr.s_addr = htonl(INADDR_ANY);
    addr.sin_port = htons(9000);

    if (bind(listen_fd, (struct sockaddr*)&addr, sizeof(addr)) < 0) {
        perror("bind"); exit(1);
    }
    if (listen(listen_fd, 128) < 0) {
        perror("listen"); exit(1);
    }
    printf("Listening on 0.0.0.0:9000 ...\n");

    for (;;) {
        struct sockaddr_in cli;
        socklen_t len = sizeof(cli);
        int fd = accept(listen_fd, (struct sockaddr*)&cli, &len);
        if (fd < 0) { perror("accept"); continue; }

        char ip[INET_ADDRSTRLEN];
        inet_ntop(AF_INET, &cli.sin_addr, ip, sizeof(ip));
        printf("New client from %s:%d\n", ip, ntohs(cli.sin_port));

        char buf[4096];
        for (;;) {
            ssize_t n = read(fd, buf, sizeof(buf));
            if (n == 0) {  // FIN 수신
                printf("Client closed\n");
                break;
            }
            if (n < 0) { perror("read"); break; }
            ssize_t w = write(fd, buf, n);
            if (w < 0) { perror("write"); break; }
        }
        close(fd);
    }
    close(listen_fd);
    return 0;
}
```

- `accept()` 후 클라이언트 소켓에서 읽은 데이터를 그대로 다시 돌려주는 단순 에코.
- Wireshark나 `tcpdump`로 3-way 핸드셰이크, 데이터 전송, FIN/ACK 종료를 직접 확인할 수 있다.

### 9.2 파이썬 에코 클라이언트

```python
# tcp_echo_client.py
import socket

HOST = "127.0.0.1"
PORT = 9000

with socket.create_connection((HOST, PORT)) as s:
    s.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)  # Nagle 비활성
    while True:
        msg = input("> ")
        if not msg:
            break
        s.sendall(msg.encode("utf-8"))
        data = s.recv(4096)
        if not data:
            break
        print("echo:", data.decode("utf-8", errors="ignore"))
```

---

## 10. 관찰/디버깅 도구 활용

### 10.1 `ss` / `netstat`로 상태 보기

```bash
ss -tin         # 각 TCP 소켓의 상태, cwnd, rtt, rto, send/recv-q 등
ss -s           # 프로토콜별 요약 통계
```

리눅스에서는 `ss -tin` 출력에 다음과 같은 필드가 보일 수 있다.

- `cwnd` : 현재 congestion window (세그먼트 단위 또는 바이트 단위)
- `rtt` / `rttvar`
- `bbr` 또는 `cubic` 등의 혼잡제어 알고리즘 상태 (커널 버전에 따라 다름)  

### 10.2 `tcpdump`로 패킷 캡처

```bash
# SYN/FIN/RST 관찰
tcpdump -nnvvvi eth0 'tcp[tcpflags] & (tcp-syn|tcp-fin|tcp-rst) != 0'

# 특정 호스트와의 모든 TCP 세션
tcpdump -nnvi eth0 'tcp and host 203.0.113.10'
```

Wireshark를 사용하면:

- SYN, SYN/ACK, ACK 핸드셰이크
- 윈도우 스케일, SACK Permitted, Timestamps 옵션
- SACK 블록, ECN(ECE/CWR) 플래그

등을 한눈에 확인할 수 있다.

### 10.3 `iperf3`로 CC 알고리즘 비교

```bash
iperf3 -s                 # 서버 (한쪽)
iperf3 -c <server-ip> -t 30

# CUBIC ↔ BBR 비교 (클라이언트 측에서)
iperf3 -c <server-ip> -t 30 -C cubic
iperf3 -c <server-ip> -t 30 -C bbr --pacing-timer 1000
```

지표:

- 스루풋(Throughput)
- RTT(P95/P99)
- 재전송률, 큐 길이(스위치 모니터링), ECN 마킹 비율

---

## 11. 트러블슈팅 체크리스트 (정리)

### 11.1 연결/종료 문제

```text
- SYN 보냄? SYN/ACK 수신? (방화벽/보안장비/백로그/SYN cookies 확인)
- SYN_SENT 또는 SYN_RCVD에 오래 머무르는 소켓은 없는가?
- FIN/ACK 순서/상태 전이 정상? RST가 갑자기 오지는 않는가?
- TIME_WAIT 소켓이 대량으로 쌓이는 원인은?
  - 단일 클라이언트에서 매우 많은 짧은 연결을 만드는 패턴?
  - 로드밸런서/프록시 사이에서의 half-close 처리 문제?
```

### 11.2 성능/지연 문제

```text
- BDP 대비 윈도우/버퍼가 충분한가?
  (BDP 계산 후 tcp_rmem/tcp_wmem, Window Scale 적용 여부 확인)
- 재전송률/중복 ACK가 높은가?
  (손실/재주문, 큐 정책(AQM/ECN) or 무선 구간 문제)
- PMTUD/ICMP가 차단되어 있지 않은가?
  (큰 응답만 끊기거나 느려지는 현상)
- Tinygram(작은 패킷) 폭주인가?
  (Nagle/Delayed ACK 조합, 앱의 write 패턴 재검토)
- 특정 구간에서만 지연이 급증하는가?
  (버퍼블로트, 큐 정책, 피크 트래픽 시기)
```

### 11.3 서버/OS 튜닝 항목

```text
- 큐디스크: fq 혹은 fq_codel 사용 여부
- 혼잡제어: cubic vs bbr (서비스별 A/B 테스트)
- SACK, Timestamps, RACK/TLP 활성화 여부
- 리스닝 소켓 backlog (tcp_max_syn_backlog, somaxconn) 값 적절성
- rmem/wmem 상한, TIME_WAIT / 포트 재사용 정책
- sendfile / MSG_ZEROCOPY / io_uring 등 zero-copy 가능 여부
```

---

## 12. 공식 수식/감각 메모

### 12.1 처리량 근사

$$
\text{Throughput} \approx \frac{\text{cwnd}}{\text{RTT}}
\qquad (\text{바이트/초})
$$

예:

- cwnd = 10 MB, RTT = 100 ms → 약 100 MB/s ≈ 800 Mbps.

### 12.2 BDP와 최소 윈도우 조건

$$
\text{필요 창 크기} \ge \text{BDP} = \text{Bandwidth} \times \text{RTT}
$$

- 링크를 꽉 채우려면 cwnd와 rwnd, 네트워크·NIC 버퍼가 **BDP 이상**이어야 한다.

### 12.3 AIMD 요약

앞서 본 대로, RTT마다 +1 MSS, 손실마다 ×β(≈0.5) 감소:

$$
\text{cwnd}_{k+1} =
\begin{cases}
\text{cwnd}_k + \text{MSS}, & \text{손실 없음 (RTT 하나)} \\
\beta \cdot \text{cwnd}_k, & \text{손실 발생}
\end{cases}
$$

---

## 13. 베스트 프랙티스(요약 카드)

- **핸드셰이크/종료**: 3-way/4-way, TIME\_WAIT 이유까지 이해하고 설계.
- **신뢰성**: SACK/RACK/TLP를 이용해 빠른 손실 복구, RTT/RTO 튜닝.
- **혼잡제어**: CUBIC를 기본으로, BBR는 서비스 특성에 맞춰 A/B 테스트 후 도입.
- **윈도우/BDP**: Window Scale 필수, 고BDP 환경에서는 rmem/wmem 상한↑.
- **작은 패킷 지연**: Nagle↔Delayed ACK 상호작용 이해, 필요시 NODELAY.
- **MTU/MSS**: 오버레이/터널 환경에서 MSS 클램핑, PMTUD/ICMP 허용.
- **큐/페이싱**: fq(_codel)+pacing으로 버퍼블로트 억제.
- **현대 기능**: ECN, RACK/TLP, MPTCP, zero-copy 등을 적절히 활용해 성능/지연을 동시에 최적화.

---

## 14. 한 장 요약(포스터 느낌)

- **TCP = 신뢰·순서·혼잡친화 스트림**.
- **슬라이딩 윈도우** + **누적 ACK/SACK**으로 손실·재전송 관리.
- **AIMD 혼잡제어**: Reno/NewReno → CUBIC(기본) → BBR(대역폭·RTT 기반).
- **옵션**: Window Scale, SACK, Timestamps, ECN, RACK/TLP, MPTCP.
- **튜닝 키워드**: BDP, 윈도우/버퍼, MTU/MSS/PMTUD, 큐(AQM/ECN), Nagle/ACK 전략, zero-copy.

---

### 부록 A — 리눅스 주요 sysctl 예시

```bash
# 혼잡제어/큐/ECN
sysctl -w net.ipv4.tcp_congestion_control=cubic     # or bbr
sysctl -w net.ipv4.tcp_ecn=1
tc qdisc replace dev eth0 root fq                   # pacing 친화적인 큐

# 윈도우/버퍼 오토튜닝 상한
sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728"
sysctl -w net.core.rmem_max=134217728
sysctl -w net.core.wmem_max=134217728

# SACK/Timestamps/RACK/TLP (커널에 따라 기본 ON)
sysctl -w net.ipv4.tcp_sack=1
sysctl -w net.ipv4.tcp_timestamps=1
sysctl -w net.ipv4.tcp_recovery=1      # RACK
sysctl -w net.ipv4.tcp_early_retrans=1 # TLP 유사 기능

# TIME_WAIT/백로그
sysctl -w net.ipv4.tcp_max_syn_backlog=4096
sysctl -w net.core.somaxconn=4096
```

### 부록 B — 애플리케이션 설계 체크리스트

```text
[전송] write() 코알레싱 / NODELAY 여부 / sendfile·zerocopy 활용
[복구] SACK 가정, 타임아웃과 재시도 정책 (중복 수신 허용 설계)
[프레이밍] 길이 프리픽스 / 고정 헤더+가변 바디 / 메시지 경계
[보안] TLS(또는 QUIC)로 암호화 / 인증 / 키 교환
[측정] RTT, 재전송률, ACK 지연, 큐 지표, ECN 마킹, 초당 트랜잭션
```

### 부록 C — 문제 분석 템플릿

```text
[환경] RTT, 대역폭, BDP, MTU, 큐 정책(AQM/ECN), NAT/방화벽, 터널/오버레이
[증상] 낮은 스루풋 / 지연 증가 / 짧은 메시지 지연 / 꼬리 손실 / 연결 빈번 종료
[가설] 윈도우/버퍼 부족, 버퍼블로트, PMTUD 문제, Nagle/ACK 상호작용,
       손실/재주문, CC 알고리즘 선택 문제
[근거] ss -tin, tcpdump, 큐 길이/ECN 비율, iperf3 테스트, sysctl 설정
[조치] 윈도우/버퍼 상향, 큐디스크 변경, ECN 활성, MSS 클램핑,
       NODELAY 적용, CC 알고리즘 교체/튜닝
[후속] A/B 테스트, 대시보드, 런북 업데이트
```