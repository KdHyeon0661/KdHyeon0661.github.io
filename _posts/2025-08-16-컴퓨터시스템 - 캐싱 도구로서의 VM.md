---
layout: post
title: 컴퓨터시스템 - 캐싱 도구로서의 VM
date: 2025-08-16 21:20:23 +0900
category: 컴퓨터시스템
---
# VM(가상 메모리, Virtual Memory)

> **핵심 관점**  
> VM은 “큰 메모리를 흉내내는 장치”를 넘어, **디스크(백스토어)** ↔ **DRAM(캐시)** 사이에서 **페이지 단위 캐싱**을 수행하는 거대한 계층(cache hierarchy)의 일부다.  
> 운영체제는 **LRU·Workingset·MGLRU** 같은 정책으로 **핫 페이지를 DRAM에 유지**하고, **콜드 페이지는 스왑/디스크**로 밀어낸다.  
> 본 문서는 **리눅스 6.x 기준 최신 동향(멀티-젠 LRU·Large Folio·PSI·zswap/zram·cgroup v2)**를 반영한다.

---

## 0. 큰 그림: “메모리 안의 캐시”

- **디스크(파일·스왑)** = 원본 저장소(백스토어)  
- **DRAM** = 디스크의 **페이지 캐시**(file-backed) + **익명 페이지 캐시**(anon + 스왑 백드)  
- **CPU 캐시(L1/L2/L3)** = DRAM의 캐시

3단계 계층(개념):

| 계층 | 단위 | 대략 지연 | 역할 |
|---|---|---|---|
| CPU 캐시 | 라인(64B) | ns | DRAM을 캐싱 |
| DRAM | 페이지(4KB/2MB/1GB) | μs | **디스크/스왑의 캐시** |
| 디스크(SSD/HDD) | 블록(4KB~) | 0.1~10ms | 원본 저장소 |

> **요지**: “VM = **DRAM을 디스크의 캐시로 사용하는 거대한 페이지 캐시 체계**”.

---

## 1. 페이지 단위 캐싱과 폴트의 의미

### 1.1 페이지/폴트/히트
- **페이지(Page)**: 가상↔물리 전송/보호의 최소 단위(기본 4KB, **Large Folio**로 16KB~ 수십 KB 묶음 운용, **THP**로 2MB/1GB).
- **페이지 히트**: 접근 페이지가 **이미 DRAM에 상주** → 곧바로 사용.
- **페이지 폴트(Page Fault)**:
  - **Minor fault**: PTE·페이지 테이블 등 **메타데이터만 필요**(DRAM에 데이터 있음, 매핑만 생성).
  - **Major fault**: **디스크 I/O 필요**(파일에서 로드, 또는 스왑 인).

### 1.2 파일/익명 두 세계
- **File-backed**: VFS 페이지 캐시(읽기/쓰기 후 **write-back**).  
- **Anonymous(익명)**: 힙/스택 등. 콜드 시 **스왑 아웃**되어 디스크의 스왑 영역으로 이동.

---

## 2. 교체 정책: LRU에서 **MGLRU**까지

### 2.1 고전: Active/Inactive + LRU 근사
- 리눅스는 **활성/비활성 리스트**로 LRU 유사 정책을 근사(CLOCK·LRU 혼합).
- **Refault 거리**로 진짜 워킹셋을 판별(금방 다시 참조되면 승격).

### 2.2 최신: **MGLRU(Multi-Gen LRU)**  
- 5.18에 합류, 6.x에서 광범위 개선. **세대(Generation)** 로 참조 시점을 기록 →  
  **콜드/핫 분리 정확도↑**, 스캔/회수 효율↑, **스래싱 감소**.

### 2.3 Working Set Model
- **최근 Δ 기간** 동안 접근한 페이지 집합을 **워킹셋**이라 하고, DRAM으로 유지하려 시도.
- 워킹셋 > DRAM ⇒ **스래싱**(과도한 Major fault) 발생 → 성능 급락.

---

## 3. 쓰기 정책(Write Policy)와 Dirty 관리

- **Write-back + Dirty bit**: 페이지를 메모리에서 먼저 변경(**dirty**로 마킹) →  
  **백그라운드 writeback** 스레드가 배출(pdflush→`kswapd`/`flush-*`/bdi).  
- **Write-through**는 안정적이나 느리므로 일반적이지 않다(저널링 FS + sync 지점으로 보장).
- 튜닝 포인트(예: `vm.dirty_background_bytes`, `vm.dirty_bytes`/`*_ratio`):  
  **Dirty 임계치** 도달 시 쓰기 시작, 앱의 **fsync/msync** 타이밍과 상호작용.

---

## 4. 수요 페이징과 프리페치(리드어헤드)

- **Demand Paging**: 접근 시점까지 로드 지연. 처음 접근 = page fault.
- **Readahead**: **공간 지역성** 가정, 연속 접근 탐지 시 **앞선 페이지**를 선제 로드.  
  - 순차 I/O에서 대폭 유리. 랜덤 I/O는 과한 Readahead가 오히려 낭비.
- 사용자 힌트:
  - `posix_fadvise(fd, off, len, POSIX_FADV_SEQUENTIAL/RANDOM/WILLNEED/DONTNEED)`  
  - `madvise(MADV_WILLNEED/DONTNEED, MADV_RANDOM, MADV_SEQUENTIAL, **MADV_POPULATE_READ**|WRITE)`

---

## 5. 스왑·압축·메모리 압박

- **swappiness**: 익명 vs 파일 캐시 회수 균형(0~100).  
  DB/캐시 서버는 낮추는 편, 워크스테이션은 기본/중간.
- **zswap**: 스왑-아웃 전에 **RAM에 압축 캐시**(백엔드 스왑 장치 필요). Major fault 비용↓.  
- **zram**: **압축 RAM 블록 장치** 자체를 스왑으로 사용(디스크 없이도 스왑 효과).
- **메모리 압축/회수** + **PSI(Pressure Stall Information)**: CPU가 **메모리 대기**로 멈춰있는 시간 비율을 계량 → 과압박 탐지.

---

## 6. NUMA·Huge Page·Large Folio

- **NUMA**: first-touch 정책으로 접근 노드에 물리 프레임 할당, **원격 접근 지연↑**.  
  `numactl`, `mbind`, `set_mempolicy`, **AutoNUMA** 가 페이지 마이그레이션 수행.
- **THP(Transparent Huge Pages)**: 2MB(또는 1GB)로 TLB 압박 완화(주로 익명).  
  파일 캐시는 **Large Folio(멀티페이지 폴리오)** 로 대형 I/O/리드어헤드 효율↑(6.x).
- THP는 랜덤 접근·분산 워킹셋에 역효과 가능 → **`madvise(MADV_NOHUGEPAGE)`** 등으로 제어.

---

## 7. 캐시 관점 **수학 모델**

### 7.1 AMAT(평균 메모리 접근 시간) 확장
DRAM을 **디스크의 캐시**로 보는 AMAT 근사:
$$
\text{AMAT} \approx T_\text{DRAM} + P_\text{pf}\cdot T_\text{fault}
$$
여기서
- \(T_\text{DRAM}\): DRAM 접근 시간(μs 수준보다 더 낮음, 실측은 ns~μs 구간의 복합)
- \(P_\text{pf}\): 페이지 폴트 확률
- \(T_\text{fault}\): Major fault 시 디스크 I/O + 스케줄링 + 페이지 테이블 갱신 시간(ms)

⇒ \(P_\text{pf}\) 를 **워킹셋 유지**·**Readahead**·**스왑 압축**·**Large Folio** 로 줄이는 것이 핵심.

### 7.2 스래싱(Thrashing) 조건(개념식)
워킹셋 크기 \(W(\Delta)\) 가 DRAM 캐시 용량 \(C\) 를 초과:
$$
W(\Delta) \gg C \ \Rightarrow \ P_\text{pf} \uparrow, \ \text{IO}\uparrow, \ \text{실효 IPC}\downarrow
$$

---

## 8. Buffered I/O vs Direct I/O

- **Buffered**: 페이지 캐시 경유(대부분 앱 기본). 작은 I/O·중복 읽기에 유리.
- **O_DIRECT**: 페이지 캐시 **우회**. DBMS(자체 버퍼 풀 운용), 대용량 순차 I/O에 적합.  
  단, **정렬·크기 제약**(파일시스템/디바이스 블록 크기)에 맞춰야 함.
- 혼용 주의: 같은 파일을 일부는 O_DIRECT, 일부는 buffered로 접근하면 **일관성/성능 충돌**.

---

## 9. 실전 튜닝 체크리스트(요약)

- [ ] **워킹셋 파악**: 랜덤/순차 비중, 시간별 footprint.
- [ ] **Readahead 힌트**: `posix_fadvise`, `madvise`.
- [ ] **Write-back 제어**: dirty 임계치, fsync 경계, 저널 모드·배치.
- [ ] **swappiness/zswap/zram**: 워크로드 성격에 맞게.
- [ ] **THP/Large Folio**: TLB 압박/대형 I/O에 유리한지 검증.
- [ ] **NUMA 바인딩**: 첫 터치/스레드 배치 일치.
- [ ] **PSI·cgroup v2(memory.high/low/max)**: 압박 감시·격리·SLO 보호.
- [ ] **O_DIRECT**: DB/엔진이 자체 캐시를 가질 때만 신중히.

---

## 10. 관찰·진단 도구 모음

### 10.1 빠른 관찰
```bash
vmstat 1
sar -B 1          # 페이지 폴트/캐시 통계
free -h
```

### 10.2 세부 카운터
```bash
cat /proc/meminfo
cat /proc/vmstat | egrep 'pgfault|pgmajfault|pgscan|pgsteal|workingset|swap'
cat /proc/pressure/memory     # PSI
cat /sys/kernel/mm/lru_gen/enabled  # MGLRU 활성화 여부/모드
```

### 10.3 프로세스 단위
```bash
ps -o pid,comm,min_flt,maj_flt,vsz,rss | head
cat /proc/$$/smaps | less
```

---

## 11. 코드 예제 — 페이지 캐시/폴트/힌트

### 11.1 Major/Minor 폴트 관찰
```c
// faults.c
#define _GNU_SOURCE
#include <sys/resource.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

static void print_faults(const char *tag){
    struct rusage u; getrusage(RUSAGE_SELF, &u);
    printf("[%s] minflt=%ld majflt=%ld\n", tag, u.ru_minflt, u.ru_majflt);
}

int main(int argc, char **argv){
    if (argc < 2){ fprintf(stderr, "usage: %s file\n", argv[0]); return 2; }
    int fd = open(argv[1], O_RDONLY);
    if (fd < 0){ perror("open"); return 1; }

    struct stat st; fstat(fd, &st);
    size_t len = st.st_size ? st.st_size : (1<<20);
    void *p = mmap(NULL, len, PROT_READ, MAP_PRIVATE, fd, 0);
    if (p == MAP_FAILED){ perror("mmap"); return 1; }

    print_faults("start");
    // 랜덤 접근: Major fault 유발 가능
    volatile unsigned long sum = 0;
    size_t step = 4096 * 17; // 페이지 경계를 일부러 건너뛰기
    for (size_t off = 0; off < len; off += step){
        sum += ((unsigned char*)p)[off];
    }
    print_faults("after-rand");

    // 순차 접근 + readahead 힌트
    posix_fadvise(fd, 0, 0, POSIX_FADV_SEQUENTIAL);
    for (size_t off = 0; off < len; off += 4096){
        sum += ((unsigned char*)p)[off];
    }
    print_faults("after-seq");

    // 캐시 버리기(실험용, 주의)
    posix_fadvise(fd, 0, 0, POSIX_FADV_DONTNEED);
    print_faults("after-dontneed");

    munmap(p, len);
    close(fd);
    fprintf(stderr, "sum=%lu\n", sum);
    return 0;
}
```

### 11.2 익명 메모리와 스왑 압박 힌트
```c
// anon.c
#define _GNU_SOURCE
#include <sys/mman.h>
#include <unistd.h>
#include <stdio.h>
#include <string.h>

int main(){
    size_t ps = sysconf(_SC_PAGESIZE);
    size_t n = (size_t)256<<20; // 256MB
    void *p = mmap(NULL, n, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
    if (p == MAP_FAILED){ perror("mmap"); return 1; }

    // 쓰기(확정 커밋)
    for (size_t i=0;i<n;i+=ps) ((char*)p)[i] = 1;

    // 일부는 콜드로 간주하도록 힌트
    madvise((char*)p + n/2, n/2, MADV_PAGEOUT);   // 커널에 회수 제안
    madvise((char*)p, n/2, MADV_WILLNEED);        // 앞 절반은 유지
    // 또는 MADV_DONTNEED(익명)으로 재초기화 효과

    munmap(p, n);
}
```

### 11.3 write-back과 msync
```c
// wback.c
#include <sys/mman.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <stdio.h>
#include <string.h>

int main(int argc,char**argv){
    if (argc<2){ fprintf(stderr,"usage: %s file\n",argv[0]); return 2; }
    int fd = open(argv[1], O_RDWR|O_CREAT, 0644);
    if (fd<0){ perror("open"); return 1; }
    ftruncate(fd, 1<<20);
    void *p = mmap(NULL, 1<<20, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);
    if (p==MAP_FAILED){ perror("mmap"); return 1; }
    memset(p, 0x5A, 1<<20);             // dirty 페이지 생성
    if (msync(p, 1<<20, MS_SYNC) < 0)   // 동기 write-back
        perror("msync");
    munmap(p, 1<<20);
    close(fd);
}
```

### 11.4 Direct I/O(O_DIRECT) 예제(정렬 주의)
```c
// odirect.c — 블록 정렬(예: 4096) 맞춘 버퍼 필요
#define _GNU_SOURCE
#include <fcntl.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <errno.h>

int main(int argc,char**argv){
    if (argc<2){ fprintf(stderr,"usage: %s file\n",argv[0]); return 2; }
    int fd = open(argv[1], O_RDONLY|O_DIRECT);
    if (fd<0){ perror("open"); return 1; }
    size_t align = 4096, len = 4096*4;
    void *buf;
    if (posix_memalign(&buf, align, len)) return 1;
    ssize_t r = read(fd, buf, len);
    if (r<0) perror("read");
    printf("read=%zd\n", r);
    free(buf); close(fd);
}
```

---

## 12. cgroup v2로 **캐시 격리/보호**

- **memory.max / high / low**: 메모리 상한·압박선·SLO 보호선.  
- **memory.swap.max**: 스왑 사용 한계.  
- **oom.group**: 그룹 단위 OOM 정책.  
→ 혼합 워크로드에서 **캐시/워킹셋 간섭 최소화**.

---

## 13. PSI(Pressure Stall Information)로 압박 감시

`/proc/pressure/memory` 예:
```
some avg10=0.00 avg60=0.00 avg300=0.00 total=123456
full avg10=1.23 avg60=0.78 avg300=0.45 total=789012
```
- **some**: 일부 태스크가 메모리 자원 대기로 지연  
- **full**: 모든 태스크가 영향 받는 총체 지연(심각)

PSI 기반 자동 완화(스로틀·프리페치 강화·일시적 스왑 확장 등)를 설계 가능.

---

## 14. 워크로드별 권장 패턴

### 14.1 데이터베이스(엔진 버퍼 풀 보유)
- **O_DIRECT** + 엔진 내부 캐시(버퍼 풀) → **이중 캐싱 방지**  
- `swappiness` 낮춤, **zswap은 신중**(추가 CPU 비용)  
- 체크포인트/redo 주기 ↔ dirty 임계치 조율, **fsync 파동 방지**

### 14.2 분석/ML(큰 순차 스캔)
- **Readahead 확대** + `POSIX_FADV_SEQUENTIAL/WILLNEED`  
- **Large Folio** 이점 큼(6.x)  
- 샘플링·랜덤 읽기 비중이 높다면 **리드어헤드 최소화**로 전환

### 14.3 웹/캐시 서버(핫셋 작음·랜덤)
- **핫셋 크기**를 DRAM 내로 맞추고 **zswap**으로 꼬리 완충  
- **PSI 알람**으로 스레드 수/큐 심도 자동 조절

---

## 15. 함정 및 안전수칙

| 항목 | 설명 | 권고 |
|---|---|---|
| `echo 3 > /proc/sys/vm/drop_caches` | 실시간 트래픽 중 캐시 파괴 | 실험 전용, 운영 금지 |
| THP 남용 | 랜덤·파편 워킹셋에서 역효과 | `MADV_NOHUGEPAGE` 또는 `thp=never` 검증 |
| O_DIRECT 혼용 | 동일 파일을 buffered와 혼용 | 한 방식으로 일관화 |
| 무제한 mlock | 페이지 고정으로 회수 불가 | 한도/권한/실측 기반 최소화 |
| 과도한 swappiness=0 | 파일 캐시만 희생, OOM↑ | 워크로드 기반 **균형값** |

---

## 16. 실험 시나리오(재현 가능한 관찰)

1) **순차 vs 랜덤**: 같은 파일에 대해 위 `faults.c`로 `POSIX_FADV_*` 의 효과 비교  
2) **zswap on/off**: Major fault 비용·PSI 변화 측정  
3) **THP vs NOHUGEPAGE**: 큰 배열 랜덤 접근에서 TLB·fault 차이  
4) **cgroup 격리**: memory.low로 중요 서비스 워킹셋 보호 → tail latency 관찰

---

## 17. 요약 한 줄

**가상 메모리의 본질** 중 하나는 **캐싱**이다.  
운영체제는 **DRAM을 디스크의 캐시**로 운용하며, **정교한 교체 정책(MGLRU/Workingset/Refault)**,  
**write-back/압축 스왑**, **Readahead/Large Folio/THP/NUMA** 로 **폴트율을 낮추고** 효율을 극대화한다.  
워크로드 특성을 계측(PSI·vmstat·/proc/vmstat)하고 **힌트/튜닝**을 더하면,  
동일 하드웨어에서도 **두 자릿수 성능 향상**을 흔히 얻을 수 있다.