---
layout: post
title: 기계학습 - 랜덤 포레스트
date: 2025-08-19 18:25:23 +0900
category: 기계학습
---
# 랜덤 포레스트(Random Forest)

## 1. 개요

**랜덤 포레스트(Random Forest)**는 다수의 **결정트리(decision trees)**를 서로 **무작위성(randomness)**으로 다양화하여 학습하고, 분류는 **다수결**, 회귀는 **평균**으로 앙상블하는 기법이다. 단일 트리의 **높은 분산(불안정성)**과 **과적합**을 **배깅(Bootstrap Aggregating)**과 **특징 무작위 선택**으로 억제한다.

- 분류: 다수결 \( \hat{y} = \mathrm{mode}\{h_m(x)\}_{m=1}^M \)
- 회귀: 평균 \( \hat{y} = \frac{1}{M}\sum_{m=1}^M h_m(x) \)

---

## 2. 동작 원리

### 2.1 배깅(Bagging)
1) 원본 데이터에서 **중복허용 부트스트랩 샘플** \(M\)개 생성  
2) 각 샘플로 **독립 트리** 학습  
3) 예측 시 **투표/평균**으로 집계

배깅은 **분산을 줄여** 일반화 성능을 높인다.

### 2.2 추가 무작위성(Feature Subsampling)
각 **노드 분할** 시 전체 특징 중 무작위로 뽑은 \(d_{\text{sub}}\)개만 후보로 고려(예: 분류는 \(\sqrt{d}\), 회귀는 \(d/3\) 관례).  
→ 트리 간 **상관 감소** → 앙상블 평균의 분산 추가 감소.

---

## 3. 수학적 정당성

### 3.1 분산 감소 공식(직관 모델)
트리 \(M\)개, 단일 트리 분산 \(\sigma^2\), 트리 간 상관 \(\rho\)일 때 앙상블 예측의 분산:

$$
\mathrm{Var}(\hat{f}(x))=\rho\sigma^2+\frac{1-\rho}{M}\sigma^2
$$

- \(M \to \infty\)일 때 분산 하한은 \(\rho\sigma^2\).  
- **핵심**: \(M\)을 늘리는 것(두 번째 항 감소)과 **\(\rho\)를 줄이는 것**(첫 번째 항 감소)이 모두 중요.  
- 랜덤 포레스트는 **부트스트랩**과 **특징 무작위 선택**으로 \(\rho\)를 낮춘다.

### 3.2 분류의 확률적 집계
각 트리의 클래스 \(k\) 확률 \(\hat{p}_m(k|x)\)를 평균:

$$
\hat{p}(k|x)=\frac{1}{M}\sum_{m=1}^M \hat{p}_m(k|x),\quad \hat{y}=\arg\max_k \hat{p}(k|x)
$$

---

## 4. 학습 알고리즘 (요약·의사코드)

1) for \(m=1,\dots,M\):  
 a) **부트스트랩** 샘플 \(S_m\) 추출  
 b) 트리 \(h_m\) 학습: 각 노드에서 무작위 특징 하위집합 선택 → 최적 분할(불순도 감소 최대)  
2) 예측: 분류는 **투표**, 회귀는 **평균**

```text
for m in 1..M:
    Sm ← bootstrap_sample(S)
    hm ← grow_tree(Sm) with feature_subsampling
return H(x) = majority_vote{hm(x)}  # 회귀면 평균
```

---

## 5. 분할 기준(불순도/지표)

- 분류: 지니(Gini) 또는 엔트로피(Information Gain)
- 회귀: 분산 감소(또는 MSE 감소)

지니 감소량(노드 \(t\), 분할 후 자식 \(t_L,t_R\)):

$$
\Delta G = G(t) - \frac{N_{L}}{N}G(t_L) - \frac{N_{R}}{N}G(t_R)
$$

---

## 6. 핵심 하이퍼파라미터 (실전 가이드)

| 파라미터 | 효과/권장 |
|---|---|
| `n_estimators` | 트리 수. 많을수록 안정하지만 시간·메모리↑. 100~1000부터 탐색. |
| `max_depth` | 트리 깊이 제한. 과적합 시 줄이기. |
| `min_samples_leaf` | 리프 최소 샘플 수. **일반화↑**(특히 노이즈 환경). 1→5/10 검토. |
| `max_features` | 노드 분할 시 특징 수. 분류: `sqrt`, 회귀: `1.0` 또는 `log2/sqrt`. |
| `bootstrap` | True면 부트스트랩, False면 전체 샘플(ExtraTrees 유사). |
| `class_weight` | 불균형 분류 시 `"balanced"` 권장. |
| `oob_score` | OOB 일반화 성능 추정(bootstrap=True일 때만). |
| `max_samples` | (sklearn) 부트스트랩 샘플 크기(비율/개수). 속도·편향 제어. |
| `ccp_alpha` | 비용복잡도 가지치기(미세 조정). |

---

## 7. OOB(Out-of-Bag) 추정

부트스트랩에서 각 관측치가 **약 \(1-1/e≈0.632\)** 확률로 샘플링되므로, **약 36.8%**의 트리에는 포함되지 않는다(OOB). 이를 해당 관측치의 **검증**으로 사용하면 **추가 검증셋 없이** 성능을 추정 가능.

$$
\hat{y}_i^{\text{OOB}} = \mathrm{agg}\{h_m(x_i): x_i \notin S_m\}
$$

```python
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=400, oob_score=True, n_jobs=-1, random_state=42)
rf.fit(X, y)
print("OOB score:", rf.oob_score_)
```

> OOB는 빠르고 편리하지만, **최종 보고용**으로는 별도 **잠금 테스트** 권장.

---

## 8. 특징 중요도(Feature Importance) — 장점과 함정

### 8.1 Gini Importance(분할 불순도 감소 합)
학습 중 **해당 특징으로 인한 불순도 감소량**을 노드 샘플 수로 가중 평균.  
**함정**: **많은 고유값/연속형** 특징이 **과대평가**되는 경향(분할 기회가 많아서).

### 8.2 Permutation Importance(순열 중요도)
검증셋에서 특정 특징 값을 **무작위 섞기 → 성능 감소량**을 중요도로 정의.  
**해석↑/편향↓**. 상관 피처가 많으면 **조건부 순열 중요도**(다른 피처 조건화) 고려.

```python
from sklearn.inspection import permutation_importance
r = permutation_importance(rf, X_val, y_val, n_repeats=20, random_state=42)
for i in r.importances_mean.argsort()[::-1]:
    print(i, r.importances_mean[i], r.importances_std[i])
```

### 8.3 SHAP 값(트리 SHAP)
트리 모델의 **기여도 분해**(로컬·글로벌 해석) 가능. 상호작용/방향성 파악에 유리.

---

## 9. 확률 보정(Probability Calibration)

랜덤 포레스트의 클래스 확률은 **과신/과소신뢰**가 있을 수 있다. **의사결정/랭킹** 업무라면 **캘리브레이션** 권장.

- Platt(로지스틱) / Isotonic

```python
from sklearn.calibration import CalibratedClassifierCV
cal = CalibratedClassifierCV(rf, method="isotonic", cv=5)
cal.fit(X_tr, y_tr)
proba = cal.predict_proba(X_te)[:,1]
```

---

## 10. 불균형·그룹·시계열 — 분할·학습 전략

### 10.1 불균형 분류
- `class_weight="balanced"` 또는 **커스텀 가중치**
- **Stratified** 분할, **PR-AUC/F1** 기반 평가
- **언더/오버샘플링**(주의: 누수 방지 위해 **파이프라인 내**에서 CV 폴드별로 적용)

### 10.2 그룹 데이터(GroupKFold)
**같은 사용자/세션**이 train과 val에 동시에 들어가면 누수. **그룹 분할** 필수.

### 10.3 시계열(TimeSeriesSplit)
미래정보 누수를 막기 위해 **과거→미래**로만 평가. 필요 시 **롤링/확장 창**.

---

## 11. 랜덤 포레스트 변형·친척 모델

| 모델 | 핵심 차이 | 장단점 |
|---|---|---|
| **ExtraTrees** | 분할 임계값까지 **무작위**(데이터 적합 최소화) | 상관↓, 분산↓(속도↑), 편향↑ 가능 |
| **Oblique RF** | 축 정렬 분할 대신 선형 조합 분할 | 경계 복잡도↑, 계산↑ |
| **Isolation Forest** | 분리 기반 이상치 탐지 | 비지도 이상치 탐지에 특화 |
| **Quantile RF** | 조건부 분포 추정(분위수) | 예측 구간 산출(별도 구현 필요) |

---

## 12. 성능·복잡도·자원

- 트리 1개의 학습 복잡도(대략): \(O(N \log N \cdot d_{\text{sub}})\)  
- 전체는 \(\times M\). **병렬화**(`n_jobs=-1`)로 완화.  
- 메모리: 노드/분할 정보 저장 비용. `max_depth`·`min_samples_leaf`로 제어.  
- 지연 허용이 적은 **실시간**이면 **트리 수 축소**·**ExtraTrees**·**Distillation**(RF→작은 모델) 고려.

---

## 13. 실전 코드 레시피

### 13.1 분류(파이프라인 + StratifiedKFold + 중요도)
```python
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.inspection import permutation_importance

# 예시: 수치/범주 혼합 데이터
num_cols = [0,1,2]
cat_cols = [3,4]

pre = ColumnTransformer([
    ("num", StandardScaler(with_mean=False), num_cols),  # 트리에 스케일 필수는 아님(혼합 데이터일 때 관례)
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

rf = RandomForestClassifier(
    n_estimators=500, max_depth=None, min_samples_leaf=3,
    max_features="sqrt", class_weight="balanced",
    n_jobs=-1, random_state=42, oob_score=True
)

pipe = Pipeline([("pre", pre), ("rf", rf)])

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(pipe, X, y, cv=skf, scoring="roc_auc", n_jobs=-1)
print("CV AUC:", scores.mean(), "±", scores.std())

pipe.fit(X_tr, y_tr)
proba = pipe.predict_proba(X_te)[:,1]
print("Test AUC:", roc_auc_score(y_te, proba))
print(classification_report(y_te, pipe.predict(X_te)))

# Permutation Importance (검증/테스트 기준)
r = permutation_importance(pipe, X_te, y_te, n_repeats=20, random_state=42, n_jobs=-1)
print("Permutation importances (mean±std):", r.importances_mean, r.importances_std)
```

### 13.2 회귀(튜닝 + OOB + 퍼뮤테이션 중요도)
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, RandomizedSearchCV
from sklearn.metrics import mean_squared_error

rf = RandomForestRegressor(
    n_estimators=800, max_depth=None, min_samples_leaf=2,
    max_features=1.0, bootstrap=True, oob_score=True,
    n_jobs=-1, random_state=42
)

param_dist = {
    "n_estimators": [400, 800, 1200],
    "max_features": [0.5, 0.75, 1.0],
    "min_samples_leaf": [1, 2, 5, 10],
    "max_depth": [None, 10, 20]
}

kfold = KFold(n_splits=5, shuffle=True, random_state=42)
rs = RandomizedSearchCV(rf, param_dist, n_iter=20, cv=kfold, n_jobs=-1, scoring="neg_root_mean_squared_error", random_state=7)
rs.fit(X_tr, y_tr)

best = rs.best_estimator_
print("OOB R^2:", best.oob_score_)
pred = best.predict(X_te)
print("Test RMSE:", mean_squared_error(y_te, pred, squared=False))
```

### 13.3 임계값 최적화(불균형 분류)
```python
from sklearn.metrics import precision_recall_curve, f1_score

proba = pipe.predict_proba(X_val)[:,1]
p, r, th = precision_recall_curve(y_val, proba)
f1 = 2 * p * r / (p + r + 1e-12)
best_t = th[f1[:-1].argmax()]
y_hat = (proba >= best_t).astype(int)
print("Best F1:", f1.max(), "Threshold:", best_t)
```

---

## 14. 해석 기법(모델 설명)

- **Permutation Importance**: 전역 중요도(편향 적음)  
- **SHAP(TreeSHAP)**: 전역/국소 기여도, 상호작용 해석  
- **PDP/ICE**: 개별 피처 변화에 따른 예측 반응(비선형·상호작용 주의)  
- **부분종속(Partial Dependence)**는 상관 피처가 강하면 잘못된 해석 유의 → **누적현대응(ALE)** 고려

---

## 15. 데이터 전처리·카테고리·결측

- 트리는 **스케일 불변**(정규화 필수 아님). 다만 **혼합 파이프라인**에서 일관성 위해 사용할 수 있음.  
- **고유값 많은 범주형**은 원-핫 차원 폭발 → **빈도 상위만 인코딩** / **타겟인코딩(누수 주의, CV 내 적용)** 고려.  
- 결측: sklearn RF는 결측 직접 처리 미지원 → **간단 임퓨테이션(SimpleImputer)**을 **파이프라인 내**에서 폴드별 `fit`.

---

## 16. 랜덤 포레스트 vs. 부스팅

| 항목 | 랜덤 포레스트 | 그래디언트 부스팅(XGB/LGBM/Cat) |
|---|---|---|
| 학습 방식 | 병렬적 트리(배깅) | 순차적 트리(오차 보정) |
| 튜닝 민감도 | 상대적 낮음(견고) | 높음(학습률·깊이·트리 수) |
| 학습 속도 | 빠름(병렬 용이) | 상대 느림(직렬) |
| 성능 상한 | 보통~좋음 | 보통~매우 좋음 |
| 확률 보정 | 추가 캘리브레이션 권장 | 보정 양호(모델·데이터 따라 다름) |

**베이스라인**으로 RF를 먼저 구축하고, 필요 시 부스팅으로 고도화하는 전략이 현실적이다.

---

## 17. 체크리스트 (실전 운영)

1. **분할 전략**: Stratified/Group/TimeSeriesSplit로 누수 방지  
2. **하이퍼파라미터**: `min_samples_leaf`, `max_features`부터 조정 → 과적합 줄이기  
3. **OOB**: 빠른 내부 추정(참고용), 최종 평가는 **잠금 테스트**  
4. **불균형**: `class_weight`, 적절 지표(PR-AUC), 임계값 최적화  
5. **해석**: Permutation/SHAP로 중요도 검증, Gini 편향 주의  
6. **확률**: 의사결정 사용 시 캘리브레이션  
7. **자원**: `n_estimators`·`n_jobs`·병렬화/캐싱, 모델 크기 관리  
8. **배포**: 버전·시드·특징 스키마 고정, 데이터 드리프트 모니터링

---

## 18. 수식·요약

- 앙상블 분산:  
  $$\mathrm{Var}(\hat{f})=\rho\sigma^2+\frac{1-\rho}{M}\sigma^2$$
- 목표: \(M\) 증가 + \(\rho\) 감소(부트스트랩 + 특징 무작위 선택)
- 분류 예측:  
  $$\hat{p}(k|x)=\tfrac{1}{M}\sum_m \hat{p}_m(k|x),\ \hat{y}=\arg\max_k \hat{p}(k|x)$$

---

## 19. 끝맺음

랜덤 포레스트는 **강건성·간편한 튜닝·탄탄한 기본 성능**으로 **베이스라인**이자 **프로덕션급** 선택지다. **분할·누수 방지·중요도 해석·캘리브레이션·임계값 최적화**를 결합하면, 다양한 도메인에서 **안정적인 일반화 성능**을 제공한다.