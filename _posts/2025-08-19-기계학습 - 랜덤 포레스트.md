---
layout: post
title: 기계학습 - 의사결정나무
date: 2025-08-19 17:25:23 +0900
category: 기계학습
---
# 🌲 랜덤 포레스트(Random Forest)

## 1. 개요
**랜덤 포레스트(Random Forest)**는 여러 개의 **의사결정나무(Decision Trees)**를 학습시킨 후, 그 예측 결과를 **앙상블(Ensemble)**하는 알고리즘입니다.  
단일 의사결정나무의 **과적합 문제**와 **불안정성**을 해결하기 위해 제안되었습니다.  

- 분류 문제 → 다수결 투표(Majority Voting)  
- 회귀 문제 → 평균(Averaging)  

> 핵심 아이디어: **무작위성(Randomness)**을 도입해 다양한 트리를 만들고, **집단 지성(Ensemble)**으로 더 강력하고 안정적인 예측을 한다.

---

## 2. 동작 원리

### 2.1 배깅(Bagging: Bootstrap Aggregating)
1. 원본 데이터셋에서 **중복 허용 샘플링(bootstrap sampling)**으로 \(n\)개의 학습용 데이터셋을 생성.
2. 각 데이터셋으로 **하나의 의사결정나무**를 학습.
3. 예측 시 각 트리의 결과를 취합:
   - 분류: 다수결 투표
   - 회귀: 평균값

배깅은 **분산(variance)**을 줄여 예측의 안정성을 높이는 효과가 있습니다.

---

### 2.2 랜덤 포레스트의 추가 무작위성
랜덤 포레스트는 배깅 외에 **특징(feature) 무작위 선택**을 도입합니다.
- 각 노드에서 분할 기준을 고를 때, **전체 특징 중 일부를 무작위로 선택**.
- 그 부분집합에서 최적의 분할을 찾음.

이로써 각 트리는 서로 더 독립적이고 다양한 구조를 가질 수 있습니다.

---

## 3. 수학적 정당성

### 3.1 배깅의 분산 감소
단일 의사결정나무 예측값을 \(h(x)\)라 하고, 랜덤 포레스트의 예측값을 \( \hat{f}(x) = \frac{1}{M} \sum_{m=1}^M h_m(x) \)라고 합시다.  

예측 오차 분산은 다음과 같이 감소합니다:

$$
Var(\hat{f}(x)) = \rho \sigma^2 + \frac{1-\rho}{M} \sigma^2
$$

- \(M\): 트리 개수
- \(\sigma^2\): 단일 트리 분산
- \(\rho\): 트리 간 상관관계

즉, 트리 수 \(M \to \infty\)일 때, 분산은 \(\rho \sigma^2\)까지 감소합니다.  
→ **트리 간 상관을 줄이는 것이 중요**하며, 이를 위해 무작위 특성 선택이 도입됩니다.

---

### 3.2 분류에서의 확률적 해석
각 트리가 클래스 \(k\)를 예측할 확률을 \(\hat{p}_m(y=k|x)\)라고 하면, 랜덤 포레스트는 이를 평균화:

$$
\hat{p}(y=k|x) = \frac{1}{M} \sum_{m=1}^M \hat{p}_m(y=k|x)
$$

최종 예측:
$$
\hat{y} = \arg \max_{k} \hat{p}(y=k|x)
$$

---

## 4. 학습 알고리즘 요약
1. 훈련 데이터로부터 \(M\)개의 bootstrap 샘플을 생성.
2. 각 샘플마다 의사결정나무를 학습하되, 노드 분할 시 특징 일부만 무작위로 고려.
3. 모든 트리의 결과를 **집계(앙상블)**하여 최종 예측 수행.

---

## 5. 특징 중요도 (Feature Importance)
랜덤 포레스트는 학습 과정에서 **특징 중요도(Feature Importance)**를 추출할 수 있습니다.  
일반적으로 두 가지 방법이 사용됩니다:

1. **Gini Importance (불순도 감소 합)**  
   특정 특징이 분할에 사용될 때 불순도 감소량의 평균.

2. **Permutation Importance (순열 중요도)**  
   특정 특징의 값을 무작위로 섞은 뒤 성능 감소량을 측정.

---

## 6. 장단점

✅ 장점
- 과적합 완화 (여러 트리 평균으로 분산 감소).
- 비선형 데이터, 복잡한 패턴에도 강력.
- 특징 중요도 제공.
- 데이터 전처리(정규화, 스케일링) 불필요.

❌ 단점
- 학습/예측 속도가 단일 트리보다 느림.
- 메모리 사용량 많음.
- 너무 많은 트리를 학습하면 **실시간 응답**에는 부적합.
- 해석력이 단일 트리보다 떨어짐.

---

## 7. 파이썬 예제 (Scikit-Learn)

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# 데이터 불러오기
iris = load_iris()
X, y = iris.data, iris.target

# 모델 학습
clf = RandomForestClassifier(
    n_estimators=100,     # 트리 개수
    max_depth=None,       # 최대 깊이 제한 없음
    random_state=42
)
clf.fit(X, y)

# 예측
y_pred = clf.predict(X)
print("훈련 정확도:", accuracy_score(y, y_pred))

# 특징 중요도 출력
importances = clf.feature_importances_
for f, imp in zip(iris.feature_names, importances):
    print(f"{f}: {imp:.3f}")
```

---

## 8. 랜덤 포레스트 vs 다른 앙상블

- **Bagging Tree**: 랜덤 포레스트와 유사하지만, 특징 무작위 선택 없음.
- **Boosting (예: XGBoost, LightGBM)**: 이전 모델의 오차를 보완하는 방식 → 일반적으로 성능은 더 높음.
- **랜덤 포레스트**: 병렬 학습 가능, 안정적, 해석력 있음.

---

## 📌 요약
- 랜덤 포레스트는 **배깅 + 무작위 특징 선택**을 통한 앙상블 기법.
- 단일 트리보다 과적합에 강하고 일반화 성능이 뛰어남.
- 트리 개수를 늘리면 안정적이지만, 계산량 증가.
- 현대 머신러닝에서는 기본 baseline 모델로 널리 사용.