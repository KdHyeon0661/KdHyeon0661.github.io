---
layout: post
title: 운영체제 - 리눅스 시스템 (1)
date: 2025-11-05 14:25:23 +0900
category: 운영체제
---
# Chapter 20 — The Linux System (1)

## Linux History

### “어떻게 여기까지 왔나”

- **1991**: Linus Torvalds가 x86용 초기 커널 공개.
- **GPLv2** 채택 → **열린 개발 모델** 정착, 드라이버/FS/네트워크 스택이 폭발적으로 확장.
- **모놀리식 커널 + 모듈** 구조: 시스템 콜·스케줄러·MM·I/O·네트워크가 하나의 주소공간에서 동작하되, **로드 가능한 모듈(LKM)** 로 기능을 동적으로 추가/제거.
- **아키텍처 범용성**: x86/ARM/RISC-V/Power 등 다수 ISA 지원.
- **릴리스 사이클**: 대략 **수 주 단위**로 메이저 버전이 전진하며, **LTS** 브랜치가 일정 기간 장기 지원.
- **생태계**: GNU 유저랜드(코어유틸/툴체인) + 수많은 배포판(Debian/Ubuntu/Fedora/Arch/Alpine 등).

### 확인해보기 — “커널과 배포판”

```bash
uname -a                 # 커널 아키/버전/빌드 문자열
cat /proc/version        # 빌드 컴파일러/버전 문자열
cat /etc/os-release      # 배포판(유저랜드) 정보
```

### 리눅스의 큰 구성

- **프로세스/스레드**: `task_struct`로 표준화, CFS/RT 스케줄러.
- **메모리**: Demand paging, 페이지 캐시, 슬랩/SLUB, THP/hugepage, OOM.
- **파일시스템**: VFS 추상 위 ext4/xfs/btrfs …
- **블록/네트워크**: `blk-mq`, io_uring, TCP/IP/QUIC, netfilter.
- **보안**: Capabilities, LSM(SELinux/AppArmor/Smack), seccomp, namespaces/cgroups.
- **관측성**: `ftrace`, `perf`, **eBPF**(bcc/bpftrace), tracepoint.

---

## Design Principles

### “Everything is a file” (그리고 FD)

- **VFS** 위에서 파일·디렉터리·파이프·소켓·디바이스가 **파일 디스크립터(FD)** 로 통일.
- **Pseudo FS**: `/proc`(프로세스/커널 상태), `/sys`(sysfs, 디바이스 모델/파라미터), `tmpfs`(메모리 기반), `devtmpfs`(장치 노드).

```bash
# FD는 /proc/<pid>/fd에서 보인다

ls -l /proc/self/fd
```

### 커널과 유저 공간, 안정성의 원칙

- **모놀리식 커널**이지만 기능은 **LKM** 으로 동적 확장.
- **User-kernel ABI는 안정**(시스템 콜·`ioctl`·netlink 등), **내부 커널 API는 비안정**(드라이버/모듈은 커널 버전 종속 가능).
- **정책 vs 메커니즘**: 커널은 메커니즘(예: cgroup)을 제공, 정책은 유저랜드(systemd, 컨테이너 런타임 등)가 결정.

### 선점형 멀티태스킹과 스케줄러

- 기본 **CFS(Completely Fair Scheduler)**: 각 태스크에 **가중치(=nice 기반 weight)** 를 주고 **가상시간(vruntime)** 이 작은 순으로 CPU를 배분.
- 핵심 공식(정성적):
  $$\Delta \text{vruntime} = \Delta t \cdot \frac{W_0}{W_{\text{task}}}$$
  여기서 \(W_{\text{task}}\) 는 nice에 따른 가중치, \(W_0\) 는 기준 가중치. **가중치가 클수록** 같은 실제 시간 \(\Delta t\) 에서 **vruntime 증가가 작아** 더 오래 CPU를 얻는다.

### 메모리 관리의 통합 캐시

- **페이지 캐시**가 파일 I/O와 메모리 매핑을 통합 → 디스크 읽기 재사용/쓰기 지연(writeback).
- **Demand paging** + **Copy-on-Write(COW)** 로 fork/exec 효율화.
- **OOM Killer**: 메모리 고갈 시 희생자 선별 정책.

### I/O, 네트워킹, 보안의 현대화

- **blk-mq**(멀티큐 블록), **io_uring**(커널<->유저 공간 링으로 저오버헤드 I/O).
- 네트워크는 소켓 계층 + netfilter/eBPF 오프로딩.
- **LSM**: MAC(SELinux/AppArmor), **Capabilities** 로 root 권한 세분화, **seccomp** 로 시스템콜 필터.

### 관측성

- **ftrace/perf** 로 커널 함수/스케줄/캐시미스 분석.
- **eBPF**: 커널 안전 확장, bpftrace/bcc로 시스템 프로브.

---

## Kernel Modules

> **목표**: “Hello, kernel”에서 시작해, **모듈 파라미터/로그/의존성**, `procfs`/`sysfs`와의 상호작용, 빌드·삽입·제거까지 한 번에 익힌다.

### 가장 작은 LKM

**소스: `hello.c`**
```c
// hello.c — 최소 커널 모듈
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>

MODULE_LICENSE("GPL");
MODULE_AUTHOR("you");
MODULE_DESCRIPTION("hello world lkm");
MODULE_VERSION("0.1");

static int __init hello_init(void) {
    pr_info("hello: init (jiffies=%lu)\n", jiffies);
    return 0;
}
static void __exit hello_exit(void) {
    pr_info("hello: exit\n");
}
module_init(hello_init);
module_exit(hello_exit);
```

**Kbuild: `Makefile`**
```make
obj-m := hello.o
KDIR  := /lib/modules/$(shell uname -r)/build
PWD   := $(shell pwd)

all:
	$(MAKE) -C $(KDIR) M=$(PWD) modules
clean:
	$(MAKE) -C $(KDIR) M=$(PWD) clean
```

**빌드·적재·확인**
```bash
make
sudo insmod hello.ko
dmesg | tail -n 5       # "hello: init ..." 확인
lsmod | grep hello
sudo rmmod hello
dmesg | tail -n 5
```

### 모듈 파라미터 & sysfs 노출

```c
// params.c — 모듈 파라미터 예시
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/moduleparam.h>

MODULE_LICENSE("GPL");

static int answer = 42;
module_param(answer, int, 0644);
MODULE_PARM_DESC(answer, "ultimate answer");

static char *who = "world";
module_param(who, charp, 0644);
MODULE_PARM_DESC(who, "target name");

static int __init p_init(void){
    pr_info("params: who=%s answer=%d\n", who, answer);
    return 0;
}
static void __exit p_exit(void){
    pr_info("params: exit\n");
}
module_init(p_init); module_exit(p_exit);
```

```bash
make
sudo insmod params.ko who=linux answer=7
# sysfs에서 확인/수정(모듈명/파라미터명)

cat /sys/module/params/parameters/who
echo "kernel" | sudo tee /sys/module/params/parameters/who
sudo rmmod params
```

### `procfs` / `debugfs` 훅 추가

```c
// proc_hello.c — /proc/hello 항목 생성
#include <linux/init.h>
#include <linux/module.h>
#include <linux/proc_fs.h>
#include <linux/seq_file.h>

MODULE_LICENSE("GPL");
static int show(struct seq_file *m, void *v){ seq_printf(m, "hello from kernel\n"); return 0; }
static int openp(struct inode* i, struct file* f){ return single_open(f, show, NULL); }
static const struct proc_ops pops = {
    .proc_open    = openp,
    .proc_read    = seq_read,
    .proc_lseek   = seq_lseek,
    .proc_release = single_release,
};
static int __init ph_init(void){
    proc_create("hello", 0444, NULL, &pops);
    pr_info("/proc/hello ready\n"); return 0;
}
static void __exit ph_exit(void){
    remove_proc_entry("hello", NULL);
    pr_info("/proc/hello removed\n");
}
module_init(ph_init); module_exit(ph_exit);
```

```bash
make
sudo insmod proc_hello.ko
cat /proc/hello
sudo rmmod proc_hello
```

### 모듈 의존성과 자동 적재

- 심볼 의존은 **exported symbol** 로 해소(`EXPORT_SYMBOL_GPL`).
- `modprobe` 는 **의존 모듈을 함께** 적재, `depmod` 가 의존 그래프를 만듦.
- `MODULE_ALIAS()` 로 장치 ID와 연결하면 **udev** 가 자동 로드.

```c
// libsym.c — 심볼을 외부에 제공
#include <linux/module.h>

MODULE_LICENSE("GPL");
int add42(int x){ return x+42; }
EXPORT_SYMBOL_GPL(add42);
```

```c
// use.c — 외부 심볼 사용
#include <linux/module.h>

MODULE_LICENSE("GPL");
extern int add42(int);
static int __init uinit(void){ pr_info("use: %d\n", add42(8)); return 0; }
static void __exit uexit(void){ }
module_init(uinit); module_exit(uexit);
```

```make
obj-m := libsym.o use.o
```

빌드 후 `sudo insmod libsym.ko && sudo insmod use.ko` → `dmesg` 로 값 확인.

### 디버깅·문제 해결

- **로그**: `pr_info/pr_err`, `dmesg -w`.
- **심볼/정보**: `modinfo`, `/sys/module/<name>`.
- **락/경합**: `lockdep`, `CONFIG_DEBUG_SPINLOCK`.
- **크래시 덤프**: `kdump` / `crash`.
- **eBPF/ftrace** 로 모듈 함수 트레이싱.

---

## Process Management

> 프로세스는 리눅스에서 **태스크(task)** 라 부른다. 스레드 또한 태스크이며, **스레드 그룹**이 하나의 프로세스를 이룬다.

### 식별자와 상태

- **PID / TID**: 전통적으로 프로세스 ID, 스레드 ID(=태스크 ID).
- `/proc/<pid>/task/<tid>` 트리에서 스레드별 정보를 볼 수 있다.
- **상태 코드**(ps `STAT`):
  - `R`(실행 가능), `S`(대기), `D`(디스크 대기, 인터럽트 불가), `T`(정지), `Z`(좀비).

```bash
ps -o pid,ppid,tid,stat,comm -T -p $$   # 현재 셸의 스레드 상태 확인
```

### 생성·실행·종료 — fork/exec/wait

- **fork()**: 부모 주소공간을 **COW** 로 복제한 자식 생성.
- **execve()**: 현재 프로세스를 **새 실행파일**로 대체.
- **wait()/waitid()**: 자식 종료/신호 전달 상태 회수(좀비 수거).

```c
// fork_exec.c — 자식이 exec, 부모가 wait
#define _GNU_SOURCE
#include <unistd.h>
#include <sys/wait.h>
#include <stdio.h>
#include <stdlib.h>

int main(){
    pid_t pid = fork();
    if (pid==0){
        execlp("sh","sh","-c","echo child && ls -1 | head -3", (char*)NULL);
        _exit(127); // exec 실패
    } else if (pid>0){
        int st; waitpid(pid,&st,0);
        if (WIFEXITED(st)) printf("exit=%d\n", WEXITSTATUS(st));
    } else { perror("fork"); exit(1); }
}
```

### clone / clone3 — 세밀한 분리

- 리눅스는 **네임스페이스·cgroup·스레드/프로세스 경계**를 `clone()` 플래그로 제어.
- 예: `CLONE_VM`(메모리 공유), `CLONE_FS`(fs 컨텍스트 공유), `CLONE_NEWUTS`/`NEWNET`/`NEWPID`(네임스페이스) 등.

```c
// clone_pidns.c — PID 네임스페이스에서 /bin/sh 실행 (root 필요)
#define _GNU_SOURCE
#include <sched.h>
#include <sys/wait.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>

static int child(void *arg){
    printf("child pid in new ns: %d\n", getpid());
    execlp("sh","sh",(char*)NULL);
    return 1;
}
int main(){
    const int stack_sz = 1<<20;
    char *stack = malloc(stack_sz);
    int flags = CLONE_NEWPID | CLONE_NEWUTS | SIGCHLD;
    pid_t cpid = clone(child, stack+stack_sz, flags, NULL);
    waitpid(cpid, NULL, 0);
    return 0;
}
```

### 스케줄링 — CFS & RT

#### CFS의 핵심 아이디어

- **레드블랙트리**로 `vruntime` 최소 태스크를 선택.
- 동일 기간 동안 각 태스크가 **비례 배분**이 되도록 한다.
- nice 레벨별 **가중치** \(W\) 로 보정:
  $$\text{share}_i \propto W_i,\quad \sum_i \text{share}_i = 1$$

간단 시뮬(유저 공간):
```python
# cfs_share.py — nice별 CPU 점유 비율 근사

weights = { 0:1024, 5:335, 10:110 }  # (예시) nice→가중치
tot = sum(weights.values())
for nice,w in weights.items():
    print(f"nice {nice}: {w/tot:.3f} CPU share")
```

#### 고정 우선순위 RT: FIFO/RR

- `SCHED_FIFO`/`SCHED_RR` 는 **우선순위 1~99**. 같은 우선순위에서는 FIFO/Round-Robin.
- **주의**: 실수로 CPU 독점 가능 → `mlockall`/`sched_yield`/워치독 필요.

```c
// rt_fifo.c — root 필요
#define _GNU_SOURCE
#include <sched.h>
#include <unistd.h>
#include <stdio.h>
#include <string.h>
#include <sys/mman.h>

int main(){
    struct sched_param sp = {.sched_priority=50};
    if (mlockall(MCL_CURRENT|MCL_FUTURE)!=0) perror("mlockall");
    if (sched_setscheduler(0, SCHED_FIFO, &sp)!=0) perror("sched_setscheduler");
    for(;;){ /* 짧은 작업 후 잠깐 양보 */
        write(1,".",1);
        sched_yield();
        usleep(1000);
    }
}
```

#### SCHED_DEADLINE (EDF)

- **주기(Period) / 예약시간(Runtime) / 데드라인** 기반 **EDF**(Earliest Deadline First).
- 커널 `sched_setattr` 시스템콜로 설정. (root/권한 필요)

수학적으로, 단일 CPU에서 \(n\) 태스크가 각각 실행시간 \(C_i\), 주기 \(T_i\) 일 때 **부하율**
$$U = \sum_{i=1}^{n} \frac{C_i}{T_i} \le 1$$
이면 EDF는 스케줄 가능(이론적 조건).

### CPU 친화도·NUMA·우선순위

```c
// pin.c — 현재 프로세스를 CPU 0에 고정
#define _GNU_SOURCE
#include <sched.h>
#include <stdio.h>

int main(){
    cpu_set_t set; CPU_ZERO(&set); CPU_SET(0,&set);
    if (sched_setaffinity(0,sizeof(set),&set)!=0) perror("aff");
    printf("pinned to cpu0\n"); for(volatile long i=0;i<1e9;i++);
}
```

```bash
nice -n 10 ./worker         # 낮은 우선순위
chrt -f 50 ./rt_fifo        # RT FIFO 50
taskset -c 0-1 ./server     # CPU 0-1에 고정
```

### cgroups v2 — 자원 격리

> 컨테이너의 핵심 기반. 루트 권한 필요.

```bash
# cgroup v2가 /sys/fs/cgroup 에 마운트되었다고 가정

cd /sys/fs/cgroup
sudo mkdir demo && cd demo
echo $$ | sudo tee cgroup.procs        # 현재 셸 이동(주의!)
echo "200000 100000" | sudo tee cpu.max  # 200ms/100ms -> 2배? (형식: max period)
echo 50000 | sudo tee memory.max       # 50KiB로 극단 제한(실험용)
# 다른 셸에서 CPU/MEM 영향 관찰

```
- `pids.max`, `io.max`, `memory.max/swap.high`, `cpu.max` 등으로 세밀 제어.
- `cgroup.events`, `pressure/*` 로 압력/스로틀 관측.

### 신호/마스크/파이프라인

```c
// sig.c — SIGINT를 블록하고 타임아웃 대기
#define _GNU_SOURCE
#include <signal.h>
#include <stdio.h>
#include <unistd.h>

int main(){
    sigset_t set; sigemptyset(&set); sigaddset(&set, SIGINT);
    sigprocmask(SIG_BLOCK,&set,NULL);
    printf("SIGINT blocked for 5s\n");
    sleep(5);
    sigprocmask(SIG_UNBLOCK,&set,NULL);
    printf("unblocked\n");
}
```

### 관측: strace / procfs / perf

```bash
strace -f -tt -o trace.log ./fork_exec
grep -E 'clone|execve|wait' trace.log | head
```
```bash
cat /proc/self/status | sed -n '1,25p'
perf stat -d -- sleep 1
```

---

# Chapter 20 — The Linux System (Ⅱ)

*(계속: 20.4의 심화 예제와 실제 운영 팁)*

## 20.4.A ELF 로딩·execve 흐름(간단 개념)

1) 유저공간이 `execve(path, argv, envp)` 호출
2) 커널이 파일 열고 **BINFMT** 검사(ELF/스크립트 shebang 등)
3) ELF 헤더 파싱 → **프로그램 헤더(PT_LOAD)** 를 따라 **페이지 매핑**
4) 초기 스택에 `argv/envp/auxv` 를 적재 → **엔트리 포인트**로 점프

```bash
readelf -h /bin/ls
readelf -l /bin/ls | grep LOAD
```

## 20.4.B io_uring 로 비동기 파일 I/O (개념 맛보기)

```c
// uring_read.c — liburing 필요(간단 스니펫)
#include <liburing.h>
#include <fcntl.h>
#include <string.h>
#include <stdio.h>

int main(){
    struct io_uring ring; io_uring_queue_init(16,&ring,0);
    int fd=open("bigfile",O_RDONLY|O_DIRECT);
    char *buf; posix_memalign((void**)&buf,4096,4096);
    struct io_uring_sqe *sqe=io_uring_get_sqe(&ring);
    io_uring_prep_read(sqe,fd,buf,4096,0);
    io_uring_submit(&ring);
    struct io_uring_cqe *cqe; io_uring_wait_cqe(&ring,&cqe);
    printf("read=%d\n", cqe->res); io_uring_cqe_seen(&ring,cqe);
}
```

## 20.4.C eBPF로 스케줄 이벤트 관측(개념)

```bash
# bpftrace 예: 스케줄 스위치 카운트

sudo bpftrace -e 'tracepoint:sched:sched_switch { @[comm] = count(); } interval:s:3 { exit(); }'
```

---

## 운영 체크리스트(요약)

1) **프로세스**: fork/exec/wait, 신호/exit code, 좀비 정리.
2) **스케줄링**: nice/affinity/cgroups로 공정성·격리 달성. RT는 신중히.
3) **메모리**: 페이지 캐시·NUMA·THP·OOM 정책 이해.
4) **I/O**: 블록 멀티큐, AIO/io_uring, 파일 캐시/다이렉트 I/O 선택.
5) **보안**: LSM/Capabilities/seccomp/NS, 최소 권한.
6) **관측**: /proc, perf, ftrace/eBPF, cgroup pressure·events.
7) **커널 모듈**: 라이선스·의존·sysfs·procfs, 로그/디버깅 루틴화.

---

## 마무리

- 리눅스는 **모놀리식 + 모듈형** 철학 위에서 **범용 OS** 로 진화했다.
- **사용자 ABI 안정성**, **강력한 스케줄/메모리/파일/네트워크 스택**, **관측성(eBPF)** 이 결합되어 **서버·임베디드·클라우드/컨테이너** 모든 영역에서 표준이 되었다.
- 본 장의 예제를 직접 실습하면서, **유저 공간→커널 경계**와 **자원 격리/관측** 감을 몸으로 익히자.
