---
layout: post
title: 딥러닝 - 경량 미세튜닝
date: 2025-10-07 21:25:23 +0900
category: 딥러닝
---
# 경량 미세튜닝(LoRA/QLoRA) 완전정복  

## 0) 배경과 큰그림

### 0.1 왜 “경량(PEFT)”인가?
- **Full Fine-Tuning(FFT)**: 모든 파라미터를 업데이트 → **메모리↑, 비용↑**, 데이터/시간 많이 필요.
- **PEFT(파라미터 효율 튜닝)**: 파라미터 일부(또는 별도 어댑터)만 학습 → **메모리↓, 속도↑**, 전이성↑.
- 대표 기법: **LoRA**, **Prefix/Prompt/Adapter 튜닝**, **QLoRA**(4bit 양자화+LoRA).

### 0.2 LoRA vs QLoRA 한눈에
- **LoRA**: 기존 가중치 \(W\)는 **동결**, **저랭크(낮은 차원) 어댑터 \(\Delta W=BA\)**만 학습.  
- **QLoRA**: **기저 모델 가중치를 4bit**로 로드(메모리 극소화) + **LoRA 어댑터는 BF16/FP16/FP32**로 학습.  
  - 메모리 절감 + 큰 모델(수십 B)도 **단일(또는 소수) GPU**에서 미세튜닝 가능.

---

## 1) LoRA 개념과 수식

### 1.1 핵심 아이디어
- 원래 선형층:  
  $$
  h = W x \quad (W\in\mathbb{R}^{o\times i})
  $$
- LoRA:  
  $$
  W' = W + \Delta W,\quad \Delta W = B A
  $$
  - \(A\in\mathbb{R}^{i\times r}\) (down-proj), \(B\in\mathbb{R}^{r\times o}\) (up-proj), \(r\ll\min(i,o)\).
  - 학습 파라미터 수: **\(r(i+o)\)** (원본 \(oi\) 대비 매우 작음).
- 스케일링(중요):  
  $$
  W' = W + \alpha \cdot \frac{BA}{r}
  $$
  - \(\alpha\)는 LoRA 스케일. 일반적으로 \(\alpha \in [4,32]\).

### 1.2 적용 위치
- 트랜스포머 **어텐션 투영층(q/k/v/o)**, **피드포워드(FFN) 내 프로젝션**에 주로 적용.  
- LLM(예: LLaMA류): 보통 **q_proj, v_proj**(또는 q,k,v,o 전부)부터 시작 → 필요 시 FFN까지 확대.

### 1.3 장점/한계
- 장점: **메모리/연산** 대폭 절감, **멀티-어댑터**(태스크별 LoRA 헤드) 조합 쉬움, 병합/분리 쉬움.
- 한계: 매우 큰 도메인 변환/아키텍처 수정이 필요한 경우 **FFT가 더 강함**.  
- **랭크 r**가 낮으면 **표현력 제한** → 도메인 난도↑/데이터 다양↑이면 **r↑** 권장.

---

## 2) QLoRA 개념과 핵심 구성요소

### 2.1 무엇이 다른가?
- LoRA만 쓰면 여전히 **기저 모델 가중치(BF16/FP16)** 로드를 위해 **수~수십 GB** 필요.
- **QLoRA**: 기저 가중치 **4bit(NF4) 양자화** + **LoRA 어댑터는 고정밀(BF16/FP16/FP32)** 로 학습.
  - **메모리 사용량 극저**, 대형 모델(예: 7B/13B/33B…)을 **소수 GPU**로도 튜닝.

### 2.2 NF4 / Double Quant / Paged Optimizer
- **NF4(NormalFloat4)**: 분포 적응(normal-aware) 4bit 양자화로 정밀도 손실 최소화.
- **Double Quantization**: 양자화 스케일 자체를 다시 양자화 → 저장/메모리 추가 절감.
- **Paged Optimizer(AdamW 등)**: 활성화 메모리 스파이크를 완화, OOM 최소화.

---

## 3) 메모리·정확도 트레이드오프

### 3.1 메모리 근사 공식(개념적)
- 파라미터 수 \(P\), dtype 바이트 크기 \(b\)(BF16=2, FP16=2, FP32=4, 4bit=0.5)라면  
  - **가중치 저장**: \(P\cdot b\)  
  - **그라디언트**: 보통 \(P\cdot b\) (모든 파라미터 학습 시)  
  - **옵티마 상태(Adam 1·2차 모멘트)**: \(\approx 2P\cdot b\)  
  - 합계(매우 러프): \(\approx 4P\cdot b + \text{activations}\)
- **LoRA**: 학습 파라미터가 전체 \(P\)가 아니라 **\(P_\text{LoRA}\)**  
  → 옵티마/그래드도 **LoRA 부분만** 생성. 기저 \(W\)는 **동결**.  
- **QLoRA**: 기저 \(W\)는 **4bit**로 저장(메모리↑↓), 연산 시 dequantize(내부 구현 최적화),  
  LoRA 파라미터는 고정밀로 학습.

> **정확도**:  
> - 일반적으로 **LoRA(r=8~64, α=16~64)**로 **FFT 근접** 성능 가능.  
> - 도메인/태스크 복잡도↑ → **r/적용 모듈 수↑** 또는 일부 계층 FFT 병행 고려.  
> - **QLoRA**도 잘 튜닝하면 FFT 대비 **근접 성능**. 아주 미세한 차이는 남을 수 있음.

### 3.2 간단한 수치 예시(개념)
- 가령, \(W: \mathbb{R}^{o\times i}\) 에 LoRA \(r\) 적용 시, **추가 학습 파라미터** \(=r(i+o)\).  
- **예**: \(i=o=4096, r=16\) → 추가 파라미터 \(=16(4096+4096)=131{,}072\).  
  - 동일 층이 24개라면 \(\approx 3.1M\) 추파라미터.  
  - 같은 모델 FFT(수십~수백 M~B) 대비 아주 작음 → **GPU 메모리/속도 이득**.

---

## 4) 실전 레시피(현업 하이퍼파라미터)

- **랭크 \(r\)**: 8/16/32(시작은 8 또는 16 권장, 고난도/다국어/정밀도 중요→32).  
- **LoRA \(\alpha\)**: 16/32(보통 \( \alpha \approx 2r\) 근처부터 탐색).  
- **LoRA Dropout**: 0.05~0.1(작은 과적합 방지).  
- **대상 모듈**: `"q_proj","v_proj"` → 필요 시 `"k_proj","o_proj","gate_proj","up_proj","down_proj"` 확대.  
- **학습률**: 1e-4 ~ 3e-4(LoRA 어댑터), 스케줄러는 Cosine/Linear.  
- **정밀도**: AMP/BF16 우선. QLoRA는 **bf16 연산 + 4bit 저장** 조합 권장.  
- **배치**: 마이크로배치 작게 + **Gradient Accumulation**(메모리 예산 내).  
- **정규화**: weight decay 0.0~0.01, label smoothing(생성형은 보통 0).  
- **길이**: 최대 토큰 길이/패킹 → OOM/품질 균형.

---

## 5) 미니 수식 정리 (LoRA)

- 원 선형:  
  $$
  y = W x
  $$
- LoRA 적용:  
  $$
  y = (W + \alpha \tfrac{BA}{r}) x = Wx + \alpha\tfrac{B(Ax)}{r}
  $$
  - **추가 연산**: \(Ax(i\to r)\), \(B(\cdot)(r\to o)\). \(r \ll i,o\)라 오버헤드 작음.
- **학습 파라미터 수**:  
  $$
  \#\text{Params}(\Delta W) = r\cdot i + r\cdot o
  $$

---

## 6) 초간단 **Pure PyTorch** LoRA 래퍼 (교육용)

```python
import torch
import torch.nn as nn
import math

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, r=8, alpha=16, dropout=0.0, bias=True):
        super().__init__()
        self.r = r
        self.alpha = alpha
        self.scaling = alpha / r if r > 0 else 1.0

        self.weight = nn.Parameter(torch.empty(out_features, in_features))  # 동결 예정
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None

        if r > 0:
            self.A = nn.Parameter(torch.empty(r, in_features))
            self.B = nn.Parameter(torch.empty(out_features, r))
            nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))
            nn.init.zeros_(self.B)
            self.lora_dropout = nn.Dropout(dropout)
        else:
            self.register_parameter("A", None)
            self.register_parameter("B", None)
            self.lora_dropout = nn.Identity()

        # 기본 가중치는 동결(옵션)
        self.weight.requires_grad_(False)

    def forward(self, x):
        # x: [B, T, in]
        base = x @ self.weight.T
        if self.bias is not None:
            base = base + self.bias
        if self.r > 0:
            # LoRA 경로
            dx = self.lora_dropout(x)
            lora = (dx @ self.A.T) @ self.B.T  # Ax → B(Ax)
            return base + self.scaling * lora
        else:
            return base
```

> **주의**: 실제 트랜스포머에 붙일 때는 `nn.Linear` 대체 또는 병렬 경로로 합산합니다.  
> 실무에서는 아래 **PEFT 라이브러리** 사용을 권장.

---

## 7) **PEFT(LoRA)** 실전 코드 — 분류/생성 예시

### 7.1 세팅
```bash
# (선택) 설치
pip install -U transformers peft datasets accelerate bitsandbytes
```

### 7.2 분류 예시(DistilBERT + LoRA)
```python
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType

model_name = "distilbert-base-uncased"
tok = AutoTokenizer.from_pretrained(model_name)
ds = load_dataset("imdb")  # 25k 영화평(양/음성)

def tok_fn(batch):
    return tok(batch["text"], truncation=True, padding="max_length", max_length=256)
ds = ds.map(tok_fn, batched=True)
ds = ds.rename_column("label", "labels")
ds.set_format(type="torch", columns=["input_ids","attention_mask","labels"])

base = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# LoRA 설정
config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=16, lora_alpha=32, lora_dropout=0.1,
    target_modules=["q_lin","k_lin","v_lin","out_lin"]  # DistilBERT 명칭에 맞춤
)
model = get_peft_model(base, config)
print(model.print_trainable_parameters())  # 몇 %만 학습되는지 확인

args = TrainingArguments(
    output_dir="./lora-imdb",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    learning_rate=2e-4,
    num_train_epochs=2,
    bf16=True,
    evaluation_strategy="epoch",
    logging_steps=50,
    save_strategy="epoch",
    report_to="none"
)

trainer = Trainer(
    model=model, args=args,
    train_dataset=ds["train"].shuffle(seed=42).select(range(20000)),  # 데모
    eval_dataset=ds["test"].select(range(5000))
)
trainer.train()
print(trainer.evaluate())

# (선택) LoRA 병합하여 추론 단순화
# model = model.merge_and_unload()  # 가중치에 ΔW 병합, 추후 CPU 추론 편리
```

### 7.3 생성 예시(소형 Causal LM + LoRA)
```python
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset

base_name = "gpt2"  # 데모용(작음). 실제는 Llama/phi3 등
tok = AutoTokenizer.from_pretrained(base_name)
tok.pad_token = tok.eos_token

# 간단 지시튜닝 데이터 (prompt→response)
pairs = [
    {"text": "<s>Instruction: 인공지능 정의\nResponse: 인공지능은 인간의 학습/추론 능력을 기계로 구현한 분야이다.</s>"},
    {"text": "<s>Instruction: LoRA가 뭐야?\nResponse: 선형층에 저랭크 어댑터를 추가해 적은 파라미터만 학습하는 기법.</s>"},
] * 200  # 데모
ds = Dataset.from_list(pairs)
def tok_fn(ex): return tok(ex["text"], truncation=True, padding="max_length", max_length=256)
ds = ds.map(tok_fn, batched=True)
ds.set_format(type="torch", columns=["input_ids","attention_mask"])

base = AutoModelForCausalLM.from_pretrained(base_name)
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16, lora_alpha=32, lora_dropout=0.05,
    target_modules=["c_attn","attn.c_attn","q_proj","v_proj","k_proj","o_proj"]  # 모델 구조에 맞게
)
model = get_peft_model(base, config)
print(model.print_trainable_parameters())

args = TrainingArguments(
    output_dir="./lora-gpt2",
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=1,
    bf16=True,
    logging_steps=20,
    save_steps=200,
    report_to="none"
)
dc = DataCollatorForLanguageModeling(tok, mlm=False)

trainer = Trainer(model=model, args=args, train_dataset=ds, data_collator=dc)
trainer.train()

prompt = tok("<s>Instruction: LoRA의 장점은?\nResponse:", return_tensors="pt").to(model.device)
out = model.generate(**prompt, max_new_tokens=80)
print(tok.decode(out[0]))
```

---

## 8) **QLoRA** 실전 코드 — 4bit + LoRA

> 전형적 스택: **bitsandbytes(4bit 로드)** + **PEFT(LoRA)** + **prepare_model_for_kbit_training**.

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
import torch

base_name = "meta-llama/Llama-3-8B-Instruct"  # 예시. 실제 사용 시 액세스 권한/모델명 확인.
bnb = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",      # 핵심: NF4
    bnb_4bit_use_double_quant=True, # double quant
    bnb_4bit_compute_dtype=torch.bfloat16
)

tok = AutoTokenizer.from_pretrained(base_name, use_fast=True)
tok.pad_token = tok.eos_token

model = AutoModelForCausalLM.from_pretrained(
    base_name, quantization_config=bnb, device_map="auto"
)

# 4bit 학습 준비(gradient checkpointing 등 옵션적으로 함께)
model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)

lora_cfg = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16, lora_alpha=32, lora_dropout=0.05,
    target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
)
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()  # LoRA 파라미터 수 확인

# 데이터/Trainer는 위 생성 예시와 동일하게 구성
```

**팁**
- **OOM 방지**: `gradient_checkpointing=True`, `flash_attention` 사용 가능 시 활성, 마이크로배치 줄이고 **accumulation**로 메우기.
- **paged optimizer**: `bitsandbytes`의 paged AdamW가 메모리 스파이크 줄여줌(Transformers가 내부에서 선택적으로 사용).

---

## 9) 멀티-어댑터 & 병합

- **멀티-도메인**: 태스크마다 **별도 LoRA 어댑터** 학습 → **동일 베이스 모델**에 **adapters.load**로 교체/합성.  
- **병합(merge_and_unload)**: 배포 시 **\(\Delta W\)** 를 원 가중치에 합침 → 실행 간단/추가 지연↓.  
  - 단, 4bit 기반 QLoRA에서의 **영구 병합**은 주의(고정밀 가중치 필요). 보통 **런타임 어댑터 로드**가 유연.

---

## 10) 트러블슈팅 & 품질 진단

1) **정확도 부족**  
   - **r↑**(8→16→32), **α↑**, 대상 모듈 확대(q/v→qkv/ffn), 학습률 소폭↑, 에폭↑.  
   - 데이터 노이즈/라벨 품질 확인(특히 지시튜닝).  
2) **모델 환각/근거성↓(RAG 연동 시)**  
   - 프롬프트 페널티(컨텍스트 밖 금지), 컨텍스트 패킹 개선, 인용 의무화.  
3) **OOM/속도 문제**  
   - QLoRA, gradient checkpointing, 시퀀스 길이↓, 마이크로배치↓+accum↑, 혼합정밀(BF16).  
4) **학습 불안정/NaN**  
   - lr↓, grad clip(1.0), AMP 스케일 조정, weight decay=0.0~0.01, β1/β2 기본값 유지.  
5) **수렴 느림**  
   - 워밍업 스텝(1~3%), 스케줄러(Cosine/Linear), LoRA dropout 0.1.

---

## 11) 메모리 계산 미니 워크시트(개념용)

- 가정: 베이스 7B, BF16 풀 파인튜닝 vs LoRA vs QLoRA  
- **풀 튜닝**(BF16 추산):  
  - 가중치(14GB) + 그라디언트(14GB) + 옵티마(≈28GB) + 활성화(가변) → **수십 GB + 활성화**  
- **LoRA**(BF16, r=16, q/v만):  
  - 베이스 가중치(14GB, 동결) + **LoRA 파라미터/옵티마만**(수백 MB~수 GB) + 활성화  
  - GPU 1~2장에서도 충분한 경우 다수  
- **QLoRA**:  
  - 베이스 **4bit(≈3.5GB)** + LoRA/옵티마(BF16, 수백 MB~) + 활성화 → **단일 24GB GPU**로도 가능 사례 다수

> 위 수치는 **개념적 예시**입니다. 실제 메모리는 시퀀스 길이/배치/아키텍처/프레임워크에 따라 크게 변합니다.  
> 프로젝트 초기에 **샘플 배치**로 `torch.cuda.memory_allocated()`을 찍어 **실측 추정**을 만드세요.

---

## 12) 작은 실험 설계(정확도·메모리 트레이드오프 확인)

- **고정 조건**: 동일 데이터/스텝/스케줄/시드.  
- **변수**: (A) FFT, (B) LoRA(r=8), (C) LoRA(r=16), (D) QLoRA(r=16).  
- **로그**: 검증 점수(F1/ROUGE/BLEU 등), `trainable_params`, `max memory alloc`, `tokens/sec`.  
- **리포트**:  
  - 비용(시간, 전력, 메모리) 대비 성능 곡선.  
  - **추천 설정**: 성능 99% 달성의 **최소 메모리** 포인트.

---

## 13) LoRA/QLoRA 체크리스트(요약)

- [ ] **대상 모듈** 올바른가? (모델 구조 이름 주의)  
- [ ] **r/α/dropout** 기본값에서 시작, 검증으로 톤업  
- [ ] **학습률/스케줄**(2e-4, Cosine/Linear)  
- [ ] **BF16/AMP** 활성화, **grad clip**(1.0)  
- [ ] **QLoRA**: NF4+double quant, `prepare_model_for_kbit_training` 호출  
- [ ] **OOM**: seq len↓, micro-batch↓, checkpointing↑, paged optimizer  
- [ ] **멀티-어댑터 관리**: 태스크별 로드/병합 전략 명확화  
- [ ] **평가**: 정량(EM/F1/ROUGE) + **지시 준수/근거성**(RAG이면) + 속도/비용  
- [ ] **배포**: 어댑터 파일만 배포(작음) 또는 병합 배포

---

## 14) 보너스: 로깅/모니터링 스니펫

```python
import torch, time

class MemMeter:
    def __init__(self): self.peak=0
    def step(self, tag=""):
        torch.cuda.synchronize()
        cur = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0
        self.peak = max(self.peak, cur)
        print(f"[{tag}] peak_mem={self.peak/1024**3:.2f} GB")

mm = MemMeter()
# 학습 루프 중간중간 mm.step("after batch") 호출
```

---

# 마무리

- **LoRA**: “어텐션/FFN 선형층에 **저랭크 어댑터**만 학습” → **메모리와 시간 절감**으로 **FFT 근접 성능** 달성.  
- **QLoRA**: “**4bit 양자화 베이스** + LoRA” → **대형 모델**도 **단일/소수 GPU**에서 튜닝 가능.  
- **트레이드오프**는 **랭크 r/적용 모듈/정밀도/배치/길이**로 조절.  
- 본문 코드를 그대로 실행하면 **분류/지시튜닝/생성** R&D를 빠르게 시작할 수 있습니다.