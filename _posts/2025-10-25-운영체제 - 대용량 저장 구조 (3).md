---
layout: post
title: 운영체제 - 대용량 저장 구조 (3)
date: 2025-10-25 21:30:23 +0900
category: 운영체제
---
# Storage Attachment & RAID Structure

## 11.7 Storage Attachment

### 11.7.1 분류 개관
- **직접/호스트 부착(Host-Attached, DAS)**
  - **SATA / SAS / NVMe(PCIe)**: OS가 **블록 디바이스**로 인식.
  - 장점: 저지연, 단순. 단점: 호스트 단일 장애점, 공유 어려움.
- **네트워크 부착**
  - **NAS**: 파일 레벨 공유(**NFS/SMB**). 호스트는 **파일시스템**을 통해 접근.
  - **SAN**: 블록 레벨 공유(**Fibre Channel, iSCSI, NVMe-oF**) — 호스트는 **LUN/Namespace**를 로컬 디스크처럼 사용.

> 선택 가이드(요지): **단일 서버/저지연** → NVMe(DAS). **여러 서버에서 같은 볼륨 공유** → SAN. **파일 공유/권한** → NAS.

---

### 11.7.2 SATA/SAS/NVMe 물리·프로토콜 요약
- **SATA**: 저가, 단일 x1 링크(6 Gb/s), **AHCI** 큐(단일 큐, 32 entries).
- **SAS**: 엔터프라이즈급, 듀얼 포트(HA), **확장기(expander)** 통해 팬아웃.
- **NVMe(PCIe)**: **다중 큐**(수천 큐 × 큐당 수천 엔트리), **MSI-X**/코어당 큐 매핑 → 락 경합↓.

**NVMe 큐 성능 직관**
TLB 유사하게, 큐 심도 \(Q\)에서 평균 처리량 \(B\) 근사:
$$
B \approx \frac{Q}{\bar{S}} \quad (\bar{S}: \text{평균 서비스 시간})
$$
그러나 **p99 지연**은 큐잉으로 증가:
$$
\text{Tail} \uparrow \quad \text{as} \quad Q \uparrow
$$

---

### 11.7.3 SAN: FC / iSCSI / NVMe-oF
- **Fibre Channel(FC)**: 전용 패브릭(Switch/Director), **WWN** 기반 주소, **Zoning**(보안/격리), **LUN Masking**. 지연/지터 낮고 안정적.
- **iSCSI**: IP 네트워크 상의 SCSI. **Target/Initiator** 개념, **CHAP** 인증, **MTU/Jumbo** 및 **멀티패스**로 신뢰성 확보.
- **NVMe-oF**: NVMe를 **RoCEv2 / TCP / FC-NVMe**로 확장. 큐/서브시스템 모델 유지, 지연·CPU 오버헤드↓.

**멀티패스(MPIO) 핵심**
- **ALUA**: Active/Optimized vs Non-optimized 경로 인지.
- **Path selector**: round-robin, queue-length, service-time.
- **Failover**: 경로 장애 시 **투명 전환**(I/O 재시도).

#### (Linux) dm-multipath 스케치
```bash
# 설치/상태
sudo multipath -ll
# /etc/multipath.conf 예시 (ALUA + 우선 경로)
defaults {
  user_friendly_names yes
  find_multipaths yes
}
devices {
  device {
    vendor "PURE"
    product "FlashArray"
    path_selector "service-time 0"
    path_grouping_policy "group_by_prio"
    features "1 queue_if_no_path"
    hardware_handler "1 alua"
    prio "alua"
  }
}
```

---

### 11.7.4 NAS: NFS/SMB 설계 포인트
- **NFSv3**: stateless, 성능 좋음. **NFSv4**: 상태/락/ACL 통합, WAN 친화.
- **SMB3**: **멀티채널**, **암호화**, **RDMA(SMB-Direct)**.
- **캐시 일관성**: 클라이언트 캐시/속성 재검증 시간, **sync/async export**.
- **락/권한**: POSIX vs Windows ACL 매핑 주의.

#### (Linux) NFS 클라이언트 마운트 옵션
```bash
sudo mount -t nfs -o vers=4.1,nconnect=4,noatime,rsize=1M,wsize=1M server:/export/data /mnt/data
```

---

### 11.7.5 주소/보안/격리
- **FC Zoning**(WWPN 기반), **LUN Masking**(스토리지 어레이에서 호스트 매핑).
- **iSCSI**: IQN, **CHAP** / **Mutual CHAP**, 이니시에이터 IP 제한.
- **NVMe-TCP/RoCE**: **Discovery controller**, **DH-HMAC-CHAP**(보안 옵션).

---

### 11.7.6 성능·신뢰성 체크리스트
1) **큐/IRQ 코어 매핑**: NVMe 큐 ↔ 코어 NUMA 로컬.
2) **MTU/Jumbo**: iSCSI/NVMe-TCP에서 **엔드-투-엔드 동일**하게.
3) **MPIO**: ALUA 인지, 최적화 경로로 선호 라우팅.
4) **QoS/Rate limit**: noisy neighbor 억제.
5) **모니터링**: `iostat -x`, `nvme smart-log`, 어레이 측 로그(경로 전환, 큐 길이).

---

### 11.7.7 멀티패스 레이턴시 Toy 시뮬레이터
```python
# mpio_latency.py — 2경로(최적/비최적) + 장애/복구 시나리오
import random
OPT=0.4e-3; NONOPT=1.2e-3; FAIL_P=0.01; RECOVER_P=0.005
def simulate(n=20000, selector="service-time"):
    path="opt"; lat=[]; inflight=0
    for _ in range(n):
        # 장애/복구
        if path=="opt" and random.random()<FAIL_P: path="nonopt"
        elif path=="nonopt" and random.random()<RECOVER_P: path="opt"
        # 선택 정책
        if selector=="round-robin":
            l = OPT if random.random()<0.5 else NONOPT
        else: # service-time: 최적 경로 선호
            l = OPT if path=="opt" else NONOPT
        lat.append(l*(1+0.2*random.random()))  # 소폭 분산
    lat.sort()
    return sum(lat)/len(lat), lat[int(0.99*len(lat))]
print("avg,p99:", simulate())
```

---

## 11.8 RAID Structure

### 11.8.1 기본 개념
- **스트라이프(Stripe)**: 여러 디스크에 **블록 단위**로 분산 배치(병렬성↑).
- **패리티(Parity)**: XOR(**RAID-5**) 또는 RS/BCH(**RAID-6**)로 **내고장성** 확보.
- **쓰기 경로**:
  - **Read-Modify-Write(RMW)**: 작은 쓰기 시, 기존 데이터/패리티 읽고 **수정** 후 다시 기록.
  - **Reconstruct Write(Full-stripe write)**: 스트라이프 전체를 덮어쓸 수 있을 때 **읽기 없이** 새 패리티 계산.

**작은 쓰기의 RMW 오버헤드**(RAID-5):
- 1블록 쓰기 → **데이터 읽기 + 패리티 읽기 + 데이터 쓰기 + 패리티 쓰기 = 4 I/O**
- 스트라이프 정렬/배치 쓰기로 **RMW 회피**가 중요.

---

### 11.8.2 RAID 레벨 정리
- **RAID-0 (Striping)**: 성능↑, 장애 허용 0.
- **RAID-1 (Mirroring)**: 1장애 허용(2-way), 읽기 병렬성, 쓰기=최저 디스크 속도.
- **RAID-5 (Single Parity)**: N-1 용량 사용, **1장애 허용**. 작은 쓰기 RMW 비용.
- **RAID-6 (Dual Parity)**: N-2 용량, **2장애 허용**(대용량/리빌드 긴 환경에서 선호).
- **RAID-10 (1+0)**: 미러 후 스트라이프. 성능/가용성↑, 용량 효율 50%.
- **중첩(50/60)**: 5/6을 상위 스트라이핑. 대규모 확장, 리빌드 영향 분산.

---

### 11.8.3 패리티 수학 (요약)

#### RAID-5 XOR
$$
P = D_0 \oplus D_1 \oplus \cdots \oplus D_{k-1}
$$

#### RAID-6 이중 패리티(개념적 RS/GF)
$$
\begin{aligned}
P &= \bigoplus_i D_i\\
Q &= \bigoplus_i \alpha^i \cdot D_i \quad (\alpha: GF(2^8)\ \text{의 생성원})
\end{aligned}
$$

> 두 디스크 손실 시 선형 방정식으로 **복원** 가능.

```python
# raid56_math.py — XOR P 및 GF(2^8) Q 패리티 데모
GF_POLY=0x11D
def gmul(a,b):
    p=0
    for _ in range(8):
        if b&1: p^=a
        hi=a&0x80
        a=(a<<1)&0xFF
        if hi: a^=(GF_POLY&0xFF)
        b>>=1
    return p

def raid5_p(stripes):
    from functools import reduce
    return bytes([reduce(lambda x,y:x^y, col) for col in zip(*stripes)])

def raid6_pq(stripes):
    P = raid5_p(stripes)
    Q = bytearray(len(P))
    for i,blk in enumerate(stripes):
        coef = pow(2, i, 256)  # α^i 근사
        for j,b in enumerate(blk):
            Q[j] ^= gmul(b, coef)
    return P, bytes(Q)
```

---

### 11.8.4 리빌드 시간/위험 모델

#### 리빌드 시간 근사
디스크 용량 \(C\), 유효 리빌드 대역폭 \(B\), 백그라운드 간섭 계수 \(\gamma\)(0–1)면,
$$
T_{\text{rebuild}} \approx \frac{C}{\gamma B}
$$

예: 16 TB HDD, 백그라운드 200 MB/s, \(\gamma=0.5\) →
$$
T\approx \frac{16\times 10^{12}}{0.5\times 200\times 10^6}\approx 160{,}000\ \text{s}\approx 44\ \text{h}
$$

#### URE(복구불가 읽기 오류) 노출 위험(직관)
비트 단위 URE 확률 \(p\), 리빌드 중 읽는 총 비트 \(N\)일 때,
$$
P(\text{at least one URE}) \approx 1 - (1-p)^N \approx 1 - e^{-pN}
$$
대용량 HDD + RAID-5 리빌드에서 위험이 커져 **RAID-6**가 권장되는 이유.

---

### 11.8.5 성능 모델(간단 근사)

#### 읽기 대역폭
- RAID-0/10/5/6: **병렬 디스크 수** \(\times\) 디스크 단일 읽기 대역폭(메타/경계/큐 제한 고려).

#### 작은 쓰기(블록 \(b\), 스트라이프 너비 \(k\))
- RAID-5 RMW: **4 I/O** 패턴 → IOPS 한계 \(\approx \frac{\text{디스크 총 IOPS}}{4}\)
- Full-stripe 쓰기(정렬): RMW 회피 → **2 I/O**(데이터/패리티) 수준

---

### 11.8.6 운영·구성 예시

#### (Linux mdraid) RAID-10/6 만들기
```bash
# 4디스크 RAID-10
sudo mdadm --create /dev/md10 --level=10 --raid-devices=4 /dev/sd{b,c,d,e}
# 6디스크 RAID-6, 256KiB 스트라이프
sudo mdadm --create /dev/md6 --level=6 --raid-devices=6 --chunk=256K /dev/sd{b,c,d,e,f,g}
sudo mkfs.xfs -f /dev/md6
sudo mount -o noatime,logbufs=8 /dev/md6 /data
```

#### (LVM RAID) 논리볼륨 단계에서 RAID
```bash
vgcreate vg0 /dev/sd{b,c,d,e,f,g}
lvcreate -i 4 -I 256k -l 100%FREE -n lv_striped vg0         # 스트라이프 LV
lvconvert --type raid6 -m 2 vg0/lv_striped                    # RAID6로 변환
```

#### (ZFS) RAID-Z2(=RAID-6 유사)
```bash
sudo zpool create tank raidz2 sdb sdc sdd sde sdf sdg
sudo zfs set compression=lz4 atime=off tank
```

#### (Windows Storage Spaces) PowerShell 스케치
```powershell
New-StoragePool -FriendlyName Pool01 -PhysicalDisks (Get-PhysicalDisk | ? CanPool -eq $True)
New-VirtualDisk -FriendlyName VD01 -StoragePoolFriendlyName Pool01 -ResiliencySettingName Mirror -NumberOfDataCopies 2 -Interleave 262144 -Size 2TB
Initialize-Disk -VirtualDisk (Get-VirtualDisk VD01)
New-Volume -FriendlyName Data -FileSystem ReFS -DriveLetter F
```

---

### 11.8.7 스트라이프 정렬과 파일시스템
- **Stripe unit**(chunk) = 예: 256 KiB, **Full-stripe** = \( (N_\text{data}) \times \text{chunk} \).
- 파일시스템에서 **RAID geometry**(sunit/swidth)를 설정하여 할당/저널 쓰기가 **스트라이프에 정렬**되도록.

```bash
# XFS: sunit(섹터)과 swidth 설정 예 (256KiB chunk, 8 data 디스크 가정)
mkfs.xfs -f -d su=256k,sw=8 /dev/md6
mount -o noatime,allocsize=1m /dev/md6 /data
```

---

### 11.8.8 스냅샷/증분·COW와의 상호작용
- **COW 파일시스템(ZFS/Btrfs)**: 스냅샷/클론이 빈번하면 쓰기 **랜덤화** → RAID-5/6 작은 쓰기 비용이 더 민감.
- **로그 구조**(LFS) 방식은 장점(순차 쓰기)도 있으나, **GC 단계**의 I/O 폭증을 고려.

---

### 11.8.9 실습 시뮬레이션: RMW vs Full-Stripe
```python
# raid_write_cost.py — 작은 쓰기와 정렬 쓰기의 IOPS/지연 차이
def rmw_cost(iops_disk, ndisks):
    return (iops_disk*ndisks)/4.0   # 4 I/O 패턴 근사
def fullstripe_cost(iops_disk, ndisks):
    return (iops_disk*ndisks)/2.0   # 데이터/패리티 2 I/O 근사

for n in (4,6,8):
    print(n, rmw_cost(200, n), fullstripe_cost(200, n))
```

---

### 11.8.10 모니터링·복구·운영 체크리스트
1) **SMART/NVMe Log**: 리맵, 미디어 오류, 온도.
2) **Scrub**: 주기 스크럽으로 숨은 오류 조기 검출(월 1회 등).
3) **Rebuild Window**: 야간/비혼잡 시간대, **리빌드 우선순위**/스로틀 조정.
4) **Hot Spare**: 즉시 리빌드 착수, RAID-6 권장(대용량 HDD).
5) **Write intent log**(일부 RAID): 장애 중 복구 범위 축소.
6) **URE 모델링**: URE 스펙 vs 풀 용량 → RAID-5 위험 평가.
7) **백업/오프사이트**: RAID는 백업이 아니다 — **버전·오프사이트** 필수.

---

## 통합 시나리오

**요구**: OLTP(읽기 70%, p99≤5 ms), 8×NVMe SSD, 공유 LUN(이중 컨트롤러), 스냅샷 빈번.
**설계**
- **Attachment**: NVMe-oF(TCP), 큐/코어 핀, **MPIO(ALUA)**.
- **RAID**: 컨트롤러 레벨 **RAID-10** → 작은 쓰기 유리, 리빌드 빠름.
- **FS**: XFS(또는 ext4), 배치 fsync(그룹 커밋), 저널 옵션 최적화.
- **QoS**: 읽기 승격, 쓰기 쓰로틀, 큐딥=32~64 튜닝(p99 관측).
- **보수**: 월 1회 스크럽, SMART 경고 시 예비 디스크로 선교체.

---

## 핵심 요약
- **Storage Attachment**: DAS는 단순/저지연, SAN은 블록 공유/MPIO, NAS는 파일 공유. NVMe-oF로 NVMe의 **멀티큐/저지연** 이점을 네트워크로 확장.
- **RAID**: RAID-10은 작은 쓰기에 강하고 빠른 복구, RAID-6은 대용량/장시간 리빌드 환경에서 **2장애 허용**으로 안전. **스트라이프 정렬**과 **RMW 회피**가 성능의 핵심.
- **운영**: 경로/큐/NUMA 정렬, 정기 스크럽·SMART, 리빌드 창 관리, URE 위험 모델링, 백업은 별개.
