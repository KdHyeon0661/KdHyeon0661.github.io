---
layout: post
title: 파이썬 심화 - 이터레이터와 제너레이터 (3)
date: 2025-11-28 16:25:23 +0900
category: 파이썬 심화
---
# 이터레이터와 제너레이터 (3)

## 서로 다른 컨테이너의 아이템 순환하기

여러 컨테이너(리스트, 튜플, 세트 등)에 걸쳐 순차적으로 아이템을 처리해야 할 때 `itertools.chain()`을 사용하면 효율적으로 작업할 수 있습니다.

### 기본적인 체인(chain) 사용법

```python
from itertools import chain

# 여러 리스트를 하나의 이터레이터로 연결
list1 = [1, 2, 3]
list2 = [4, 5, 6]
list3 = [7, 8, 9]

# 기본적인 연결
for item in chain(list1, list2, list3):
    print(item, end=' ')
# 출력: 1 2 3 4 5 6 7 8 9

# 다양한 컨테이너 타입 혼합
mixed_containers = [
    [1, 2, 3],           # 리스트
    (4, 5, 6),           # 튜플  
    {7, 8, 9},           # 세트
    {'a': 10, 'b': 11},  # 딕셔너리 (키만 순회)
    range(12, 15)        # 레인지 객체
]

print("\n혼합 컨테이너 순회:")
for item in chain.from_iterable(mixed_containers):
    print(item, end=' ')
# 출력: 1 2 3 4 5 6 8 9 7 a b 12 13 14
```

### 체인을 활용한 실제 예제

```python
def process_multiple_data_sources():
    """여러 데이터 소스를 통합 처리"""
    # 다양한 데이터 소스 예시
    database_results = ['user001', 'user002', 'user003']
    api_response = ['itemA', 'itemB', 'itemC']
    file_contents = ['data1', 'data2', 'data3']
    cache_entries = ['cache_item1', 'cache_item2']
    
    # 모든 데이터 소스를 하나의 순회 가능 객체로 통합
    all_data = chain(database_results, api_response, file_contents, cache_entries)
    
    print("통합 데이터 처리:")
    total_items = 0
    
    for i, item in enumerate(all_data, 1):
        print(f"{i:2d}. {item}")
        total_items += 1
    
    print(f"\n총 처리 항목: {total_items}개")
    
    # 결과를 리스트로 변환
    combined_list = list(chain(database_results, api_response, file_contents, cache_entries))
    print(f"결합된 리스트: {combined_list}")
    
    # 필터링과 결합
    filtered_data = [item for item in chain(database_results, api_response) 
                    if item.startswith('user') or item.endswith('A')]
    print(f"필터링된 데이터: {filtered_data}")

process_multiple_data_sources()
```

### 체인의 메모리 효율성

```python
def demonstrate_memory_efficiency():
    """체인의 메모리 효율성 시연"""
    # 큰 데이터 세트 생성
    large_dataset1 = [f"data1_{i}" for i in range(10000)]
    large_dataset2 = [f"data2_{i}" for i in range(10000)]
    large_dataset3 = [f"data3_{i}" for i in range(10000)]
    
    # 비효율적인 방법: 모든 데이터를 하나의 리스트로 합침
    # combined = large_dataset1 + large_dataset2 + large_dataset3  # 메모리 낭비
    
    # 효율적인 방법: 체인 사용
    print("체인을 사용한 메모리 효율적 순회 시작...")
    
    total_count = 0
    for item in chain(large_dataset1, large_dataset2, large_dataset3):
        # 각 아이템 처리
        total_count += 1
        if total_count % 5000 == 0:  # 진행 상황 표시
            print(f"처리된 항목: {total_count}")
    
    print(f"총 처리된 항목 수: {total_count}")
    
    # 메모리 사용량 비교
    import sys
    
    # 체인 객체의 메모리 사용량
    chained = chain(large_dataset1, large_dataset2, large_dataset3)
    chain_memory = sys.getsizeof(chained)
    
    # 리스트 결합의 메모리 사용량 (샘플로 작은 크기로 테스트)
    sample1 = [1, 2, 3]
    sample2 = [4, 5, 6]
    sample3 = [7, 8, 9]
    
    chained_sample = chain(sample1, sample2, sample3)
    combined_sample = sample1 + sample2 + sample3
    
    print(f"\n체인 객체 크기: {sys.getsizeof(chained_sample)} bytes")
    print(f"결합된 리스트 크기: {sys.getsizeof(combined_sample)} bytes")
    print("체인이 메모리 효율적입니다!")

demonstrate_memory_efficiency()
```

## 데이터 처리 파이프라인 생성

제너레이터와 이터레이터를 활용하면 메모리 효율적인 데이터 처리 파이프라인을 구축할 수 있습니다.

### 기본 파이프라인 패턴

```python
def read_data(source):
    """데이터 소스에서 읽기"""
    for item in source:
        print(f"읽기: {item}")
        yield item

def filter_data(items, condition):
    """조건에 따라 데이터 필터링"""
    for item in items:
        if condition(item):
            print(f"필터 통과: {item}")
            yield item

def transform_data(items, transform_func):
    """데이터 변환"""
    for item in items:
        transformed = transform_func(item)
        print(f"변환: {item} → {transformed}")
        yield transformed

def write_data(items, output_list):
    """데이터 출력 또는 저장"""
    for item in items:
        output_list.append(item)
        print(f"저장: {item}")
        yield item

# 파이프라인 구성
def create_data_pipeline(data_source, condition, transform_func):
    """완전한 데이터 처리 파이프라인 생성"""
    # 파이프라인 구성
    pipeline = write_data(
        transform_data(
            filter_data(
                read_data(data_source),
                condition
            ),
            transform_func
        ),
        []  # 출력을 저장할 리스트
    )
    
    # 파이프라인 실행
    result = list(pipeline)
    return result

# 사용 예제
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# 파이프라인 조건과 변환 함수 정의
def is_even(x):
    return x % 2 == 0

def square(x):
    return x ** 2

print("데이터 처리 파이프라인 실행:")
print("=" * 40)

result = create_data_pipeline(data, is_even, square)
print(f"\n최종 결과: {result}")
```

### 실전적인 로그 처리 파이프라인

```python
import re
from datetime import datetime

def log_file_reader(filename):
    """로그 파일을 한 줄씩 읽는 제너레이터"""
    with open(filename, 'r', encoding='utf-8') as file:
        for line in file:
            yield line.strip()

def parse_log_lines(lines):
    """로그 라인 파싱"""
    log_pattern = re.compile(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(\w+)\] (.+)')
    
    for line in lines:
        match = log_pattern.match(line)
        if match:
            timestamp_str, level, message = match.groups()
            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
            yield {
                'timestamp': timestamp,
                'level': level,
                'message': message,
                'raw': line
            }

def filter_by_level(log_entries, min_level='INFO'):
    """로그 레벨에 따라 필터링"""
    level_priority = {'DEBUG': 0, 'INFO': 1, 'WARNING': 2, 'ERROR': 3, 'CRITICAL': 4}
    min_priority = level_priority.get(min_level, 1)
    
    for entry in log_entries:
        if level_priority.get(entry['level'], 0) >= min_priority:
            yield entry

def extract_errors(log_entries):
    """에러 메시지에서 중요한 정보 추출"""
    error_pattern = re.compile(r'error: (.+?) at (.+)', re.IGNORECASE)
    
    for entry in log_entries:
        if entry['level'] in ('ERROR', 'CRITICAL'):
            match = error_pattern.search(entry['message'])
            if match:
                error_msg, location = match.groups()
                yield {
                    'time': entry['timestamp'],
                    'error': error_msg.strip(),
                    'location': location.strip(),
                    'severity': entry['level']
                }

# 샘플 로그 파일 생성 (실제 파일이 없는 경우를 위한 대체)
sample_logs = [
    "2024-01-03 10:15:30 [INFO] Application started successfully",
    "2024-01-03 10:16:45 [DEBUG] Loading configuration from config.ini",
    "2024-01-03 10:17:20 [WARNING] Disk usage above 80%",
    "2024-01-03 10:18:05 [ERROR] Database connection failed: error: Connection timeout at db_server:3306",
    "2024-01-03 10:19:15 [INFO] Retrying connection...",
    "2024-01-03 10:20:30 [CRITICAL] System crash imminent: error: Memory overflow at process:1234",
]

# 파일에 샘플 로그 쓰기 (데모용)
with open('sample.log', 'w', encoding='utf-8') as f:
    f.write('\n'.join(sample_logs))

# 로그 처리 파이프라인 실행
print("로그 처리 파이프라인:")
print("=" * 50)

# 파이프라인 구성 및 실행
log_entries = parse_log_lines(log_file_reader('sample.log'))
filtered_logs = filter_by_level(log_entries, 'WARNING')
errors = extract_errors(filtered_logs)

print("\n중요 로그 및 에러 리포트:")
print("-" * 50)

for error in errors:
    print(f"시간: {error['time'].strftime('%H:%M:%S')}")
    print(f"심각도: {error['severity']}")
    print(f"에러: {error['error']}")
    print(f"위치: {error['location']}")
    print("-" * 30)
```

### 제너레이터 표현식을 활용한 파이프라인

```python
def create_efficient_pipeline(data):
    """제너레이터 표현식을 활용한 효율적인 파이프라인"""
    # 각 단계가 제너레이터 표현식으로 구성됨
    pipeline = (
        item ** 3              # 3단계: 세제곱
        for item in (
            item * 2           # 2단계: 2배
            for item in (
                item for item in data if item > 0  # 1단계: 양수만 필터링
            )
        )
        if item < 1000         # 4단계: 1000 미만만 필터링
    )
    
    return pipeline

# 대용량 데이터 처리 데모
import random

# 큰 데이터 세트 생성 (메모리 효율적으로 처리)
def generate_large_dataset(n):
    """큰 데이터 세트 생성기"""
    for i in range(n):
        yield random.randint(-100, 100)

print("대용량 데이터 파이프라인 처리:")
print("처리 중...", end=' ')

# 파이프라인 생성 및 실행
data_stream = generate_large_dataset(10000)
pipeline = create_efficient_pipeline(data_stream)

# 결과 일부만 확인
results = []
for i, item in enumerate(pipeline):
    results.append(item)
    if i >= 9:  # 처음 10개만 표시
        break

print("완료!")
print(f"처리된 샘플 결과 (첫 10개): {results}")
print(f"총 처리 가능 항목 수는 스트림의 모든 데이터를 처리할 수 있습니다.")
```

## 중첩된 시퀀스 풀기

중첩된 리스트나 튜플을 평평하게 만들어야 할 때 `itertools.chain.from_iterable()`을 사용하거나 재귀 함수를 작성할 수 있습니다.

### 단일 레벨 풀기

```python
from itertools import chain

def flatten_one_level(nested_list):
    """단일 레벨의 중첩 리스트 풀기"""
    # 방법 1: chain.from_iterable 사용
    result1 = list(chain.from_iterable(nested_list))
    
    # 방법 2: 리스트 컴프리헨션 사용
    result2 = [item for sublist in nested_list for item in sublist]
    
    # 방법 3: 제너레이터 사용
    def flatten_gen(lst):
        for sublist in lst:
            for item in sublist:
                yield item
    
    result3 = list(flatten_gen(nested_list))
    
    return result1, result2, result3

# 예제 데이터
nested_data = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
]

print("단일 레벨 중첩 리스트 풀기:")
result1, result2, result3 = flatten_one_level(nested_data)
print(f"chain.from_iterable: {result1}")
print(f"리스트 컴프리헨션: {result2}")
print(f"제너레이터: {result3}")
```

### 다중 레벨 재귀적 풀기

```python
def flatten_deep(nested_iterable, max_depth=None):
    """임의의 깊이를 가진 중첩 시퀀스를 재귀적으로 풀기"""
    def _flatten(iterable, current_depth=0):
        for element in iterable:
            # 최대 깊이 제한 확인
            if max_depth is not None and current_depth >= max_depth:
                yield element
                continue
                
            # 요소가 시퀀스이고 문자열이 아닌 경우 (문자열은 풀지 않음)
            if (isinstance(element, (list, tuple, set)) and 
                not isinstance(element, (str, bytes))):
                yield from _flatten(element, current_depth + 1)
            else:
                yield element
    
    return list(_flatten(nested_iterable))

def flatten_deep_generator(nested_iterable):
    """제너레이터 버전 - 메모리 효율적"""
    for element in nested_iterable:
        if isinstance(element, (list, tuple, set)) and not isinstance(element, (str, bytes)):
            yield from flatten_deep_generator(element)
        else:
            yield element

# 다양한 중첩 구조 테스트
complex_nested = [
    1,
    [2, 3],
    [4, [5, 6, [7, 8]]],
    (9, (10, 11)),
    [12, [13, [14, [15]]]],
    "문자열은 풀지 않음",
    ["리스트", ["안의", "문자열"]],
    [],
    [16, [], 17]  # 빈 리스트 포함
]

print("다중 레벨 중첩 구조 풀기:")
print("=" * 50)

# 완전히 풀기
fully_flattened = flatten_deep(complex_nested)
print(f"완전히 풀린 결과: {fully_flattened}")

# 깊이 제한해서 풀기
depth_limited = flatten_deep(complex_nested, max_depth=2)
print(f"2레벨 깊이만 풀기: {depth_limited}")

# 제너레이터 사용
print("\n제너레이터를 사용한 순차적 처리:")
for i, item in enumerate(flatten_deep_generator(complex_nested)):
    print(f"항목 {i+1}: {item}")
```

### 실제 데이터 처리 예제

```python
def process_nested_json_data():
    """중첩된 JSON 데이터 처리 예제"""
    # 중첩된 JSON 구조 (예시)
    json_data = {
        "users": [
            {
                "id": 1,
                "name": "Alice",
                "orders": [
                    {"order_id": "A100", "items": ["book", "pen"], "amount": 45.50},
                    {"order_id": "A101", "items": ["laptop", "mouse"], "amount": 1200.00}
                ]
            },
            {
                "id": 2,
                "name": "Bob",
                "orders": [
                    {"order_id": "B200", "items": ["shirt", "pants"], "amount": 85.75},
                    {"order_id": "B201", "items": ["shoes"], "amount": 120.00},
                    {"order_id": "B202", "items": ["watch", "belt"], "amount": 250.50}
                ]
            }
        ],
        "metadata": {
            "total_users": 2,
            "total_orders": 5
        }
    }
    
    # 모든 주문 아이템 추출
    def extract_all_items(data):
        """모든 사용자의 모든 주문에서 아이템 추출"""
        for user in data["users"]:
            for order in user["orders"]:
                for item in order["items"]:
                    yield {
                        "user": user["name"],
                        "order_id": order["order_id"],
                        "item": item,
                        "amount": order["amount"]
                    }
    
    # 모든 주문 금액 추출
    def extract_all_amounts(data):
        """모든 주문 금액 추출"""
        for user in data["users"]:
            for order in user["orders"]:
                yield order["amount"]
    
    print("중첩된 JSON 데이터 처리:")
    print("=" * 50)
    
    # 모든 아이템 출력
    print("\n모든 주문 아이템:")
    for i, item_info in enumerate(extract_all_items(json_data), 1):
        print(f"{i}. {item_info['user']}의 주문 {item_info['order_id']}: "
              f"{item_info['item']} (${item_info['amount']})")
    
    # 금액 통계
    amounts = list(extract_all_amounts(json_data))
    print(f"\n금액 통계:")
    print(f"총 주문 수: {len(amounts)}")
    print(f"총 금액: ${sum(amounts):.2f}")
    print(f"평균 금액: ${sum(amounts)/len(amounts):.2f}")
    print(f"최대 금액: ${max(amounts):.2f}")
    print(f"최소 금액: ${min(amounts):.2f}")

process_nested_json_data()
```

## 정렬된 여러 시퀀스를 병합 후 순환

`heapq.merge()` 함수를 사용하면 정렬된 여러 시퀀스를 효율적으로 병합할 수 있습니다.

### 기본적인 병합

```python
import heapq
import random

def merge_sorted_sequences():
    """정렬된 시퀀스 병합"""
    # 정렬된 시퀀스 생성
    seq1 = sorted([random.randint(1, 100) for _ in range(5)])
    seq2 = sorted([random.randint(1, 100) for _ in range(5)])
    seq3 = sorted([random.randint(1, 100) for _ in range(5)])
    
    print(f"시퀀스 1: {seq1}")
    print(f"시퀀스 2: {seq2}")
    print(f"시퀀스 3: {seq3}")
    
    # 병합
    merged = list(heapq.merge(seq1, seq2, seq3))
    print(f"병합 결과: {merged}")
    
    # 정렬 확인
    is_sorted = all(merged[i] <= merged[i+1] for i in range(len(merged)-1))
    print(f"정렬 상태: {'정렬됨' if is_sorted else '정렬 안됨'}")

merge_sorted_sequences()
```

### 대용량 정렬된 파일 병합

```python
def merge_large_sorted_files():
    """대용량 정렬된 파일 병합 시뮬레이션"""
    
    # 메모리 효율적인 시퀀스 생성기
    def generate_sorted_sequence(name, count, start, end):
        print(f"{name} 생성 중...")
        return sorted(random.randint(start, end) for _ in range(count))
    
    def file_like_generator(sequence):
        """파일처럼 시퀀스를 한 줄씩 yield하는 제너레이터"""
        for item in sequence:
            yield item
    
    # 대용량 시퀀스 생성
    large_seq1 = generate_sorted_sequence("큰 시퀀스 1", 10000, 1, 100000)
    large_seq2 = generate_sorted_sequence("큰 시퀀스 2", 10000, 1, 100000)
    large_seq3 = generate_sorted_sequence("큰 시퀀스 3", 10000, 1, 100000)
    
    print("\n대용량 시퀀스 병합 시작...")
    
    # 제너레이터로 변환
    gen1 = file_like_generator(large_seq1)
    gen2 = file_like_generator(large_seq2)
    gen3 = file_like_generator(large_seq3)
    
    # 효율적인 병합
    merged_gen = heapq.merge(gen1, gen2, gen3)
    
    # 결과 확인 (일부만)
    print("병합된 결과 (첫 20개):")
    first_20 = []
    for i, item in enumerate(merged_gen):
        if i < 20:
            first_20.append(item)
        else:
            break
    
    print(first_20)
    
    # 전체 병합 결과 확인
    print("\n전체 병합 통계:")
    # 다시 제너레이터 생성
    gen1 = file_like_generator(large_seq1)
    gen2 = file_like_generator(large_seq2)
    gen3 = file_like_generator(large_seq3)
    
    merged = heapq.merge(gen1, gen2, gen3)
    
    total_count = 0
    min_val = float('inf')
    max_val = float('-inf')
    
    for item in merged:
        total_count += 1
        min_val = min(min_val, item)
        max_val = max(max_val, item)
    
    print(f"총 항목 수: {total_count}")
    print(f"최소값: {min_val}")
    print(f"최대값: {max_val}")

merge_large_sorted_files()
```

### 사용자 정의 객체 병합

```python
import heapq
from dataclasses import dataclass
from typing import Any

@dataclass(order=True)  # order=True로 비교 메서드 자동 생성
class LogEntry:
    """정렬 가능한 로그 엔트리 클래스"""
    timestamp: int  # 타임스탬프 (정렬 키)
    level: str
    message: str
    source: str
    
    def __str__(self):
        return f"{self.timestamp} [{self.level}] {self.source}: {self.message}"

def merge_log_files():
    """여러 로그 파일 병합 시뮬레이션"""
    
    # 샘플 로그 데이터 생성
    def create_log_entries(source_name, num_entries):
        entries = []
        base_time = random.randint(1000, 5000)
        
        for i in range(num_entries):
            timestamp = base_time + i * random.randint(1, 10)
            level = random.choice(['INFO', 'WARNING', 'ERROR', 'DEBUG'])
            message = f"Log message {i} from {source_name}"
            entries.append(LogEntry(timestamp, level, message, source_name))
        
        return sorted(entries)  # 타임스탬프로 정렬
    
    # 여러 소스의 로그 생성
    server_logs = create_log_entries("Server", 5)
    application_logs = create_log_entries("Application", 5)
    database_logs = create_log_entries("Database", 5)
    
    print("원본 로그 (각 소스별 정렬됨):")
    print("-" * 50)
    print("서버 로그:")
    for entry in server_logs:
        print(f"  {entry}")
    
    print("\n애플리케이션 로그:")
    for entry in application_logs:
        print(f"  {entry}")
    
    print("\n데이터베이스 로그:")
    for entry in database_logs:
        print(f"  {entry}")
    
    # 모든 로그를 타임스탬프 기준으로 병합
    print("\n" + "=" * 50)
    print("병합된 타임라인 (시간순 정렬):")
    print("-" * 50)
    
    merged_logs = heapq.merge(server_logs, application_logs, database_logs)
    
    for entry in merged_logs:
        print(entry)

merge_log_files()
```

## 무한 while 순환문을 이터레이터로 대체하기

이터레이터와 제너레이터를 사용하면 무한 루프를 더 안전하고 표현력 있게 작성할 수 있습니다.

### 기본적인 무한 시퀀스 생성

```python
from itertools import count, cycle, islice

def infinite_sequences_demo():
    """무한 시퀀스 생성기 데모"""
    
    # 1. count: 무한 카운터
    print("count() 예제 - 무한 숫자 시퀀스:")
    counter = count(start=10, step=2)
    first_5 = list(islice(counter, 5))
    print(f"첫 5개: {first_5}")
    
    # 2. cycle: 무한 반복
    print("\ncycle() 예제 - 시퀀스 무한 반복:")
    colors = ['red', 'green', 'blue']
    color_cycler = cycle(colors)
    first_8_colors = list(islice(color_cycler, 8))
    print(f"첫 8개 색상: {first_8_colors}")
    
    # 3. 사용자 정의 무한 제너레이터
    print("\n사용자 정의 무한 피보나치 제너레이터:")
    
    def fibonacci():
        """무한 피보나치 수열 생성기"""
        a, b = 0, 1
        while True:
            yield a
            a, b = b, a + b
    
    fib = fibonacci()
    first_10_fib = list(islice(fib, 10))
    print(f"첫 10개 피보나치 수: {first_10_fib}")

infinite_sequences_demo()
```

### 네트워크 폴링을 이터레이터로 대체

```python
import time
import random
from itertools import islice

def traditional_polling():
    """전통적인 무한 while 루프 폴링"""
    print("전통적인 폴링 방식:")
    attempts = 0
    max_attempts = 5
    
    while True:
        attempts += 1
        print(f"폴링 시도 {attempts}...")
        
        # 시뮬레이션: 20% 확률로 성공
        if random.random() < 0.2:
            print("작업 성공!")
            break
        
        if attempts >= max_attempts:
            print("최대 시도 횟수 초과")
            break
        
        time.sleep(0.5)  # 잠시 대기

def iterator_based_polling():
    """이터레이터 기반 폴링"""
    print("\n이터레이터 기반 폴링 방식:")
    
    def polling_attempts(max_tries=5):
        """폴링 시도 제너레이터"""
        for attempt in range(1, max_tries + 1):
            yield attempt
            time.sleep(0.5)  # 각 시도 사이 대기
    
    for attempt in polling_attempts():
        print(f"폴링 시도 {attempt}...")
        
        # 시뮬레이션: 20% 확률로 성공
        if random.random() < 0.2:
            print("작업 성공!")
            break
    else:
        print("최대 시도 횟수 초과")

# 실행
traditional_polling()
iterator_based_polling()
```

### 상태 기반 무한 프로세스 처리

```python
def stateful_iterator_example():
    """상태를 가진 이터레이터 예제"""
    
    class DataProcessor:
        """데이터 처리기 (상태 보존)"""
        def __init__(self, data_stream):
            self.data_stream = data_stream
            self.processed_count = 0
            self.error_count = 0
            self.total_value = 0
        
        def __iter__(self):
            return self
        
        def __next__(self):
            try:
                # 데이터 스트림에서 다음 항목 가져오기
                data = next(self.data_stream)
                
                # 처리 (여기서는 간단히 값 더하기)
                processed_value = self.process_data(data)
                
                # 상태 업데이트
                self.processed_count += 1
                self.total_value += processed_value
                
                return {
                    'data': data,
                    'processed_value': processed_value,
                    'total_processed': self.processed_count,
                    'running_total': self.total_value
                }
                
            except StopIteration:
                # 데이터 스트림이 끝나면 종료
                raise
    
        def process_data(self, data):
            """데이터 처리 로직"""
            # 간단한 처리: 숫자인 경우 그대로, 아니면 0
            try:
                return float(data)
            except (ValueError, TypeError):
                self.error_count += 1
                return 0
        
        def get_stats(self):
            """현재 통계 반환"""
            return {
                'processed': self.processed_count,
                'errors': self.error_count,
                'total_value': self.total_value,
                'average': self.total_value / self.processed_count if self.processed_count > 0 else 0
            }
    
    # 무한 데이터 스트림 생성
    def infinite_data_stream():
        """무한 데이터 스트림 생성기"""
        count = 0
        while True:
            count += 1
            # 다양한 타입의 데이터 생성
            if count % 3 == 0:
                yield count  # 숫자
            elif count % 3 == 1:
                yield f"item_{count}"  # 문자열
            else:
                yield None  # None
            
            if count > 20:  # 데모용 제한
                break
    
    print("상태 기반 데이터 처리:")
    print("=" * 50)
    
    # 프로세서 생성 및 실행
    processor = DataProcessor(infinite_data_stream())
    
    # 처리 결과 출력
    for i, result in enumerate(processor, 1):
        print(f"처리 {i}: 데이터={result['data']}, "
              f"값={result['processed_value']}, "
              f"누적={result['running_total']}")
        
        if i >= 10:  # 데모용으로 10개만 처리
            print("\n중간 통계:")
            stats = processor.get_stats()
            for key, value in stats.items():
                print(f"{key}: {value}")
            break

stateful_iterator_example()
```

### 이벤트 루프 시뮬레이션

```python
def event_loop_simulation():
    """이터레이터를 사용한 이벤트 루프 시뮬레이션"""
    
    class Event:
        def __init__(self, name, priority=1):
            self.name = name
            self.priority = priority
            self.processed = False
        
        def __lt__(self, other):
            # 우선순위가 높을수록(숫자가 작을수록) 먼저 처리
            return self.priority < other.priority
        
        def __str__(self):
            return f"Event({self.name}, priority={self.priority})"
    
    class EventLoop:
        def __init__(self):
            self.events = []
            self.running = True
            self.processed_count = 0
        
        def add_event(self, event):
            """이벤트 추가"""
            self.events.append(event)
        
        def event_generator(self):
            """이벤트 처리 제너레이터"""
            import heapq
            
            while self.running and self.events:
                # 우선순위 큐에서 다음 이벤트 가져오기
                heapq.heapify(self.events)  # 최소 힙 구성
                next_event = heapq.heappop(self.events)
                
                yield next_event
                
                # 처리 완료 표시
                next_event.processed = True
                self.processed_count += 1
                
                # 조건부 종료
                if self.processed_count >= 10:
                    self.running = False
        
        def run(self):
            """이벤트 루프 실행"""
            print("이벤트 루프 시작:")
            print("-" * 40)
            
            for i, event in enumerate(self.event_generator(), 1):
                print(f"이벤트 {i}: {event} 처리 중...")
                # 이벤트 처리 시뮬레이션
                time.sleep(0.1)
                print(f"       {event} 처리 완료")
            
            print("-" * 40)
            print(f"이벤트 루프 종료. 총 처리된 이벤트: {self.processed_count}")
    
    # 이벤트 루프 생성 및 실행
    loop = EventLoop()
    
    # 다양한 이벤트 추가
    events_to_add = [
        Event("네트워크 요청", priority=2),
        Event("사용자 입력", priority=1),  # 가장 높은 우선순위
        Event("파일 저장", priority=3),
        Event("데이터베이스 백업", priority=4),
        Event("시스템 로깅", priority=5),
        Event("주기적 검사", priority=3),
        Event("에러 처리", priority=1),  # 높은 우선순위
        Event("메모리 정리", priority=4),
        Event("UI 업데이트", priority=2),
        Event("배치 작업", priority=5),
        Event("실시간 알림", priority=2),
        Event("보안 검사", priority=1),  # 높은 우선순위
    ]
    
    for event in events_to_add:
        loop.add_event(event)
    
    # 이벤트 루프 실행
    loop.run()

event_loop_simulation()
```

## 결론

이터레이터와 제너레이터는 파이썬에서 데이터 처리를 위한 강력한 패러다임을 제공합니다. `itertools.chain()`을 사용하면 여러 컨테이너를 효율적으로 순회할 수 있고, 제너레이터 파이프라인을 구성하면 복잡한 데이터 처리 흐름을 명확하게 표현하면서도 메모리 사용을 최적화할 수 있습니다.

중첩된 데이터 구조를 다룰 때는 재귀적 풀기 기법을 활용하고, 정렬된 데이터를 병합할 때는 `heapq.merge()`를 사용하면 효율적으로 작업할 수 있습니다. 특히 무한 루프나 상태 기반 프로세스를 이터레이터로 대체하면 코드의 가독성과 안정성을 크게 향상시킬 수 있습니다.

이러한 기법들은 대용량 데이터 처리, 실시간 스트리밍, 복잡한 알고리즘 구현 등 다양한 상황에서 유용하게 적용될 수 있습니다. 각 도구의 특징을 이해하고 상황에 맞게 조합하면 더 효율적이고 유지보수하기 좋은 코드를 작성할 수 있습니다.