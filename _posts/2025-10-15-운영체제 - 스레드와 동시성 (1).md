---
layout: post
title: 운영체제 - 스레드와 동시성 (1)
date: 2025-10-15 23:25:23 +0900
category: 운영체제
---
# Chapter 4 — Threads & Concurrency
*(4.1 Overview • 4.2 Multicore Programming • 4.3 Multithreading Models — 실행/관측 가능한 예제 포함)*

이 장은 **스레드와 동시성**을 실용적으로 이해하기 위해,
1) 기본 개념(4.1),
2) 멀티코어에서의 성능/일관성/튜닝(4.2),
3) 스레딩 모델(4.3)
을 **직접 실행 가능한 코드**와 **관측 포인트**로 설명한다. 예제는 주로 **리눅스 + g++/clang + pthreads/C++ 표준 스레드**를 사용한다.

---

## 4.1 Overview

### 4.1.1 프로세스 vs 스레드
- **프로세스**: 독립된 가상 주소 공간, 커널이 격리/보호. 프로세스 간 통신은 IPC 필요.
- **스레드**: **한 프로세스의 주소 공간을 공유**하는 실행 단위. 코드/데이터/힙, 열린 파일 등을 공유하고 **스택/레지스터**는 분리.

**장점**
- 컨텍스트 스위치 비용/메모리 오버헤드가 프로세스보다 적음
- 공유 메모리 기반 협업이 쉬움

**위험**
- **데이터 레이스**, **경쟁 조건**, **교착 상태(데드락)**

### 4.1.2 동시성과 병렬성
- **동시성(Concurrency)**: 시간 분할로 여러 작업을 *겹쳐* 진행하는 설계(단일 코어에서도 성립).
- **병렬성(Parallelism)**: 실제로 **복수 코어가 동시에** 실행.

### 4.1.3 스레드의 상태와 동기화 원시(Primitives)
- **뮤텍스**: 상호 배제
- **읽기-쓰기 락**: 읽기 다수/쓰기 1
- **세마포어**: 정량적 자원
- **조건 변수**: 이벤트 대기/깨우기
- **바리어**: 스레드 집합의 동기화 지점
- **원자적 연산/메모리 오더**: lock-free/low-lock 설계의 기반

---

## 4.1 예제 1 — 데이터 레이스와 해결

### (A) 의도적 데이터 레이스
```cpp
// race.cpp : 두 스레드가 같은 카운터를 증가 — 보호 없음(UB)
#include <thread>
#include <vector>
#include <iostream>
int main(){
    int counter = 0;
    std::vector<std::thread> ts;
    for(int i=0;i<4;i++){
        ts.emplace_back([&]{ for(int k=0;k<500000;k++) counter++; });
    }
    for(auto& t:ts) t.join();
    std::cout << "counter=" << counter << "\n"; // 기대: 2,000,000; 실제: 종종 작음
}
```
```bash
g++ -O2 -std=c++17 race.cpp -o race && ./race
```

### (B) 뮤텍스로 해결
```cpp
// race_mutex.cpp
#include <thread>
#include <mutex>
#include <vector>
#include <iostream>
int main(){
    long long counter = 0;
    std::mutex m;
    std::vector<std::thread> ts;
    for(int i=0;i<4;i++){
        ts.emplace_back([&]{
            for(int k=0;k<500000;k++){
                std::lock_guard<std::mutex> g(m);
                counter++;
            }
        });
    }
    for(auto& t:ts) t.join();
    std::cout << "counter=" << counter << "\n";
}
```

### (C) 원자적 연산으로 해결
```cpp
// race_atomic.cpp
#include <thread>
#include <atomic>
#include <vector>
#include <iostream>
int main(){
    std::atomic<long long> counter{0};
    std::vector<std::thread> ts;
    for(int i=0;i<4;i++){
        ts.emplace_back([&]{ for(int k=0;k<500000;k++) counter.fetch_add(1,std::memory_order_relaxed); });
    }
    for(auto& t:ts) t.join();
    std::cout << "counter=" << counter.load() << "\n";
}
```

**관측 포인트**
- 원자 연산은 **락 해제/획득 오버헤드 없음** → 스루풋↑.
- 그러나 복잡한 불변식을 동시에 유지하려면 락이 여전히 필요.

---

## 4.2 Multicore Programming

멀티코어에서 성능을 얻으려면 **작업 분해, 캐시/메모리 일관성, NUMA, false sharing**을 고려해야 한다.

### 4.2.1 Amdahl & Gustafson 법칙

- **암달의 법칙(Amdahl’s law)**: 전체 작업 중 병렬화 가능한 비율을 $$p$$, 코어 수를 $$N$$이라 할 때 이론적 최대 가속도 $$S$$
  $$
  S(N) = \frac{1}{(1-p) + \frac{p}{N}}
  $$
  직렬 부분이 크면 코어를 늘려도 한계가 있음.

- **거스탠슨 법칙(Gustafson’s law)**: 문제 크기를 늘릴 때의 선형 확장성 강조
  $$
  S(N) = N - (N-1)\cdot (1-p)
  $$

### 4.2.2 작업 분해 전략
- **데이터 병렬(Data parallel)**: 큰 배열/벡터를 **균등 분할**
- **태스크 병렬(Task parallel)**: 서로 다른 독립 작업을 큐에 등록하고 **워크-스틸링**으로 균형
- **파이프라인 병렬(Pipeline)**: 단계별로 병렬화

### 4.2.3 False Sharing 방지
서로 다른 스레드가 **같은 캐시 라인의 다른 변수**를 갱신하면 **일관성 트래픽**이 폭증.

```cpp
// false_sharing.cpp : 패딩 없이 vs 패딩 포함
#include <thread>
#include <vector>
#include <iostream>
#include <chrono>
struct alignas(64) Padded { long long v; char pad[64-sizeof(long long)]; };
int main(){
    const int T=8, I=30'000'000;
    // 케이스 1: 인접 배열(의도적 충돌)
    std::vector<long long> a(T);
    auto t1 = std::chrono::high_resolution_clock::now();
    std::vector<std::thread> ts1;
    for(int i=0;i<T;i++) ts1.emplace_back([&,i]{ for(int k=0;k<I;k++) a[i]++; });
    for(auto& t:ts1) t.join();
    auto t2 = std::chrono::high_resolution_clock::now();

    // 케이스 2: 캐시라인 패딩
    std::vector<Padded> b(T);
    std::vector<std::thread> ts2;
    for(int i=0;i<T;i++) ts2.emplace_back([&,i]{ for(int k=0;k<I;k++) b[i].v++; });
    for(auto& t:ts2) t.join();
    auto t3 = std::chrono::high_resolution_clock::now();

    auto d1=std::chrono::duration<double>(t2-t1).count();
    auto d2=std::chrono::duration<double>(t3-t2).count();
    std::cout<<"no padding: "<<d1<<"s, padded: "<<d2<<"s\n";
}
```

**관측 포인트**
- 패딩 버전이 **대폭 빠른** 경우가 많다(코어/플랫폼에 따라 상이).

### 4.2.4 메모리 모델 & Happens-Before

- **Happens-Before**: 한 스레드의 **release** 쓰기와 다른 스레드의 **acquire** 읽기 사이의 **순서 보장**.
- **C++ 원자 메모리 오더**:
  - `memory_order_relaxed` — 순서 제약 없음(동일 원자 변수에만 일관)
  - `memory_order_release` / `memory_order_acquire` — 단방향 장벽
  - `memory_order_seq_cst` — 가장 엄격, 간단하지만 비용↑

```cpp
// release_acquire.cpp : 간단한 신호-데이터 패턴
#include <atomic>
#include <thread>
#include <iostream>
int main(){
    std::atomic<bool> ready{false};
    int data=0;
    std::thread prod([&]{ data=42; ready.store(true,std::memory_order_release); });
    std::thread cons([&]{
        while(!ready.load(std::memory_order_acquire)) {}
        std::cout<<data<<"\n"; // 42 보장
    });
    prod.join(); cons.join();
}
```

### 4.2.5 NUMA와 CPU Affinity

- **NUMA**: 코어가 가까운 메모리 뱅크에서 **더 낮은 지연**.
- **정책**: first-touch, 바인딩(스레드/메모리), 워크-스틸 시 **원격 접근 최소화**.

```cpp
// affinity.cpp : 리눅스에서 스레드를 특정 CPU에 바인딩(축약)
#include <pthread.h>
#include <thread>
#include <iostream>
int main(){
    std::thread t([]{
        cpu_set_t set; CPU_ZERO(&set); CPU_SET(0, &set);
        pthread_setaffinity_np(pthread_self(), sizeof(set), &set);
        volatile long long x=0; for(long long i=0;i<1e8;i++) x+=i;
        std::cout<<"done on CPU0\n";
    });
    t.join();
}
```

실행 시 `taskset`/`numactl`과 함께 측정해 **원격 메모리 접근**이 미치는 영향을 관찰한다.

### 4.2.6 스레드 풀 & 워크-스틸링(개념)

- **스레드 풀**: 고정 워커 수 + 작업 큐. 컨텍스트 스위치/생성 비용 상쇄.
- **워크-스틸링**: 워커마다 덱(Deque). 자기 덱이 비면 **다른 워커의 뒤에서** 작업을 훔침 → **부하 균형**.

```cpp
// pool.cpp : 간단 스레드풀 (학습용, 종료/예외 생략)
#include <thread>
#include <vector>
#include <queue>
#include <mutex>
#include <condition_variable>
#include <functional>
#include <iostream>

struct Pool{
    std::vector<std::thread> ws; std::queue<std::function<void()>> q;
    std::mutex m; std::condition_variable cv; bool stop=false;
    Pool(int n){
        for(int i=0;i<n;i++)
            ws.emplace_back([this]{
                for(;;){
                    std::function<void()> f;
                    { std::unique_lock<std::mutex> lk(m);
                      cv.wait(lk,[&]{return stop||!q.empty();});
                      if(stop&&q.empty()) return; f=std::move(q.front()); q.pop(); }
                    f();
                }
            });
    }
    template<class F> void submit(F&& f){
        { std::lock_guard<std::mutex> lk(m); q.emplace(std::forward<F>(f)); }
        cv.notify_one();
    }
    ~Pool(){ { std::lock_guard<std::mutex> lk(m); stop=true; } cv.notify_all();
             for(auto& t:ws) t.join(); }
};

int main(){
    Pool p(std::thread::hardware_concurrency());
    std::atomic<long long> sum{0};
    for(int b=0;b<16;b++)
        p.submit([b,&sum]{
            long long s=0; for(long long i=1;i<=10'000'00;i++) s+= (i+b)%97;
            sum.fetch_add(s,std::memory_order_relaxed);
        });
    // Pool 소멸자에서 join
    std::cout<<"sum="<<sum.load()<<"\n";
}
```

---

## 4.2 예제 2 — 병렬 합계: OpenMP vs std::thread 비교

### OpenMP (간단/선언적)
```cpp
// omp_sum.cpp
#include <vector>
#include <numeric>
#include <iostream>
#ifdef _OPENMP
#include <omp.h>
#endif
int main(){
    const size_t N=50'000'000;
    std::vector<int> v(N,1);
    long long s=0;
    #pragma omp parallel for reduction(+:s) schedule(static)
    for(size_t i=0;i<N;i++) s+=v[i];
    std::cout<<"sum="<<s<<"\n";
}
```
```bash
g++ -O3 -fopenmp omp_sum.cpp -o omp_sum && ./omp_sum
```

### std::thread (세밀 제어)
```cpp
// thr_sum.cpp
#include <thread>
#include <vector>
#include <iostream>
int main(){
    const size_t N=50'000'000, T=std::thread::hardware_concurrency();
    std::vector<int> v(N,1); std::vector<long long> part(T);
    std::vector<std::thread> ts;
    for(size_t t=0;t<T;t++){
        size_t L = N*t/T, R = N*(t+1)/T;
        ts.emplace_back([&,t,L,R]{ long long s=0; for(size_t i=L;i<R;i++) s+=v[i]; part[t]=s; });
    }
    for(auto& th:ts) th.join();
    long long sum=0; for(auto x:part) sum+=x;
    std::cout<<"sum="<<sum<<"\n";
}
```

**비교 포인트**
- **OpenMP**: 코드가 간결, 스케줄링 정책만 조정하면 됨.
- **std::thread**: **바인딩/NUMA/캐시** 고려한 미세튜닝 가능.

---

## 4.3 Multithreading Models

스레드가 “사용자 수준”과 “커널 수준” 어디에 존재하는지, 어떻게 매핑되는지가 **모델**이다.

### 4.3.1 Many-to-One (여러 ULT → 하나의 KLT)
- **사용자 수준 스레드(ULT)** 라이브러리가 스케줄링, 커널에는 **단일 스레드**로 보임.
- 장점: 생성/스위치 비용 매우 낮음.
- 단점: **시스템콜 블로킹**이 **전체를 멈춤**. 멀티코어 병렬 실행 불가.
- **예**: 고전적 `green threads`, 일부 언어의 fiber/coroutine 런타임(블로킹 회피 시).

### 4.3.2 One-to-One (각 ULT ↔ 각 KLT)
- 사용자 스레드마다 **커널 스레드**가 1:1 매핑. 현대 **Linux NPTL**, **Windows**, **macOS**가 채택.
- 장점: 커널이 스케줄링 → **진짜 병렬성**. 블로킹 시스템콜도 다른 스레드에 영향 적음.
- 단점: 스레드 수가 많으면 **메모리/스케줄링 오버헤드** 증가 → **스레드풀**이 실전 기본.

### 4.3.3 Many-to-Many (N ULT ↔ M KLT)
- 사용자 스레드를 **소수의 커널 스레드 풀**에 매핑. 런타임이 **사용자 스케줄링**(work-stealing 등) 수행.
- 장점: 유연/효율 — 수천~수만 ULT를 소수 KLT에 매핑.
- 단점: 구현 복잡, 커널/언어 런타임 협력 필요.
- **예**: 과거 Solaris **LWP** 모델, 최신에는 **Go**의 G-M-P(수만 Goroutine ↔ M OS thread), **Erlang/BEAM**(스케줄러 스레드 + 경량 프로세스).

### 4.3.4 사용자 수준 스레드(Fiber/Coroutine)와 비동기
- **Coroutine**/Fiber는 **협력적 스케줄링**(yield)로 문맥 전환 비용↓.
- I/O는 **비동기**로 처리하여 **블로킹 회피**(epoll/kqueue/IOCP + 런타임) → **엄밀한 의미의 스레드가 아니어도 동시성 제공**.
- **Python**: GIL로 CPU 바운드는 멀티스레딩 이득 제한 → `multiprocessing`/C 확장/`numba`/`cython` 또는 I/O 바운드에 `asyncio`가 적합.
- **Rust**: `std::thread` + `tokio`(비동기) 혼용, `Send/Sync` 트레이트로 데이터 경합을 컴파일 타임에 제약.

---

## 4.3 예제 1 — 사용자 수준 스케줄링(코루틴 느낌의 협력)

```cpp
// coop_threads.cpp : 두 "코루틴"이 협력적으로 yield
#include <ucontext.h>
#include <cstdio>
char stack1[64<<10], stack2[64<<10];
ucontext_t mainc, c1, c2;

void f1(){ for(int i=0;i<3;i++){ printf("f1 step %d\n",i); swapcontext(&c1,&c2);} swapcontext(&c1,&mainc); }
void f2(){ for(int i=0;i<3;i++){ printf("f2 step %d\n",i); swapcontext(&c2,&c1);} swapcontext(&c2,&mainc); }

int main(){
    getcontext(&c1); c1.uc_stack.ss_sp=stack1; c1.uc_stack.ss_size=sizeof(stack1); c1.uc_link=&mainc; makecontext(&c1,f1,0);
    getcontext(&c2); c2.uc_stack.ss_sp=stack2; c2.uc_stack.ss_size=sizeof(stack2); c2.uc_link=&mainc; makecontext(&c2,f2,0);
    swapcontext(&mainc,&c1); // 시작
    puts("done");
}
```
> 실제 서비스에서는 **언어 런타임(Go/Erlang/Rust async)**가 안전한 스케줄링/런루프/타이머/IO를 제공한다.

---

## 4.3 예제 2 — Go의 M:N 개념 요약(설명 중심)

- **G**: Goroutine(사용자 스레드)
- **M**: OS 스레드
- **P**: 실행권(스케줄러 context, 런 큐 보유). `G`가 많을 때 **워크-스틸링**으로 균형.
- 블로킹 시스템콜은 **별도의 M**으로 격리; 런타임이 자동으로 스레드 증가/감소.

**학습 실험**: 동일한 CPU 작업을 Go에서 10만 개 Goroutine으로 돌리고, C++에서 10만 `std::thread`를 만들려 하면 메모리/스케줄러 오버헤드 차이를 체감할 수 있다(실험 시 시스템 주의).

---

## 4.3 예제 3 — Readers–Writers 문제 (rwlock vs mutex)

```cpp
// rwlock.cpp : 읽기 많은 워크로드에서 rwlock 이점
#include <pthread.h>
#include <thread>
#include <vector>
#include <iostream>
#include <atomic>
pthread_rwlock_t rw = PTHREAD_RWLOCK_INITIALIZER;
int shared_value=0;

void reader(){ for(int i=0;i<100000;i++){ pthread_rwlock_rdlock(&rw); int x=shared_value; (void)x; pthread_rwlock_unlock(&rw);} }
void writer(){ for(int i=0;i<10000;i++){ pthread_rwlock_wrlock(&rw); shared_value++; pthread_rwlock_unlock(&rw);} }

int main(){
    int R=8, W=1; std::vector<std::thread> ts;
    for(int i=0;i<R;i++) ts.emplace_back(reader);
    for(int i=0;i<W;i++) ts.emplace_back(writer);
    for(auto& t:ts) t.join();
    std::cout<<"value="<<shared_value<<"\n";
}
```

**관측 포인트**
- 읽기 위주에서 `rwlock`가 유리. 쓰기가 많아지면 락 경합으로 **되려 느려질 수 있음** → 벤치마크로 의사결정.

---

## 4.3 예제 4 — Lock-Free 구조(학습용) : MPMC 원형 큐 스케치

> 실전급 MPMC는 매우 까다롭다(ABA, 세대 태깅, 캐시 라인 패딩, acquire/release 세밀 제어).
> 여기서는 **아이디어**와 **관측 포인트**를 위한 축약 스케치만 제시.

```cpp
// mpmc_ring_sketched.cpp (정확성 검증/경계 체크 생략 — 학습용)
#include <atomic>
#include <vector>
#include <thread>
#include <iostream>

struct Slot { std::atomic<size_t> seq; int val; };
struct Ring {
    size_t cap, mask; std::vector<Slot> v;
    Ring(size_t c):cap(c),mask(c-1),v(c){ for(size_t i=0;i<c;i++) v[i].seq=i; }
    std::atomic<size_t> head{0}, tail{0};
    bool push(int x){
        size_t t = tail.load(std::memory_order_relaxed);
        for(;;){
            Slot& s = v[t & mask];
            size_t seq = s.seq.load(std::memory_order_acquire);
            intptr_t diff = (intptr_t)seq - (intptr_t)t;
            if(diff==0){
                if(tail.compare_exchange_weak(t,t+1,std::memory_order_relaxed)){
                    s.val=x; s.seq.store(t+1,std::memory_order_release);
                    return true;
                }
            } else if(diff<0) return false; // full
            else t = tail.load(std::memory_order_relaxed);
        }
    }
    bool pop(int& out){
        size_t h = head.load(std::memory_order_relaxed);
        for(;;){
            Slot& s = v[h & mask];
            size_t seq = s.seq.load(std::memory_order_acquire);
            intptr_t diff = (intptr_t)seq - (intptr_t)(h+1);
            if(diff==0){
                if(head.compare_exchange_weak(h,h+1,std::memory_order_relaxed)){
                    out=s.val; s.seq.store(h+cap,std::memory_order_release);
                    return true;
                }
            } else if(diff<0) return false; // empty
            else h = head.load(std::memory_order_relaxed);
        }
    }
};

int main(){
    Ring q(1<<12);
    std::atomic<bool> go{false};
    std::thread p([&]{ while(!go.load()); for(int i=0;i<200000;i++) while(!q.push(i)){} });
    std::thread c([&]{ while(!go.load()); int x; long long sum=0; for(int i=0;i<200000;i++){ while(!q.pop(x)){} sum+=x; std::cout.flush(); } std::cout<<"ok\n"; });
    go.store(true);
    p.join(); c.join();
}
```
**관측 포인트**
- `seq`(세대번호)로 **ABA 회피/상태 구분**.
- `acquire/release`로 **HB 관계**를 만든다.
- 실제 서비스에서는 **검증/퍼징/경계**가 필수.

---

## 4.3 예제 5 — I/O 바운드: async + threadpool 하이브리드(개념)

- 네트워크는 `epoll/kqueue`로 수만 연결을 관리하고, **CPU 바운드 작업만** 스레드풀로 이관.
- Python `asyncio + run_in_executor`, C++ `io_uring + CPU pool`, Java NIO + `ForkJoinPool`이 대표 패턴.

```python
# aio_pool.py : asyncio + thread pool (CPU 태스크만 offload)
import asyncio, concurrent.futures, time
def cpu_job(n): s=0
;
 for i in range(10_000_00): s=(s+i)%n; return s
async def main():
    loop=asyncio.get_running_loop()
    with concurrent.futures.ThreadPoolExecutor() as pool:
        t1=loop.run_in_executor(pool, cpu_job, 97)
        t2=loop.run_in_executor(pool, cpu_job, 193)
        print(await t1, await t2)
asyncio.run(main())
```

---

## 운영 체크리스트

1) **측정 우선**: p50/p95, 컨텍스트 스위치, 런큐 길이(`vmstat`, `pidstat -w`), 캐시미스(`perf stat`).
2) **락 경쟁**: `perf lock`, bpftrace로 `futex` hot spot 확인.
3) **false sharing**: 캐시라인 패딩/배치.
4) **NUMA**: first-touch, 바인딩, 메모리 정책.
5) **스레드 수**: 하드웨어 동시성 + 외부 I/O 지연/DB 커넥션 한도에 맞춤.
6) **모델 선택**:
   - CPU 바운드: 1:1 + 풀/워크-스틸
   - 대규모 I/O: 비동기 + 제한된 풀
   - 초대량 경량 태스크: M:N(Go/Erlang/언어 런타임) 고려

---

## 요약

- **4.1**: 스레드는 메모리 공유로 빠른 협업을 가능케 하지만, 데이터 레이스/메모리 모델 이해가 필수.
- **4.2**: 멀티코어 성능은 **작업 분해 + 캐시/NUMA/false sharing 회피 + 적절한 동기화**에서 나온다. 암달/거스탠슨으로 한계를 읽어라.
- **4.3**: 스레딩 모델(1:1, M:1, M:N) 각자의 **성능–복잡성–확장성** 트레이드오프를 이해하고, 워크로드에 맞는 **실용 설계**(풀/워크-스틸/async)를 택하라.

---

## 추가 과제

1) `false_sharing.cpp`에서 **패딩 간격/정렬**을 바꿔 성능 곡선을 얻어라.
2) `pool.cpp`에 **작업 선점/기한**(deadline)을 고려하는 **우선순위 큐**를 도입하고 p95 지연을 비교하라.
3) NUMA 2소켓 시스템에서 **cross-node** vs **local** 할당으로 `thr_sum.cpp` 성능을 비교하라.
4) `release_acquire.cpp`를 `relaxed`로 바꿔 **데이터 가시성 문제**를 재현하고, `seq_cst`로의 변화도 비교하라.
5) Go 또는 Erlang으로 **같은 문제**(대량 경량 태스크)를 구현하고 C++ 1:1 스레드와 **메모리 사용량/Context switch**를 수치 비교하라.
