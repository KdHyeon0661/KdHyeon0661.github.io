---
layout: post
title: 딥러닝 - RAG 파이프라인
date: 2025-10-07 19:25:23 +0900
category: 딥러닝
---
# RAG 파이프라인 총정리  

## 0) 파이프라인 한눈에 보기

```
[원문서] ──▶ [청크/전처리] ──▶ [색인(토큰/BM25 + 임베딩/FAISS)]
                                 ▲                         │
                                 │                         ▼
질의 ──▶ [질의전처리/임베딩] ──▶ [하이브리드 검색] ──▶ [재순위(CrossEncoder)]
                                                         │
                                                         ▼
                                       [컨텍스트 패킹(토큰 예산/중복제거/하이라이트)]
                                                         │
                                                         ▼
                                         [생성(LLM) + 출처주석 + 후처리(검증)]
                                                         │
                                                         ▼
                                     [캐시(질의→문서/생성), 로그, 회귀 테스트]
```

- **하이브리드**: 토큰기반(BM25/TF-IDF) + 벡터(코사인)를 **가중합/CRF/RRF**로 융합  
- **재순위**: 상위 k개 후보를 **크로스 인코더(질의+문서 한 번에)**로 재평가  
- **컨텍스트 길이 관리**: **토큰 예산** 안에서 **중복 제거/요약/슬라이딩**으로 패킹  
- **캐시**: *문서 임베딩*, *질의→Top-k*, *프롬프트→응답*, *KV-cache* (추론 가속)

---

## 1) 데이터 전처리 & 청크 전략

### 1.1 왜 청크가 중요한가?
- LLM 컨텍스트 창 제한 → **너무 길면 삭감**  
- 벡터 검색은 길이/주제가 **균일한 청크**에서 잘 작동  
- 권장: **의미 단락 기준 + 토큰 길이 상한(예: 256~512)**

### 1.2 청크 크기/겹침
- 일반 텍스트: 300~500 토큰, **stride(겹침)** 50~100  
- 코드/매뉴얼: 섹션/함수 경계 우선, 200~400 토큰  
- PDF: 문단 추출 + 헤더/푸터 제거, 표/그림 캡션은 별도 청크

### 1.3 간단 청크 코드 (문단 + 토큰 상한)
```python
import re
from transformers import AutoTokenizer
from typing import List, Dict

def split_paragraphs(text: str) -> List[str]:
    paras = [p.strip() for p in re.split(r'\n{2,}', text) if p.strip()]
    return paras

def tokenize_len(tok, s): 
    return len(tok(s, add_special_tokens=False)["input_ids"])

def chunk_text(text: str, tok, max_tokens=400, overlap=60) -> List[str]:
    paras = split_paragraphs(text)
    chunks = []
    buf, buf_len = [], 0
    for p in paras:
        L = tokenize_len(tok, p)
        if L > max_tokens:  # 문단이 너무 긴 경우 슬라이딩
            tokens = tok(p, add_special_tokens=False)["input_ids"]
            start = 0
            while start < len(tokens):
                end = min(start + max_tokens, len(tokens))
                seg = tok.decode(tokens[start:end])
                chunks.append(seg.strip())
                start = end - overlap
                if start < 0: start = 0
        else:
            if buf_len + L <= max_tokens:
                buf.append(p); buf_len += L
            else:
                if buf:
                    chunks.append("\n\n".join(buf).strip())
                buf, buf_len = [p], L
    if buf: chunks.append("\n\n".join(buf).strip())
    return chunks

# 사용 예: tok = AutoTokenizer.from_pretrained("intfloat/e5-base") 등
```

---

## 2) 인덱싱(색인): 토큰기반 + 벡터기반

### 2.1 BM25 (간단 구현)
> 전통 IR의 표준. 질의 단어가 **문서에 얼마나 잘 매칭**되는지 평가.

- **BM25 점수**  
  $$
  \mathrm{BM25}(q,d)=\sum_{t\in q} \mathrm{IDF}(t)\cdot
  \frac{f(t,d)\,(k_1+1)}{f(t,d)+k_1\left(1-b+b\frac{|d|}{\mathrm{avgdl}}\right)}
  $$
  - \(f(t,d)\): 단어 \(t\)의 문서 \(d\) 내 빈도  
  - \(k_1\in[1.2, 2.0]\), \(b\in[0.5,0.9]\)

```python
import math, collections, re
from typing import Tuple, List

class BM25Index:
    def __init__(self, docs: List[str], k1=1.5, b=0.75):
        self.k1, self.b = k1, b
        self.N = len(docs)
        self.docs = docs
        self.tokenized = [self._tok(d) for d in docs]
        self.df = collections.Counter()
        self.len = []
        for toks in self.tokenized:
            self.len.append(len(toks))
            for w in set(toks): self.df[w]+=1
        self.avgdl = sum(self.len)/self.N

    def _tok(self, s: str) -> List[str]:
        return re.findall(r"[A-Za-z0-9가-힣_]+", s.lower())

    def idf(self, w):
        n = self.df.get(w, 0)+0.5
        return math.log((self.N - n + 0.5)/n + 1.0)

    def score(self, q: str, doc_id: int):
        toks = self.tokenized[doc_id]
        tf = collections.Counter(toks)
        score = 0.0
        for w in self._tok(q):
            if w not in tf: 
                continue
            idf = self.idf(w)
            f = tf[w]
            dl = self.len[doc_id]
            denom = f + self.k1*(1 - self.b + self.b*dl/self.avgdl)
            score += idf * f*(self.k1+1)/denom
        return score

    def topk(self, q: str, k=20):
        scores = [(i, self.score(q, i)) for i in range(self.N)]
        scores.sort(key=lambda x:x[1], reverse=True)
        return scores[:k]
```

### 2.2 Dense 임베딩(Encoder + Mean Pooling) → FAISS
- **코사인 유사도**  
  $$\cos(\mathbf{q},\mathbf{d})=\frac{\mathbf{q}\cdot \mathbf{d}}{\|\mathbf{q}\|\,\|\mathbf{d}\|}$$
- **권장 프롬프트 규약**(E5/Contriever류):
  - 쿼리: `"query: {질문}"`  
  - 문서: `"passage: {청크}"`

```python
import torch, numpy as np
from transformers import AutoTokenizer, AutoModel

class DenseEncoder(torch.nn.Module):
    def __init__(self, name="intfloat/e5-base-v2"):
        super().__init__()
        self.tok = AutoTokenizer.from_pretrained(name)
        self.enc = AutoModel.from_pretrained(name)
        self.dim = self.enc.config.hidden_size

    @torch.no_grad()
    def embed_texts(self, texts: List[str], batch_size=32, device="cuda"):
        self.enc.to(device).eval()
        outs = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            x = self.tok(batch, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
            h = self.enc(**x).last_hidden_state    # [B,T,H]
            mask = x["attention_mask"].unsqueeze(-1).float()  # [B,T,1]
            v = (h*mask).sum(1) / mask.sum(1).clamp(min=1e-6) # mean-pooling
            v = torch.nn.functional.normalize(v, p=2, dim=1)  # cosine용 정규화
            outs.append(v.cpu())
        return torch.cat(outs, 0)  # [N, H]
```

**FAISS 색인 (옵션: 없으면 NumPy로 대체)**
```python
def build_faiss(emb: np.ndarray, use_hnsw=True):
    try:
        import faiss
    except ImportError:
        return None  # 추후 NumPy 검색으로 대체
    d = emb.shape[1]
    if use_hnsw:
        index = faiss.IndexHNSWFlat(d, 32)       # HNSW 그래프
        index.hnsw.efConstruction = 80
    else:
        index = faiss.IndexFlatIP(d)             # 내적 = 코사인(정규화했으므로)
    index.add(emb.astype(np.float32))
    return index

@torch.no_grad()
def dense_topk(query: str, encoder: DenseEncoder, d_emb: np.ndarray, index=None, k=20):
    q = encoder.embed_texts([f"query: {query}"])[0].cpu().numpy().reshape(1, -1)
    if index is None:
        # NumPy 브루트포스
        sims = (d_emb @ q.T).squeeze(1)          # 정규화되어 있으므로 코사인
        idx = np.argpartition(-sims, range(k))[:k]
        idx = idx[np.argsort(-sims[idx])]
        return [(int(i), float(sims[i])) for i in idx]
    else:
        import faiss
        D, I = index.search(q.astype(np.float32), k)
        return [(int(I[0, j]), float(D[0, j])) for j in range(k)]
```

---

## 3) 하이브리드 검색 & 점수 융합

### 3.1 융합 방식
- **정규화 가중합**:  
  $$
  s = \alpha \cdot \widetilde{s}_{\text{BM25}} + (1-\alpha)\cdot \widetilde{s}_{\text{dense}}
  $$
  (각 점수는 Min-Max 정규화)
- **RRF(Reciprocal Rank Fusion)**:  
  $$
  \mathrm{RRF}(d) = \sum_{r \in \{\text{BM25}, \text{dense}\}} \frac{1}{k + \mathrm{rank}_r(d)}
  $$
  *(k=60 권장)*

- **MMR(diversity)**:  
  $$\mathrm{MMR}=\arg\max_{d\in C\setminus S}\ \lambda\,\mathrm{sim}(q,d)-(1-\lambda)\max_{s\in S}\mathrm{sim}(d,s)$$  
  (서로 유사한 문서 중복 방지)

### 3.2 코드: 하이브리드 + MMR
```python
import numpy as np

def minmax_norm(pairs):
    if not pairs: return pairs
    vals = np.array([s for _,s in pairs], dtype=np.float32)
    lo, hi = float(vals.min()), float(vals.max())
    if hi - lo < 1e-9: 
        return [(i, 0.0) for i,_ in pairs]
    return [(i, (s - lo)/(hi - lo)) for i,s in pairs]

def hybrid_fusion(bm25_pairs, dense_pairs, alpha=0.5, topk=20, method="sum"):
    # 입력: [(doc_id, score), ...] 각각 정렬된 리스트
    b = {i:s for i,s in minmax_norm(bm25_pairs)}
    d = {i:s for i,s in minmax_norm(dense_pairs)}
    ids = set(b.keys()) | set(d.keys())
    fused = []
    if method=="sum":
        for i in ids:
            fused.append((i, alpha*b.get(i,0.0) + (1-alpha)*d.get(i,0.0)))
    elif method=="rrf":
        # 입력이 rank 순으로 들어왔다는 가정
        k = 60
        brank = {i:r for r,(i,_) in enumerate(bm25_pairs)}
        drank = {i:r for r,(i,_) in enumerate(dense_pairs)}
        for i in ids:
            rrf = 1.0/(k+brank.get(i, 10**9)) + 1.0/(k+drank.get(i, 10**9))
            fused.append((i, rrf))
    fused.sort(key=lambda x:x[1], reverse=True)
    return fused[:topk]

def mmr_dedup(q_vec: np.ndarray, cand_idx: List[int], d_emb: np.ndarray, lam=0.65, k=10):
    # q_vec: [H], d_emb: [N,H] (정규화 가정)
    sel = []
    cand = cand_idx[:]
    while cand and len(sel) < k:
        best, best_val = None, -1e9
        for i in cand:
            rel = float(d_emb[i].dot(q_vec))
            div = 0.0 if not sel else max([float(d_emb[i].dot(d_emb[j])) for j in sel])
            val = lam*rel - (1-lam)*div
            if val > best_val: best, best_val = i, val
        sel.append(best)
        cand.remove(best)
    return sel
```

---

## 4) 재순위(Re-ranking) — 크로스 인코더

- 질의와 문서 청크를 **한 시퀀스**로 묶어 **유사도 스칼라**를 예측 (회귀/2-class 로지스틱)  
- 장점: 문맥 상호작용을 직접 모델링 → **정확도↑**  
- 단점: 후보 k개에 대해 **모델 k회 추론** → 비용

### 4.1 수식
- 회귀 점수 \(s(q,d)=\mathbf{w}^\top h_{[\mathrm{CLS}]}\)  
- 훈련:  
  $$\mathcal{L}=\sum_{(q,d^+,d^-)}\max\{0, 1 - s(q,d^+) + s(q,d^-)\}$$  
  (랭킹 hinge) 또는 **BCE**로 시그모이드 점수 학습

### 4.2 코드 (사전학습 모델로 점수만 뽑기)
```python
import torch
from transformers import AutoTokenizer, AutoModel

class CrossEncoder:
    def __init__(self, name="cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.tok = AutoTokenizer.from_pretrained(name)
        self.enc = AutoModel.from_pretrained(name)
        self.enc.eval()

    @torch.no_grad()
    def score_pairs(self, query: str, passages: List[str], device="cuda"):
        self.enc.to(device)
        batch = [f"Query: {query} Passage: {p}" for p in passages]
        x = self.tok(batch, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
        h = self.enc(**x).last_hidden_state[:,0]  # CLS
        # 선형헤드가 없다는 가정: 코사인 유사도로 근사
        h = torch.nn.functional.normalize(h, p=2, dim=1)
        q = self.tok([f"Query: {query}"], return_tensors="pt").to(device)
        qh = self.enc(**q).last_hidden_state[:,0]
        qh = torch.nn.functional.normalize(qh, p=2, dim=1)
        sims = (h @ qh.T).squeeze(1).cpu().numpy().tolist()
        return sims
```

> 실제 운영에서는 **크로스 인코더 전용 체크포인트**(회귀 헤드 포함)를 권장. 위 코드는 의존성 최소화를 위해 간략화.

---

## 5) 생성(LLM) — 컨텍스트 패킹 & 프롬프트

### 5.1 토큰 예산 & 패킹
- 총 창 \(C\) (예: 8k, 32k)에서
  - **질의+시스템 프롬프트**: \(B\)
  - **컨텍스트**: \(K\)
  - **최대 출력**: \(O\)  
  $$K = C - (B + O)$$

- **우선순위**: (1) 재순위 상위, (2) 질의와 **Overlap 점수↑**, (3) **중복제거**, (4) **제목/하이라이트 우대**  
- **하이라이팅**(선택): 질의 키워드와 일치하는 문장만 발췌

### 5.2 패킹 코드 (토큰 예산에 맞춰 청크 선택)
```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

def token_len(tok, text): 
    return len(tok(text, add_special_tokens=False)["input_ids"])

def pack_context(query, docs, tok, budget_tokens=2000):
    # docs: [{"id":i,"text":..., "score":..., "meta":{...}}, ...] (랭크 순)
    used, picked, seen = 0, [], set()
    for d in docs:
        t = d["text"].strip()
        # 간단 중복 제거 (문장 해시)
        sig = hash(t[:2000])
        if sig in seen: 
            continue
        L = token_len(tok, t)
        if used + L <= budget_tokens:
            picked.append(d)
            seen.add(sig)
            used += L
        if used >= budget_tokens: break
    return picked

SYS_TEMPLATE = """You are a precise assistant. Answer the user using ONLY the provided context.
If you are uncertain or the answer is not in the context, say you don't know.
Cite sources as [#doc_id]. Keep the answer concise and factual."""

def build_prompt(query, picked):
    ctx = ""
    for d in picked:
        ctx += f"\n[#{d['id']}] {d.get('title','')}\n{d['text']}\n"
    prompt = f"{SYS_TEMPLATE}\n\n# Question\n{query}\n\n# Context\n{ctx}\n\n# Answer"
    return prompt
```

### 5.3 LLM 호출 (T5/혹은 임의 LLM)
```python
def generate_answer(query, picked, model_name="google/flan-t5-base", max_new_tokens=512, device="cuda"):
    tok = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device).eval()
    prompt = build_prompt(query, picked)
    x = tok(prompt, return_tensors="pt", truncation=True, max_length=8192).to(device)
    y = model.generate(**x, max_new_tokens=max_new_tokens, do_sample=False)
    ans = tok.decode(y[0], skip_special_tokens=True)
    return ans
```

> 대형 LLM(예: Llama/Command 등)을 쓸 때도 **패킹 로직은 동일**합니다.  
> KV-Cache(디코딩 캐시)는 프레임워크 내부에서 자동 사용.

---

## 6) 캐시 전략 (속도·비용·재현성의 핵심)

### 6.1 캐시 종류
1) **임베딩 캐시**: `doc_hash → vector` (문서 업데이트 시 무효화)  
2) **리트리벌 캐시**: `(query_hash, index_hash) → doc_ids`  
3) **생성 캐시**: `prompt_hash → answer` (모델/샘플링 파라미터 포함)  
4) **KV 캐시**: 디코딩 중 내부 캐시(모델 내부), 외부로는 **스트리밍 재활용**시 고려

### 6.2 안전한 키 설계
- `hash = SHA256(json.dumps(obj, sort_keys=True) + model_name + version)`  
- **TTL**(예: 7일) + **최대 개수**(LRU)로 메모리 관리

```python
import hashlib, json, time
from collections import OrderedDict

class LRUCache:
    def __init__(self, cap=2048, ttl=86400):
        self.cap, self.ttl = cap, ttl
        self.data = OrderedDict()

    def _prune(self):
        now = time.time()
        keys = [k for k,(v,t) in self.data.items() if now - t > self.ttl]
        for k in keys: self.data.pop(k, None)
        while len(self.data) > self.cap: self.data.popitem(last=False)

    def get(self, key):
        self._prune()
        if key in self.data:
            v,t = self.data.pop(key); self.data[key]=(v,time.time()); return v
        return None
    def set(self, key, val):
        self._prune()
        if key in self.data: self.data.pop(key)
        self.data[key] = (val, time.time())

def key_of(obj, *args):
    s = json.dumps(obj, ensure_ascii=False, sort_keys=True) + "||" + "||".join(args)
    return hashlib.sha256(s.encode("utf-8")).hexdigest()
```

---

## 7) 엔드투엔드 예제 (RAGEngine)

> **데모 데이터**로 5개 문서를 색인 → 하이브리드 검색 → 재순위 → 패킹 → 생성까지 한 번에.

```python
# -*- coding: utf-8 -*-
import numpy as np, torch

class RAGEngine:
    def __init__(self, enc_name="intfloat/e5-base-v2", gen_name="google/flan-t5-base", use_faiss=False):
        self.encoder = DenseEncoder(enc_name)
        self.gen_name = gen_name
        self.gen_tok = None
        self.gen_model = None
        self.bm25 = None
        self.faiss = None
        self.emb_cache = {}
        self.ret_cache = LRUCache(cap=4096, ttl=86400)
        self.doc_meta = []       # [{"id":i,"text":...,"title":...}...]
        self.dense_mat = None    # np.ndarray [N,H]
        self.use_faiss = use_faiss

    def build(self, docs: List[Dict], use_hnsw=True):
        # docs: [{"title":..., "text":...}, ...]
        self.doc_meta = [{"id":i, **d} for i,d in enumerate(docs)]
        # BM25
        self.bm25 = BM25Index([d["text"] for d in self.doc_meta])
        # Dense
        passages = [f"passage: {d['text']}" for d in self.doc_meta]
        emb = self.encoder.embed_texts(passages, device="cuda" if torch.cuda.is_available() else "cpu")
        self.dense_mat = emb.cpu().numpy()
        # FAISS
        if self.use_faiss:
            self.faiss = build_faiss(self.dense_mat, use_hnsw=use_hnsw)

    def retrieve(self, query, k=20, alpha=0.5, method="sum", mmr=True, mmr_k=10):
        key = key_of({"q":query}, f"idx={len(self.doc_meta)}", f"a={alpha}", method)
        cached = self.ret_cache.get(key)
        if cached: return cached

        bm = self.bm25.topk(query, k=max(k, 50))
        dn = dense_topk(query, self.encoder, self.dense_mat, index=self.faiss, k=max(k, 50))
        fused = hybrid_fusion(bm, dn, alpha=alpha, topk=max(k, 50), method=method)

        # MMR로 다양성
        if mmr:
            qv = self.encoder.embed_texts([f"query: {query}"])[0].cpu().numpy()
            order = [i for i,_ in fused]
            order = mmr_dedup(qv, order, self.dense_mat, lam=0.65, k=k)
        else:
            order = [i for i,_ in fused[:k]]

        out = [{"id":i, "title": self.doc_meta[i].get("title",""),
                "text": self.doc_meta[i]["text"], "score": 0.0} for i in order]
        self.ret_cache.set(key, out)
        return out

    def generate(self, query, top_docs, out_tokens=512):
        tok = AutoTokenizer.from_pretrained(self.gen_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(self.gen_name).eval().to("cuda" if torch.cuda.is_available() else "cpu")
        budget = 4096 - 512 - 256  # (모델 창 - 출력 - 시스템/질의 여유)
        picked = pack_context(query, top_docs, tok, budget_tokens=max(512, budget))
        prompt = build_prompt(query, picked)
        x = tok(prompt, return_tensors="pt", truncation=True, max_length=8192).to(model.device)
        y = model.generate(**x, max_new_tokens=out_tokens, do_sample=False)
        ans = tok.decode(y[0], skip_special_tokens=True)
        return ans, picked

# --- 데모 실행 ---
docs = [
    {"title":"GPU 사용법", "text":"CUDA는 병렬 프로그래밍 모델이며 ... 커널, 스레드 블록 ..."},
    {"title":"PyTorch 텐서", "text":"PyTorch 텐서는 autograd를 통한 자동미분을 지원한다 ..."},
    {"title":"Docker 네트워킹", "text":"브리지 네트워크, 포트 매핑, 컨테이너 간 통신 ..."},
    {"title":"FAISS 소개", "text":"FAISS는 벡터 검색 라이브러리로 HNSW, IVF-Flat 인덱스 제공 ..."},
    {"title":"BM25", "text":"BM25는 고전 정보검색 점수로 k1, b 하이퍼파라미터 ..."}
]
engine = RAGEngine(use_faiss=False)
engine.build(docs)
cand = engine.retrieve("벡터 검색 라이브러리 뭐 쓰지? HNSW가 뭐야", k=5, alpha=0.4, method="sum", mmr=True)
ans, used = engine.generate("벡터 검색 라이브러리 뭐 쓰지? HNSW가 뭐야", cand, out_tokens=256)
print(ans)
print("sources:", [d["id"] for d in used])
```

---

## 8) 컨텍스트 길이 관리(필승 패턴)

1) **토큰 예산 계산**: 모델 창 대비 **BOS(프롬프트 고정) + 질의 + 컨텍스트 + 여유 출력** 분리  
2) **중복 제거**: 동일 문장/문단 해시로 제거  
3) **하이라이트 추출**: 쿼리와 **BM25 문장 점수 상위**만 발췌해 넣기  
4) **문서 요약 백오프**: 길면 *문서→문단 요약*을 캐싱해 사용  
5) **멀티턴 이어붙임 제한**: 이전 턴 컨텍스트를 **최소화**(핵심 근거만 유지)  
6) **역색인 경계 유지**: 청크를 도중 절단하지 말고 **원문 경계**를 살려 인용 가능하게

**문장 발췌 예시**
```python
def sentence_extract(text, query, top_m=3):
    sents = re.split(r'(?<=[.!?])\s+', text)
    bm = BM25Index(sents)  # 문장 단위 BM25
    top = bm.topk(query, k=min(top_m, len(sents)))
    return " ".join([sents[i] for i,_ in top])
```

---

## 9) 품질을 끌어올리는 리트리벌 팁

- **하이브리드**는 거의 항상 이득(특히 키워드 질의/숫자/기호 포함 시)  
- **MMR/클러스터링**으로 다양성 확보 → **중복 컨텍스트로 낭비되는 토큰** 절감  
- **재순위**는 상위 20~100개에 적용 → 최종 5~15개 패킹  
- **메타데이터 필터**: 시간/버전/언어/섹션 타입으로 후보를 줄이기  
- **RRF**는 robust, 점수 튜닝 귀찮을 때 좋은 기본값

---

## 10) 안전장치 & 후처리(정확도/책임성)

- **출처 주석**: `[ #doc_id ]` 형태 → **복원가능성/감사추적**  
- **정합성 점검(선택)**: 생성 답변과 인용 문맥 간 **NLI(Entailment)** 체크로 *환각* 감지  
- **금지 응답 정책**: 근거 부족 시 **“문맥에 없습니다”** 강제  
- **PII/보안**: 색인 시 **민감정보 마스킹/필터링** (regex/NER)

---

## 11) 평가(오프라인 회귀)

### 11.1 리트리벌 지표
- **Recall@k / Precision@k / MRR**  
  $$
  \mathrm{MRR}=\frac{1}{|Q|}\sum_{q}\frac{1}{\mathrm{rank}_q}
  $$

### 11.2 생성 지표
- **ROUGE-L**, **BLEU**(요약/번역형), **Faithfulness**(사실성)  
- **소스 커버리지**: 인용된 문서가 **정답 근거**와 겹치는지

**간단 스켈레톤**
```python
def recall_at_k(golds, preds, k=5):
    hit = 0
    for qid in golds:
        gold_set = set(golds[qid])  # 정답 문서 ID들
        pred = preds.get(qid, [])[:k]
        if gold_set & set(pred): hit += 1
    return hit/len(golds)
```

---

## 12) 운영 체크리스트

- [ ] **인덱스 스냅샷**(문서→청크→임베딩→FAISS) 버전/해시 보관  
- [ ] **캐시 키**에 *모델명/버전/하이퍼* 포함  
- [ ] **시간/버전 필터**(“최신 매뉴얼만”)  
- [ ] **관측/로깅**: 질의·클릭(출처 열람)·응답시간·토큰 사용량  
- [ ] **회귀 테스트**: 질문집 N개로 **R@k/EM/ROUGE** 자동 비교  
- [ ] **스케일링**: 색인은 **오프라인 배치**, 리트리벌은 **서빙 GPU/CPU 분리**

---

## 13) 고급 토픽(간단 개관)

- **ColBERT/스파스+덴스 멀티벡터**: 단어/토큰 별 임베딩 매칭으로 미세 정렬  
- **Distillation**: 크로스 인코더 신뢰 점수로 **dual encoder** 재학습  
- **RAG-Fusion**: 질의 변형(paraphrase) 여러 개로 검색 → RRF 융합  
- **Graph-RAG**: 문서 간 링크/엔티티 그래프를 통해 hop-aware 검색  
- **Long-Context 모델**: 128k 이상 창 모델 + sliding-window rerank 혼용

---

## 14) 자주 묻는 질문(FAQ)

**Q. BM25와 벡터 중 무엇을 우선?**  
A. 둘 다. **하이브리드 + RRF**가 초기에 안전한 기본값입니다.

**Q. 청크가 너무 길어요.**  
A. 토큰 상한(400±100)과 **stride 10~20%**를 권장. 긴 표/코드/목차는 별도.

**Q. 생성이 근거를 무시합니다.**  
A. 시스템 프롬프트에 **“문맥 밖 추론 금지”** 를 명시, 컨텍스트 패킹 시 **하이라이트 추출**을 우선.

**Q. 속도가 느립니다.**  
A. 임베딩/리트리벌 **캐시**, 크로스 인코더 **상위 20~50** 제한, LLM **빔 대신 그리디**.

---

## 15) 수식 요약

- **BM25**
$$
\mathrm{BM25}(q,d)=\sum_{t\in q} \mathrm{IDF}(t)\cdot
\frac{f(t,d)\,(k_1+1)}{f(t,d)+k_1\left(1-b+b\frac{|d|}{\mathrm{avgdl}}\right)}
$$

- **코사인**
$$
\cos(\mathbf{q},\mathbf{d})=\frac{\mathbf{q}\cdot \mathbf{d}}{\|\mathbf{q}\|\,\|\mathbf{d}\|}
$$

- **MMR**
$$
\mathrm{MMR}=\arg\max_{d\in C\setminus S}\ \lambda\,\mathrm{sim}(q,d)-(1-\lambda)\max_{s\in S}\mathrm{sim}(d,s)
$$

- **RRF**
$$
\mathrm{RRF}(d) = \sum_{r} \frac{1}{k + \mathrm{rank}_r(d)}
$$

---

# 마무리

- **좋은 RAG = (청크/색인)×(하이브리드 검색)×(재순위)×(똑똑한 패킹)** 입니다.  
- **캐시**는 성능과 비용의 생명줄, **컨텍스트 길이 관리**는 품질의 생명줄입니다.  
- 본문 코드를 복사해 **문서→청크→인덱스**를 만든 뒤, `RAGEngine`에 연결하면 즉시 동작합니다.