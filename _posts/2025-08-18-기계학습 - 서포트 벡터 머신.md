---
layout: post
title: 기계학습 - 서포트 벡터 머신
date: 2025-08-18 21:25:23 +0900
category: 기계학습
---
# 서포트 벡터 머신(Support Vector Machine, SVM)

## 0. 직관과 핵심 요약

- 목표: **마진(여유 거리)을 최대화**하는 결정 초평면을 찾아 **일반화**를 극대화한다.
- 최적화: 하드 마진은 선형 완전분리 가정, 실제는 **소프트 마진**으로 슬랙 \(\xi_i\) 허용.
- 듀얼: 결정경계는 **서포트 벡터**의 선형결합으로만 결정된다.
- 커널: \(\phi(x)\)를 명시하지 않고 **내적**만으로 비선형 경계를 학습한다.
- 실전: 스케일링, \(C\)/\(\gamma\) 조절, **클래스 불균형**(class\_weight)과 **확률 보정**(Platt) 등을 신중히.

---

## 1. 결정 초평면과 마진의 기하

### 1.1 초평면과 분류 규칙
- 초평면:
$$
\mathbf{w}^\top \mathbf{x} + b = 0
$$
- 분류:
$$
\hat{y}=\mathrm{sign}(\mathbf{w}^\top \mathbf{x}+b)
$$

### 1.2 마진의 정의와 폭
- 정규화(함수 마진을 1로 맞춤) 제약:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1
$$
- 기하 마진(두 지지 초평면 사이 거리):
$$
\text{Margin}=\frac{2}{\|\mathbf{w}\|_2}
$$

---

## 2. 하드 마진 SVM: 프라이멀 → KKT → 듀얼

### 2.1 프라이멀 문제
$$
\min_{\mathbf{w}, b} \ \frac{1}{2}\|\mathbf{w}\|_2^2
\quad \text{s.t.}\quad
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1\ \ (\forall i)
$$

### 2.2 라그랑지안과 KKT 조건
- 라그랑지안:
$$
\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha})=
\frac{1}{2}\|\mathbf{w}\|^2-
\sum_{i=1}^n \alpha_i\big(y_i(\mathbf{w}^\top \mathbf{x}_i+b)-1\big),\ \ \alpha_i\ge 0
$$
- 정지 조건(Stationarity):
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{w}}=0 \Rightarrow
\mathbf{w}=\sum_i \alpha_i y_i \mathbf{x}_i,\qquad
\frac{\partial \mathcal{L}}{\partial b}=0 \Rightarrow \sum_i \alpha_i y_i=0
$$
- 보완 슬랙(Complementary Slackness):
$$
\alpha_i\big(y_i(\mathbf{w}^\top \mathbf{x}_i+b)-1\big)=0
$$

### 2.3 듀얼 문제와 결정함수
- 듀얼(볼록 이차계획):
$$
\max_{\boldsymbol{\alpha}\ge 0}\ \sum_{i=1}^n \alpha_i-\frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j\ \mathbf{x}_i^\top \mathbf{x}_j
\quad \text{s.t.}\ \sum_i \alpha_i y_i=0
$$
- 최적 \(\alpha_i>0\) 인 샘플이 **서포트 벡터**.  
- 결정함수:
$$
f(\mathbf{x})=\sum_{i\in SV}\alpha_i y_i\, \mathbf{x}_i^\top \mathbf{x} + b
$$

---

## 3. 소프트 마진 SVM: 현실 데이터에 대한 완화

### 3.1 소프트 마진 프라이멀
$$
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \ \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^n \xi_i
\quad \text{s.t.}\quad
y_i(\mathbf{w}^\top \mathbf{x}_i+b)\ge 1-\xi_i,\ \ \xi_i\ge 0
$$
- \(C>0\): **제약 위반 패널티**(큰 \(C\)는 위반에 가혹→저바이어스/고분산).

### 3.2 소프트 마진 듀얼과 제약
$$
\max_{\boldsymbol{\alpha}} \ \sum_i \alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j\,\mathbf{x}_i^\top \mathbf{x}_j
\quad \text{s.t.}\quad 0\le \alpha_i \le C,\ \sum_i \alpha_i y_i=0
$$

### 3.3 힌지 손실 관점
- 등가 풀림(정규화 형태):
$$
\min_{\mathbf{w},b} \ \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_i \max(0, 1-y_i(\mathbf{w}^\top \mathbf{x}_i+b))
$$
- 변형: **Squared Hinge**(\(\max(0,\cdot)^2\)) → 매끈하지만 이상치에 더 민감.

---

## 4. 커널 트릭: 비선형 결정경계

### 4.1 핵심 아이디어
- 어떤 \(\phi(\cdot)\)에 대해
$$
K(\mathbf{x},\mathbf{z})=\phi(\mathbf{x})^\top \phi(\mathbf{z})
$$
이면, \(\phi\)를 명시하지 않고 **내적만**으로 듀얼을 계산:
$$
f(\mathbf{x})=\sum_{i\in SV}\alpha_i y_i\, K(\mathbf{x}_i,\mathbf{x}) + b
$$

### 4.2 대표 커널과 해석

| 커널 | 식 | 직관/파라미터 |
|---|---|---|
| 선형 | $$K=\mathbf{x}^\top \mathbf{z}$$ | 고차원 희소 텍스트에 강함 |
| 다항 | $$K=(\mathbf{x}^\top \mathbf{z}+c)^d$$ | 차수 \(d\)가 복잡도, \(c\)는 오프셋 |
| RBF | $$K=\exp(-\gamma\|\mathbf{x}-\mathbf{z}\|^2)$$ | \(\gamma\)↑ → 지역적, 경계 구불거림 증가 |
| 시그모이드 | $$K=\tanh(\kappa \mathbf{x}^\top \mathbf{z}+c)$$ | 조건부 PSD(주의) |

- 실전: **RBF**가 기본 강자. \(C\)-\(\gamma\) 상호작용: \(\gamma\)가 크면 경계 복잡 → \(C\)를 낮춰 과적합 제어.

---

## 5. 회귀용 SVM: \(\varepsilon\)-SVR

### 5.1 프라이멀(ε-무감각 손실)
$$
\min_{\mathbf{w},b,\boldsymbol{\xi},\boldsymbol{\xi}^*} \ \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_i (\xi_i+\xi_i^*)
$$
$$
\text{s.t. } 
\begin{cases}
y_i - (\mathbf{w}^\top \mathbf{x}_i+b) \le \varepsilon + \xi_i \\
(\mathbf{w}^\top \mathbf{x}_i+b) - y_i \le \varepsilon + \xi_i^* \\
\xi_i,\xi_i^* \ge 0
\end{cases}
$$
- **튜닝**: \(\varepsilon\)은 잔차 허용 폭, \(C\)는 위반 패널티.

### 5.2 듀얼과 예측식
- 듀얼 변수 \(\alpha_i,\alpha_i^*\in[0,C]\):
$$
f(\mathbf{x})=\sum_{i=1}^n (\alpha_i-\alpha_i^*)\, K(\mathbf{x}_i,\mathbf{x}) + b
$$

---

## 6. 멀티클래스 SVM

- **One-vs-Rest(OVR)**: \(K\)개 클래스 → \(K\)개의 이진 SVM, 점수 최대 클래스로 예측.  
- **One-vs-One(OVO)**: \(K(K-1)/2\)개 분류기, 투표 혹은 확률 누적.  
- **Crammer–Singer**(공동최적화)도 있으나 구현/튜닝 복잡 → scikit-learn은 OVR/OVO가 표준.

---

## 7. 일반화·확률 해석·불균형 대응

### 7.1 일반화(VC 관점 직관)
- 마진 최대화는 **낮은 유효 복잡도**를 유도 → 테스트 성능 향상.

### 7.2 확률 보정(Platt Scaling / Isotonic)
- SVM 점수는 **확률이 아님**.  
- 보정:
$$
P(y=1\mid x)\approx \frac{1}{1+\exp(A f(x)+B)}
$$
- scikit-learn: `SVC(probability=True)`(내부 Platt) 또는 `CalibratedClassifierCV`.

### 7.3 클래스 불균형
- `class_weight='balanced'` 또는 가중치 딕셔너리 사용:
$$
\min \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_i w_{y_i}\,\xi_i
$$

---

## 8. 최적화·복잡도·실무 팁

### 8.1 SMO/좌표하강(듀얼)
- **SMO**는 두 변수 선택 후 해를 닫형으로 갱신 → 빠른 수렴. LIBSVM/`SVC` 기반.

### 8.2 Linear SVM (대규모·고차원)
- `LinearSVC`(liblinear, 프라이멀/이중), 또는 `SGDClassifier(loss='hinge')`(확률적 최적화).  
- **규모**: 표본 수가 매우 많고 특징이 희소/고차원 → 선형 SVM 권장.

### 8.3 스케일링과 전처리
- 거리/커널 기반 모델은 **스케일 필수**(표준화). 파이프라인으로 고정.

### 8.4 하이퍼파라미터 가이드
- RBF SVM의 기본 탐색 범위 (로그 스케일 교차검증):
  - \(C \in \{10^{-2},10^{-1},1,10,10^2,10^3\}\)
  - \(\gamma \in \{\tfrac{1}{d}, \tfrac{1}{2d}, 10^{-3},10^{-2},10^{-1},1\}\) 등
- 경향: \(\gamma\)↑ → 경계 복잡↑(과적합 위험), \(C\)↑ → 위반 패널티↑(학습오차↓/분산↑).

---

## 9. 파이썬 코드: 분류/회귀/보정/불균형/시각화

### 9.1 선형/커널 SVM, 파이프라인+그리드서치
```python
import numpy as np
from sklearn.datasets import make_classification, make_circles
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import classification_report, accuracy_score

# 1. 데이터: 선형/비선형 각각
X_lin, y_lin = make_classification(n_samples=1500, n_features=20, n_informative=5,
                                   n_redundant=0, class_sep=1.5, random_state=0)
X_nl, y_nl = make_circles(n_samples=1500, factor=0.4, noise=0.1, random_state=0)

# 2. 선형 가능 데이터: LinearSVC
Xtr, Xte, ytr, yte = train_test_split(X_lin, y_lin, test_size=0.25, random_state=0)
lin_pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LinearSVC(loss="hinge", C=1.0, random_state=0))  # 빠르고 대규모에 유리
])
lin_pipe.fit(Xtr, ytr)
print("[Linear] Acc:", accuracy_score(yte, lin_pipe.predict(Xte)))

# 3. 비선형: RBF SVC + 그리드서치
Xtr, Xte, ytr, yte = train_test_split(X_nl, y_nl, test_size=0.25, random_state=0)
rbf_pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("svc", SVC(kernel="rbf"))
])
param = {
    "svc__C": [0.1, 1, 10, 100],
    "svc__gamma": ["scale", 0.1, 0.5, 1, 2]
}
gs = GridSearchCV(rbf_pipe, param, cv=5, n_jobs=-1)
gs.fit(Xtr, ytr)
print("[RBF] Best:", gs.best_params_, " Acc:", accuracy_score(yte, gs.predict(Xte)))
print(classification_report(yte, gs.predict(Xte)))
```

### 9.2 서포트 벡터와 마진(선형 커널)
```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from numpy.linalg import norm

# 선형 커널에서만 support_vectors_와 coef_를 동시에 활용
pipe = make_pipeline(StandardScaler(), SVC(kernel="linear", C=1.0))
pipe.fit(X_lin, y_lin)

svc = pipe.named_steps["svc"]
w = svc.coef_.ravel()  # 다중클래스면 각 클래스-OVA 벡터가 존재
margin = 2.0 / norm(w)
print("지원벡터 수:", svc.support_.size, " | 추정 마진 폭:", margin)
```

### 9.3 확률 보정: CalibratedClassifierCV vs probability=True
```python
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import brier_score_loss

base = Pipeline([("scaler", StandardScaler()), ("svc", SVC(kernel="rbf", C=10, gamma="scale"))])
# 방식 1: 내부 Platt (libsvm) -> SVC(probability=True)
platt = Pipeline([("scaler", StandardScaler()), ("svc", SVC(kernel="rbf", C=10, gamma="scale", probability=True))])

# 방식 2: 외부 보정
cal = CalibratedClassifierCV(base, method="isotonic", cv=5)

Xtr, Xte, ytr, yte = train_test_split(X_nl, y_nl, test_size=0.3, random_state=0)
platt.fit(Xtr, ytr); cal.fit(Xtr, ytr)

from sklearn.metrics import log_loss
p1 = platt.predict_proba(Xte)[:,1]
p2 = cal.predict_proba(Xte)[:,1]
print("Platt logloss:", log_loss(yte, p1), " | Isotonic logloss:", log_loss(yte, p2))
```

### 9.4 클래스 불균형: class_weight
```python
from sklearn.utils.class_weight import compute_class_weight

# 인위적 불균형
X_imb, y_imb = make_classification(n_samples=5000, weights=[0.95, 0.05],
                                   n_features=20, n_informative=5, random_state=0)
Xtr, Xte, ytr, yte = train_test_split(X_imb, y_imb, test_size=0.3, random_state=0)

imb = Pipeline([
    ("scaler", StandardScaler()),
    ("svc", SVC(kernel="rbf", C=10, gamma="scale", class_weight="balanced"))
])
imb.fit(Xtr, ytr)
print("Imbalanced Acc:", accuracy_score(yte, imb.predict(Xte)))
print(classification_report(yte, imb.predict(Xte)))
```

### 9.5 SVR(회귀) 예제: \(\varepsilon\), \(C\), \(\gamma\) 튜닝
```python
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

# 비선형 회귀 데이터
rng = np.random.RandomState(0)
Xr = np.sort(5 * rng.rand(400, 1) - 2.5, axis=0)
yr = np.sinc(Xr).ravel() + 0.1 * rng.randn(400)

Xtr, Xte, ytr, yte = train_test_split(Xr, yr, test_size=0.25, random_state=0)
svr = Pipeline([("scaler", StandardScaler()), ("svr", SVR(kernel="rbf"))])
param = {
    "svr__C": [1, 10, 100, 1000],
    "svr__gamma": ["scale", 0.1, 0.5, 1, 2],
    "svr__epsilon": [0.01, 0.05, 0.1, 0.2]
}
gsr = GridSearchCV(svr, param, cv=5, n_jobs=-1)
gsr.fit(Xtr, ytr)
yp = gsr.predict(Xte)
print("SVR best:", gsr.best_params_, " | RMSE:", mean_squared_error(yte, yp, squared=False), " | R2:", r2_score(yte, yp))
```

### 9.6 2D 결정경계 시각화 스니펫(연습용)
```python
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

def plot_2d_decision(model, X, y, title="Decision boundary"):
    x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5
    y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),
                         np.linspace(y_min, y_max, 400))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(["#99ccff","#ff9999"]))
    plt.scatter(X[:,0], X[:,1], c=y, s=10, cmap=ListedColormap(["#0066cc", "#cc0000"]))
    plt.title(title); plt.show()

# 예: 비선형 데이터에 RBF SVC 학습 후 호출
X, y = make_circles(n_samples=1000, factor=0.4, noise=0.1, random_state=1)
model = Pipeline([("scaler", StandardScaler()), ("svc", SVC(kernel="rbf", C=10, gamma=1))]).fit(X, y)
plot_2d_decision(model, X, y, "RBF-SVM on two circles")
```

---

## 10. 수학 보충: 소프트 마진 듀얼 유도 스케치

- 프라이멀 라그랑지안(슬랙 포함, \(\mu_i\)는 \(\xi_i\ge0\) 제약 승수):
$$
\mathcal{L}=\frac{1}{2}\|\mathbf{w}\|^2 + C\sum_i \xi_i
 -\sum_i \alpha_i\big(y_i(\mathbf{w}^\top \mathbf{x}_i+b)-1+\xi_i\big) - \sum_i \mu_i \xi_i
$$
- 정지 조건:
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{w}}=0 \Rightarrow \mathbf{w}=\sum_i \alpha_i y_i \mathbf{x}_i,\quad
\frac{\partial \mathcal{L}}{\partial b}=0 \Rightarrow \sum_i \alpha_i y_i=0,\quad
\frac{\partial \mathcal{L}}{\partial \xi_i}=0 \Rightarrow \alpha_i+\mu_i=C
$$
- \(\mu_i\ge 0\) 로부터 \(0\le \alpha_i \le C\). 이를 대입해 듀얼(위 3.2)을 얻는다.

---

## 11. 흔한 함정과 디버깅

1. **스케일링 누락** → 경계 왜곡/수렴 저하. 파이프라인 사용.  
2. **\(\gamma\) 과대**(RBF) → 경계가 지나치게 구불거림, 훈련완벽/테스트저하.  
3. **\(C\) 과대** → 위반에 과민, 이상치에 과적합.  
4. **확률 필요** 시 `probability=True` 또는 외부 보정 사용(속도/정확도 트레이드오프).  
5. **대규모 희소 텍스트** → `LinearSVC`/`SGDClassifier` 우선 검토.  
6. **클래스 불균형** → `class_weight`, 임계값 조정, PR-AUC 모니터링.

---

## 12. 요약

- SVM은 **마진 최대화**라는 기하학적 원리와 **라그랑주 듀얼-커널 트릭**이 결합된 견고한 알고리즘이다.  
- 현실에서는 **소프트 마진**을 쓰며, 결정경계는 **서포트 벡터**로 완전히 기술된다.  
- **RBF 커널 + 스케일링 + 교차검증**이 안전한 기본 시작점.  
- 회귀는 **SVR(ε-무감각 손실)**로 확장, 멀티클래스는 **OVR/OVO** 실무 표준.  
- 하이퍼파라미터 \(C\)·\(\gamma\)와 데이터 스케일, 불균형·확률 보정 등을 함께 고려해 **일반화 성능**을 확보하라.
