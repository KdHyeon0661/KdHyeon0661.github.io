---
layout: post
title: ê¸°ê³„í•™ìŠµ - ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ 
date: 2025-08-18 21:25:23 +0900
category: ê¸°ê³„í•™ìŠµ
---
# ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (Support Vector Machine, SVM)

**ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (SVM)**ì€ ì§€ë„í•™ìŠµ ê¸°ë°˜ì˜ ê°•ë ¥í•œ ë¶„ë¥˜(Classification) ë° íšŒê·€(Regression) ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.  
ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” **ë°ì´í„°ë¥¼ ìµœëŒ€ margin(ì—¬ìœ  ê±°ë¦¬)**ì„ ê°€ì§€ë„ë¡ ë¶„ë¦¬í•˜ëŠ” **ê²°ì • ê²½ê³„(Hyperplane)**ë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.

---

## 1. ê°œë…

### (1) ëª©í‘œ
- ë°ì´í„°ê°€ ì„ í˜•ì ìœ¼ë¡œ êµ¬ë¶„ ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •í•˜ë©´,  
  **í´ë˜ìŠ¤ ê°„ ê±°ë¦¬ë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” ì´ˆí‰ë©´(Hyperplane)**ì„ ì°¾ìŒ
- ì—¬ìœ  ê±°ë¦¬(Margin)ë¥¼ ìµœëŒ€í™”í•˜ë©´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

---

### (2) ì´ˆí‰ë©´(Hyperplane)
nì°¨ì› ê³µê°„ì—ì„œ ì´ˆí‰ë©´ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë©ë‹ˆë‹¤:
$$
\mathbf{w}^\top \mathbf{x} + b = 0
$$
- \(\mathbf{w}\): ê°€ì¤‘ì¹˜ ë²¡í„° (ì´ˆí‰ë©´ì˜ ë²•ì„  ë²¡í„°)
- \(b\): í¸í–¥(Bias)
- \(\mathbf{x}\): ì…ë ¥ ë²¡í„°

---

## 2. ë§ˆì§„(Margin)ê³¼ ìµœì í™” ë¬¸ì œ

### (1) ë§ˆì§„ ì •ì˜
- ë§ˆì§„ = ë‘ í´ë˜ìŠ¤ì˜ **ì„œí¬íŠ¸ ë²¡í„°(Support Vector)** ì‚¬ì´ì˜ ê±°ë¦¬
- ì„œí¬íŠ¸ ë²¡í„°: ê²°ì • ê²½ê³„ì— ê°€ì¥ ê°€ê¹Œì´ ìˆëŠ” ë°ì´í„° í¬ì¸íŠ¸

ë§ˆì§„ í­:
$$
\text{Margin} = \frac{2}{\|\mathbf{w}\|}
$$

---

### (2) ìµœì í™” ë¬¸ì œ (í•˜ë“œ ë§ˆì§„ SVM)
ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥í•œ ê²½ìš°:
$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2
$$
subject to:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1, \quad \forall i
$$
- \(y_i \in \{-1, 1\}\)

---

### (3) ì†Œí”„íŠ¸ ë§ˆì§„ SVM
ì‹¤ì œ ë°ì´í„°ëŠ” ì™„ë²½íˆ ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ â†’ **Slack ë³€ìˆ˜ \(\xi_i\)** ì¶”ê°€:
$$
\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i
$$
subject to:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
$$
- \(C\): ê·œì œ(Regularization) íŒŒë¼ë¯¸í„°, ê³¼ì í•© ë°©ì§€

---

## 3. ë¼ê·¸ë‘ì£¼ ì´ì¤‘ ë¬¸ì œ(Lagrangian Dual Form)

### (1) ë¼ê·¸ë‘ì§€ì•ˆ êµ¬ì„±
í•˜ë“œ ë§ˆì§„ ê¸°ì¤€:
$$
L(\mathbf{w}, b, \alpha) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^n \alpha_i \left[ y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \right]
$$
- \(\alpha_i \ge 0\): ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜

---

### (2) KKT ì¡°ê±´(Karush-Kuhn-Tucker)
1. **Stationarity**:
$$
\frac{\partial L}{\partial \mathbf{w}} = 0 \implies \mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i
$$
$$
\frac{\partial L}{\partial b} = 0 \implies \sum_{i=1}^n \alpha_i y_i = 0
$$

2. **Primal feasibility**:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \ge 0
$$

3. **Dual feasibility**:
$$
\alpha_i \ge 0
$$

4. **Complementary slackness**:
$$
\alpha_i \left[ y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \right] = 0
$$

---

### (3) ì´ì¤‘ ë¬¸ì œ(Dual Problem)
ìœ„ ì¡°ê±´ì„ ì ìš©í•˜ë©´:
$$
\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j
$$
subject to:
$$
\alpha_i \ge 0, \quad \sum_{i=1}^n \alpha_i y_i = 0
$$

---

## 4. ì»¤ë„ ê¸°ë²•(Kernel Trick)

### (1) í•„ìš”ì„±
- ë°ì´í„°ê°€ ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥í•  ë•Œ, ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ì„ í˜• ë¶„ë¦¬ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•¨
- ë§¤í•‘ í•¨ìˆ˜: \(\phi(\mathbf{x})\)

---

### (2) ì»¤ë„ í•¨ìˆ˜
- ë‚´ì  \(\phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j)\)ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì§€ ì•Šê³ , **ì»¤ë„ í•¨ìˆ˜ K**ë¡œ ëŒ€ì²´
- ëŒ€í‘œì ì¸ ì»¤ë„:
  - ì„ í˜• ì»¤ë„: \(K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^\top \mathbf{x}_j\)
  - ë‹¤í•­ ì»¤ë„: \(K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^\top \mathbf{x}_j + 1)^d\)
  - RBF ì»¤ë„: \(K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)\)

---

## 5. SVM ìˆ˜í•™ì  ì¦ëª… (ë§ˆì§„ ìµœëŒ€í™” â†” L2 ë…¸ë¦„ ìµœì†Œí™”)

### **ì •ë¦¬**
ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ë¬¸ì œ:
$$
\max_{\mathbf{w}, b} \frac{2}{\|\mathbf{w}\|}
$$
subject to:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1
$$
ëŠ”,  
ë‹¤ìŒ ë¬¸ì œì™€ ë™ì¹˜:
$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2
$$
subject to ë™ì¼ ì¡°ê±´.

**ì¦ëª…**:
1. ë§ˆì§„ í­ = \(\frac{2}{\|\mathbf{w}\|}\)
2. ë§ˆì§„ ìµœëŒ€í™” â†” \(\|\mathbf{w}\|\) ìµœì†Œí™”
3. ê³„ì‚° í¸ì˜ë¥¼ ìœ„í•´ \(\frac{1}{2}\|\mathbf{w}\|^2\) ìµœì†Œí™”ë¡œ ë³€í™˜ (ë¯¸ë¶„ ì‹œ ê¹”ë”)

---

## 6. íŒŒì´ì¬ ì˜ˆì œ
```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# ë°ì´í„° ë¡œë“œ
iris = datasets.load_iris()
X, y = iris.data, iris.target

# ë‹¨ìˆœí™” (ì´ì§„ ë¶„ë¥˜: 0ê³¼ 1 í´ë˜ìŠ¤ë§Œ)
X = X[y != 2]
y = y[y != 2]

# ë°ì´í„° ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SVM ëª¨ë¸ (RBF ì»¤ë„)
svm = SVC(kernel='rbf', C=1.0, gamma=0.5)
svm.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = svm.predict(X_test)

# ì •í™•ë„
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

## ğŸ“Œ ì •ë¦¬
- SVMì€ **ë§ˆì§„ì„ ìµœëŒ€í™”**í•˜ëŠ” ì´ˆí‰ë©´ì„ ì°¾ëŠ” ë¶„ë¥˜/íšŒê·€ ì•Œê³ ë¦¬ì¦˜
- í•˜ë“œ ë§ˆì§„(ì™„ë²½ ë¶„ë¦¬)ê³¼ ì†Œí”„íŠ¸ ë§ˆì§„(ì˜¤ì°¨ í—ˆìš©) ë°©ì‹ ì¡´ì¬
- ë¼ê·¸ë‘ì£¼ ì´ì¤‘ ë¬¸ì œë¡œ ë³€í™˜ ì‹œ **ì„œí¬íŠ¸ ë²¡í„°ë§Œ** ê²°ì • ê²½ê³„ì— ì˜í–¥
- **ì»¤ë„ ê¸°ë²•**ìœ¼ë¡œ ë¹„ì„ í˜• ë°ì´í„°ë„ ì²˜ë¦¬ ê°€ëŠ¥
- KKT ì¡°ê±´ê³¼ ìµœì í™” ì´ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìˆ˜í•™ì ìœ¼ë¡œ ê²¬ê³ í•œ ì•Œê³ ë¦¬ì¦˜