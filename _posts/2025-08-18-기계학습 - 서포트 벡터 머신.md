---
layout: post
title: 기계학습 - 서포트 벡터 머신
date: 2025-08-18 21:25:23 +0900
category: 기계학습
---
# 서포트 벡터 머신(Support Vector Machine, SVM)

**서포트 벡터 머신(SVM)**은 지도학습 기반의 강력한 분류(Classification) 및 회귀(Regression) 알고리즘입니다.  
기본 아이디어는 **데이터를 최대 margin(여유 거리)**을 가지도록 분리하는 **결정 경계(Hyperplane)**를 찾는 것입니다.

---

## 1. 개념

### (1) 목표
- 데이터가 선형적으로 구분 가능하다고 가정하면,  
  **클래스 간 거리를 최대로 하는 초평면(Hyperplane)**을 찾음
- 여유 거리(Margin)를 최대화하면 새로운 데이터에 대한 일반화 성능 향상

---

### (2) 초평면(Hyperplane)
n차원 공간에서 초평면은 다음과 같이 표현됩니다:
$$
\mathbf{w}^\top \mathbf{x} + b = 0
$$
- \(\mathbf{w}\): 가중치 벡터 (초평면의 법선 벡터)
- \(b\): 편향(Bias)
- \(\mathbf{x}\): 입력 벡터

---

## 2. 마진(Margin)과 최적화 문제

### (1) 마진 정의
- 마진 = 두 클래스의 **서포트 벡터(Support Vector)** 사이의 거리
- 서포트 벡터: 결정 경계에 가장 가까이 있는 데이터 포인트

마진 폭:
$$
\text{Margin} = \frac{2}{\|\mathbf{w}\|}
$$

---

### (2) 최적화 문제 (하드 마진 SVM)
선형 분리 가능한 경우:
$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2
$$
subject to:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1, \quad \forall i
$$
- \(y_i \in \{-1, 1\}\)

---

### (3) 소프트 마진 SVM
실제 데이터는 완벽히 선형 분리 불가 → **Slack 변수 \(\xi_i\)** 추가:
$$
\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i
$$
subject to:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
$$
- \(C\): 규제(Regularization) 파라미터, 과적합 방지

---

## 3. 라그랑주 이중 문제(Lagrangian Dual Form)

### (1) 라그랑지안 구성
하드 마진 기준:
$$
L(\mathbf{w}, b, \alpha) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^n \alpha_i \left[ y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \right]
$$
- \(\alpha_i \ge 0\): 라그랑주 승수

---

### (2) KKT 조건(Karush-Kuhn-Tucker)
1. **Stationarity**:
$$
\frac{\partial L}{\partial \mathbf{w}} = 0 \implies \mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i
$$
$$
\frac{\partial L}{\partial b} = 0 \implies \sum_{i=1}^n \alpha_i y_i = 0
$$

2. **Primal feasibility**:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \ge 0
$$

3. **Dual feasibility**:
$$
\alpha_i \ge 0
$$

4. **Complementary slackness**:
$$
\alpha_i \left[ y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \right] = 0
$$

---

### (3) 이중 문제(Dual Problem)
위 조건을 적용하면:
$$
\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j
$$
subject to:
$$
\alpha_i \ge 0, \quad \sum_{i=1}^n \alpha_i y_i = 0
$$

---

## 4. 커널 기법(Kernel Trick)

### (1) 필요성
- 데이터가 선형 분리 불가능할 때, 고차원 공간으로 매핑하여 선형 분리가 가능하도록 함
- 매핑 함수: \(\phi(\mathbf{x})\)

---

### (2) 커널 함수
- 내적 \(\phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j)\)를 직접 계산하지 않고, **커널 함수 K**로 대체
- 대표적인 커널:
  - 선형 커널: \(K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^\top \mathbf{x}_j\)
  - 다항 커널: \(K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^\top \mathbf{x}_j + 1)^d\)
  - RBF 커널: \(K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)\)

---

## 5. SVM 수학적 증명 (마진 최대화 ↔ L2 노름 최소화)

### **정리**
마진을 최대화하는 문제:
$$
\max_{\mathbf{w}, b} \frac{2}{\|\mathbf{w}\|}
$$
subject to:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \ge 1
$$
는,  
다음 문제와 동치:
$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2
$$
subject to 동일 조건.

**증명**:
1. 마진 폭 = \(\frac{2}{\|\mathbf{w}\|}\)
2. 마진 최대화 ↔ \(\|\mathbf{w}\|\) 최소화
3. 계산 편의를 위해 \(\frac{1}{2}\|\mathbf{w}\|^2\) 최소화로 변환 (미분 시 깔끔)

---

## 6. 파이썬 예제
```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# 데이터 로드
iris = datasets.load_iris()
X, y = iris.data, iris.target

# 단순화 (이진 분류: 0과 1 클래스만)
X = X[y != 2]
y = y[y != 2]

# 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SVM 모델 (RBF 커널)
svm = SVC(kernel='rbf', C=1.0, gamma=0.5)
svm.fit(X_train, y_train)

# 예측
y_pred = svm.predict(X_test)

# 정확도
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

## 📌 정리
- SVM은 **마진을 최대화**하는 초평면을 찾는 분류/회귀 알고리즘
- 하드 마진(완벽 분리)과 소프트 마진(오차 허용) 방식 존재
- 라그랑주 이중 문제로 변환 시 **서포트 벡터만** 결정 경계에 영향
- **커널 기법**으로 비선형 데이터도 처리 가능
- KKT 조건과 최적화 이론을 기반으로 한 수학적으로 견고한 알고리즘