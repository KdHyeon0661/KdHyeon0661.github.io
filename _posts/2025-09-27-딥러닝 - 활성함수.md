---
layout: post
title: 딥러닝 - 활성함수
date: 2025-09-27 15:25:23 +0900
category: 딥러닝
---
# 1.4 활성함수(Activation) 총정리
**스텝 · 시그모이드 · tanh · ReLU/LeakyReLU/PReLU · ELU/GELU · Softmax**  
**포화 영역/기울기 소실·폭주 · Softmax–CrossEntropy 결합 · 수치안정성(LogSumExp)**

## A. 왜 활성함수인가?
선형층(affine)만 여러 겹 쌓으면 결국 하나의 선형 변환과 동일합니다. **비선형성**을 주는 활성함수 덕분에 네트워크가 복잡한 함수를 근사할 수 있습니다.  
활성함수를 고를 때 핵심 포인트:
- **포화 영역(saturation)**: 입력이 크면 출력이 평평해져 **기울기(도함수)가 0에 가까워지는 현상** → 학습 정체.
- **출력 범위/평균**: 음수 허용(평균 0 근처), 스케일, 연속성·미분가능성.
- **수치안정성**: Softmax/시그모이드는 큰 로짓에서 **overflow/underflow** 위험 → **LogSumExp, BCEWithLogits** 등 안정형을 사용.

---

## B. 각 활성함수의 정의·도함수·특징

### B-1. 스텝(Step / Heaviside)
- **정의(이진 출력; 미분 불가)**  
  $$
  \mathrm{step}(x) =
  \begin{cases}
  1 & x \ge 0 \\
  0 & x < 0
  \end{cases}
  $$
- **특징**: 퍼셉트론의 고전적 활성. **기울기 0 (거의 모든 곳)** → **경사하강법**에 부적합.  
- **용례**: 학습 후 추론에서 **임곗값**으로만 사용(예: 확률>0.5 → 1).

> 실무 요약: **학습에 사용하지 않음.**

---

### B-2. 시그모이드(Sigmoid / Logistic)
- **정의**  
  $$
  \sigma(x)=\frac{1}{1+e^{-x}}\in(0,1)
  $$
- **도함수**  
  $$
  \sigma'(x)=\sigma(x)\big(1-\sigma(x)\big)
  $$
- **특징**
  - (0,1)로 매핑 → **이진 확률** 해석에 적합.
  - $$|x|\gg 0$$ 에서 **포화** → 기울기 소실.  
  - 평균이 0이 아님(0.5 중심) → 이전 레이어에 **평균 이동(mean shift)** 유발.
- **안정형**: 이진 분류는 **`BCEWithLogitsLoss`** 로 **시그모이드+로그손실**을 한 번에 처리(수치안정).

> 실무 요약: **출력층(이진/멀티라벨)**에만 사용. **은닉층 활성로는 비권장**(ReLU류, GELU 선호).

---

### B-3. tanh (Hyperbolic Tangent)
- **정의**  
  $$
  \tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\in(-1,1)
  $$
- **도함수**  
  $$
  \tanh'(x)=1-\tanh^2(x)
  $$
- **특징**
  - **제로 중심**(평균 0) → 학습 안정성에서 시그모이드보다 유리.
  - 여전히 **포화** 존재(큰 |x|에서 기울기 소실).  
  - RNN 고전 구현에서 많이 사용됐으나, 최근엔 **ReLU류/GELU**가 주류.

> 실무 요약: 포화/기울기 소실을 감수할 이유가 적음. **특정 구조(RNN 일부) 또는 안정성 요구**에서 선택.

---

### B-4. ReLU (Rectified Linear Unit)
- **정의(비포화, 음수 차단)**  
  $$
  \mathrm{ReLU}(x)=\max(0,x)
  $$
- **도함수**  
  $$
  \mathrm{ReLU}'(x)=
  \begin{cases}
  1 & x>0 \\
  0 & x<0
  \end{cases}
  \quad (\text{보통 }x=0 \text{에서는 }0 \text{로 정의})
  $$
- **특징**
  - 양수 영역에서 **기울기 1** → **소실 완화**, 계산 단순.
  - **Dead ReLU**: 음수만 지속 입력되면 뉴런이 0에 붙어서 **죽음**.
  - He(Kaiming) 초기화와 궁합 좋음.

> 실무 요약: **기본값**. Dead ReLU가 걱정되면 Leaky/PReLU/ELU/GELU 고려.

---

### B-5. LeakyReLU / PReLU
- **LeakyReLU 정의(음수에 작은 기울기 부여)**  
  $$
  \mathrm{LReLU}(x)=
  \begin{cases}
  x & x\ge 0\\
  \alpha x & x<0
  \end{cases},\ \ \alpha \in (0,1)
  $$
- **PReLU 정의(학습 가능한 기울기)**  
  $$
  \mathrm{PReLU}(x)=
  \begin{cases}
  x & x\ge 0\\
  a x & x<0
  \end{cases},\ \ a \text{는 학습 파라미터}
  $$
- **특징**
  - **Dead ReLU** 감소(음수에서도 기울기 존재).
  - PReLU는 데이터에 맞게 **음수 기울기**를 학습.

> 실무 요약: ReLU의 보수적 대안. **음수 정보 유지**로 **표현력** ↑.

---

### B-6. ELU (Exponential Linear Unit)
- **정의(음수에서 지수형)**  
  $$
  \mathrm{ELU}(x)=
  \begin{cases}
  x & x\ge 0\\
  \alpha\,(e^{x}-1) & x<0
  \end{cases}
  $$
- **특징**
  - 음수 영역 **부드러운 포화**(≈ \(-\alpha\)) → **평균 0 근처** 유도.
  - Dead ReLU 완화 + 부드러움, 다만 exp 연산으로 **비용↑**.

> 실무 요약: ReLU보다 부드럽고 **제로 중심** 경향. **연산비** 감당 가능하면 선택지.

---

### B-7. GELU (Gaussian Error Linear Unit)
- **정의(확률론적 게이팅; Transformer 표준)**  
  $$
  \mathrm{GELU}(x) = x \cdot \Phi(x) \quad(\Phi:\ \text{표준정규 CDF})
  $$
  **근사(빠른 구현; PyTorch default)**  
  $$
  \mathrm{GELU}(x)\approx 0.5x\left[1+\tanh\!\Big(\sqrt{\tfrac{2}{\pi}}\,(x+0.044715\,x^3)\Big)\right]
  $$
- **특징**
  - 작은 음수/양수를 **부드럽게 게이팅** → **학습 안정성·성능** 양호(특히 Transformer).
  - ReLU처럼 하드 차단이 없어 **정보 손실↓**.

> 실무 요약: **Transformer·대형 모델 기본**. CNN에서도 종종 사용.

---

### B-8. Softmax (다중 클래스 확률)
- **정의**  
  $$
  \mathrm{softmax}(\mathbf{z})_i=
  \frac{e^{z_i}}{\sum_{j} e^{z_j}}
  $$
- **특징**
  - 각 클래스 **확률 분포**를 제공(합=1).
  - **Cross-Entropy**와 결합 시 **안정형(log-softmax)** 사용.

---

## C. 포화, 기울기 소실·폭주(Exploding), 초기화와의 관계

### C-1. 포화 영역과 소실
- **시그모이드/tanh**: $$|x|\gg 0$$에서 출력이 평평 → $$\sigma'(x)\approx 0,\ \tanh'(x)\approx 0$$ → **역전파 기울기 소실**.
- 깊은 네트워크에서 **연쇄법칙**으로 미세한 기울기가 **곱**을 통해 더 작아짐:
  $$
  \left\|\frac{\partial \mathcal{L}}{\partial \mathbf{x}}\right\|
  = \Big\|\prod_{\ell}\mathbf{J}_\ell\Big\|
  $$
  각 층 야코비안 노름이 1보다 작으면 **소실**, 크면 **폭주**.

### C-2. ReLU류와 소실 완화
- ReLU 양수에서 도함수 1 → 포화 없음(음수는 0).  
- **He 초기화**: ReLU 계열에서 **분산 보존**을 위해
  $$
  \mathrm{Var}[w]\approx \frac{2}{\mathrm{fan\_in}}
  $$

### C-3. Leaky/PReLU/ELU/GELU의 의미
- 음수에서도 기울기(또는 부드러운 게이팅) 제공 → **dead unit 감소** + **평균 0 근처** → **수렴 안정**.

---

## D. Softmax–CrossEntropy 결합과 수치안정성(LogSumExp)

### D-1. Cross-Entropy(다중 클래스, 1-hot 라벨)
- 로짓 $$\mathbf{z}\in\mathbb{R}^K$$, 정답 라벨 $$y\in\{1,\dots,K\}$$.
- **CE 손실(안정형 표현)**  
  $$
  \ell(\mathbf{z},y)=\log\!\Big(\sum_{j=1}^{K} e^{z_j}\Big) - z_y
  $$
  여기서
  $$
  \log\!\Big(\sum_{j} e^{z_j}\Big) = m + \log\!\Big(\sum_{j} e^{z_j-m}\Big),\quad m=\max_j z_j
  $$
  → **LogSumExp 트릭**으로 overflow 방지.

### D-2. 그라디언트(핵심 결과)
- 소프트맥스 확률 $$p_j=\frac{e^{z_j}}{\sum_k e^{z_k}}$$.  
  **미분 결과**  
  $$
  \frac{\partial \ell}{\partial z_j} = p_j - \mathbb{1}\{j=y\}
  $$
  즉, **예측확률−정답 1-hot**.

### D-3. 이진 분류의 안정형(BCEWithLogits)
- 이진 라벨 $$y\in\{0,1\}$$, 로짓 $$z$$.  
  **안정형 BCE**  
  $$
  \ell(z,y)=\max(z,0) - zy + \log\!\big(1+e^{-|z|}\big)
  $$
  직접 시그모이드 후 로그를 취하지 말고, **프레임워크의 로짓 입력 손실**을 사용.

---

## E. PyTorch로 보는 활성함수 사용 예

### E-1. 기본 사용(은닉층/출력층)
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SmallNet(nn.Module):
    def __init__(self, d_in=128, d_hid=256, d_out=10, act="relu"):
        super().__init__()
        self.fc1 = nn.Linear(d_in, d_hid)
        self.fc2 = nn.Linear(d_hid, d_out)
        if act == "relu":
            self.act = nn.ReLU()
        elif act == "leaky":
            self.act = nn.LeakyReLU(negative_slope=0.1)
        elif act == "prelu":
            self.act = nn.PReLU()  # learnable slope
        elif act == "elu":
            self.act = nn.ELU(alpha=1.0)
        elif act == "gelu":
            self.act = nn.GELU()   # tanh-approx by default
        elif act == "tanh":
            self.act = nn.Tanh()
        elif act == "sigmoid":
            self.act = nn.Sigmoid()
        else:
            raise ValueError("unknown act")

    def forward(self, x):
        x = self.act(self.fc1(x))
        logits = self.fc2(x)  # 다중분류 → CrossEntropyLoss와 결합(로짓 그대로)
        return logits

# 다중분류: CrossEntropyLoss는 내부적으로 log-softmax + NLLLoss(안정형)
model = SmallNet(act="gelu")
crit  = nn.CrossEntropyLoss()  # targets: int64 class ids
x = torch.randn(32, 128)
y = torch.randint(0, 10, (32,))
logits = model(x)
loss = crit(logits, y)
loss.backward()
```

### E-2. 이진/멀티라벨 출력(시그모이드 + BCEWithLogits)
```python
class BinaryHead(nn.Module):
    def __init__(self, d_in=128, d_hid=256):
        super().__init__()
        self.fc1 = nn.Linear(d_in, d_hid)
        self.fc2 = nn.Linear(d_hid, 1)  # 로짓 1개
        self.act = nn.ReLU()

    def forward(self, x):
        x = self.act(self.fc1(x))
        z = self.fc2(x).squeeze(1) # (B,)
        return z  # 로짓 반환

model = BinaryHead()
bce = nn.BCEWithLogitsLoss()   # z(로짓) 직접 입력
x = torch.randn(16, 128)
y = torch.randint(0, 2, (16,)).float()
z = model(x)
loss = bce(z, y)
loss.backward()
```

### E-3. 수치안정 CE 직접 구현(검증용)
```python
def stable_ce(logits, target):  # target: int64 class id
    # logits: (B, K)
    B, K = logits.shape
    m = logits.max(dim=1, keepdim=True).values
    logsumexp = m + torch.log(torch.exp(logits - m).sum(dim=1, keepdim=True))
    # gather z_y
    zy = logits.gather(1, target.view(-1,1))
    loss = (logsumexp - zy).mean()
    return loss

logits = torch.randn(8, 5)
target = torch.randint(0, 5, (8,))
l1 = stable_ce(logits, target)
l2 = nn.CrossEntropyLoss()(logits, target)
print((l1 - l2).abs().item())  # ~0에 가깝다
```

---

## F. 작은 실험: 포화/소실을 눈으로 확인

### F-1. 시그모이드·tanh 포화 구간의 기울기 통계
```python
import torch

x = torch.linspace(-10, 10, steps=2001)  # 넓은 입력 범위
sig = torch.sigmoid(x)
tanh = torch.tanh(x)

sig_grad = sig * (1 - sig)        # analytic
tanh_grad = 1 - tanh**2

print("sigmoid grad min/max:", sig_grad.min().item(), sig_grad.max().item())
print("tanh    grad min/max:", tanh_grad.min().item(), tanh_grad.max().item())
# 관찰: 중앙근처에서만 기울기 큼, ±큰 x에서는 ~0 (포화)
```

### F-2. ReLU Dead 문제 재현(간단 예)
```python
import torch.nn as nn

relu = nn.ReLU()
w = nn.Linear(4, 4, bias=False)
with torch.no_grad():
    w.weight.fill_(-1.0)  # 일부러 음수 가중치

x = torch.ones(2,4)      # 입력 양수 → 선형출력은 음수 → ReLU 후 모두 0
y = relu(w(x))
print("all zeros?", (y==0).all().item())  # True → dead
```
> 해결: **LeakyReLU/PReLU/ELU/GELU** 또는 **초기화 개선/정규화**.

---

## G. 초기화 가이드(활성함수별 gain)
가중치 초기화는 **입력/출력 분산**을 적절히 유지하도록 설계됩니다.

- **Xavier(Glorot)**: tanh/선형에 적합  
  $$
  \mathrm{Var}[w] \approx \frac{2}{\mathrm{fan\_in}+\mathrm{fan\_out}}
  $$
- **He(Kaiming)**: ReLU 계열에 적합  
  $$
  \mathrm{Var}[w] \approx \frac{2}{\mathrm{fan\_in}}
  $$
- **LeakyReLU**의 음수 기울기 $$\alpha$$ 를 고려한 gain  
  $$
  \text{gain} \approx \sqrt{\frac{2}{1+\alpha^2}}
  $$
  PyTorch `nn.init.kaiming_*` 의 `negative_slope` 인자로 반영.

```python
import torch.nn as nn
import torch.nn.init as init

layer = nn.Linear(256, 256)
init.kaiming_normal_(layer.weight, a=0.1, nonlinearity='leaky_relu')  # LeakyReLU(α=0.1)
nn.init.zeros_(layer.bias)
```

---

## H. 배치정규화/레이어정규화와의 상호작용
- **BN(BatchNorm)**: 배치 차원 통계로 정규화 → ReLU와 궁합 좋음. 작은 배치에서는 통계 불안정.  
- **LN(LayerNorm)**: 시퀀스·Transformer에서 표준(배치 크기 독립).  
- ELU/GELU 등 **제로 중심 성향** 활성은 **평균 이동**을 줄여 정규화 부담을 낮출 수 있음.

---

## I. 상황별 선택 가이드
- **은닉층(일반)**: ReLU 기본 → **LeakyReLU/PReLU**(dead 완화), **GELU**(Transformer/대형 모델)  
- **CNN 백본**: ReLU/LeakyReLU (배치·속도 중요), 일부 네트에서는 **SiLU/Swish**도 사용(본 장 범위 밖)  
- **Transformer**: **GELU** 표준  
- **이진 출력**: **BCEWithLogitsLoss + 로짓** (시그모이드는 로스 내부 처리)  
- **다중 클래스 출력**: **CrossEntropyLoss + 로짓** (softmax는 로스 내부 처리)

---

## J. 실전 미니 프로젝트(요약 스크립트)
다양한 활성으로 같은 데이터셋을 학습해 **수렴/일반화**를 비교합니다.

```python
import torch, torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

# 2D 분류 데이터(살짝 비선형)
def make_data(n=2000, noise=0.5):
    t = torch.rand(n) * 6.283  # [0, 2π)
    r = 2.0 + torch.randn(n)*noise
    x = torch.stack([r*torch.cos(t), r*torch.sin(t)], dim=1)
    y = (t > 3.1415).long()  # 반원 분리
    perm = torch.randperm(n)
    return x[perm], y[perm]

X, y = make_data()
Xtr, ytr = X[:1600], y[:1600]
Xte, yte = X[1600:], y[1600:]

def make_model(act):
    acts = {
        "relu": nn.ReLU(),
        "leaky": nn.LeakyReLU(0.1),
        "prelu": nn.PReLU(),
        "elu": nn.ELU(),
        "gelu": nn.GELU(),
        "tanh": nn.Tanh(),
        "sigmoid": nn.Sigmoid()
    }
    return nn.Sequential(
        nn.Linear(2, 128),
        acts[act],
        nn.Linear(128, 128),
        acts[act],
        nn.Linear(128, 2)  # 로짓
    )

def train_eval(act, epochs=50, lr=1e-3):
    model = make_model(act)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)
    ce  = nn.CrossEntropyLoss()
    ds  = TensorDataset(Xtr, ytr)
    dl  = DataLoader(ds, batch_size=128, shuffle=True)
    for _ in range(epochs):
        for xb, yb in dl:
            z = model(xb)
            loss = ce(z, yb)
            opt.zero_grad(); loss.backward(); opt.step()
    with torch.no_grad():
        acc = (model(Xte).argmax(1)==yte).float().mean().item()
    return acc

for name in ["relu","leaky","prelu","elu","gelu","tanh","sigmoid"]:
    acc = train_eval(name, epochs=60)
    print(f"{name:7s} test acc: {acc:.3f}")
```

> 경향: ReLU/Leaky/PReLU/ELU/GELU 가 대체로 **시그모이드/tanh 은닉**보다 수렴·성능에서 안정적.

---

## K. 치트시트(속성 비교)

| 함수 | 범위 | 평균 중심 | 포화 | 도함수 최대 | 특기 |
|---|---|---|---|---|---|
| Step | {0,1} | X | 완전 | 0 | 경사학습 불가 |
| Sigmoid | (0,1) | X(0.5) | 양끝 포화 | 0.25 | 이진 확률 출력층 |
| tanh | (-1,1) | O | 양끝 포화 | 1 | 제로 중심, 여전히 소실 |
| ReLU | [0,∞) | X | 음수 절단 | 1 | 빠름, Dead 위험 |
| LeakyReLU | (-∞,∞) | ~O | 없음(음수 기울기) | 1 | Dead 완화 |
| PReLU | (-∞,∞) | ~O | 없음 | 1 | 음수 기울기 학습 |
| ELU | (-α,∞) | ~O | 음수 부드러운 포화 | ≤1 | 평균 0 경향, 비용↑ |
| GELU | (-∞,∞) | O | 부드러운 게이팅 | <1 | Transformer 표준 |
| Softmax | 확률단체 | - | - | - | 다중클래스 확률 |

---

## L. 자주 하는 실수와 해결책
1. **이진 분류에 `Sigmoid` + `BCELoss`(확률 입력)**  
   → **`BCEWithLogitsLoss`** 로 바꾸세요(수치안정, 로짓 입력).
2. **`Softmax` 후 `NLLLoss` 대신 `CrossEntropyLoss`에 softmax 중복 적용**  
   → `CrossEntropyLoss`는 **로짓** 입력이 정석(내부에서 log-softmax).
3. **Dead ReLU**  
   → Leaky/PReLU/ELU/GELU, 초기화/정규화 점검.
4. **폭주/소실 불안정**  
   → He/Xavier 초기화, 적절 LR/스케줄, 정규화 레이어, Gradient Clipping.

---

## M. 수식 요약(빠른 참조)
- 시그모이드  
  $$\sigma(x)=\frac{1}{1+e^{-x}},\ \ \sigma'(x)=\sigma(x)(1-\sigma(x))$$
- tanh  
  $$\tanh'(x)=1-\tanh^2(x)$$
- ReLU  
  $$\max(0,x),\ \ \mathrm{ReLU}'(x)\in\{0,1\}$$
- ELU  
  $$\alpha(e^{x}-1)\ (x<0),\ \text{else } x$$
- GELU 근사  
  $$0.5x\left[1+\tanh\!\Big(\sqrt{\tfrac{2}{\pi}}(x+0.044715x^3)\Big)\right]$$
- Softmax–CE  
  $$\ell=\log\!\sum_j e^{z_j}-z_y,\quad \frac{\partial \ell}{\partial z_j}=p_j-\mathbb{1}\{j=y\}$$
- LogSumExp  
  $$\log\!\sum_j e^{z_j} = m + \log\!\sum_j e^{z_j-m},\ m=\max_j z_j$$

---

## N. 마무리
- **은닉층 기본은 ReLU 계열**(Leaky/PReLU/ELU/GELU로 안전성·성능 조정).  
- **출력층은 과제에 맞춘 안정형 손실과 한 쌍**(이진=BCEWithLogits, 다중=CrossEntropy).  
- **수치안정성(로그-소프트맥스/LogSumExp)** 과 **초기화** 를 챙기면 대부분의 학습 이슈의 절반은 해결됩니다.
