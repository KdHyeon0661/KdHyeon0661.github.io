---
layout: post
title: 기계학습 - 릿지 회귀, 라쏘 회귀
date: 2025-08-18 18:25:23 +0900
category: 기계학습
---
# 릿지 회귀(Ridge Regression) / 라쏘 회귀(Lasso Regression)

## 정규화(Regularization) 개념 — 왜 필요한가?

### (1) 과적합과 분산 폭증

선형 회귀의 정규방정식 해
$$
\hat{\mathbf{w}}_{\text{OLS}}=(X^\top X)^{-1}X^\top \mathbf{y}
$$
는 **\(X^\top X\)가 ill-conditioned**(조건수 큼) 이거나 **특성 간 강한 상관(다중공선성)**이 있을 때 분산이 폭증한다. 작은 데이터 잡음이 계수에 크게 증폭되어 **일반화 성능이 나빠진다**.

### (2) 정규화의 원리

패널티를 추가해 **계수의 크기**(노름)를 제한 → 분산을 줄이고(안정화) 약간의 바이어스를 도입(바이어스–분산 절충).
- **L2(릿지)**: 전체 계수를 **매끄럽게 수축**
- **L1(라쏘)**: 계수 일부를 **정확히 0**으로 만들어 **변수 선택**

### (3) 페널티형 ↔ 제약형의 동치

다음 두 문제는 \(\lambda\)–\(t\) 대응관계 하에 동치이다.
$$
\min_{\mathbf{w}} \frac{1}{2n}\| \mathbf{y}-X\mathbf{w}\|_2^2 + \lambda \|\mathbf{w}\|_q
\quad \Longleftrightarrow \quad
\min_{\|\mathbf{w}\|_q\le t} \frac{1}{2n}\|\mathbf{y}-X\mathbf{w}\|_2^2
$$
여기서 \(q=2\) 는 Ridge, \(q=1\) 은 Lasso.

---

## 릿지 회귀(Ridge) — 수학적 해, 스펙트럼 해석, 베이지안 대응

### (1) 최적화 문제와 닫힌형 해

$$
\min_{\mathbf{w}} \ \frac{1}{2n}\|\mathbf{y} - X\mathbf{w}\|_2^2 + \frac{\lambda}{2}\|\mathbf{w}\|_2^2
\quad \Rightarrow \quad
\hat{\mathbf{w}}_{\text{ridge}} = (X^\top X + n\lambda I)^{-1}X^\top \mathbf{y}.
$$
> 실무 라이브러리는 보통 \(n\) 흡수하여 \((X^\top X+\lambda I)^{-1}\) 형태로 구현. 두 표현은 \(\lambda\) 스케일만 다를 뿐 본질 동일.

### (2) SVD 기반 스펙트럼 해석(축 방향 수축)

\(X=U\Sigma V^\top\) (특이값 \(\sigma_i\)) 이면
$$
\hat{\mathbf{w}}_{\text{ridge}}
= V\,\mathrm{diag}\!\left(\frac{\sigma_i}{\sigma_i^2+\lambda'}\right)U^\top \mathbf{y}
$$
(\(\lambda'\)는 스케일 조정된 \(\lambda\)).
즉, 각 특이방향에서 **수축 계수**가
$$
\underbrace{\frac{\sigma_i^2}{\sigma_i^2+\lambda'}}_{\text{shrinkage factor}}\in(0,1)
$$
로 작용한다. 작은 \(\sigma_i\)(불안정 방향)일수록 강하게 수축 → **수치적 안정**.

### (3) 유효 자유도와 GCV

릿지의 **유효 자유도**
$$
\mathrm{df}(\lambda)=\mathrm{tr}\!\left(X(X^\top X+\lambda I)^{-1}X^\top\right)=\sum_i \frac{\sigma_i^2}{\sigma_i^2+\lambda}.
$$
이를 이용한 **GCV(Generalized Cross-Validation)** 로 \(\lambda\) 선택 가능:
$$
\mathrm{GCV}(\lambda)=\frac{\| \mathbf{y}-\hat{\mathbf{y}}_\lambda\|_2^2/n}{\big(1-\mathrm{df}(\lambda)/n\big)^2}.
$$

### (4) 베이지안 해석

사전분포 \( \mathbf{w}\sim\mathcal{N}(0,\tau^2 I)\) + 가우시안 잡음 ⇒ **MAP 추정**이 릿지와 동일.
\(\lambda=\sigma^2/\tau^2\) (노이즈분산/사전분산 비)로 해석 가능.

---

## 라쏘 회귀(Lasso) — 서브그라디언트, KKT, 소프트 임계

### (1) 최적화 문제

$$
\min_{\mathbf{w}} \ \frac{1}{2n}\|\mathbf{y}-X\mathbf{w}\|_2^2 + \lambda \|\mathbf{w}\|_1.
$$
볼록이지만 비미분(0 근방) → **좌표강하법(좌표별 폐합해)**, **LARS** 등 사용.

### (2) KKT 조건(부지표 포함)

잔차 \(\mathbf{r}=\mathbf{y}-X\hat{\mathbf{w}}\) 에 대해
$$
\frac{1}{n}X_j^\top \mathbf{r} =
\begin{cases}
\lambda \,\mathrm{sign}(\hat{w}_j) & \hat{w}_j\ne 0 \\
\in [-\lambda,\lambda] & \hat{w}_j=0
\end{cases}
$$
→ 상관이 작은 변수는 **정확히 0** 이 되기 쉬움(특성선택).

### (3) 정규직교 설계(칼럼 정규직교)에서의 **소프트 임계 연산**

\(X^\top X = nI\) 이면 좌표별 닫힌형:
$$
\hat{w}_j = \mathcal{S}\!\left(\frac{1}{n}X_j^\top \mathbf{y},\, \lambda \right),\qquad
\mathcal{S}(a,\lambda)=\mathrm{sign}(a)\max(|a|-\lambda,0).
$$
즉 **Soft-thresholding**가 정확한 해.

### (4) 상관 피처·그룹 효과

강히 상관된 변수가 여럿이면 라쏘는 **그 중 일부만 선택**(불안정). 반면 릿지는 **계수들을 함께 축소**하여 **그룹 효과**. → 상관 군집이 있으면 **Elastic Net** 권장.

### (5) 지원 크기

일반적 위치 일반조건에서 라쏘의 활성변수 개수는 \( \le n \) (관측 수보다 많아지지 않음) — \(p\gg n\) (고차원)에서 특히 유용.

---

## 엘라스틱넷(Elastic Net) — 라쏘의 희소화 + 릿지의 안정화

$$
\min_{\mathbf{w}} \ \frac{1}{2n}\|\mathbf{y}-X\mathbf{w}\|_2^2
+ \lambda\left(\alpha\|\mathbf{w}\|_1 + \frac{1-\alpha}{2}\|\mathbf{w}\|_2^2\right),\quad \alpha\in[0,1].
$$
- \(\alpha=1\): Lasso, \(\alpha=0\): Ridge.
- 상관 피처 묶음(**grouping**)을 어느 정도 유지하며 **희소성**도 제공.

---

## 바이어스–분산 트레이드오프와 모델 선택

### (1) 오류 분해(개념)

예측 MSE
$$
\mathrm{MSE} = \underbrace{\mathrm{Bias}^2}_{\lambda\uparrow\Rightarrow\uparrow}
+\underbrace{\mathrm{Var}}_{\lambda\uparrow\Rightarrow\downarrow}
+\sigma^2.
$$
\(\lambda\) 증가 → 바이어스 증가, 분산 감소. **CV로 균형점**을 찾는다.

### (2) \(\lambda\) 선택 가이드

- **K-fold CV** (표준): 로그스케일 그리드.
- 릿지: **GCV**/L-curve 보조.
- 라쏘: **LARS 경로**에서 CV로 최적 점 선택.
- **1-SE 규칙**: 최적점 근처에서 더 단순한 모델 선택(해석성↑, 과적합↓).

---

## 데이터 전처리와 실전 팁

1. **표준화(Standardization)**: 정규화는 **필수**. 스케일 차이는 페널티 영향 불균형을 초래.
   - 편향(절편)에는 패널티 적용 X (일반적 구현은 자동 처리).
2. **범주형 변수**: 원-핫 인코딩 후 라쏘/엘라스틱넷 사용 → **희소성** + 선택.
3. **다중공선성**: 릿지/엘라스틱넷이 안정적.
4. **데이터 누수 방지**: `Pipeline`으로 스케일러/인코더 **훈련셋으로만 fit**.
5. **p≫n**: 라쏘/엘라스틱넷 적합. 릿지는 해 존재(폐합해) + 안정.
6. **해석**: 라쏘의 0이 **원인–결과**는 아님. 상관·표본 크기·\(\lambda\)에 크게 의존.

---

## 수학/최적화 요약 표

| 항목 | Ridge | Lasso |
|---|---|---|
| 목적함수 | $$\frac{1}{2n}\|y-Xw\|_2^2+\frac{\lambda}{2}\|w\|_2^2$$ | $$\frac{1}{2n}\|y-Xw\|_2^2+\lambda\|w\|_1$$ |
| 해 | 닫힌형(정규방정식) | 좌표강하/LARS |
| 스펙트럼 | 특이방향별 수축 \(\frac{\sigma^2}{\sigma^2+\lambda}\) | 축 회전 불변X, 소프트 임계 |
| 희소성 | ✗ | ✓ |
| 다중공선성 | 매우 강함(안정) | 변수 선택, 불안정 가능 |
| 베이지안 | \(w\sim\mathcal{N}(0,\tau^2I)\) | 라플라스 사전 |

---

## 파이썬: Ridge/Lasso/ElasticNet — 파이프라인·경로·CV

### (A) 기본 비교 (표준화 + CV)

```python
import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error

# 상관 구조를 가진 데이터(다중공선성) 생성

X, y = make_regression(n_samples=1200, n_features=60, n_informative=10,
                       noise=15.0, effective_rank=20, random_state=42)
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=1)

cv = KFold(n_splits=5, shuffle=True, random_state=0)

pipe_ridge = Pipeline([("scaler", StandardScaler()),
                       ("model", Ridge(random_state=0))])
pipe_lasso = Pipeline([("scaler", StandardScaler()),
                       ("model", Lasso(random_state=0, max_iter=5000))])
pipe_en = Pipeline([("scaler", StandardScaler()),
                    ("model", ElasticNet(random_state=0, max_iter=5000))])

gs_ridge = GridSearchCV(pipe_ridge, {"model__alpha": np.logspace(-3, 3, 15)},
                        scoring="neg_root_mean_squared_error", cv=cv, n_jobs=-1)
gs_lasso = GridSearchCV(pipe_lasso, {"model__alpha": np.logspace(-3, 1, 15)},
                        scoring="neg_root_mean_squared_error", cv=cv, n_jobs=-1)
gs_en = GridSearchCV(pipe_en, {
    "model__alpha": np.logspace(-3, 1, 10),
    "model__l1_ratio": [0.2, 0.5, 0.8]
}, scoring="neg_root_mean_squared_error", cv=cv, n_jobs=-1)

for gs in (gs_ridge, gs_lasso, gs_en):
    gs.fit(Xtr, ytr)

def report(gs, name):
    yhat = gs.best_estimator_.predict(Xte)
    rmse = mean_squared_error(yte, yhat, squared=False)
    print(name, "best params:", gs.best_params_, "RMSE:", round(rmse, 3))

report(gs_ridge, "Ridge")
report(gs_lasso, "Lasso")
report(gs_en, "ElasticNet")
```

### (B) 라쏘 경로(계수 경로 시각화)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import lasso_path
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler().fit(Xtr)
Xs = scaler.transform(Xtr)
alphas, coefs, _ = lasso_path(Xs, ytr, alphas=np.logspace(1, -3, 50))

plt.figure()
for k in range(coefs.shape[0]):
    plt.plot(alphas, coefs[k, :])
plt.xscale("log")
plt.gca().invert_xaxis()
plt.xlabel("alpha (log scale)")
plt.ylabel("coefficients")
plt.title("Lasso coefficient paths")
plt.show()
```

### (C) 좌표강하(라쏘) — 핵심 단계 직접 구현(학습용)

```python
import numpy as np

def soft_thresh(a, lam):
    if a > lam:  return a - lam
    if a < -lam: return a + lam
    return 0.0

def lasso_cd(X, y, lam=0.1, iters=100):
    n, p = X.shape
    # 표준화 가정(평균0, 분산1), y도 중심화 가정
    w = np.zeros(p)
    for _ in range(iters):
        for j in range(p):
            r = y - (X @ w) + X[:, j]*w[j]
            rho = (X[:, j] @ r) / n
            w[j] = soft_thresh(rho, lam)
    return w

# 사용 전 X, y는 반드시 중심화/스케일링 필요

```

### (D) 릿지의 SVD 의사코드(스펙트럼 수축 확인)

```python
import numpy as np

def ridge_svd(X, y, lam):
    U, S, VT = np.linalg.svd(X, full_matrices=False)
    # 수축 계수: S / (S^2 + lam)
    shrink = S / (S**2 + lam)
    return VT.T @ (shrink * (U.T @ y))
```

---

## 해석·진단: 어떤 때 무엇을 고를까?

| 상황 | 권장 |
|---|---|
| 강한 다중공선성, 모든 피처 보유 원함 | **Ridge** (안정성, 그룹 효과) |
| 고차원 희소 해석(피처 선택) 필요 | **Lasso** (희소성) |
| 상관 피처 묶음 + 희소성 | **Elastic Net** (\(\alpha\approx0.5\) 시작) |
| p≫n, 변수 스크리닝 단계 | Lasso/ElasticNet → 후보 축소 후 Ridge/OLS 재학습 |
| 해석성을 중시, 성능 비슷 | 1-SE 규칙로 더 단순한 모델 선택 |

**주의**: 라쏘의 “선택된 변수 = ‘진짜’ 중요 변수”로 단정 금지. 표본·상관·\(\lambda\)·잡음 수준에 좌우.

---

## 확장: 일반화선형모형(GLM)·로지스틱·포아송

릿지/라쏘는 **회귀** 뿐 아니라 **분류/카운트**에도 동일 원리로 적용(패널티는 동일, 손실만 교체).
- **로지스틱 L2/L1/EN**: 분류에서 과적합 제어, 변수 선택.
- **포아송 L1/L2**: 희소 카운트 모델링.

---

## 작동 원리 그림(개념 도식 텍스트)

- **L2 구면(원)** 제약과 **평균제곱오차**의 타원 등고선 접점 → 매끄러운 **축소**.
- **L1 다이아몬드** 제약과 타원 등고선 접점 → 꼭짓점(축과 만남)에서 **정확히 0** 다수.

---

## 체크리스트(실무)

1. 스케일링(표준화) + 파이프라인.
2. \(\lambda\) (또는 `alpha`)는 **로그 스케일** CV.
3. 라쏘/EN: `max_iter` 충분히, 수렴 여부 체크.
4. 계수 안정성: 부트스트랩/반복 CV로 **선택 안정도** 확인.
5. 최종 보고: CV RMSE ± 표준오차, 선택 피처 개수, 계수 표준화값.
6. 해석은 **업무 맥락**과 함께(상관·공선성 고려).

---

## 참고 수식 모음

- **Ridge 해**
  $$\hat{w}=(X^\top X+\lambda I)^{-1}X^\top y.$$
- **Ridge 예측 함수**
  $$\hat{y}=S_\lambda y,\quad S_\lambda = X(X^\top X+\lambda I)^{-1}X^\top.$$
- **유효 자유도**
  $$\mathrm{df}(\lambda)=\mathrm{tr}(S_\lambda).$$
- **Lasso KKT**
  $$
  \frac{1}{n}X_j^\top(y-X\hat{w})\in
  \begin{cases}
  \{\lambda\,\mathrm{sign}(\hat{w}_j)\}, & \hat{w}_j\neq0\\
  [-\lambda,\lambda], & \hat{w}_j=0
  \end{cases}
  $$

---

## 요약

- **Ridge(L2)**: 스펙트럼 방향별 **연속 수축**으로 수치 안정·다중공선성 완화. 베이지안 가우시안 사전과 동형.
- **Lasso(L1)**: **희소 계수**(정확한 0)로 **변수 선택**. 좌표강하/경로 기반 최적화.
- **Elastic Net**: 둘의 장점 결합 — 상관 피처 묶음 + 희소성.
- \(\lambda\) 선택은 CV/GCV/1-SE 규칙. **표준화**, 파이프라인, 데이터 누수 방지 필수.
- 해석은 계수값 뿐 아니라 **안정성**과 **업무 맥락**을 함께 본다.
