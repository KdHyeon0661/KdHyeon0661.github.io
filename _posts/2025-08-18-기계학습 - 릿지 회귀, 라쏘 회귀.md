---
layout: post
title: 기계학습 - 릿지 회귀, 라쏘 회귀
date: 2025-08-18 18:25:23 +0900
category: 기계학습
---
# 릿지 회귀(Ridge Regression) / 라쏘 회귀(Lasso Regression)

**릿지 회귀**와 **라쏘 회귀**는 **선형 회귀(Linear Regression)** 모델에 **정규화(Regularization)**를 적용하여  
과적합(Overfitting)을 방지하고 모델의 일반화 성능을 향상시키는 기법입니다.

---

## 1. 정규화(Regularization) 개념

### (1) 왜 필요한가?
- 선형 회귀는 학습 데이터에 과도하게 맞춰져서 **과적합**이 발생할 수 있음.
- 특히 **특성(Feature)** 수가 많거나 **다중공선성(Multicollinearity)**이 존재하면 가중치 값이 불안정하게 커짐.
- **정규화**는 가중치의 크기를 제한하여 모델 복잡도를 줄이고, 일반화 성능을 향상시킴.

---

### (2) 정규화 방식
1. **L2 정규화** → 릿지 회귀(Ridge Regression)
2. **L1 정규화** → 라쏘 회귀(Lasso Regression)
3. **L1 + L2 혼합** → 엘라스틱넷(Elastic Net)

---

## 2. 릿지 회귀 (Ridge Regression)

### (1) 수식
선형 회귀의 MSE 손실 함수에 L2 패널티 항을 추가:
$$
\min_w \left[ \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p w_j^2 \right]
$$
- \(\lambda\) (정규화 파라미터): 규제 강도 조절
  - \(\lambda \to 0\) → 일반 선형 회귀와 동일
  - \(\lambda \to \infty\) → 가중치가 0에 가까워짐
- L2 정규화는 **가중치를 작게 만들지만 완전히 0으로 만들지는 않음**.

---

### (2) 특징
- 다중공선성 문제 완화
- 모든 특성이 조금씩 영향을 미침
- 특성 선택 기능 없음 (가중치가 0이 되지 않음)

---

## 3. 라쏘 회귀 (Lasso Regression)

### (1) 수식
선형 회귀의 MSE 손실 함수에 L1 패널티 항을 추가:
$$
\min_w \left[ \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p |w_j| \right]
$$
- L1 정규화는 **가중치를 완전히 0으로 만들 수 있음** → 특성 선택(Feature Selection) 기능

---

### (2) 특징
- 불필요한 특성의 가중치를 0으로 만들어 자동 변수 선택
- 고차원 데이터셋에서 유용
- 하지만 특성 수가 많은 경우, 성능이 릿지보다 불안정할 수 있음

---

## 4. 릿지 vs 라쏘 비교

| 구분 | 릿지 회귀 | 라쏘 회귀 |
|------|-----------|-----------|
| 정규화 종류 | L2 (가중치 제곱) | L1 (가중치 절댓값) |
| 가중치 0 가능 여부 | 불가능 | 가능 |
| 특성 선택 기능 | 없음 | 있음 |
| 다중공선성 처리 | 잘 처리 | 변수 제거 가능 |
| 해석 가능성 | 낮음 | 높음 (변수 줄이므로) |

---

## 5. 엘라스틱넷 (Elastic Net)
- L1과 L2를 모두 적용:
$$
\min_w \left[ MSE + \lambda_1 \sum |w_j| + \lambda_2 \sum w_j^2 \right]
$$
- Lasso의 변수 선택 기능과 Ridge의 안정성을 결합

---

## 6. 파이썬 예제
```python
import numpy as np
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 데이터 생성
np.random.seed(0)
X = np.random.randn(100, 5)
y = X @ np.array([1.5, 0, -3, 0, 2]) + np.random.randn(100) * 0.5

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Ridge 회귀
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)

# Lasso 회귀
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)

print("Ridge 회귀 MSE:", mean_squared_error(y_test, y_pred_ridge))
print("Lasso 회귀 MSE:", mean_squared_error(y_test, y_pred_lasso))
print("Ridge 가중치:", ridge.coef_)
print("Lasso 가중치:", lasso.coef_)
```

---

## 7. 활용 예시
- **Ridge 회귀**:
  - 변수들이 모두 의미 있다고 판단될 때
  - 다중공선성 문제 해결이 필요할 때
- **Lasso 회귀**:
  - 변수 선택이 필요한 경우
  - 고차원 희소 데이터(예: 텍스트 벡터화 데이터)

---

## 📌 정리
- 릿지 회귀: L2 정규화 → 모든 가중치 축소, 변수 선택 없음
- 라쏘 회귀: L1 정규화 → 일부 가중치 0, 변수 선택 가능
- 두 방법 모두 과적합 방지와 모델 일반화 향상에 기여
- 엘라스틱넷은 두 방식의 장점을 결합