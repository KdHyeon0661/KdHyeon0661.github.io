---
layout: post
title: 딥러닝 - 문서 기반 RAG 질의응답 + 근거
date: 2025-10-12 20:25:23 +0900
category: 딥러닝
---
# 문서 기반 RAG 질의응답 + 근거 하이라이트 (PyTorch 순수 구현)

> 목표
> - **문서 기반 RAG(Retrieval-Augmented Generation)** 파이프라인을 **PyTorch만**으로 끝까지 구현합니다.
> - 인덱싱(청킹·임베딩) → 리트리벌(코사인 Top-K) → **리랭킹(간단 Cross-Encoder)** → **리더(질문-증거 스팬 추출)** → **근거 하이라이트(HTML `<mark>`/오프셋)**까지 작동하는 **엔드투엔드 예제**를 제공합니다.
> - 외부 프레임워크(FAISS, scikit-learn, Hugging Face 등) 없이 **표준 라이브러리 + PyTorch**만 사용합니다.
> - 생성형 모델 없이도 **추출형 QA**로 신뢰 가능한 답변 & **출처 하이라이트**를 제공합니다(LLM 접속 전 단계의 안전한 MVP).

---

## 큰 그림 (RAG with Evidence Highlight)

```
문서 수집 → 청킹 → 임베딩(문단/청크) → 벡터 인덱스(코사인) → Top-K 리트리벌
   └──(옵션) 메타/키워드 인덱스 병행
   └→ 리랭킹(질문+청크 Cross Scoring)
      └→ 리더(질문-청크 토큰 매칭으로 스팬 추출)
         └→ 답변 문자열 + 근거 스팬(오프셋/문장) + 하이라이트 HTML
```

- **리더(Reader)**는 학습 없이도 동작하도록 설계:
  질문 임베딩과 청크 토큰 임베딩의 **유사도 행렬** \(S\in\mathbb{R}^{|Q|\times|C|}\)을 만들고, 열 방향 집계로 토큰별 점수를 만든 후 **(start, end)** 스코어 맵을 구성 → **최적 스팬**을 선택.
- **근거 하이라이트**는 스팬 오프셋을 HTML `<mark>`로 감싸거나, 문장 중요도에 따라 **문장 레벨 하이라이트**도 병행.

---

## 데이터 로더/청킹

- **권장 청크 크기**: 400~800 토큰(문장/단락 경계를 최대한 유지), **오버랩** 60~120 토큰.
- **메타데이터**: `doc_id`, `chunk_id`, `char_start/end`, `title`, `source`.

```python
# rag_chunk.py

import re, textwrap
from typing import List, Dict, Tuple

_SENT = re.compile(r'(?<=[.!?])\s+')
_WS   = re.compile(r'\s+')

def simple_sent_split(text: str) -> List[str]:
    sents = _SENT.split(text.strip())
    return [s.strip() for s in sents if s.strip()]

def whitespace_tokenize(text: str) -> List[str]:
    return [t for t in _WS.split(text.strip()) if t]

def chunk_by_tokens(text: str, max_tokens: int = 512, overlap: int = 80) -> List[Dict]:
    """
    문장 경계를 최대한 보존하면서 토큰 기준으로 청킹.
    반환: [ {chunk_text, tokens, token_offsets, char_start, char_end}, ... ]
    """
    sents = simple_sent_split(text)
    out = []
    buf_tokens, buf_offsets = [], []
    char_cursor = 0

    # 전체 텍스트의 전역 문자 오프셋 추적
    for s in sents:
        # 문장 내 token과 char offset
        start_local = 0
        for m in re.finditer(r'\S+', s):
            tok = m.group(0); a, b = m.start(), m.end()
            buf_tokens.append(tok)
            # 전역 char 오프셋 = 누적 char_cursor + 문장 내 오프셋
            buf_offsets.append((char_cursor + a, char_cursor + b))
        char_cursor += len(s) + 1  # 대략적 공백

        # 채워지면 배출
        while len(buf_tokens) >= max_tokens:
            chunk_tokens = buf_tokens[:max_tokens]
            chunk_offsets = buf_offsets[:max_tokens]
            char_a = chunk_offsets[0][0]; char_b = chunk_offsets[-1][1]
            chunk_text = text[char_a:char_b]
            out.append({
                "chunk_text": chunk_text,
                "tokens": chunk_tokens,
                "token_offsets": chunk_offsets,
                "char_start": char_a,
                "char_end": char_b
            })
            # overlap 남기고 슬라이드
            buf_tokens = buf_tokens[max_tokens-overlap:]
            buf_offsets = buf_offsets[max_tokens-overlap:]

    # 남은 것 배출
    if buf_tokens:
        char_a = buf_offsets[0][0]; char_b = buf_offsets[-1][1]
        chunk_text = text[char_a:char_b]
        out.append({
            "chunk_text": chunk_text,
            "tokens": buf_tokens,
            "token_offsets": buf_offsets,
            "char_start": char_a,
            "char_end": char_b
        })
    return out

def naive_clean(text: str) -> str:
    # 표/코드 블록 등 가벼운 정리
    text = text.replace('\u3000', ' ').replace('\xa0',' ')
    text = re.sub(r'[^\S\r\n]+', ' ', text)
    return text.strip()
```

---

## 임베딩(토치 전용, 학습 없이도 안정)

**설계 목표**: 외부 모델 없이도 “질문↔문단” 유사도와 “질문↔토큰” 매칭이 **그럴듯하게** 나오도록,
- (a) **해싱 임베딩**(고정 랜덤) +
- (b) **얕은 컨텍스트 인코더**(1D CNN 또는 BiGRU)를 사용합니다.

```python
# rag_embed.py

import torch, torch.nn as nn
import math, random
from typing import List, Tuple

def build_vocab(tokens_list: List[List[str]], max_size: int = 50000, min_freq: int = 1):
    # 간단 빈도 기반 (실전은 BPE 추천이나 여기선 PyTorch-only 제약)
    from collections import Counter
    cnt = Counter()
    for toks in tokens_list: cnt.update(toks)
    vocab = ["<PAD>", "<UNK>"]
    for w, f in cnt.most_common(max_size):
        if f >= min_freq and w not in ("<PAD>","<UNK>"):
            vocab.append(w)
    stoi = {w:i for i,w in enumerate(vocab)}
    return vocab, stoi

def encode_tokens(tokens: List[str], stoi: dict, max_len: int):
    ids = [stoi.get(t, 1) for t in tokens][:max_len]  # 1: UNK
    if len(ids) < max_len: ids += [0]*(max_len-len(ids))  # 0: PAD
    return torch.tensor(ids, dtype=torch.long)

class HashEmbed(nn.Module):
    """
    - 단어 ID 대신 해시로 고정 임베딩 테이블 인덱싱(Out-of-vocab 강건)
    - 모델 학습 없이 랜덤 초기화(고정 seed)로 일관 표현
    """
    def __init__(self, dim=128, buckets=200000, seed=13):
        super().__init__()
        g = torch.Generator().manual_seed(seed)
        self.table = nn.Embedding(buckets, dim)
        with torch.no_grad():
            # Xavier 유사 초기화
            self.table.weight.uniform_(-1.0/math.sqrt(dim), 1.0/math.sqrt(dim), generator=g)
        self.buckets = buckets

    def fhash(self, token: str) -> int:
        # 빠른 고정 해시(파이썬 hash는 세션마다 달라질 수 있어 고정)
        import hashlib
        return int(hashlib.md5(token.encode()).hexdigest()[:8], 16) % self.buckets

    def forward(self, tokens: List[str]) -> torch.Tensor:
        idx = torch.tensor([self.fhash(t) for t in tokens], dtype=torch.long)
        return self.table(idx)  # [T, D]

class TinyContextEncoder(nn.Module):
    """얕은 컨텍스트 인코더(1D CNN)로 순서/국소 맥락 반영"""
    def __init__(self, dim=128, hidden=256, kernel_sizes=(3,5)):
        super().__init__()
        self.convs = nn.ModuleList([nn.Conv1d(dim, hidden, k, padding=k//2) for k in kernel_sizes])
        self.proj = nn.Linear(hidden*len(kernel_sizes), dim)
        self.act = nn.ReLU()

    def forward(self, X: torch.Tensor) -> torch.Tensor:
        # X: [T, D] → [1, D, T]
        x = X.transpose(0,1).unsqueeze(0)
        hs=[]
        for conv in self.convs:
            h = self.act(conv(x))  # [1, H, T]
            hs.append(h)
        hcat = torch.cat(hs, dim=1)   # [1, H*len(K), T]
        y = self.proj(hcat.transpose(1,2)).squeeze(0)  # [T, D]
        return y

@torch.no_grad()
def sentence_embedding(token_embeds: torch.Tensor, pool="mean"):
    # token_embeds: [T, D]
    if token_embeds.numel()==0: return torch.zeros(128)
    if pool=="mean":
        v = token_embeds.mean(dim=0)
    elif pool=="max":
        v = token_embeds.max(dim=0).values
    else:
        v = torch.cat([token_embeds.mean(0), token_embeds.max(0).values], dim=0)
    v = v / (v.norm(p=2)+1e-8)
    return v
```

> **참고**: 실제 제품에선 SBERT/Instructor/MPNet 등 사전학습 임베딩이 더 좋습니다. 여기선 **PyTorch 단독**으로도 “동작하는” 베이스라인을 구현합니다.

---

## 인덱스(코사인 Top-K, Torch만)

- **저장 구조**: `index["vecs"]`(FP32, L2-normed), `index["meta"]`(doc_id, chunk_id, text, offsets).
- 검색은 **브루트포스 점수화**(수만 청크까지 실용). 더 커지면 IVF/HNSW는 별도 구현 또는 전용 라이브러리 고려.

```python
# rag_index.py

import torch
from typing import List, Dict, Tuple

class TorchIndex:
    def __init__(self, dim=128, device="cpu"):
        self.dim = dim
        self.device = device
        self.vecs = torch.empty(0, dim, dtype=torch.float32, device=device)
        self.meta: List[Dict] = []

    @torch.no_grad()
    def add(self, vecs: torch.Tensor, metas: List[Dict]):
        # vecs: [N, D] (L2 정규화 권장)
        assert vecs.shape[0] == len(metas)
        if vecs.device != self.device:
            vecs = vecs.to(self.device)
        if self.vecs.numel() == 0:
            self.vecs = vecs.clone()
        else:
            self.vecs = torch.cat([self.vecs, vecs], dim=0)
        self.meta.extend(metas)

    @torch.no_grad()
    def search(self, q: torch.Tensor, topk=5):
        # q: [D] normalized
        if q.device != self.device: q = q.to(self.device)
        sims = (self.vecs @ q)  # [N]
        val, idx = torch.topk(sims, k=min(topk, sims.numel()))
        results=[]
        for s, i in zip(val.tolist(), idx.tolist()):
            results.append({"score": float(s), "meta": self.meta[i]})
        return results
```

---

## 리랭커(Cross-Encoder 간이판)

- 리트리버는 문단 단위 유사도, **리랭커**는 **질문+청크 상호작용을 반영**.
- 간단히 **[질문 평균벡터 `q̄`, 청크 평균벡터 `c̄`, cos(q̄,c̄), |q̄−c̄|, q̄∘c̄]**를 결합 후 **MLP 점수**. (학습이 있다면 소량 파인튜닝, 여기서는 **무학습 가중치 초기값**으로도 대략 필터 역할)

```python
# rag_rerank.py

import torch, torch.nn as nn
from typing import List, Dict

class SimpleCrossScorer(nn.Module):
    def __init__(self, dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(dim*3+2, 256), nn.ReLU(),
            nn.Linear(256, 1)
        )
        with torch.no_grad():
            for m in self.mlp:
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)

    @torch.no_grad()
    def forward(self, q_vec: torch.Tensor, c_vec: torch.Tensor):
        # q_vec, c_vec: [D] normalized
        cos = (q_vec @ c_vec).unsqueeze(0)  # [1]
        l1  = torch.abs(q_vec - c_vec)
        had = q_vec * c_vec
        feats = torch.cat([q_vec, c_vec, l1, had, cos, cos], dim=0)  # [D*3 + D + 2] = D*4+2
        return self.mlp(feats.unsqueeze(0)).squeeze(0).squeeze(-1)  # scalar
```

> **팁**: 실제로는 **미세튜닝**하면 정밀도가 눈에 띄게 올라갑니다. (여기선 무학습 가중치로 “다시 점수화”만)

---

## 리더(Reader): 질문-청크 스팬 추출(무학습)

**아이디어**
- 질문 토큰 임베딩 \(Q\in\mathbb{R}^{m\times d}\), 청크 토큰 임베딩 \(C\in\mathbb{R}^{n\times d}\).
- 유사도 \(S = Q C^\top\) 후, 질문 차원으로 **맥스/평균 집계** → 토큰별 점수 \(t\in\mathbb{R}^n\).
- 1D 컨볼루션으로 **스팬 시작/끝 점수**를 만든 뒤 **가장 높은 합(start+end−거리 패널티)** 조합을 선택.

```python
# rag_reader.py

import torch, torch.nn as nn

class SpanSelector(nn.Module):
    def __init__(self, dim=128, k=5):
        super().__init__()
        self.conv_start = nn.Conv1d(dim, 1, kernel_size=3, padding=1)
        self.conv_end   = nn.Conv1d(dim, 1, kernel_size=3, padding=1)
        self.k = k
        with torch.no_grad():
            nn.init.xavier_uniform_(self.conv_start.weight); nn.init.zeros_(self.conv_start.bias)
            nn.init.xavier_uniform_(self.conv_end.weight);   nn.init.zeros_(self.conv_end.bias)

    @torch.no_grad()
    def forward(self, Q: torch.Tensor, C: torch.Tensor):
        """
        Q: [m, d] question token embeds
        C: [n, d] chunk token embeds
        return: start_idx, end_idx, score
        """
        Qn = Q / (Q.norm(dim=-1, keepdim=True) + 1e-8)
        Cn = C / (C.norm(dim=-1, keepdim=True) + 1e-8)
        S = Qn @ Cn.T  # [m, n]
        # 질문 차원 집계
        t_max = S.max(dim=0).values        # [n]
        t_mean = S.mean(dim=0)             # [n]
        t = torch.stack([t_max, t_mean], dim=0).mean(0)  # [n]
        # 토큰별 컨텍스트 강화: C를 conv로 흘려 start/end 점수 생성
        X = Cn.transpose(0,1).unsqueeze(0)  # [1, d, n]
        s = self.conv_start(X).squeeze(0).squeeze(0) + t  # [n]
        e = self.conv_end(X).squeeze(0).squeeze(0) + t    # [n]

        # 후보 조합 탐색(간단 Top-K)
        top_s = torch.topk(s, k=min(self.k, s.numel())).indices.tolist()
        top_e = torch.topk(e, k=min(self.k, e.numel())).indices.tolist()
        best = (-1e9, 0, 0)
        for i in top_s:
            for j in top_e:
                if j < i: continue
                length_penalty = 0.01*(j - i)
                score = float(s[i] + e[j] - length_penalty)
                if score > best[0]:
                    best = (score, i, j)
        return best  # (score, start, end)
```

---

## 근거 하이라이트(오프셋/문장/HTML)

- **토큰 오프셋** → **문자 오프셋**을 이용해 **정확 하이라이트**.
- 보조로 **문장 중요도**(질문-문장 유사도) 상위 문장에 박스/밑줄 표시.

```python
# rag_highlight.py

from typing import Dict, List, Tuple
import html

def highlight_span_html(text: str, char_start: int, char_end: int, extra_spans: List[Tuple[int,int]]=[]):
    """
    text: 원문 전체
    주요 스팬 [char_start, char_end) 를 <mark>로 감싸고,
    extra_spans(문장 등)는 얇은 배경으로 감쌉니다.
    """
    def esc(s): return html.escape(s, quote=False)
    pieces=[]
    last=0
    # 보조 스팬(문장 강조)을 먼저 병합
    spans = [(char_start, char_end, "main")] + [(a,b,"aux") for a,b in extra_spans if b>a]
    spans = sorted(spans, key=lambda x: x[0])
    for a,b,typ in spans:
        a=max(a,0); b=min(b,len(text))
        if a>last: pieces.append(esc(text[last:a]))
        if typ=="main":
            pieces.append(f"<mark>{esc(text[a:b])}</mark>")
        else:
            pieces.append(f"<span style='background:rgba(255,230,150,0.35)'>{esc(text[a:b])}</span>")
        last=b
    if last<len(text): pieces.append(esc(text[last:]))
    return "".join(pieces)

def sentence_offsets(text: str) -> List[Tuple[int,int]]:
    # 매우 단순한 문장 오프셋 추출
    import re
    spans=[]
    start=0
    for m in re.finditer(r'(?<=[.!?])\s+', text):
        end=m.start()
        spans.append((start, end))
        start=m.end()
    spans.append((start, len(text)))
    return spans
```

---

## 엔진 조립 (RAGEngine)

```python
# rag_engine.py

import torch
from typing import List, Dict, Any
from rag_chunk import naive_clean, chunk_by_tokens, whitespace_tokenize
from rag_embed import HashEmbed, TinyContextEncoder, sentence_embedding
from rag_index import TorchIndex
from rag_rerank import SimpleCrossScorer
from rag_reader import SpanSelector
from rag_highlight import highlight_span_html, sentence_offsets

class RAGEngine:
    def __init__(self, dim=128, device="cpu", max_chunk_tokens=512, overlap=80):
        self.dim = dim; self.device=device
        self.embed = HashEmbed(dim=dim)               # 토큰 → 벡터
        self.ctx   = TinyContextEncoder(dim=dim)      # 컨텍스트 보강
        self.index = TorchIndex(dim=dim, device=device)
        self.rerank = SimpleCrossScorer(dim=dim)
        self.reader = SpanSelector(dim=dim)
        self.docs: Dict[str, Dict] = {}  # doc_id → {"text":..., "chunks":[...]}
        self.max_chunk_tokens=max_chunk_tokens; self.overlap=overlap

    @torch.no_grad()
    def _embed_tokens(self, tokens: List[str]) -> torch.Tensor:
        return self.ctx(self.embed(tokens))  # [T,D]

    @torch.no_grad()
    def add_document(self, doc_id: str, text: str, title: str = "", source: str = ""):
        text = naive_clean(text)
        chunks = chunk_by_tokens(text, max_tokens=self.max_chunk_tokens, overlap=self.overlap)
        metas = []
        vecs = []
        for ci, ch in enumerate(chunks):
            C = self._embed_tokens(ch["tokens"])                   # [T,D]
            cvec = sentence_embedding(C, pool="mean").float()      # [D]
            vecs.append(cvec.unsqueeze(0))
            metas.append({
                "doc_id": doc_id, "chunk_id": ci, "title": title, "source": source,
                "chunk_text": ch["chunk_text"],
                "token_offsets": ch["token_offsets"],
                "char_range": (ch["char_start"], ch["char_end"])
            })
        if len(vecs)>0:
            V = torch.cat(vecs, dim=0)  # [N,D]
            self.index.add(V, metas)
        self.docs[doc_id] = {"text": text, "title": title, "source": source, "chunks": metas}

    @torch.no_grad()
    def retrieve(self, question: str, topk=8):
        q_tokens = whitespace_tokenize(question)
        Q = self._embed_tokens(q_tokens)                     # [m,D]
        q_vec = sentence_embedding(Q, pool="mean")           # [D]
        # 1) 1차 리트리벌
        cand = self.index.search(q_vec, topk=topk)
        if not cand: return Q, q_vec, []
        # 2) 리랭킹
        scored=[]
        for c in cand:
            cvec = torch.tensor(c["meta"].get("vec",""), dtype=torch.float32)  # not stored: recompute
            # vec이 메타에 없다면 chunk_text 재임베딩
            if cvec.numel()==0:
                C = self._embed_tokens(whitespace_tokenize(c["meta"]["chunk_text"]))
                cvec = sentence_embedding(C)
            s = float(self.rerank(q_vec, cvec))
            c["rerank"] = s
            scored.append(c)
        scored = sorted(scored, key=lambda x: x["rerank"], reverse=True)
        return Q, q_vec, scored

    @torch.no_grad()
    def read_answer(self, Q: torch.Tensor, top_scored: List[Dict], max_passages=3):
        answers=[]
        for c in top_scored[:max_passages]:
            meta = c["meta"]
            toks = whitespace_tokenize(meta["chunk_text"])
            C = self._embed_tokens(toks)
            score, si, sj = self.reader(Q, C)
            # 토큰 → 문자 오프셋
            tofs = meta["token_offsets"]
            if si >= len(tofs) or sj >= len(tofs): continue
            a, b = tofs[si][0], tofs[sj][1]
            # 전역 텍스트 오프셋
            char_a = meta["char_range"][0] + (a - meta["char_range"][0])
            char_b = meta["char_range"][0] + (b - meta["char_range"][0])
            # 실제 스팬 텍스트
            full_text = meta["chunk_text"]
            # chunk 내 상대 오프셋
            span_text = full_text[a - meta["char_range"][0]: b - meta["char_range"][0]]
            answers.append({
                "doc_id": meta["doc_id"], "chunk_id": meta["chunk_id"], "span_text": span_text,
                "span_local": (a, b), "span_score": score, "chunk_text": meta["chunk_text"],
                "title": meta["title"], "source": meta["source"], "char_range": meta["char_range"]
            })
        # 최고 점수 스팬 1개를 최종 답변으로
        if not answers: return None, []
        best = max(answers, key=lambda x: x["span_score"])
        return best, answers

    def highlight(self, best: Dict):
        # 주스팬 + 문장 보조 하이라이트
        text = best["chunk_text"]
        a, b = best["span_local"]
        # chunk 내부 문장 오프셋
        sents = sentence_offsets(text)
        # 스팬을 포함하는 문장 앞뒤 한 문장씩 보조표시
        main_sent = None
        for sa, sb in sents:
            if sa <= a < sb: main_sent = (sa, sb); break
        extras=[]
        if main_sent:
            idx = sents.index(main_sent)
            if idx>0: extras.append(sents[idx-1])
            if idx+1<len(sents): extras.append(sents[idx+1])
        html = highlight_span_html(text, a, b, extra_spans=extras)
        return html
```

---

## 엔드투엔드 데모 (문서 추가 → 질의 → 답변+하이라이트)

```python
# demo_rag.py

from rag_engine import RAGEngine

if __name__=="__main__":
    engine = RAGEngine(dim=128, device="cpu", max_chunk_tokens=256, overlap=64)
    # 문서 등록 (실전은 파일/DB에서 로드)
    doc = """
    트랜스포머(Transformer)는 자기어텐션(self-attention)에 기반한 모델 구조다.
    순환 없이도 전역 컨텍스트를 포착할 수 있으며, 포지셔널 인코딩으로 순서를 주입한다.
    인코더-디코더 구조에서는 인코더가 입력을 표현으로 변환하고, 디코더는 마스킹된 어텐션으로 생성한다.
    """
    engine.add_document("doc:transformer", doc, title="Transformer 소개", source="internal_note")

    q = "트랜스포머가 순서를 표현하는 방법은?"
    Q, qv, cands = engine.retrieve(q, topk=5)
    best, alts = engine.read_answer(Q, cands, max_passages=2)
    if best:
        print("[답변]", best["span_text"])
        html = engine.highlight(best)
        # 콘솔 출력용 간단 표시(실제는 HTML로 렌더)
        print("[근거]\n", html)
    else:
        print("관련 근거를 찾지 못했습니다.")
```

**예상 출력(콘솔)**
```
[답변] 포지셔널 인코딩으로 순서를 주입한다
[근거]
 ... 트랜스포머(Transformer)는 ... <mark>포지셔널 인코딩으로 순서를 주입한다</mark>. 인코더-디코더 ...
```

---

## — `/add`, `/query`

```python
# rag_server.py

import json, argparse
from http.server import BaseHTTPRequestHandler, HTTPServer
from rag_engine import RAGEngine

ENGINE = None

class Handler(BaseHTTPRequestHandler):
    def _json(self, code, obj):
        b=json.dumps(obj, ensure_ascii=False).encode()
        self.send_response(code); self.send_header("Content-Type","application/json")
        self.send_header("Content-Length", str(len(b))); self.end_headers(); self.wfile.write(b)

    def do_POST(self):
        ln=int(self.headers.get("Content-Length","0")); raw=self.rfile.read(ln)
        try:
            req=json.loads(raw.decode())
        except Exception as e:
            self._json(400, {"error": f"bad json: {e}"}); return

        if self.path=="/add":
            doc_id=req.get("doc_id"); text=req.get("text",""); title=req.get("title",""); src=req.get("source","")
            if not doc_id or not text: self._json(400, {"error":"doc_id and text required"}); return
            ENGINE.add_document(doc_id, text, title, src)
            self._json(200, {"ok":True, "added": doc_id}); return

        if self.path=="/query":
            q=req.get("question","")
            if not q: self._json(400, {"error":"question required"}); return
            Q, qv, cands = ENGINE.retrieve(q, topk=req.get("topk",8))
            best, alts = ENGINE.read_answer(Q, cands, max_passages=req.get("max_passages",3))
            if not best: self._json(200, {"answer": None, "evidence": []}); return
            html = ENGINE.highlight(best)
            self._json(200, {
                "answer": best["span_text"],
                "doc_id": best["doc_id"], "chunk_id": best["chunk_id"],
                "evidence_html": html,
                "alternatives": [{"span_text":a["span_text"], "score":a["span_score"]} for a in alts]
            }); return

        self._json(404, {"error":"not found"})

def main(args):
    global ENGINE
    ENGINE = RAGEngine(dim=args.dim, device="cpu", max_chunk_tokens=args.max_tokens, overlap=args.overlap)
    httpd = HTTPServer(("0.0.0.0", args.port), Handler)
    print(f"RAG listening on {args.port}"); httpd.serve_forever()

if __name__=="__main__":
    ap=argparse.ArgumentParser()
    ap.add_argument("--port", type=int, default=8090)
    ap.add_argument("--dim", type=int, default=128)
    ap.add_argument("--max_tokens", type=int, default=256)
    ap.add_argument("--overlap", type=int, default=64)
    main(ap.parse_args())
```

---

## 품질/정확성 점검 (라벨 없이도 가능한 평가)

라벨(정답)이 없는 상황에서도 다음 **카나리 게이트**를 적용할 수 있습니다.

1) **리트리벌 품질(프록시)**
   - 질문-청크 **최대 토큰 유사도**(reader의 `t_max`)의 상위/하위 분위 비교 → 하위 10%는 점검/보강 큐로.
   - `문장 중요도 상위 1~2문장`이 답변 스팬 주변인지 비율.

2) **리더 일관성**
   - (다중 Top-K 청크) 서로 다른 청크에서 **스팬 텍스트가 동일/중복** 비율이 높은가? (높을수록 신뢰↑)
   - 스팬 길이 분포(너무 길면 요약/후처리, 너무 짧으면 키워드 추출에 그칠 수 있음).

3) **근거 포함성(Attribution)**
   - 답변 문자열이 **반드시 근거 스팬 내부**에 존재해야 함. (본 구현은 추출형이므로 자동 충족)
   - 클라이언트에선 “**복사 아이콘**” 대신 **출처 링크**만 허용해 **환각** 위험 차단.

> 정답 라벨이 있으면 **EM/F1**(SQuAD 스타일), **Recall@K**(정답이 포함된 청크가 Top-K에 있는지), **MRR** 등의 정식 평가를 추가하세요.

---

## 운영 체크리스트 & 모니터링 포인트

- [ ] **리트리벌 시간** p50/p95 (ms), 청크 개수/길이 분포
- [ ] **인덱스 크기**(MB), 재빌드/증분 업데이트 시간
- [ ] **Top-K 리콜**(가능 시), **스팬 길이**/유사도 분위
- [ ] **오탐 케이스 리뷰 큐** 자동 적재(낮은 유사도/지나치게 긴 스팬)
- [ ] **알람**: p95 지연↑, 인덱스 누락(문서 추가 실패), 리더 실패율(스팬 None)↑

---

## 한계와 확장

- 본 구현은 **추출형**으로 **환각 위험이 매우 낮음**. 하지만 **자유로운 요약/생성**은 제한적.
- 확장안:
  - **압축 생성**: 스팬 1~3개를 조합해 **문장 추출 요약**(문장 순서/연결어 규칙)
  - **학습 가능 리랭커**: 소량 QA 페어로 SimpleCrossScorer 미세튜닝
  - **다중 증거 결합**: 서로 다른 청크의 스팬을 연결, 각 스팬 링크 제공
  - **멀티모달**: 도표/코드 블록 감지 후 정규화(텍스트화) 규칙 추가

---

## 성능 팁

- 임베딩 차원 **128~256** 권장(메모리/속도 균형).
- 청킹 오버랩 **64~128**로 **경계 손실** 줄이기.
- 질문/문단 임베딩은 **L2 정규화** 후 코사인.
- 서버에서 **마이크로배칭**(질문 여러 개를 묶어 임베딩/리랭킹) 적용 가능.
- 인덱스는 **CPU 텐서**로 두되, 점수화 시 **GPU로 복사**해 Top-K를 빠르게 뽑는 것도 한 방법(여기선 CPU 기준).

---

## 단위테스트/리그레션(간단)

```python
# test_rag.py

from rag_engine import RAGEngine

def test_basic():
    e=RAGEngine()
    e.add_document("d1", "파이토치는 텐서 연산과 자동미분을 제공한다. RAG는 검색과 생성을 결합한다.")
    q="RAG는 무엇의 결합인가?"
    Q, qv, c=e.retrieve(q, topk=3)
    best, alts = e.read_answer(Q, c, max_passages=2)
    assert best and ("검색" in best["span_text"] or "결합" in best["span_text"])
    print("OK", best["span_text"])

if __name__=="__main__":
    test_basic()
```

---

## FAQ

**Q1. 왜 LLM 없이 추출형인가요?**
A. **근거 하이라이트와 환각 최소화**가 목적이라면 추출형이 기본값입니다. RAG의 리더를 **LLM 요약자**로 바꾸는 건 다음 단계이며, 그때도 **스팬 하이라이트**를 병행해 **출처 일치**를 검증하세요.

**Q2. 임베딩이 랜덤이면 성능이 나쁘지 않나요?**
A. 사전학습 대비 성능은 낮습니다. 그러나 **문서 내 질의응답**과 같이 범위가 좁고 문체가 유사한 경우, **해싱 임베딩 + 얕은 컨텍스트**만으로도 충분히 유의미한 베이스라인을 얻을 수 있습니다. (실전은 사전학습 임베딩 권장)

**Q3. 리랭커/리더는 학습해야 하나요?**
A. 여기선 **무학습**이지만, 소량 라벨로 **미세튜닝**하면 품질이 크게 개선됩니다. 특히 Cross-Encoder 리랭커의 개선효과가 큽니다.

**Q4. 다국어는요?**
A. 해싱 임베딩 기반이라 **언어 불문**으로 작동합니다. 단, 분절/표기 규칙 차이로 인해 토큰화·청킹 튜닝이 필요합니다.

---

## 요약

- 본 문서는 **PyTorch만**으로 구현한 **문서 기반 RAG + 근거 하이라이트**의 **작동 가능한 최소 제품(MVP)**를 제공합니다.
- **청킹/임베딩/인덱스/리랭킹/스팬 리더/하이라이트/서버**까지 엔드투엔드 코드를 포함합니다.
- 이 베이스라인을 바탕으로 **사전학습 임베딩**·**학습형 리랭커**·**요약자(생성)**를 단계적으로 도입하면, 높은 **정확성 + 신뢰성(Attribution)**을 달성할 수 있습니다.
