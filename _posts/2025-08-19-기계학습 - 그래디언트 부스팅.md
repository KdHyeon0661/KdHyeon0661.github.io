---
layout: post
title: 기계학습 - 그래디언트 부스팅
date: 2025-08-19 19:25:23 +0900
category: 기계학습
---
# 🚀 그래디언트 부스팅(Gradient Boosting)

## 1. 개요
**그래디언트 부스팅(Gradient Boosting)**은 약한 학습기(주로 얕은 결정 트리)를 순차적으로 학습시키면서, 이전 모델의 **오차(Residual Error)**를 점진적으로 보완해 나가는 **앙상블 학습 기법**입니다.  

- **랜덤 포레스트**: 독립적으로 여러 트리를 학습 후 평균/투표 → 병렬적 앙상블  
- **그래디언트 부스팅**: 순차적으로 트리를 학습하며 이전의 오차를 줄여감 → 직렬적 앙상블  

즉, 이전 모델이 틀린 부분을 집중적으로 학습하여 최종적으로 강력한 예측기를 만듭니다.

---

## 2. 기본 아이디어
### 2.1 예측 함수의 단계적 개선
목표: 예측 함수 \(F(x)\)를 점차 개선해 최적의 함수 \(\hat{F}(x)\)에 수렴시키는 것.  

초기 모델:
\[
F_0(x) = \arg \min_{\gamma} \sum_{i=1}^n L(y_i, \gamma)
\]

여기서:
- \(L(y, F(x))\): 손실 함수 (예: MSE, 로그 손실 등)
- \(F_0(x)\): 초기 예측값 (평균, 로짓 변환 등)

그 후, 각 단계 \(m\)에서 새로운 트리 \(h_m(x)\)를 추가하여 업데이트:
\[
F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)
\]

- \(\nu\): 학습률(Learning rate) → 과적합 방지 및 세밀한 보정  
- \(h_m(x)\): 잔차를 학습한 약한 학습기(보통 결정 트리)

---

### 2.2 그래디언트 기반 접근
부스팅은 단순한 잔차(실제값 - 예측값)를 학습하는 대신, **손실 함수의 그래디언트** 방향으로 개선합니다.  

잔차 대신 **음의 그래디언트(negative gradient)**를 학습:
\[
r_{im} = - \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x) = F_{m-1}(x)}
\]

즉, "손실 함수를 가장 빠르게 줄이는 방향"을 따라 새로운 트리를 학습합니다.

---

## 3. 알고리즘 절차
1. 초기 모델 \(F_0(x)\) 생성
   - 회귀: \(F_0(x) = \text{평균}(y)\)
   - 분류: 로그 오즈(log-odds)

2. 반복 (\(m = 1, 2, \dots, M\)):
   - 각 샘플 \(i\)에 대해 음의 그래디언트 계산:
     \[
     r_{im} = -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}
     \]
   - 이 \(r_{im}\)을 타깃 값으로 새로운 결정 트리 \(h_m(x)\) 학습
   - 최적의 계수 \(\gamma_m\) 계산:
     \[
     \gamma_m = \arg \min_\gamma \sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i))
     \]
   - 모델 업데이트:
     \[
     F_m(x) = F_{m-1}(x) + \nu \cdot \gamma_m h_m(x)
     \]

3. 최종 모델:
   \[
   \hat{F}(x) = F_M(x)
   \]

---

## 4. 손실 함수 예시
- 회귀 (MSE):
  \[
  L(y, F(x)) = \frac{1}{2}(y - F(x))^2
  \]
  → 그래디언트: \( r_{im} = y_i - F(x_i) \) (즉, 잔차)

- 분류 (로지스틱 손실):
  \[
  L(y, F(x)) = \log(1 + e^{-yF(x)})
  \]
  → 그래디언트: \( r_{im} = \frac{y_i}{1 + e^{y_i F(x_i)}} \)

---

## 5. 장단점

✅ 장점
- 다양한 손실 함수 적용 가능 (회귀, 분류, 순위 등).
- 높은 예측 성능 (특히 XGBoost, LightGBM, CatBoost 등 최신 구현체는 캐글 대회 우승 단골).
- 비선형 복잡한 데이터에서도 잘 작동.

❌ 단점
- 순차적 학습 → 병렬화 어려움, 학습 속도 느림.
- 트리 개수 많아지면 과적합 위험.
- 하이퍼파라미터(학습률, 트리 깊이, 트리 개수 등) 조정이 민감.

---

## 6. 하이퍼파라미터
- **n_estimators (M)**: 트리 개수 (많을수록 성능 ↑, 과적합 위험 ↑)
- **learning_rate (ν)**: 학습률 (작을수록 성능 ↑, 속도 ↓)
- **max_depth**: 각 트리 깊이 (작게 설정해 "약한 학습기" 유지)
- **subsample**: 학습에 사용하는 샘플 비율 (랜덤성 부여, 과적합 완화)

---

## 7. 파이썬 예제 (Scikit-Learn)

```python
from sklearn.datasets import load_boston
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# 데이터 불러오기
X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 모델 학습
gbr = GradientBoostingRegressor(
    n_estimators=200,    # 트리 개수
    learning_rate=0.1,   # 학습률
    max_depth=3,         # 트리 깊이
    random_state=42
)
gbr.fit(X_train, y_train)

# 예측
y_pred = gbr.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred))
```

---

## 8. 확장 알고리즘
- **XGBoost (Extreme Gradient Boosting)**  
  → 정규화 추가, 계산 최적화, 분산 환경 지원  
- **LightGBM**  
  → 히스토그램 기반 학습, 대규모 데이터 처리 최적화  
- **CatBoost**  
  → 범주형 변수 처리 강화  

이들은 모두 **그래디언트 부스팅의 변형/최적화 버전**으로, 현재 산업/연구에서 가장 널리 사용됩니다.

---

## 📌 요약
- 그래디언트 부스팅은 **순차적 트리 학습**으로 오차를 점진적으로 줄이는 앙상블 기법.
- 손실 함수의 **음의 그래디언트**를 학습하는 것이 핵심.
- 높은 성능을 보장하지만, 하이퍼파라미터 튜닝과 계산 비용이 크다.
- XGBoost, LightGBM, CatBoost는 그래디언트 부스팅의 강력한 확장판.