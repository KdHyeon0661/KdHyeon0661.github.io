---
layout: post
title: 기계학습 - 그래디언트 부스팅
date: 2025-08-19 19:25:23 +0900
category: 기계학습
---
# 그래디언트 부스팅(Gradient Boosting)

## 0. 한눈에 보는 요약(치트시트)

| 구성요소 | 핵심 |
|---|---|
| 모델형태 | **가법모형(additive model)**:  $$F_M(x)=F_0(x)+\sum_{m=1}^M \nu \gamma_m h_m(x)$$ |
| 최적화 | **Functional Gradient Descent**: 음의 그래디언트를 근사하는 트리를 순차적으로 적합 |
| 약한학습기 | 보통 **얕은 결정트리**(depth=3~8). 리프의 예측값은 선형 탐색(line search)로 갱신 |
| 정규화 | **Shrinkage(learning rate \(\nu\))**, **subsample(확률적 부스팅)**, **max_depth / min_samples_leaf** |
| 대표 손실 | 회귀: MSE/MAE/Huber/Quantile, 분류: 로지스틱(이진)/소프트맥스(다중), 순위: 랭킹 손실 |
| 장점 | 다양한 손실·강력한 예측력·특성 스케일 민감도 낮음(트리) |
| 단점 | 직렬 학습(느릴 수 있음)·튜닝 민감·나무 수 많을수록 과적합 위험 |
| 실무 팁 | **작은 \(\nu\)** + **큰 M** + **subsample** + **조기중단(Early Stopping)** + 적절한 **depth** |

---

## 1. 직관: “잔차(오차)를 줄이는 방향으로 트리를 더한다”

- 초기 예측 \(F_0\) (예: 평균/로그오즈)에서 시작해, **현재 모델이 틀린 부분**을 다음 트리가 설명한다.
- 최종적으로 **여러 약한 트리의 가중합**이 강한 예측기를 이룬다.

---

## 2. 함수최적화 관점: Functional Gradient Descent

### 2.1 가법모형과 업데이트
학습 단계 \(m=1,\dots,M\)에서
$$
F_m(x) = F_{m-1}(x) + \nu \cdot \gamma_m h_m(x)
$$
- \(h_m(x)\): **새로 추가되는 트리**(기저함수)
- \(\nu \in (0,1]\): **학습률(shrinkage)**
- \(\gamma_m\): 라인서치로 추정(리프별 상이할 수도 있음)

### 2.2 음의 그래디언트(“유사 잔차”)를 타깃으로
손실 \(L(y,F)\) 에 대해
$$
r_{im} = -\left.\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right|_{F=F_{m-1}}.
$$
- \(r_{im}\)을 목표값으로 **회귀트리 \(h_m\)** 를 적합한다(노드 분할은 \(r_{im}\) 기준).
- 리프 \(R_{jm}\)마다 **선형 탐색**으로 \(\gamma_{jm}\) 산출:
$$
\gamma_{jm} = \arg\min_{\gamma} \sum_{x_i\in R_{jm}} L\big(y_i,\, F_{m-1}(x_i)+\gamma\big).
$$

---

## 3. 손실 함수와 그래디언트(핵심식 총정리)

### 3.1 회귀

- **제곱오차(MSE)**
  $$
  L = \tfrac{1}{2}(y-F)^2,\quad r_{im} = y_i - F_{m-1}(x_i) \quad(\text{= 잔차})
  $$
  리프의 \(\gamma_{jm}\)는 **리프 내 잔차 평균**.

- **절대오차(MAE)**
  $$
  L = |y-F|,\quad r_{im} = \operatorname{sgn}\big(y_i - F_{m-1}(x_i)\big)
  $$
  리프의 \(\gamma_{jm}\)는 **리프 내 중앙값**(median).

- **Huber(혼합 강건손실)**
  임계 \(\delta\)에서 이차→일차 전환:
  $$
  L = \begin{cases}
  \tfrac{1}{2}(y-F)^2,& |y-F|\le \delta\\
  \delta|y-F| - \tfrac{1}{2}\delta^2,& \text{else}
  \end{cases}
  $$
  그래디언트는 작은 오차에선 잔차, 큰 오차에선 부호.

- **Quantile(분위수) \(\tau\in(0,1)\)**
  $$
  L = \rho_\tau(y-F)=
  \begin{cases}
  \tau(y-F),& y\ge F\\
  (1-\tau)(F-y),& y<F
  \end{cases}
  $$
  리프 \(\gamma_{jm}\)는 **\(\tau\)-분위수**(예: \(\tau=0.5\)는 중앙값).

> 실무: **허버/퀀타일**은 이상치·불균형 분포에서 **신뢰구간/강건 회귀**로 유용.

### 3.2 이진 분류(로지스틱 손실, \(y\in\{-1,+1\}\))
$$
L = \log\big(1+e^{-yF}\big), \quad
r_{im} = \frac{y_i}{1+\exp\left(y_i F_{m-1}(x_i)\right)}.
$$
- 예측확률:  $$\hat p(x)=\sigma(F(x))=\frac{1}{1+e^{-F(x)}}.$$
- 라인서치는 보통 **뉴턴 근사**나 **폐형식**(리프별 최적 \(\gamma_{jm}\))을 사용.

### 3.3 다중 분류(소프트맥스, \(K\) 클래스)
클래스별 함수 \(F_k(x)\), 원-핫 \(y_{ik}\):
$$
L = -\sum_{k=1}^K y_{ik}\log\frac{e^{F_k(x_i)}}{\sum_{t=1}^K e^{F_t(x_i)}},\quad
r_{im}^{(k)} = y_{ik} - \hat p_k(x_i),
$$
$$
\hat p_k(x) = \frac{e^{F_k(x)}}{\sum_{t} e^{F_t(x)}}.
$$
- 각 단계에 **클래스별 트리 \(h_m^{(k)}\)** 를 적합(공유 트리 구조를 쓰기도 함).

---

## 4. 약한 학습기로서 “결정트리” 설계 포인트

- **깊이(depth)**: 3~8 권장(너무 깊으면 과적합, 너무 얕으면 상호작용을 못 모형화)
- **리프 최소 샘플수**: `min_samples_leaf` ↑ → **부드러운 함수**, 강건화
- **분할 기준**: 회귀트리이므로 **분산/잔차 감소(impurity decrease)** 최대
- **max_features**: 특성 부분 샘플링으로 **분산·과적합 완화**

---

## 5. 정규화와 확률적 변형(성능/일반화의 핵심)

- **Shrinkage(학습률 \(\nu\))**: 작을수록 일반화 ↑ (대신 더 많은 트리 필요)
- **Subsample(확률적 GB)**: 각 단계 **무작위 샘플**(예: 0.5~0.9) 사용 → RF의 배깅과 유사한 **분산 감소**
- **Column Subsampling**: `max_features`로 특성 부분 선택 → **다양성 확보**
- **Early Stopping**: 검증 손실 **상승 시작** 시 중단 → **과적합 방지**
- **Tree Constraints**: 깊이/리프 수 제한, `min_impurity_decrease` 등

> 경험칙: **\(\nu=0.01\sim0.1\)** + **수백~수천 트리** + **subsample=0.5~0.8** + **depth=3~6**.

---

## 6. 알고리즘(회귀/일반손실) 의사코드

```
입력: 데이터 {(x_i,y_i)}_{i=1}^n, 손실 L, 학습률 ν, 트리개수 M
1) F0(x) = argmin_γ Σ L(y_i, γ)        # 평균/로짓 등
2) for m = 1..M:
      r_i = -∂L(y_i, F(x_i)) / ∂F |_{F=F_{m-1}}
      적합: 회귀트리 h_m(x) ← {(x_i, r_i)}
      각 리프 R_{jm}에 대해:
          γ_{jm} = argmin_γ Σ_{x_i∈R_{jm}} L(y_i, F_{m-1}(x_i)+γ)
      업데이트:
          F_m(x) = F_{m-1}(x) + ν * Σ_j γ_{jm} 1{x∈R_{jm}}
반환: F_M(x)
```

---

## 7. AdaBoost와의 관계(지수손실)

- 지수손실 \(L=\exp(-yF)\) 에 대한 **Functional GD**는 **AdaBoost**와 밀접한 동치.
- 로지스틱 손실을 쓰면 **확률적 해석/캘리브레이션**이 더 자연스럽다.

---

## 8. 평가·튜닝 전략(실무 체크리스트)

1) **검증 곡선**: 트리 수에 따른 검증손실(staged_predict) 관찰 → **조기중단 포인트** 결정
2) **학습률-트리수 트레이드오프**: \(\nu\)↓ → M↑ (일반화 ↑)
3) **깊이/리프**: 상호작용 차수 제어(깊이=상호작용 차수 상한)
4) **Subsample/Max_features**: 0.5~0.9/√p 수준 시도
5) **클래스 불균형**: `class_weight="balanced"`/샘플가중치/임계값 조정/PR-AUC 모니터
6) **손실선택**: 이상치 많으면 Huber/Quantile, 예측구간은 Quantile(0.1/0.5/0.9)
7) **특성 엔지니어링**: 트리는 비선형·상호작용을 잘 잡지만, **시간/계절/로그변환**은 여전히 도움
8) **결측/범주**: 전통 GB는 NaN 처리 불가(사전 임퓨트). 히스토그램 기반(일부 구현)은 NaN 경로학습 지원
9) **해석**: 중요도(분할이득/빈도·Permutation), PDP/ICE, SHAP로 정책적 설명성 확보

---

## 9. 예측 해석: 중요도·PDP/ICE·SHAP

- **특성 중요도**: (1) 이득(gain), (2) 분할횟수, (3) **Permutation 중요도**(권장)
- **PDP/ICE**: 개별/평균 반응 곡선으로 **변수-타깃 관계** 시각화
- **SHAP**: 개별 예측의 **기여도 분해**(로컬 설명)

---

## 10. 파이썬 실전 코드

> 아래 코드는 **scikit-learn** 표준 `GradientBoosting*` 기반. (조기중단은 `n_iter_no_change` 사용)
> *참고*: `HistGradientBoosting*`는 대규모/결측 처리에 유리하며 `early_stopping=True`를 기본 지원.

### 10.1 회귀 — MSE vs Huber vs Quantile (예측구간)
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_friedman1
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# 1. 데이터
X, y = make_friedman1(n_samples=3000, n_features=10, noise=1.0, random_state=7)
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. MSE (기본)
gbr_mse = GradientBoostingRegressor(
    loss="squared_error", learning_rate=0.05, n_estimators=1200,
    max_depth=3, subsample=0.7, max_features="sqrt",
    n_iter_no_change=20, validation_fraction=0.1, random_state=42
).fit(X_tr, y_tr)

# 3. Huber (이상치 강건)
gbr_huber = GradientBoostingRegressor(
    loss="huber", alpha=0.9, learning_rate=0.05, n_estimators=1200,
    max_depth=3, subsample=0.7, max_features="sqrt",
    n_iter_no_change=20, validation_fraction=0.1, random_state=42
).fit(X_tr, y_tr)

# 4. Quantile (예측구간: 10%, 50%, 90%)
q10 = GradientBoostingRegressor(loss="quantile", alpha=0.10, learning_rate=0.05,
                                n_estimators=1200, max_depth=3, subsample=0.7,
                                max_features="sqrt", random_state=42).fit(X_tr, y_tr)
q50 = GradientBoostingRegressor(loss="quantile", alpha=0.50, learning_rate=0.05,
                                n_estimators=1200, max_depth=3, subsample=0.7,
                                max_features="sqrt", random_state=42).fit(X_tr, y_tr)
q90 = GradientBoostingRegressor(loss="quantile", alpha=0.90, learning_rate=0.05,
                                n_estimators=1200, max_depth=3, subsample=0.7,
                                max_features="sqrt", random_state=42).fit(X_tr, y_tr)

# 5. 평가
for name, mdl in [("MSE", gbr_mse), ("Huber", gbr_huber), ("Q50", q50)]:
    pred = mdl.predict(X_te)
    print(f"{name} MSE: {mean_squared_error(y_te, pred):.4f}")

# 6. 예측구간 시각화(샘플 200개 정렬)
idx = np.argsort(q50.predict(X_te))[:200]
yp = q50.predict(X_te)[idx]
lo = q10.predict(X_te)[idx]
hi = q90.predict(X_te)[idx]
yt = y_te[idx]

plt.figure(figsize=(8,4))
plt.plot(range(len(yp)), yt, ".", label="y_true")
plt.plot(range(len(yp)), yp, "-", label="Q50")
plt.fill_between(range(len(yp)), lo, hi, alpha=0.2, label="Q10~Q90")
plt.title("Quantile GB: Prediction Interval")
plt.legend(); plt.tight_layout(); plt.show()
```

### 10.2 이진 분류 — 로지스틱 손실 + 조기중단 + 중요도·PDP
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score, RocCurveDisplay
from sklearn.inspection import PartialDependenceDisplay

# 1. 데이터 (불균형 조금 부여)
X, y = make_classification(n_samples=10000, n_features=20, n_informative=8,
                           weights=[0.7, 0.3], random_state=0)
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. 모델 (log_loss = 이진 로지스틱)
gbc = GradientBoostingClassifier(
    loss="log_loss", learning_rate=0.05, n_estimators=1500,
    max_depth=3, subsample=0.7, max_features="sqrt",
    n_iter_no_change=30, validation_fraction=0.1, random_state=42
)
gbc.fit(X_tr, y_tr)

# 3. 성능
proba = gbc.predict_proba(X_te)[:,1]
auc = roc_auc_score(y_te, proba)
print("ROC-AUC:", round(auc, 4))

RocCurveDisplay.from_predictions(y_te, proba)
plt.show()

# 4. 특성 중요도 (기본 gain 기반 + permutation 권장)
imp = gbc.feature_importances_
topk = np.argsort(imp)[::-1][:10]
print("Top-10 features:", topk, np.round(imp[topk],4))

# 5. PDP: 상위 2개 변수
fig, ax = plt.subplots(1, 2, figsize=(10,4))
PartialDependenceDisplay.from_estimator(gbc, X_te, [topk[0]], ax=ax[0])
PartialDependenceDisplay.from_estimator(gbc, X_te, [topk[1]], ax=ax[1])
plt.tight_layout(); plt.show()
```

### 10.3 학습 곡선(트리 수 vs 검증손실) 관찰 — 조기중단 근거
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss

X, y = make_classification(n_samples=6000, n_features=30, n_informative=10, random_state=7)
X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

gb = GradientBoostingClassifier(
    loss="log_loss", learning_rate=0.05, n_estimators=2000,
    max_depth=3, subsample=0.7, max_features="sqrt", random_state=42
).fit(X_tr, y_tr)

train_ll, val_ll = [], []
for proba_tr, proba_val in zip(gb.staged_predict_proba(X_tr), gb.staged_predict_proba(X_val)):
    train_ll.append(log_loss(y_tr, proba_tr))
    val_ll.append(log_loss(y_val, proba_val))

best_m = np.argmin(val_ll)+1
print("Best #trees:", best_m, " Val LogLoss:", round(val_ll[best_m-1], 4))

plt.figure(figsize=(7,4))
plt.plot(train_ll, label="train")
plt.plot(val_ll, label="val")
plt.axvline(best_m, color="r", ls="--", label="early-stop point")
plt.xlabel("#trees"); plt.ylabel("log loss"); plt.legend(); plt.tight_layout(); plt.show()
```

---

## 11. 실무 고급 주제

1) **순위학습(GBRank/LambdaRank)**: 쌍(pairwise)·리스트(listwise) 손실에서 그래디언트 부스팅 응용(검색/추천).
2) **모노토닉 제약/상호작용 제약**: 라이브러리(XGBoost/LightGBM/CatBoost 등)에서 정책적 설명가능성/규제 준수를 위해 제공.
3) **결측/범주형 처리**:
   - 전통 GB는 **임퓨트 후** 학습.
   - 히스토그램 기반/특정 구현은 **결측 방향 학습**, **범주 분할**을 네이티브 지원.
4) **스케일링**: 트리는 **특성 스케일에 둔감**, 다만 **정규화는 결측·이상치 탐지/시각화** 측면에 도움.

---

## 12. 요약

- 그래디언트 부스팅은 **음의 그래디언트를 따르는 가법모형**으로, 약한 트리를 순차적으로 쌓아 **손실을 최소화**한다.
- **손실별 그래디언트/리프 폐형식**(MSE=평균, MAE=중앙값, Quantile=분위수)을 이해하면 **튜닝 직감**이 생긴다.
- **Shrinkage + Subsample + 적절한 Depth + Early Stopping**이 **일반화 성능**의 핵심.
- **PDP/ICE/SHAP/Permutation 중요도**로 모델을 **설명 가능**하게 운용하라.
- 대규모·범주·결측·순위 등 현실적 요구는 **현대 구현체(XGBoost/LightGBM/CatBoost/HistGB)**로 확장하라.
