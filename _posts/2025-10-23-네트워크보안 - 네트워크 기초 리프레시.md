---
layout: post
title: 네트워크보안 - 네트워크 기초 리프레시
date: 2025-10-23 15:25:23 +0900
category: 네트워크보안
---
# 1. 네트워크 기초 리프레시

> 목표: **OSI ↔ TCP/IP 계층 연결**을 머릿속에 한 장으로 정리하고, **MTU/MSS/프래그먼테이션**과 **성능·지연·손실·재전송**의 상호작용을 **측정 가능한 지표**로 이해합니다.  
> 모든 실습은 **격리된 개인 랩**(로컬 네임스페이스 / Docker 브릿지 / 사설 VPC)에서 수행하세요.

---

## 1.1 OSI 7계층 & TCP/IP 스택, 캡슐화/역캡슐화

### 1.1.1 OSI ↔ TCP/IP 매핑(실용 관점)

| OSI | 키워드 | TCP/IP 대응 | 주요 예시 |
|---|---|---|---|
| L7 Application | HTTP, DNS, TLS, SMTP | Application | `curl`, `dig`, `openssl s_client` |
| L6 Presentation | 인코딩/암호화 | (Application 내 포함) | TLS 레코드, JSON/CBOR |
| L5 Session | 대화 관리 | (Application 내 포함) | HTTP/2 stream, gRPC stream |
| L4 Transport | TCP/UDP/QUIC | Transport | `tcpdump 'tcp'`, `ss -tn` |
| L3 Network | IP(IPv4/IPv6), ICMP | Internet | 라우팅, `ip route`, `icmp` |
| L2 Data Link | 이더넷, ARP, VLAN | Link | `ip -d link`, `arp -n` |
| L1 Physical | 케이블/무선 | Physical | 속도/듀플렉스, RSSI |

> 요점: 실무에서는 L6/L5가 **앱 계층**으로 녹아 있고, **TCP/IP 모델**은 4계층(호스트 ↔ 네트워크 중심)로 단순화됩니다.

### 1.1.2 캡슐화/역캡슐화 흐름 이해

#### 송신(캡슐화)
```
[L7 Payload]
   ↓ (HTTP 헤더/바디 등)
[L4] TCP 세그먼트 (src port, dst port, seq/ack, flags, window)
   ↓
[L3] IP 패킷 (src IP, dst IP, TTL, DF/MF, ID, proto)
   ↓
[L2] 이더넷 프레임 (dst MAC, src MAC, EtherType, FCS)
   ↓
[PHY]
```

#### 수신(역캡슐화)
```
PHY → L2 프레임 파싱 → L3 IP 검증/라우팅 → L4 TCP/UDP 헤더 검증 → L7 넘김
```

#### Wireshark로 눈으로 확인하기 (pcap 분석 팁)
- **Packet Details** 패널에서 `Frame → Ethernet → IP → TCP → HTTP` 순으로 펼치며 각 계층 헤더가 **중첩**되어 있음을 관찰.
- **Follow TCP Stream**: L7(HTTP) 페이로드를 재조립해서 보여줍니다.
- **Statistics → Protocol Hierarchy**: 캡처된 트래픽이 어느 계층 프로토콜로 구성되는지 비율 확인.

### 1.1.3 실습: 캡슐화 구조를 만드는 가장 작은 예

#### A) 로컬 HTTP 요청 후 패킷 보기
```bash
# 1) 임시 HTTP 서버(로컬)
python3 -m http.server 8080 &
# 2) 다른 터미널에서 요청
curl -sS http://127.0.0.1:8080/ > /dev/null
# 3) 패킷 캡처(루프백 인터페이스, OS별 이름 확인: lo/lo0/Loopback Pseudo-Interface 등)
sudo tcpdump -i lo -nn -vvv -c 10 'tcp port 8080' -w http_lo.pcap
# 4) Wireshark로 http_lo.pcap 열어 캡슐화/역캡슐화 관찰
```

#### B) 캡슐화 크기 감(IPv4/TCP 기본 헤더 크기)
- IPv4 헤더 = **20B**(옵션 없음)  
- TCP 헤더 = **20B**(옵션 없음)  
- 이더넷 헤더 = **14B**(802.1Q VLAN 태그 시 +4B)  
- **즉, L2+L3+L4 최소 오버헤드**(VLAN X) ≈ 14 + 20 + 20 = **54B** (+FCS 4B, IFG/프리앰블 제외)

---

## 1.2 MTU/MSS/프래그먼테이션, 라우팅 vs 스위칭, ARP/ND, NAT 유형

### 1.2.1 MTU, MSS, 프래그먼테이션의 상관관계

- **MTU (Maximum Transmission Unit)**: L2 프레임 페이로드 **최대 크기**(이더넷 기본 1500B).
- **MSS (Maximum Segment Size)**: TCP가 **한 세그먼트에 실을 수 있는 L7 데이터 최대 크기**.  
  - 대개 `MSS = MTU - (IP 헤더 20 + TCP 헤더 20)` = 1460B(옵션 없음, IPv4 기준).
- **프래그먼테이션**: L3(IP)가 패킷을 **쪼개어 전송**.  
  - IPv4: 라우터/송신 호스트가 분할 가능(DF 비트=0일 때).  
  - IPv6: **라우터는 분할하지 않음**(송신 호스트만 Fragmentation Header로 분할). → Path MTU Discovery 중요.

#### 실습: MSS 계산 스크립트(빠른 계산)
```python
# mss_calc.py
# MTU와 IP/TCP 옵션 크기에 따른 MSS 계산기 (IPv4 가정)
def calc_mss(mtu=1500, ip_opt=0, tcp_opt=0):
    ip_header = 20 + ip_opt
    tcp_header = 20 + tcp_opt
    return mtu - ip_header - tcp_header

# 예시
for mtu in (1500, 1492, 9000):
    print(f"MTU={mtu}, MSS(옵션X)={calc_mss(mtu)}")

# IPv6의 경우 IP 헤더=40, 고정 오버헤드가 증가
def calc_mss_ipv6(mtu=1500, tcp_opt=0):
    ip_header = 40  # IPv6
    tcp_header = 20 + tcp_opt
    return mtu - ip_header - tcp_header

print(f"IPv6 MTU=1500 → MSS≈{calc_mss_ipv6(1500)}")
```

#### MTU 미스매치 증상 & 탐지
- **증상**: 특정 경로에서만 느림, 대용량 다운로드 중 **RST/재전송 증가**, ICMP 차단 시 PMTUD 실패.
- **tcpdump 관찰**  
  - IPv4 DF=1인 큰 패킷 + ICMP “Fragmentation needed”가 안 보이면 PMTUD 실패 의심.
  - Wireshark: `ip.flags.df == 1 && tcp.len > 0` 필터로 DF 세그먼트 확인.

#### PMTUD (Path MTU Discovery)
- IPv4: DF=1로 보내고, 경로상 작은 MTU 라우터가 ICMP Type3 Code4("Frag Needed") 응답 → MSS 축소.  
- IPv6: 라우터는 프래그먼트하지 않으므로 **PMTUD 또는 PLPMTUD** 필수.

> **주의**: 일부 네트워크에서 ICMP 블록 → PMTUD 실패 → 심각한 성능 저하. 운영 정책에서 **ICMP 관련 메시지 허용**(필요 최소) 권장.

---

### 1.2.2 라우팅 vs 스위칭 (L3 vs L2)

- **스위칭(L2)**: MAC 주소 기반 **프레임 포워딩**.  
  - 학습된 MAC 테이블에 따라 포트 결정. 미학습/브로드캐스트는 **플러딩**.
- **라우팅(L3)**: IP 주소와 라우팅 테이블 기반 **패킷 포워딩**.  
  - TTL 감소, L2 헤더 재작성(디폴트 게이트웨이에서 MAC 변경), 경로 선택.

#### 실습: 라우터/스위치 동작 관찰(네임스페이스 간 라우팅)
```bash
# 간단한 라우팅 확인
ip netns exec ns-client ip route    # default via 10.10.0.1
ip netns exec ns-client ping -c 2 10.10.0.1
# 게이트웨이(ns-gw)에서 패킷이 들어오고 나갈 때 L2 MAC이 바뀌는지 tcpdump로 확인
ip netns exec ns-gw tcpdump -i veth-g -nn -e -c 10
```

---

### 1.2.3 ARP/ND (IPv4/IPv6 이웃 탐색), GARP, 이슈 포인트

- **ARP (IPv4)**: IP→MAC 매핑 질의/응답.  
  - `arp -n` 또는 `ip neigh`로 캐시 확인.  
  - **Gratuitous ARP**: 자신의 IP/MAC 정보를 브로드캐스트로 알림(충돌 감지/업데이트).
- **ND (Neighbor Discovery, IPv6)**: **ICMPv6** 기반 이웃 탐색/주소 자동 설정(SLAAC), 라우터 광고(RA).

#### 실습: ARP 테이블 변화 관찰
```bash
ip netns exec ns-client ip neigh flush all
ip netns exec ns-client ping -c 1 10.10.0.1
ip netns exec ns-client ip neigh show
# → ns-gw의 MAC이 학습된 것을 확인
```

> **운영 포인트**: ARP 스푸핑 방지(스위치의 **DAI**, 정적 ARP(제한적으로), ARP rate limit),  
> IPv6에서는 **RA 가짜 주입** 방어(라우터 가드, RA-Guard 적용).

---

### 1.2.4 NAT 유형과 동작

- **SNAT (Source NAT)**: 내부 → 외부로 나갈 때 **소스 IP 변경**(공유 공인 IP).  
- **DNAT (Destination NAT)**: 외부 → 내부 서비스에 **목적지 IP 변경**(포트 포워딩).  
- **PAT (NAPT)**: IP뿐 아니라 **포트**까지 매핑(다수 클라이언트 동시 인터넷 접속).  
- **Hairpin NAT**: 내부에서 내부 공인주소로 접근 시 NAT 장비가 다시 내부로 hairpin 포워딩.

#### 실습: 간단 SNAT (참고용, 실제 인터페이스명 수정)
```bash
# ns-gw를 라우터로 쓰고, 호스트 eth0이 외부라고 가정
# 1) IPv4 포워딩
ip netns exec ns-gw sysctl -w net.ipv4.ip_forward=1

# 2) SNAT/MASQUERADE 규칙(호스트에서 iptables 사용 시 네임스페이스 경계 고려)
# (네임스페이스 내부 NAT 구성은 veth 밖으로 나가는 인터페이스 기준으로 수정 필요)
# 예시: 호스트에서 10.10.0.0/24 → eth0로 나가는 트래픽을 MASQUERADE
sudo iptables -t nat -A POSTROUTING -s 10.10.0.0/24 -o eth0 -j MASQUERADE
```

> **운영 포인트**: NAT은 **연결 추적 테이블**(conntrack)에 의존 → 대규모 트래픽/플로우 폭증 시 **테이블 포화**로 성능 저하/드롭 발생 가능.  
> 적절한 `nf_conntrack_max`, idle timeout 튜닝, LB/프록시 앞단 분산이 필요.

---

## 1.3 성능·지연·손실·재전송의 상호작용(지표 해석 기초)

### 1.3.1 지표 파악의 기본: RTT, 대역폭, BDP

- **RTT (Round-Trip Time)**: 왕복 지연(ms).  
- **대역폭**: 링크 전송률(Mbps/Gbps).  
- **BDP (Bandwidth-Delay Product)**: 한 번에 파이프에 채워 넣어야 할 데이터 양.  
  - $$ \text{BDP} = \text{Bandwidth} \times \text{RTT} $$
  - 예: 1Gbps, RTT 50ms → BDP = \(1\times10^9 \ \text{bit/s} \times 0.05\ \text{s} = 50\ \text{Mb} \approx 6.25\ \text{MB}\).  
    → TCP 윈도우/버퍼가 최소 이 정도는 되어야 **링크 포화** 가능.

#### 실습: BDP 계산(간단 Python)
```python
# bdp_calc.py
def bdp_bytes(bw_mbps, rtt_ms):
    bw_bps = bw_mbps * 1_000_000
    rtt_s = rtt_ms / 1000
    bdp_bits = bw_bps * rtt_s
    return int(bdp_bits / 8)

print("1Gbps, 50ms →", bdp_bytes(1000, 50), "bytes (~6.25MB)")
print("100Mbps, 10ms →", bdp_bytes(100, 10), "bytes (~125KB)")
```

> **튜닝 메시지**: **창(윈도우)·버퍼 크기가 BDP보다 작으면 링크를 다 못 씀** → 고RTT/고대역 환경에서 성능 저하.

---

### 1.3.2 손실과 TCP 스루풋(간이 모델)

- 경험적 상한(전통적 Mathis 공식 근사):  
  $$ \text{Throughput} \approx \frac{\text{MSS}}{\text{RTT}} \cdot \frac{C}{\sqrt{p}} $$
  - \(p\): 손실률(0~1), \(C\): 상수(대략 ≈ 1.22 등), MSS/RTT는 세션 특성.
  - 손실이 아주 조금만 늘어도 **스루풋 급감**(특히 고RTT 환경).

> 실무 해석: 손실률이 낮아도 RTT가 크면 **스루풋이 제약**. 반대로 RTT가 작아도 손실이 크면 **혼잡 윈도우가 자주 줄어 성능 악화**.

---

### 1.3.3 재전송, Duplicate ACK, RTO, Fast Retransmit

- **재전송( retransmission )**: 패킷 손실/순서 뒤바뀜 시 송신 측이 시도.
- **DupACK**: 수신 측이 같은 ACK 번호를 반복 전송 → 송신 측은 **Fast Retransmit** 트리거(일반적으로 3 중복 ACK).
- **RTO (Retransmission Timeout)**: 지정 시간 내 ACK가 없으면 타이머 만료로 재전송.

#### tcpdump로 재전송/dupACK 관찰
```bash
# 재전송/dupACK 힌트(완벽한 필터는 아님)
sudo tcpdump -i any -nn -vv 'tcp[tcpflags] & (tcp-ack|tcp-syn|tcp-fin|tcp-rst) != 0'

# Wireshark 표현식:
# tcp.analysis.retransmission
# tcp.analysis.fast_retransmission
# tcp.analysis.duplicate_ack
```

> **현상 해석**: RTT 증가→RTO 증가, 손실 발생→dupACK/재전송 증가.  
> 그래프(시계열)로 **RTT/재전송율/dupACK 빈도**를 함께 본다면 원인 추적이 빨라집니다.

---

### 1.3.4 MTU/MSS 미스매치가 성능에 미치는 영향(사례)

- **현상**: 특정 경로에서 대용량 다운로드가 느려지고, Wireshark에서 `TCP Previous segment not captured` 또는 `ICMP Frag Needed` 부재가 관찰.
- **원인**: 중간 장비가 **ICMP 차단** → PMTUD 실패 → DF=1 세그먼트가 계속 드롭 → 앱에서 **응답 지연/타임아웃**.
- **해결**: ICMP Type3 Code4 허용, **MSS 클램핑**(L3/4 장비에서 SYN 시 MSS 낮추기), 서버/클라이언트 MTU도 경로에 맞춰 조정.

#### 예: iptables로 MSS 클램핑(리눅스 라우터에서)
```bash
# SYN 패킷의 TCP MSS를 자동 클램핑 (pppoe/터널 등 MTU 작은 경로 보호)
sudo iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN \
  -j TCPMSS --clamp-mss-to-pmtu
```

---

### 1.3.5 손실(랜덤/버스트)과 큐 빌드업, 버퍼블로트

- **랜덤 손실**: 무선/잡음/간헐적 충돌 → 낮은 확률이지만 스루풋에 치명적.
- **버스트 손실**: 큐 포화(버퍼 오버플로우) → 다발 재전송/지연 상승.
- **버퍼블로트(Bufferbloat)**: 지나치게 큰 버퍼가 **지연만 증가**시킴. 대역폭은 남는데 **RTT가 급증**.

#### 실습: `tc netem`으로 지연/손실 주입
```bash
# 예: egress에 50ms 지연, 1% 손실
sudo tc qdisc add dev eth0 root netem delay 50ms loss 1%

# 결과 확인 후 제거
sudo tc qdisc del dev eth0 root
```

> **관찰 포인트**: `ping` RTT 상승, `iperf3` 스루풋 저하, Wireshark 재전송 증가.

---

### 1.3.6 Throughput 측정과 윈도우/버퍼 튜닝

- **iperf3**로 TCP/UDP 측정:
```bash
# 서버
iperf3 -s
# 클라이언트(10초 테스트, 윈도우 자동 조정 보고)
iperf3 -c <server_ip> -t 10 -i 1 --get-server-output
```

- **TCP 윈도우/버퍼 확인**: `ss -ti`에서 `send cwnd`, `rto`, `rtt` 등 확인.
```bash
ss -ti dst <server_ip>
```

- **OS 튜닝(예: Linux)**: 자동 튜닝 범위 확장
```bash
# 리드/라이트 버퍼(참고값, 환경에 맞게)
sudo sysctl -w net.core.rmem_max=268435456
sudo sysctl -w net.core.wmem_max=268435456
sudo sysctl -w net.ipv4.tcp_rmem="4096 87380 268435456"
sudo sysctl -w net.ipv4.tcp_wmem="4096 65536 268435456"
```

> **원리**: **BDP 이상**의 윈도우/버퍼를 제공해야 RTT가 큰 링크에서도 포화.  
> 단, 너무 크게만 키우면 큐 지연/메모리 낭비 → **관측 기반**으로 조정.

---

### 1.3.7 지표 기반 트러블슈팅 체크리스트

1) **문제 정의**  
   - 특정 앱/경로/시간대/패킷 크기에서만 느린가?  
   - 엔드투엔드 vs 구간(분할 정복) 문제인가?

2) **계층별 가설**  
   - L1/2: 링크 에러, 듀플렉스 미스매치, RSSI/무선 간섭?  
   - L3: 라우팅 우회/리플렉션, ICMP 차단(PMTUD 실패)?  
   - L4: 손실/재전송 증가, cwnd 수축?  
   - L7: 서버 리소스 부족, 응답 지연, TLS 핸드셰이크 문제?

3) **측정/관찰**  
   - RTT, 재전송율, dupACK, RTO, cwnd, `ss -ti`, `iperf3`.  
   - Wireshark: `tcp.analysis.*` 플래그, ICMP “Frag Needed”.

4) **개선 실험**  
   - MSS 클램핑, MTU 조정, ICMP 허용.  
   - 큐 관리(qdisc, AQM: fq_codel), 트래픽 쉐이핑.  
   - 서버/클라이언트 버퍼/윈도우 조정.

5) **결론/표준화**  
   - 재현 절차 기록, 경로 특성 문서화, 네트워크·시스템 기본값 템플릿 반영.

---

## 1.1~1.3 통합 미니 랩: “캡슐화부터 성능까지 한 번에”

### 토폴로지(로컬/컨테이너)
```
client ──(eth0)── [router/NAT] ──(eth1)── server
            │
          mirror (tcpdump)
```

### 단계

1) **캡슐화 관찰**  
   - `curl http://server` → mirror에서 `tcpdump -i any -w flow.pcap`  
   - Wireshark로 L2/L3/L4/L7 계층 확인, 헤더 길이와 MSS 관찰.

2) **MTU 이슈 재현**  
   - router에서 `--clamp-mss-to-pmtu` 끄기, ICMP Type3 Code4 드롭(테스트용).  
   - 대용량 전송( `wget` / `iperf3` ) 중 느려짐/타임아웃 관찰.  
   - ICMP 허용 후 개선 확인.

3) **손실/지연 주입**  
   - `tc netem delay 50ms loss 0.5%` → RTT/재전송/스루풋 변화 측정.  
   - `ss -ti`로 cwnd 축소/확대 흐름 관찰.

4) **튜닝**  
   - TCP 버퍼 상향, AQM(fq_codel) 적용 후 변화 비교.  
   - 결과를 표로 기록(BDP, RTT, 스루풋, 재전송율).

---

## 부록: 유용한 명령/필터/지식

### tcpdump 필터 모음
```bash
# 3-way handshake만
tcpdump -i any -nn 'tcp[tcpflags] & (tcp-syn|tcp-ack) != 0 and tcp[tcpflags] & tcp-syn != 0'

# DF=1 IPv4 TCP 세그먼트(프래그 문제 탐색)
tcpdump -i any -nn 'ip[6] & 0x40 != 0 and tcp'

# ICMP Frag Needed (Type 3 Code 4)
tcpdump -i any -nn 'icmp[0] == 3 and icmp[1] == 4'
```

### Wireshark Display Filter 모음
- Handshake: `tcp.flags.syn==1 && tcp.flags.ack==0`  
- 재전송: `tcp.analysis.retransmission`  
- DupACK: `tcp.analysis.duplicate_ack`  
- PMTUD 실패 의심: `ip.flags.df==1 && tcp.len>0` + ICMP Type3 Code4 미보임  
- HTTP 요청: `http.request` / DNS 쿼리: `dns.flags.response==0`

### 스루풋 근사 계산(노트)
- $$ \text{BDP(bytes)} = \frac{\text{bw(bps)} \times \text{RTT(s)}}{8} $$
- $$ \text{Throughput} \approx \frac{\text{MSS}}{\text{RTT}} \cdot \frac{C}{\sqrt{p}} $$
- PMTUD 실패 시 MSS가 비현실적으로 커진 세그먼트가 드롭 → **스루풋 급락**.

---

## 마무리

- **1.1**: 캡슐화/역캡슐화는 “헤더들이 층층이 싸여 있다”로 기억하고, Wireshark로 **계층별 헤더**를 직접 확인하세요.  
- **1.2**: MTU↔MSS↔프래그의 **기하(크기)**가 성능을 좌우합니다. ICMP 차단은 **PMTUD 파괴** → MSS 클램핑/정책 조정으로 해결.  
- **1.3**: 성능은 **RTT, 손실, 재전송, 윈도우/버퍼**의 상호작용 결과입니다. 지표를 모아 **가설→실험→개선** 루프를 돌리면 현상이 수학으로 설명됩니다.