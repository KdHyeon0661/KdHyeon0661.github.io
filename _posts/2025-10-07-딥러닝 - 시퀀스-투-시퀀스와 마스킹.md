---
layout: post
title: 딥러닝 - 시퀀스-투-시퀀스와 마스킹
date: 2025-10-07 18:25:23 +0900
category: 딥러닝
---
# 시퀀스-투-시퀀스와 마스킹 총정리  

## 0) 왜 Seq2Seq인가?

- **문제 정의**: 입력 \(X=(x_1,\dots,x_S)\)를 받아 출력 \(Y=(y_1,\dots,y_T)\)를 **오토리그레시브**하게 예측.
- **대표 태스크**: 기계번역, 추상적 요약, 데이터-to-텍스트, 질문→답변 생성, 코드 변환 등.
- **핵심 난제**  
  1) **출력은 미래 의존** → 디코더는 **자기회귀(causal)** 제약 필요  
  2) **패딩** → 미니배치에서 다양한 길이를 맞추기 위해 **패딩 마스크** 필요  
  3) **노출 편향(exposure bias)** → 학습 때는 정답을 보고, 추론 때는 본인 출력을 본다 → **교사강요/Scheduled Sampling** 논의

---

## 1) 인코더–디코더 개요 (RNN→Transformer)

### 1.1 RNN(LSTM/GRU) 기반 Seq2Seq
- **인코더**: \(h_t = \mathrm{RNN}(x_t, h_{t-1})\) → 문맥 \(H=(h_1,\dots,h_S)\)  
- **디코더**: \(s_t = \mathrm{RNN}(y_{t-1}, s_{t-1}, \mathrm{context}_t)\),  
  \(\mathrm{context}_t\)는 보통 어텐션으로 \(H\)에서 가중합
- **교사강요**: 학습 시 \(y_{t-1}\) 대신 **정답 토큰**을 입력으로 투여 → 빠른 수렴

### 1.2 Transformer 기반 Seq2Seq
- **인코더**: 입력에 **셀프 어텐션** \( \mathrm{SelfAttn}(X) \)  
- **디코더**:  
  1) **캐주얼 마스크**가 적용된 **디코더 셀프 어텐션**(미래 차단)  
  2) **인코더–디코더(크로스) 어텐션**: 디코더의 쿼리 \(Q\)가 인코더의 \(K,V\)를 바라봄  
  3) 포지션-와이즈 FFN

> 어텐션 수식(스케일드 닷-프로덕트):
> $$
> \mathrm{Attention}(Q,K,V) = \mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V
> $$
> 여기서 \(M\)은 **마스크(패딩·캐주얼)**로 마스킹 위치에 **\(-\infty\)** 를 더해 softmax 확률을 0으로 만든다.

---

## 2) 마스크의 3종류

1) **패딩 마스크(padding mask)**  
   - 목적: **패딩 토큰**이 가중치/로스를 먹지 않게.  
   - 모양: \([B, L]\) (배치×길이) **bool** → True=**패딩**(무시).

2) **캐주얼(look-ahead) 마스크**  
   - 목적: 시점 \(t\)에서 **미래 토큰** \(t+1,\dots\)을 보지 못하게.  
   - 모양: \([T, T]\) 상삼각을 \(-\infty\)로 마스킹.

3) **크로스-어텐션 마스크(memory padding mask)**  
   - 목적: **인코더 메모리의 패딩**을 디코더가 보지 않게.  
   - 모양: \([B, S]\) **bool** → 인코더 길이 기준으로 True=패딩.

> PyTorch `nn.Transformer` 인자 요약(배치우선 `batch_first=True` 기준):  
> - `src_key_padding_mask`: \([B,S]\)  
> - `tgt_key_padding_mask`: \([B,T]\)  
> - `memory_key_padding_mask`: \([B,S]\)  
> - `tgt_mask`: \([T,T]\) (캐주얼 마스크, float 또는 bool)

---

## 3) 교사강요(Teacher Forcing)와 Scheduled Sampling

- **교사강요**: 학습 시 디코더 입력 \(y_{t-1}^{\text{in}}\)에 **모델 추정치** 대신 **정답 토큰**을 넣는 전략.
- **장점**: 학습 안정, 수렴 속도↑.  
- **단점(노출 편향)**: 추론 시 **오류 전파**(초반 한 두 토큰의 오류가 연쇄적으로 커짐).
- **Scheduled Sampling**:  
  - 확률 \(\epsilon_t\)로 **모델 예측**을, \(1-\epsilon_t\)로 **정답**을 투입.  
  - \(\epsilon_t\)를 시간/스텝에 따라 증가시키는 스케줄을 둔다.
- **Transformer에서는?**  
  - 디코더 입력은 **정답의 한 칸 오른쪽 쉬프트**가 기본 → **묵시적 교사강요**.  
  - Scheduled Sampling을 적용하려면 **부분적으로 모델 예측을 넣어** 토큰 시퀀스를 구성해야 한다(실전에서는 드뭄).

---

## 4) 수식으로 보는 마스킹

### 4.1 캐주얼 마스크
- 길이 \(T\)에서,  
  $$
  M_{ij}=
  \begin{cases}
  0,& j \le i\\
  -\infty, & j > i
  \end{cases}
  \quad (i,j=1..T)
  $$
  로 정의.  
  \(\mathrm{softmax}(z+M)\)에서 \(j>i\)인 위치의 확률은 0.

### 4.2 패딩 마스크
- 패딩 위치 인덱스 집합 \(\mathcal{P}\)가 있을 때,  
  $$
  M_j = \begin{cases}
  0 & j \notin \mathcal{P}\\
  -\infty & j \in \mathcal{P}
  \end{cases}
  $$
  를 점수(유사도)에 더하거나, 프레임워크에서 **key padding mask**로 전달.

---

## 5) 실전 코드 ①: **PyTorch Transformer**로 미니 Seq2Seq

> 목적: **토이 번역/복사** 데이터로 **마스크·교사강요**를 완전히 이해.  
> 포인트: `nn.Transformer(batch_first=True)`를 쓰고 **모든 마스크**를 정확히 넣는다.

### 5.1 유틸: 토큰/사전/포지셔널 인코딩/마스크 생성

```python
# PyTorch만 사용 (pip 필요 없음)
import math, random
from dataclasses import dataclass
from typing import List, Tuple
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
PAD, BOS, EOS, UNK = 0, 1, 2, 3   # 토큰 인덱스 규약

# 간단한 vocab
@dataclass
class Vocab:
    stoi: dict
    itos: list
    pad_idx: int = PAD
    bos_idx: int = BOS
    eos_idx: int = EOS
    unk_idx: int = UNK
    def __len__(self): return len(self.itos)

def build_vocab(pairs: List[Tuple[List[str], List[str]]], min_freq=1) -> Vocab:
    from collections import Counter
    cnt = Counter()
    for src, tgt in pairs:
        cnt.update(src); cnt.update(tgt)
    itos = ["[PAD]","<bos>","<eos>","[UNK]"]
    for w,c in cnt.items():
        if c >= min_freq and w not in itos:
            itos.append(w)
    stoi = {w:i for i,w in enumerate(itos)}
    return Vocab(stoi, itos)

def tokens_to_ids(v: Vocab, toks: List[str]) -> List[int]:
    return [v.stoi.get(t, v.unk_idx) for t in toks]

def add_bos_eos(ids: List[int], v: Vocab):
    return [v.bos_idx] + ids + [v.eos_idx]

# 포지셔널 인코딩 (Transformer 기본)
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout=0.1, max_len=10000):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        pe = torch.zeros(max_len, d_model)                 # [L, D]
        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        pe = pe.unsqueeze(0)                               # [1, L, D]
        self.register_buffer("pe", pe)                     # no grad

    def forward(self, x):                                   # x: [B, L, D]
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)

def make_padding_mask(seqs: torch.Tensor, pad_idx: int) -> torch.Tensor:
    # seqs: [B, L] → True for PAD
    return (seqs == pad_idx)  # [B, L] bool

def make_causal_mask(sz: int) -> torch.Tensor:
    # [T, T] 상삼각을 -inf로
    mask = torch.full((sz, sz), float("-inf"))
    return torch.triu(mask, diagonal=1)  # (i<j) 위치가 -inf
```

### 5.2 데이터셋: **토이 복사/뒤집기**(개념 이해에 최고)

```python
# 예시 태스크: src를 조금 변형해서 tgt로 (ex. 뒤집기 or 접미사 추가)
class ToySeq2Seq(Dataset):
    def __init__(self, n=2000, vocab_tokens=None, max_len=10, mode="reverse"):
        self.mode = mode
        base = vocab_tokens or ["a","b","c","d","e","f","g","h","i","j"]
        self.pairs = []
        for _ in range(n):
            L = random.randint(3, max_len)
            src = [random.choice(base) for _ in range(L)]
            if mode == "reverse":
                tgt = src[::-1]
            elif mode == "copy":
                tgt = src[:]
            else:
                tgt = src[:] + ["x"]  # 임의 변형
            self.pairs.append((src, tgt))
        self.vocab = build_vocab(self.pairs)

    def __len__(self): return len(self.pairs)
    def __getitem__(self, i): return self.pairs[i]

def collate_batch(batch, vocab: Vocab, max_len=32):
    # 1) 토큰→ID + BOS/EOS
    src_ids, tgt_in_ids, tgt_out_ids = [], [], []
    for src, tgt in batch:
        s = tokens_to_ids(vocab, src)[:max_len-2]
        t = tokens_to_ids(vocab, tgt)[:max_len-2]
        s = add_bos_eos(s, vocab)      # 인코더는 BOS/EOS 없어도 되지만 통일
        t_in  = [vocab.bos_idx] + t    # 디코더 입력
        t_out = t + [vocab.eos_idx]    # 디코더 정답(shifted)
        src_ids.append(s); tgt_in_ids.append(t_in); tgt_out_ids.append(t_out)
    # 2) 패딩
    def pad_to_max(xs, pad=PAD):
        M = max(len(x) for x in xs)
        return torch.tensor([x + [pad]*(M-len(x)) for x in xs], dtype=torch.long)
    src_ids = pad_to_max(src_ids, vocab.pad_idx)
    tgt_in_ids  = pad_to_max(tgt_in_ids, vocab.pad_idx)
    tgt_out_ids = pad_to_max(tgt_out_ids, vocab.pad_idx)
    # 3) 마스크
    src_key_padding_mask = make_padding_mask(src_ids, vocab.pad_idx)  # [B,S]
    tgt_key_padding_mask = make_padding_mask(tgt_in_ids, vocab.pad_idx)  # [B,T]
    tgt_mask = make_causal_mask(tgt_in_ids.size(1)).to(src_ids.device)   # [T,T]
    return (src_ids, tgt_in_ids, tgt_out_ids,
            src_key_padding_mask, tgt_key_padding_mask, tgt_mask)
```

### 5.3 모델: 임베딩 + Transformer + 생성기

```python
class Seq2SeqTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, dim_ff=1024, dropout=0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.src_emb = nn.Embedding(vocab_size, d_model)
        self.tgt_emb = nn.Embedding(vocab_size, d_model)
        self.pos = PositionalEncoding(d_model, dropout)
        self.trans = nn.Transformer(
            d_model=d_model, nhead=nhead,
            num_encoder_layers=num_layers,
            num_decoder_layers=num_layers,
            dim_feedforward=dim_ff, dropout=dropout,
            batch_first=True  # 중요: [B,L,D]
        )
        self.generator = nn.Linear(d_model, vocab_size)

    def encode(self, src, src_key_padding_mask):
        x = self.pos(self.src_emb(src))
        memory = self.trans.encoder(
            x, src_key_padding_mask=src_key_padding_mask
        )
        return memory

    def decode(self, tgt, memory, tgt_mask, tgt_key_padding_mask, memory_key_padding_mask):
        y = self.pos(self.tgt_emb(tgt))
        out = self.trans.decoder(
            y, memory, tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask
        )
        return out

    def forward(self, src, tgt_in, src_key_padding_mask, tgt_key_padding_mask, tgt_mask):
        memory = self.encode(src, src_key_padding_mask)
        out = self.decode(tgt_in, memory, tgt_mask, tgt_key_padding_mask, src_key_padding_mask)
        logits = self.generator(out)  # [B,T,V]
        return logits
```

### 5.4 학습 루프 (교사강요=정답 쉬프트 입력, 패딩 무시 CE)

```python
def train_one_epoch(model, dl, vocab: Vocab, opt, scaler=None, clip=1.0):
    model.train()
    ce = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx, label_smoothing=0.0)  # 필요시 smoothing=0.1
    total = 0.0
    for (src, tgt_in, tgt_out, src_pad, tgt_pad, tgt_mask) in dl:
        src, tgt_in, tgt_out = src.to(DEVICE), tgt_in.to(DEVICE), tgt_out.to(DEVICE)
        src_pad, tgt_pad = src_pad.to(DEVICE), tgt_pad.to(DEVICE)
        tgt_mask = tgt_mask.to(DEVICE)
        opt.zero_grad(set_to_none=True)
        if scaler:
            with torch.cuda.amp.autocast():
                logits = model(src, tgt_in, src_pad, tgt_pad, tgt_mask)   # [B,T,V]
                loss = ce(logits.view(-1, logits.size(-1)), tgt_out.view(-1))
            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
            scaler.step(opt); scaler.update()
        else:
            logits = model(src, tgt_in, src_pad, tgt_pad, tgt_mask)
            loss = ce(logits.view(-1, logits.size(-1)), tgt_out.view(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
            opt.step()
        total += loss.item()
    return total / len(dl)

@torch.no_grad()
def greedy_decode(model, src, vocab: Vocab, max_len=30):
    model.eval()
    src = src.to(DEVICE)
    src_pad = make_padding_mask(src, vocab.pad_idx).to(DEVICE)
    memory = model.encode(src, src_pad)  # [B,S,D]
    B = src.size(0)
    ys = torch.full((B,1), vocab.bos_idx, dtype=torch.long, device=DEVICE)
    for _ in range(max_len-1):
        tgt_pad = make_padding_mask(ys, vocab.pad_idx)
        tgt_mask = make_causal_mask(ys.size(1)).to(DEVICE)
        out = model.decode(ys, memory, tgt_mask, tgt_pad, src_pad)    # [B,T,D]
        logits = model.generator(out[:, -1])                          # [B,V]
        next_tok = torch.argmax(logits, dim=-1, keepdim=True)         # [B,1]
        ys = torch.cat([ys, next_tok], dim=1)
        if (next_tok == vocab.eos_idx).all(): break
    return ys  # [B,T']

def demo_train():
    ds = ToySeq2Seq(n=2000, mode="reverse")
    vocab = ds.vocab
    dl = DataLoader(ds, batch_size=64, shuffle=True,
                    collate_fn=lambda b: collate_batch(b, vocab))
    model = Seq2SeqTransformer(len(vocab), d_model=256, nhead=8, num_layers=3).to(DEVICE)
    opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)
    scaler = torch.cuda.amp.GradScaler(enabled=True)

    for ep in range(1, 8):
        loss = train_one_epoch(model, dl, vocab, opt, scaler)
        print(f"[ep {ep}] loss={loss:.4f}")

    # 추론 데모
    batch = [ds[i] for i in range(4)]
    src_ids = [add_bos_eos(tokens_to_ids(vocab, s), vocab) for s,_ in batch]
    L = max(len(x) for x in src_ids)
    src_ids = torch.tensor([x + [PAD]*(L-len(x)) for x in src_ids], dtype=torch.long)
    pred = greedy_decode(model, src_ids, vocab, max_len=L+3)
    # 디토큰
    def ids_to_toks(ids):
        toks = []
        for j in ids:
            w = vocab.itos[int(j)]
            if w in ("[PAD]","<bos>"): continue
            if w=="<eos>": break
            toks.append(w)
        return toks
    for i,(src,tgt) in enumerate(batch):
        print("SRC:", src)
        print("GOLD:", tgt)
        print("PRED:", ids_to_toks(pred[i].tolist()))
        print("---")

# if __name__ == "__main__": demo_train()
```

**핵심 체크**
- **교사강요**: 학습 시 `tgt_in=[BOS]+gold[:-1]` / 정답은 `tgt_out=gold+[EOS]`  
- **캐주얼 마스크**: `make_causal_mask(T)`를 `tgt_mask`로 전달  
- **패딩 마스크**: `src_key_padding_mask`, `tgt_key_padding_mask`, `memory_key_padding_mask`를 정확히 전달  
- **로스**: `ignore_index=PAD`로 **패딩 무시** (중요!)  
- **라벨 스무딩**: `label_smoothing=0.1` 등으로 과신완화 가능

---

## 6) 실전 코드 ②: **Beam Search** & **Top-k/Top-p** 디코딩

```python
@torch.no_grad()
def beam_search(model, src, vocab: Vocab, beam=4, max_len=40, len_penalty=0.6):
    # 간단 빔서치 (배치=1 가정). 실제 배치 버전은 확장 필요.
    model.eval()
    src = src.to(DEVICE)
    src_pad = make_padding_mask(src, vocab.pad_idx).to(DEVICE)
    memory = model.encode(src, src_pad)

    beams = [(torch.tensor([[vocab.bos_idx]], device=DEVICE), 0.0)]  # (seq, logprob)
    for _ in range(max_len-1):
        cand = []
        for seq, lp in beams:
            if seq[0,-1].item() == vocab.eos_idx:
                cand.append((seq, lp)); continue
            tgt_pad = make_padding_mask(seq, vocab.pad_idx)
            tgt_mask = make_causal_mask(seq.size(1)).to(DEVICE)
            out = model.decode(seq, memory, tgt_mask, tgt_pad, src_pad)
            logits = model.generator(out[:, -1])
            logp = torch.log_softmax(logits, dim=-1).squeeze(0)  # [V]
            topk = torch.topk(logp, beam)
            for idx, score in zip(topk.indices, topk.values):
                cand.append((torch.cat([seq, idx.view(1,1)], dim=1),
                             lp + float(score)))
        # 길이 보정
        def score_fn(item):
            seq, lp = item
            L = seq.size(1)
            lp_norm = lp / ((5+L)**len_penalty / (5+1)**len_penalty)
            return lp_norm
        beams = sorted(cand, key=score_fn, reverse=True)[:beam]
        if all(seq[0,-1].item()==vocab.eos_idx for seq,_ in beams):
            break
    return beams[0][0]
```

```python
@torch.no_grad()
def topk_topp_sampling(model, src, vocab: Vocab, max_len=40, top_k=50, top_p=0.9, temperature=1.0):
    import torch
    model.eval()
    src = src.to(DEVICE)
    src_pad = make_padding_mask(src, vocab.pad_idx).to(DEVICE)
    memory = model.encode(src, src_pad)
    ys = torch.tensor([[vocab.bos_idx]], device=DEVICE)

    for _ in range(max_len-1):
        tgt_pad = make_padding_mask(ys, vocab.pad_idx)
        tgt_mask = make_causal_mask(ys.size(1)).to(DEVICE)
        out = model.decode(ys, memory, tgt_mask, tgt_pad, src_pad)
        logits = model.generator(out[:, -1]) / temperature
        probs = torch.softmax(logits, dim=-1).squeeze(0)

        # top-k
        if top_k > 0:
            v, ix = torch.topk(probs, top_k)
            mask = torch.ones_like(probs, dtype=torch.bool)
            mask[ix] = False
            probs = probs.masked_fill(mask, 0.0)

        # top-p (nucleus)
        if 0 < top_p < 1.0:
            sorted_probs, sorted_idx = torch.sort(probs, descending=True)
            cumsum = torch.cumsum(sorted_probs, dim=-1)
            keep = cumsum <= top_p
            keep[0] = True
            mask = torch.ones_like(probs, dtype=torch.bool)
            mask[sorted_idx[keep]] = False
            probs = probs.masked_fill(mask, 0.0)

        probs = probs / probs.sum()
        next_tok = torch.multinomial(probs, num_samples=1).view(1,1)
        ys = torch.cat([ys, next_tok.to(ys.device)], dim=1)
        if next_tok.item() == vocab.eos_idx:
            break
    return ys
```

---

## 7) 교사강요 데모 ③: **RNN(LSTM) Seq2Seq + Scheduled Sampling**

> Transformer에서는 학습 입력이 “정답 쉬프트”라 **교사강요가 기본**.  
> 개념을 명확히 하려면 RNN 디코더에서 **teacher_forcing_ratio** 를 시연해 보는 게 좋다.

```python
class RNNEncoder(nn.Module):
    def __init__(self, vocab_size, d_model=256, hidden=512, n_layers=1):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, d_model)
        self.rnn = nn.LSTM(d_model, hidden, num_layers=n_layers, batch_first=True, bidirectional=False)
        self.proj = nn.Linear(hidden, d_model)
    def forward(self, src, src_len=None):
        x = self.emb(src)
        out, (h,c) = self.rnn(x)     # out: [B,S,H]
        ctx = self.proj(h[-1])       # [B,D]
        return out, (h,c), ctx

class RNNAttnDecoder(nn.Module):
    def __init__(self, vocab_size, d_model=256, hidden=512):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, d_model)
        self.rnn = nn.LSTM(d_model + d_model, hidden, batch_first=True)
        self.W_q = nn.Linear(hidden, d_model)   # for attention query
        self.W_k = nn.Linear(d_model, d_model)  # encoder out proj
        self.W_v = nn.Linear(d_model, d_model)
        self.gen = nn.Linear(hidden, vocab_size)

    def step(self, y_prev, state, enc_out, pad_mask):
        # y_prev: [B] (이전 토큰), state: (h,c), enc_out: [B,S,H_enc→D]
        y_emb = self.emb(y_prev).unsqueeze(1)         # [B,1,D]
        # 어텐션: q from decoder state, k/v from encoder outputs
        h, c = state
        q = self.W_q(h[-1]).unsqueeze(1)              # [B,1,D]
        k = self.W_k(enc_out)                         # [B,S,D]
        v = self.W_v(enc_out)                         # [B,S,D]
        attn = torch.matmul(q, k.transpose(1,2)) / math.sqrt(k.size(-1))  # [B,1,S]
        if pad_mask is not None:                      # pad_mask [B,S] True=pad
            attn = attn.masked_fill(pad_mask.unsqueeze(1), float("-inf"))
        attn = torch.softmax(attn, dim=-1)            # [B,1,S]
        ctx = torch.matmul(attn, v)                   # [B,1,D]

        rnn_in = torch.cat([y_emb, ctx], dim=-1)      # [B,1,2D]
        out, state = self.rnn(rnn_in, state)          # out: [B,1,H]
        logits = self.gen(out.squeeze(1))             # [B,V]
        return logits, state

def train_rnn_teacher_forcing(encoder, decoder, dl, vocab: Vocab, tf_ratio=0.5, epochs=5):
    ce = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)
    opt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)
    encoder.to(DEVICE); decoder.to(DEVICE)
    for ep in range(1, epochs+1):
        encoder.train(); decoder.train(); total=0.0
        for (src, tgt_in, tgt_out, src_pad, tgt_pad, _) in dl:
            src, tgt_in, tgt_out = src.to(DEVICE), tgt_in.to(DEVICE), tgt_out.to(DEVICE)
            src_pad = src_pad.to(DEVICE)
            opt.zero_grad()
            enc_out, state, _ = encoder(src)
            B, T = tgt_in.size()
            logits_list = []
            y_prev = tgt_in[:,0]  # BOS
            for t in range(1, T):
                logits, state = decoder.step(y_prev, state, enc_out, src_pad)
                logits_list.append(logits)
                # Scheduled Sampling
                teacher = (torch.rand(B, device=DEVICE) > tf_ratio)
                y_prev = torch.where(teacher, torch.argmax(logits, dim=-1), tgt_in[:,t])
            logits = torch.stack(logits_list, dim=1)  # [B,T-1,V]
            loss = ce(logits.reshape(-1, logits.size(-1)), tgt_out[:, :logits.size(1)].reshape(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(list(encoder.parameters())+list(decoder.parameters()), 1.0)
            opt.step()
            total += loss.item()
        print(f"[ep {ep}] loss={total/len(dl):.4f}")
```

- `tf_ratio=0.5`이면 **절반은 정답**, **절반은 모델 예측**을 다음 입력으로 사용.  
- 노출 편향을 완화하지만 과도하면 훈련 흔들릴 수 있음 → 점진 스케줄 권장(초기 0.0→후기 0.3 등).

---

## 8) 인코더–디코더 어텐션(크로스 어텐션) 해부

- 디코더 레이어의 2번째 블록:
  $$
  Q = H^{\text{dec}} W_Q,\quad 
  K = H^{\text{enc}} W_K,\quad 
  V = H^{\text{enc}} W_V
  $$
  $$
  \mathrm{Attn}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}} + M_{\text{mem}}\right)V
  $$
- \(M_{\text{mem}}\): **memory key padding mask** (인코더 패딩 위치 \(-\infty\)).  
- **역할**: 디코더가 **입력 전체 문맥**을 참고하여, 번역/요약 시 **정렬(alignment)**에 해당.

> 실무 팁  
> - 긴 입력(수천 토큰)에서 **크로스 어텐션 비용**이 커짐 → **Longformer/Perceiver**류, **압축**(selective attention) 고려  
> - 문서-수준 요약: **hierarchical encoder**(문단→문서) + 크로스 어텐션 조합이 흔함

---

## 9) 손실·스케줄링·평가

### 9.1 손실
- **토큰 단위 CE**:
  $$
  \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T_i} \log p(y_{i,t}\mid y_{i,<t}, X_i)
  $$
- **패딩 무시**: `ignore_index=PAD` 필수.  
- **라벨 스무딩**: \(\epsilon=0.1\) 정도로 과신 완화/일반화 ↑.

### 9.2 학습률 스케줄
- **워밍업 → 디케이**(Cosine/Linear): 안정 수렴.  
- **그라디언트 클리핑**: \(1.0\) 근방.

### 9.3 평가
- **BLEU**(n-gram 정합) / **ROUGE-L**(Longest Common Subsequence).  
- 디코딩은 **beam 4~8** 기본, **길이 보정** 적용.

---

## 10) 마스킹 실수 방지 체크리스트

- [ ] **dtype**: 마스크는 보통 **bool**([B,L])이지만, `tgt_mask`는 **[T,T] float**로 \(-\infty\) 필요할 때가 있음.  
- [ ] **True=패딩**(PyTorch key padding mask 규약).  
- [ ] `memory_key_padding_mask`를 **잊지 말 것**(크로스 어텐션에서 중요).  
- [ ] **캐주얼 마스크**는 디코더 **셀프 어텐션**에만.  
- [ ] **로스에서 PAD 무시**.  
- [ ] **BOS/EOF 쉬프트** 정렬 확인(오프바이원 오류 빈발).  
- [ ] AMP 혼합정밀 시 **-inf 마스킹** 연산이 NaN을 유발하지 않게 순서/타입 주의.

---

## 11) 성능·서빙 팁

- **KV 캐시**(추론): Transformer 디코더에서 스텝별 \(K,V\) 캐싱 → **O(T)** 시간.  
  (순정 `nn.Transformer`는 기본 제공 X, 커스텀 MHA 또는 HF `generate` 사용)
- **가속**: ONNX/TensorRT로 내릴 때도 **마스크 텐서 형태**는 동일하게 전달.  
- **길이 제약**: `max_new_tokens`, `min_length`, **금지 토큰** 리스트로 제어.

---

## 12) 수식 모음

- **크로스 엔트로피**  
  $$
  \mathrm{CE}(\mathbf{y},\hat{\mathbf{p}}) = -\sum_{c} y_c \log \hat{p}_c
  $$
- **스케일드 닷-프로덕트 어텐션**  
  $$
  \mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V
  $$
- **길이 보정(빔)**  
  $$
  \mathrm{score}=\frac{\log P(y\mid x)}{((5+|y|)^\alpha/(5+1)^\alpha)}
  $$

---

## 13) 미니 시나리오: 짧은 변환 태스크

- **데이터**: `ToySeq2Seq(mode="reverse")`  
- **학습**: 위 `demo_train()` 실행 → 몇 에폭 후 **정확 복원**  
- **마스크 관찰**:  
  - `tgt_mask[i,j]=-inf (j>i)` → 미래 차단  
  - `src_key_padding_mask[b,s]=True` → 어텐션에서 무시

---

## 14) 자주 묻는 질문(FAQ)

- **Q. 왜 인코더에는 캐주얼 마스크가 없나요?**  
  A. 인코더는 **양방향 문맥**을 모두 보게 하여 더 풍부한 표현을 만들기 위해서입니다.
- **Q. 교사강요를 반드시 해야 하나요?**  
  A. Transformer 학습에는 **정답 쉬프트 입력**이 사실상 표준입니다. RNN에서는 **teacher forcing ratio**를 조절해 노출 편향을 완화할 수 있습니다.
- **Q. 길이 불일치/패딩으로 정답이 어긋납니다.**  
  A. `tgt_in`/`tgt_out` 정렬, `ignore_index=PAD`, 마스크 모양을 다시 점검하세요(가장 흔한 버그).

---

## 15) 확장 주제(간단 개관)

- **Label Smoothing** / **Sequence-level Loss**(Minimum Risk Training)  
- **Coverage Penalty**(요약/번역 재마침 방지)  
- **Copy / Pointer-Generator**(OOV 복사)  
- **Relative Positional Bias**(T5/DeBERTa류)

---

# 마무리

- **마스크**는 Seq2Seq의 안티-버그 장치다: **패딩**을 빼고, **미래**를 가리고, **메모리 패딩**도 잊지 마라.  
- **교사강요**는 빠른 수렴의 친구지만, **노출 편향**의 씨앗이다 → 필요 시 **Scheduled Sampling**을 섞어보라.  
- **인코더–디코더 어텐션**은 입력과 출력의 **정렬·참조**를 담당하는 핵심 경로다.  
- 본문 코드를 변형하면 **요약/번역/테이블-to-텍스트**에 그대로 쓸 수 있다.  
  데이터만 갈아끼우고, **마스크/쉬프트/로스 ignore** 3종을 반드시 지켜라.