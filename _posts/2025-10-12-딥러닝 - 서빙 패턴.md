---
layout: post
title: 딥러닝 - 서빙 패턴
date: 2025-10-12 14:25:23 +0900
category: 딥러닝
---
# 서빙 패턴(배치 vs 실시간, 카나리/A-B)

> 목표
> - **배치 서빙**과 **실시간(온라인) 서빙**의 차이·의사결정 기준·아키텍처 패턴을 정리합니다.
> - **카나리 배포**와 **A/B 테스트**를 위한 **트래픽 라우팅·버킷팅(일관 해싱)·측정·롤백** 절차를 코드로 보여줍니다.
> - **PyTorch만**을 기준으로 모델 로딩/최적화/동적 마이크로배칭/섀도우 테스트/관측(로그·지표) 샘플을 제공합니다.
> - 운영에서 자주 생기는 이슈(레이턴시 예산·큐 백프레셔·콜드 스타트·버전 호환·드리프트)를 체크리스트로 묶었습니다.

---

## 한 눈에 보는 결정표

| 질문 | 배치 추천 | 실시간 추천 |
|---|---|---|
| 응답 SLO | 초·분 단위 OK | **p95 < 100~300ms** 필요 |
| 요청 패턴 | 대량·주기적·오프피크 | 트래픽 변동·상시 |
| 데이터 신선도 | 분~시간 단위 | **초 단위** |
| 계산량 | 고비용 일괄 처리 | **마이크로배칭·캐시**로 얇게 |
| 실패 영향 | 재시도 가능 | **사용자 체감 큼 → 카나리/롤백 필수** |
| 예시 | 재고 예측, 추천 프리컴퓨트, 위험 스코어 야간 갱신 | 검색/챗봇/결제 사기 감지, 맞춤 추천 호출 |

> 현실에선 **하이브리드**가 많습니다. (예) 추천 임베딩·후보 프리컴퓨트는 **배치**, 최종 rerank/생성은 **실시간**.

---

## PyTorch 추론 공통 베스트 프랙티스

- **inference_mode + autocast**: 그래프·그래드 비활성화, BF16/FP16 사용
- **고정형 전처리**: 토치 텐서로 변환해 **핀드 메모리**, 이왕이면 **배치**
- **콜드 스타트 워밍업**: 가짜 배치 3~5회 돌려 커널 컴파일·캐시 채우기
- **TorchScript/`torch.compile`(옵션)**: 제약은 있지만 레이턴시↓
- **동적 마이크로배칭**: 0~10ms 윈도에 요청을 모아 GPU 효율↑
- **고정 SEED·버전 메타**: 응답 헤더/로그에 모델 해시·config 기록

```python
# pytorch_infer_core.py

import torch, time
from typing import List, Dict

class TorchPredictor:
    def __init__(self, ckpt_path: str, device=None, dtype=torch.bfloat16):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.dtype  = dtype if self.device=="cuda" else torch.float32
        self.model = self._load(ckpt_path).to(self.device).eval()
        self._warmup()

    def _load(self, path):
        # 예: 단순 MLP/Transformer를 불러온다고 가정
        from model_def import build_model  # 사용자 정의
        model = build_model()
        state = torch.load(path, map_location="cpu")
        model.load_state_dict(state["model"] if "model" in state else state)
        return model

    @torch.no_grad()
    def _warmup(self, bs=4, dim=128):
        x = torch.randn(bs, dim, device=self.device, dtype=self.dtype)
        for _ in range(5):
            with torch.autocast(device_type=("cuda" if self.device=="cuda" else "cpu"), dtype=self.dtype):
                _ = self.model(x)

    @torch.no_grad()
    def predict_batch(self, x: torch.Tensor) -> torch.Tensor:
        # x: [B, D]
        x = x.to(self.device, dtype=self.dtype, non_blocking=True)
        with torch.autocast(device_type=("cuda" if self.device=="cuda" else "cpu"), dtype=self.dtype), \
             torch.inference_mode():
            y = self.model(x)
        return y
```

---

## **배치 서빙**: 야간 일괄/주기 처리

### 설계 포인트

- 입력: 키(유저/아이템 ID) 리스트 또는 전체 테이블
- 출力: 예측 테이블(키, 점수, 버전, 생성시각) → **피쳐스토어/캐시/DB** 적재
- 실패: **재시도**·부분 재처리 전략(체크포인트·스팬 로그)
- 스케줄: cron/워크플로 엔진(이전 장에서 다룬 스케줄링)
- 검증: 샘플링 검수, 총합/분포 가드레일(예: 평균 스코어 범위)

### PyTorch 배치 추론 스크립트

```python
# batch_infer.py

import csv, json, os, time, math
import torch
from torch.utils.data import DataLoader, Dataset
from pytorch_infer_core import TorchPredictor

class CSVDataset(Dataset):
    def __init__(self, path, dim):
        self.rows = []
        with open(path) as f:
            r = csv.DictReader(f)
            for i,row in enumerate(r):
                # 예: feature_0..feature_{dim-1}
                x = [float(row.get(f"f{j}", 0.0)) for j in range(dim)]
                self.rows.append((row["id"], x))
    def __len__(self): return len(self.rows)
    def __getitem__(self, idx):
        _id, x = self.rows[idx];
        return _id, torch.tensor(x, dtype=torch.float32)

def collate(batch):
    ids, xs = zip(*batch)
    X = torch.stack(xs, dim=0)
    return list(ids), X

def main(inp_csv, out_csv, ckpt, dim=128, bs=1024):
    pred = TorchPredictor(ckpt)
    ds = CSVDataset(inp_csv, dim)
    dl = DataLoader(ds, batch_size=bs, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate)
    os.makedirs(os.path.dirname(out_csv) or ".", exist_ok=True)
    with open(out_csv, "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["id","score","model_version","ts"])
        for ids, X in dl:
            y = pred.predict_batch(X).float().softmax(-1)[:,1] if ydim_is_2(pred) else pred.predict_batch(X).squeeze(-1)
            ts = int(time.time())
            ver = model_version(ckpt)
            for i,score in zip(ids, y.detach().cpu().tolist()):
                w.writerow([i, score, ver, ts])

def model_version(ckpt):
    return os.path.basename(ckpt)

def ydim_is_2(pred: TorchPredictor):
    # 단순 판정(사용자 모델에 맞게 수정)
    return True

if __name__=="__main__":
    # 예: python batch_infer.py data/features.csv out/preds.csv ckpt/model_best.pt
    import sys
    main(sys.argv[1], sys.argv[2], sys.argv[3])
```

> **운영 팁**
> - 출력 파일 이름에 **데이터 버전 + 모델 버전** 포함: `preds-DS20250919-MVabc123.csv`
> - 완료 후 **원자적 교체**(tmp 파일 작성→rename)로 컨슈머 일관성 보장

---

## **실시간 서빙**: 저지연 API + 동적 마이크로배칭

### 아키텍처 핵심

- **요청 큐**(in-proc) + **마이크로배처**(윈도우 1~10ms) → GPU 효율
- **동시성 제한**(세마포어) + **백프레셔**: 큐가 가득 차면 빠른 실패(429)
- **섀도우(다크) 호출**: 새 모델에 **복제** 요청 보내 로그만(응답엔 영향 X)
- **헬스체크 & 워밍업**: `/health`가 **모델 해시/로드 상태** 제공
- **관측**: p50/p95/p99, 5xx/타임아웃율, 큐 길이, 배치 분포

### 외부 프레임워크 없이 간단 HTTP 서버 + 동적 배칭

```python
# rt_server.py

import json, threading, time, queue, hashlib
from http.server import BaseHTTPRequestHandler, HTTPServer
import torch
from pytorch_infer_core import TorchPredictor

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

class DynamicBatcher:
    def __init__(self, predictor: TorchPredictor, max_batch=64, window_ms=5):
        self.pred = predictor
        self.max_batch = max_batch
        self.window_ms = window_ms/1000.0
        self.q = queue.Queue(maxsize=2048)
        self.thread = threading.Thread(target=self._worker, daemon=True)
        self.thread.start()

    def submit(self, x_tensor, done_event, slot):
        self.q.put((x_tensor, done_event, slot))

    def _worker(self):
        while True:
            # 첫 요청 수집(블로킹)
            items = [self.q.get()]
            start = time.time()
            # 윈도우 동안 추가 집계
            while len(items) < self.max_batch and (time.time()-start) < self.window_ms:
                try:
                    items.append(self.q.get_nowait())
                except queue.Empty:
                    time.sleep(0.0005)
            # 배치 실행
            xs = torch.stack([it[0] for it in items], dim=0)
            ys = self.pred.predict_batch(xs).detach().cpu()
            # fan-out
            for out, (_, ev, slot) in zip(ys, items):
                slot["y"] = out.numpy().tolist()
                ev.set()

# 간단한 카나리/AB 라우팅 유틸(일관 해싱)

def stable_bucket(key: str, salt: str, buckets: int = 1000) -> int:
    h = hashlib.sha256((salt+key).encode()).hexdigest()
    return int(h[:8], 16) % buckets

def route_canary(user_id: str, weight: float, salt="canary") -> str:
    # weight: 0.0~1.0 → 그 비율만 'canary'로
    b = stable_bucket(user_id, salt)
    return "canary" if b < int(weight*1000) else "stable"

def route_ab(user_id: str, exp: str, split: float=0.5, salt="exp") -> str:
    b = stable_bucket(user_id, salt+exp)
    return "B" if b < int(split*1000) else "A"

class Handler(BaseHTTPRequestHandler):
    # 전역 리소스(실서비스면 DI 컨테이너로)
    predictor_stable = TorchPredictor("ckpt/model_stable.pt")
    predictor_canary = TorchPredictor("ckpt/model_canary.pt")
    batcher_stable = DynamicBatcher(predictor_stable, max_batch=64, window_ms=5)
    batcher_canary = DynamicBatcher(predictor_canary, max_batch=64, window_ms=5)

    def _json(self, code, obj, extra_hdr=None):
        body = json.dumps(obj).encode()
        self.send_response(code)
        self.send_header("Content-Type","application/json")
        self.send_header("Content-Length", str(len(body)))
        if extra_hdr:
            for k,v in extra_hdr.items(): self.send_header(k,v)
        self.end_headers()
        self.wfile.write(body)

    def do_GET(self):
        if self.path.startswith("/health"):
            self._json(200, {"ok":True,"device":DEVICE})
            return
        self._json(404, {"error":"not found"})

    def do_POST(self):
        t0 = time.time()
        if self.path.startswith("/predict"):
            # 입력(JSON): {"user_id": "...", "x": [..feature..], "exp": "rec-exp-1"}
            ln = int(self.headers.get("Content-Length","0"))
            raw = self.rfile.read(ln)
            try:
                req = json.loads(raw.decode())
                user = str(req.get("user_id","0"))
                x    = torch.tensor(req["x"], dtype=torch.float32)
            except Exception as e:
                self._json(400, {"error": f"bad input: {e}"}); return

            # 라우팅 결정
            lane = route_canary(user, weight=0.05)  # 5% 카나리
            arm  = route_ab(user, exp=req.get("exp","default"), split=0.5)

            # 동적 배치 제출
            done = threading.Event()
            slot = {}
            batcher = self.batcher_canary if lane=="canary" else self.batcher_stable
            try:
                batcher.submit(x, done, slot)
                ok = done.wait(timeout=0.8)  # SLA 상한
                if not ok:
                    self._json(504, {"error":"timeout"})
                    return
            except queue.Full:
                self._json(429, {"error":"busy"})
                return

            # (옵션) 섀도우 테스트: 반대 모델에도 비동기 전송(응답 영향 X)
            threading.Thread(target=self._shadow_call, args=(x, lane), daemon=True).start()

            # 응답
            hdr = {"X-Model-Lane": lane, "X-AB-Arm": arm}
            self._json(200, {"y": slot["y"], "lane": lane, "arm": arm, "ts": int(time.time())}, extra_hdr=hdr)
            # (실서비스) 여기에 관측 로그 남김(레이턴시, 2xx/4xx/5xx)
            return

        self._json(404, {"error":"not found"})

    def _shadow_call(self, x, lane):
        # 반대편으로만(정방향과 다른 모델) — 결과는 버림, 지표만 적재
        pred = (self.predictor_stable if lane=="canary" else self.predictor_canary).predict_batch(x.unsqueeze(0))
        # TODO: 파일/파이프에 로그 남기기

def run(port=8080):
    httpd = HTTPServer(("0.0.0.0", port), Handler)
    print(f"listening {port} on {DEVICE}")
    httpd.serve_forever()

if __name__=="__main__":
    run()
```

> **운영 팁**
> - 실제 환경에선 WSGI/ASGI(예: gunicorn/uvicorn)나 리버스 프록시(Nginx)를 앞단에 둡니다.
> - 위 코드는 **개념 증명**용(단일 프로세스). 멀티프로세스/멀티GPU는 **프로세스당 포트 + L4 로드밸런서**로 스케일.

---

## 카나리 배포: 안전한 점진 롤아웃

### 절차

1) **사전 검증**: 리그레션 테스트(스냅샷 입력에 대한 동일/개선 여부), p50/p95 레이턴시 확인
2) **섀도우(다크) 런**: 트래픽 복제 → 새 모델 점수 비교, **사용자 영향 없음**
3) **카나리 시작**: 1% → 5% → 10% → 25% … (각 단계 **모니터링 기간** 유지)
4) **가드레일**: KPI 하락/오류↑/p95↑ 임계 초과 시 **자동 롤백**
5) **완료**: 100% 전환, 구 모델 **드레인(drain)** 후 종료

### 가드레일/롤백(의사코드)

```python
# guardrail.py

from dataclasses import dataclass

@dataclass
class WindowMetrics:
    conv_uplift: float      # 전환율(또는 타 KPI) uplift vs control
    err_rate: float         # 5xx 비율
    p95_ms: float
    sample: int

def should_rollback(m: WindowMetrics) -> bool:
    if m.sample < 5000:  # 표본 부족
        return False
    if m.err_rate > 0.01:        # 1% 이상 5xx
        return True
    if m.p95_ms > 400:           # SLA 초과
        return True
    if m.conv_uplift < -0.02:    # KPI 2%p 하락
        return True
    return False
```

> **핵심**: “**사용자 보호** 우선”. **에러율·레이턴시**는 **절대 임계**로, KPI는 통계적 신뢰(표본·p-value) 함께 고려.

---

## A/B 테스트: 일관 버킷·측정·해석

### 버킷팅(일관 해싱)

- 같은 사용자/세션은 **항상 같은 팔(A/B)** 으로 → **일관성·오염 방지**
- 버킷 수(1000/10000) 중 일부를 A에, 나머지를 B에 할당
- 실험 ID/솔트로 **실험 간 독립성** 유지

> 위 서버 코드의 `route_ab()` 함수가 해당 패턴입니다.

### 측정

- **로그 스키마(예)**:
  ```json
  {
    "ts": 1695090001, "uid": "u123", "exp": "rec-exp-1", "arm": "B",
    "lane": "canary", "lat_ms": 87, "http": 200, "score": 0.73,
    "label": null, "model": "recnet:abc123", "features_hash": "..."
  }
  ```
- **오프라인 조인**: 나중에 **정답 라벨**(클릭/구매/정답)과 합쳐 KPI 계산(CTR/Accuracy 등)
- **분포 체크**: 팔 간 **피쳐 분포 차이**(오염/누출 여부) 점검

### 해석

- **AB는 평균 효과**를 본다. 소수군/꼬리군(p95 레이턴시, 롱테일 사용자)에 대한 **분할 분석**까지
- **장기 실험**에서 **드리프트**(사용자 적응) 고려 → 기간별 효과 추적

---

## 섀도우 테스트(다크 론칭) 패턴

- 실사용 요청을 **복제**해 새 모델에 전달, **응답은 버림**
- 장점: **안전**하게 지표 수집(레이턴시/오류/예측 분포)
- 주의: **부하 2배** → QPS 여유·리소스 배분
- 구현: 위 `Handler._shadow_call()`처럼 **비동기** 호출 + 결과만 로그

---

## 캐싱·피쳐 일관성·콜드 스타트

- **캐시 전략**: 동일 입력에 대한 재호출 빈도가 높다면 **응답 캐시**(TTL, 해시 키: 모델 버전+입력 요약)
- **피쳐 일관성**: 학습 시 전처리와 실시간 전처리가 달라지면 **훈-서스큐(Training-Serving Skew)**
  - **같은 코드/버전**을 쓰거나, 전처리를 **모델 안으로** 흡수(예: `nn.Module`에 포함)
- **콜드 스타트**: 유저 신규/피쳐 결측 → 안전 기본값/백오프 모델/탐색 정책

---

## 관측(Observability) — 무엇을 모니터링하나?

- **SLO**: p50/p95/p99, 4xx/5xx, 타임아웃율, 큐 길이, 배치 크기 분포
- **리소스**: GPU·CPU 사용률, GPU 메모리 피크, OOM/스로틀링
- **품질**: 실시간 스코어 분포(평균/표준편차/최댓값), 오프라인 조인한 KPI
- **드리프트**: 입력 피쳐의 분포 변화(KS 통계 간이)
- **릴리즈 라벨**: 로그/지표에 **모델 버전**·**데이터 버전**·**코드 커밋**을 **반드시** 남김

> 간단히라도 **JSON Lines** 로그를 남기고, 배치 집계 작업(또는 외부 관측 시스템)으로 대시보드화.

---

## 안정성·성능 최적화 체크리스트

- [ ] `torch.inference_mode()` + `autocast(bf16/fp16)`
- [ ] 마이크로배칭 윈도우(2~10ms) + 최대 배치(32~128) 튜닝
- [ ] (GPU) **고정 스트림** + 입력 텐서 **pin_memory=True**
- [ ] 워커 프로세스 수 ≥ GPU 수, **프로세스당 1 GPU** 원칙
- [ ] **워밍업** 후 헬스패스 `ready=true` 전환
- [ ] 백프레셔(큐 가득 → 429)와 타임아웃(504) 명확화
- [ ] 카나리 **자동 가드레일** + 원클릭 롤백
- [ ] 모델/피쳐 버전 **헤더** (`X-Model-Id`, `X-Feature-Hash`) 주입
- [ ] 섀도우/AB **샘플 비율** 과부하 방지
- [ ] 로그 PII 제거/익명화

---

## 예제: “배치 + 실시간” 하이브리드 추천 미니 시나리오

1) **야간 배치**: 사용자·아이템 임베딩 생성 → `emb_user.parquet`, `emb_item.parquet`
2) **온라인**: 쿼리 시 최근 클릭 피쳐 합성 + **탑K 후보**(배치 상점) 불러와
3) **실시간 rerank**(PyTorch) → 응답
4) 새 모델 릴리즈: 섀도우 → 1% 카나리 → 10% → 50% → 100%
5) KPI·레이턴시 모니터링, 가드레일 넘어가면 자동 롤백

---

## 모델 버전/아티팩트 규약

- 파일명: `model-{arch}-{ds}-{git}-{epoch}-{metric}.pt`
- 메타: `metadata.json` (학습 파라미터·데이터 버전·피쳐 스키마 해시)
- API 응답 헤더: `X-Model: recnet:abc123`, `X-Data: feats:2025-09-18`
- **하위 호환**: 입력 스키마 변경 시 **버전드 엔드포인트**(`/v2/predict`)

---

## 간단 로드테스트(표준 라이브러리)

```python
# loadgen.py

import json, time, random, threading, http.client

def worker(n, host="localhost", port=8080):
    ok=0; fail=0; lat=[]
    for _ in range(n):
        x = [random.random() for _ in range(128)]
        body = json.dumps({"user_id": f"u{random.randint(1,10_000)}", "x": x, "exp":"rec-exp-1"})
        t0=time.time()
        try:
            conn = http.client.HTTPConnection(host, port, timeout=1.0)
            conn.request("POST","/predict", body=body, headers={"Content-Type":"application/json"})
            r = conn.getresponse()
            _ = r.read()
            conn.close()
            dt = (time.time()-t0)*1000
            lat.append(dt)
            if r.status==200: ok+=1
            else: fail+=1
        except Exception:
            fail+=1
    return ok, fail, lat

if __name__=="__main__":
    threads=[]; results=[]
    for _ in range(16):
        t = threading.Thread(target=lambda: results.append(worker(200)))
        t.start(); threads.append(t)
    for t in threads: t.join()
    ok=sum(r[0] for r in results); fail=sum(r[1] for r in results)
    all_lat = [l for r in results for l in r[2]]
    all_lat.sort()
    def pct(p):
        k=int(len(all_lat)*p/100);
        return all_lat[min(k, len(all_lat)-1)] if all_lat else None
    print("OK",ok,"FAIL",fail,"p50",pct(50),"p95",pct(95),"p99",pct(99))
```

---

## 보안·거버넌스

- **입력 검증**: 타입/범위/길이(DoS·오염 방지)
- **레이트 리밋**: IP·유저 키 기반
- **비밀 관리**: 환경변수/시크릿, 응답/로그에 절대 노출 금지
- **감사 가능성**: 요청 ↔︎ 응답 상관 키, 모델 버전, 피쳐 해시로 **추적 가능성** 확보
- **개인정보**: 최소 수집, 익명화/부분 마스킹, TTL

---

## 운영 중 자주 겪는 문제와 해법

| 증상 | 원인 | 해결 |
|---|---|---|
| p95 급등, GPU 유휴 | 배치가 너무 작음/분산 과도 | 마이크로배칭 윈도우 ↑, 큐 용량 ↑, 리퀘스트 합치기 |
| 5xx 급증 | 모델 OOM/버그 | 입력 배치 상한↓, FP16→BF16, 체크포인트 안정판 롤백 |
| 카나리 KPI↓ | 피쳐 미스매치/전처리 차이 | 학습·서빙 전처리 코드 공유/버전 고정, 스키마 검증 |
| 섀도우 과부하 | 복제 QPS 과다 | 섀도우 샘플링률↓, 비동기 큐 분리 |
| AB 오염 | 버킷팅 불안정/솔트 재사용 | **일관 해싱 + 실험별 솔트** 고정 |

---

## “바로 적용” 순서(요약)

1) **모델 추론 코어** 정리: `inference_mode+autocast+warmup`
2) **실시간 서버** 배치: 동적 마이크로배칭 + 백프레셔 + 헬스체크
3) **로그 스키마** 확정(모델/데이터 버전 포함)
4) **섀도우 → 카나리** 단계와 **가드레일** 코드화
5) **A/B 버킷팅** 일관 해싱 도입, 리포트 파이프라인(오프라인 조인)
6) **배치 경로**도 마련(프리컴퓨트/백필), 하이브리드 완성

---

## 부록: TorchScript/`torch.compile` (옵션)

```python
# compile_or_script.py

import torch
def optimize_module(model: torch.nn.Module, mode="compile"):
    model.eval()
    if mode=="compile" and hasattr(torch, "compile"):
        return torch.compile(model, mode="reduce-overhead")
    elif mode=="script":
        return torch.jit.script(model)
    else:
        return model
```

> 제약: 동적 제어/커스텀 연산은 스크립팅이 어려울 수 있습니다. 먼저 **warmup + 마이크로배칭**으로 대부분의 이득을 얻은 뒤, 필요 시 도입하세요.

---

## 맺음말

- **배치 vs 실시간**은 상호 배타가 아닙니다. **프리컴퓨트(배치)**로 비용·지연을 줄이고, **실시간**으로 개인화·신선도·상호작용을 완성합니다.
- **카나리/A-B**는 **릴리즈 안전망**입니다. 트래픽 라우팅·관측·가드레일·롤백이 **툴이 아니라 절차**로 굳어야 합니다.
- 본문 코드는 **PyTorch를 기준**으로 추론 최적화와 동적 배칭·라우팅의 **핵심 뼈대**를 담았습니다. 실제 환경(멀티프로세스/멀티GPU/리버스 프록시/관측 스택)에 맞춰 센서·대시보드·CI/CD만 덧붙이면 즉시 실전에 투입 가능합니다.
