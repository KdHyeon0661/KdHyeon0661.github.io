---
layout: post
title: 기계학습 - 데이터의 종류와 특징
date: 2025-08-14 15:20:23 +0900
category: 기계학습
---
# 데이터의 종류와 특징

## 1. 데이터의 구조에 따른 분류

### 1. 정형 데이터(Structured)
- **형태**: 행-열 테이블(SQL/스프레드시트).  
- **특징**: 명확한 스키마. 조인/집계/인덱스/제약조건이 수월.  
- **예시**: 고객 테이블, 센서 시계열(정규 샘플링), 거래 로그.  
- **장점**: 전처리/통계분석 용이, 품질관리 쉬움.  
- **단점**: 자연어/이미지 등 복잡한 맥락 표현이 어려움.

### 2. 비정형 데이터(Unstructured)
- **형태**: 텍스트/이미지/오디오/비디오 등 고정 스키마 없음.  
- **특징**: 고차원·대용량·전처리(특징추출) 필요.  
- **예시**: 기사/리뷰, 사진/X-ray, 음성 콜, CCTV 영상.  
- **장점**: 맥락·풍부한 표현력.  
- **단점**: 연산·저장 비용↑, 전문 전처리/모델 필요.

### 3. 반정형 데이터(Semi-Structured)
- **형태**: 느슨한 스키마 + 메타데이터.  
- **예시**: JSON, XML, HTML, 로그(키-값), 이벤트 스트림.  
- **장점**: 교환/확장 용이, 스키마 진화에 유연.  
- **단점**: 파싱/정규화·스키마 추론 필요.

---

## 2. 데이터 값의 척도(Scale)

| 척도 | 의미 | 허용 연산 | 예시 | 전처리/모델 예시 |
|---|---|---|---|---|
| 명목(Nominal) | 구분만 의미 | =, ≠ | 성별, 국가 | 원-핫/해시, 트리계열 |
| 서열(Ordinal) | 순서 의미, 간격 불명 | =, ≠, <, > | 만족도, 등급 | 순서 인코딩, 순서 보존 손실 |
| 등간(Interval) | 간격 의미, 0 임의 | +, -, 평균 | °C, 날짜(연-월-일) | 중심화/차분, 계절성 처리 |
| 비율(Ratio) | 절대 0, 비율 의미 | 모든 산술 | 가격, 길이, 개수 | 로그변환, 스케일링 |

**주의**: 등간/비율 구분을 틀리면 비율·로그변환 해석이 무너진다.

---

## 3. 데이터 형식(Representation)

### 3.1 수치형(Numerical)
- **연속형**: 실수 범위(온도, 길이).  
- **이산형**: 정수 카운트(클릭 수).  
- 전처리: 표준화/정규화, 로그변환(양수·long-tail), 이상치 처리(IQR, winsorizing).

### 3.2 범주형(Categorical)
- **명목형**: 순서 없음(색상).  
- **서열형**: 순서 있음(등급).  
- 인코딩: 원-핫, 해시, 타겟/평균 인코딩(누수 주의), 순서 인코딩(단조 제약이 있는 모델과 궁합).

### 3.3 날짜·시간(Time/Date, Time Series)
- 성질: 계절성·추세·이상치(공휴일)·누수 위험(미래 정보).  
- 파생 변수: 요일/월/분기/영업일, **순환 인코딩(아래)**, 이동평균/차분/시차(lag) 특성.

### 3.4 순환형(Circular: 시간/각도)
- 24시간, 요일, 방위각 등은 **순환** 구조. 단순 정수로 넣으면 거리 왜곡.  
- **순환 인코딩**:
$$
x_{\sin}=\sin\frac{2\pi t}{T},\quad x_{\cos}=\cos\frac{2\pi t}{T}.
$$

```python
import numpy as np
def cyclical_encode(t, T):
    return np.column_stack([np.sin(2*np.pi*t/T), np.cos(2*np.pi*t/T)])
```

### 3.5 공간 좌표(위도/경도)
- 유클리드 거리 대신 지구 대원거리(**하버사인**) 사용.
$$
d=2R\arcsin\sqrt{\sin^2\frac{\Delta\phi}{2} + \cos\phi_1\cos\phi_2\sin^2\frac{\Delta\lambda}{2}}
$$
```python
import numpy as np
def haversine(lat1, lon1, lat2, lon2, R=6371.0):
    phi1, phi2 = np.radians(lat1), np.radians(lat2)
    dphi = np.radians(lat2 - lat1); dl = np.radians(lon2 - lon1)
    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dl/2)**2
    return 2*R*np.arcsin(np.sqrt(a))
```

### 3.6 텍스트(Text)
- 토큰화 → 벡터화(BOW/TF-IDF) → n-grams → 임베딩.  
- 지표: 희소성, 어휘 크기, 길이 분포.  
- 예제(TF-IDF + 로지스틱):
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

X = ["좋아요 품질이 좋아요", "별로에요 불량입니다", "만족합니다 재구매", "최악 다시는 안사요"]
y = [1,0,1,0]
pipe = Pipeline([
    ("tfidf", TfidfVectorizer(min_df=1, ngram_range=(1,2))),
    ("clf", LogisticRegression(max_iter=1000))
])
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)
pipe.fit(Xtr, ytr)
print("Test acc:", pipe.score(Xte, yte))
```

### 3.7 이미지(Image)
- 3D 텐서(H×W×C). 채널 정규화(평균/표준편차), 리사이즈/크롭, 증강(플립/회전/색상).  
- 단순 베이스라인(벡터화 → 선형/트리), 실전은 CNN/비전 트랜스포머.

### 3.8 오디오(Audio)
- 파형 → 스펙트로그램/STFT → **MFCC** 등의 특징.  
- 샘플링레이트 정규화, 윈도잉, 잡음 제거/증강(타임시프트).

### 3.9 그래프(Graph)/군집(Set)/다중출력
- 노드/엣지/속성, 이웃집계, GNN 고려.  
- 멀티라벨(여러 클래스 동시), 다중출력 회귀/분류 주의(지표·손실).

---

## 4. 데이터 특성이 전처리·모델 선택에 미치는 영향

### 4.1 전처리
- **수치형**: 표준화/정규화, 로그변환, 이상치 처리.  
- **범주형**: 원-핫(저/중 카디널리티), 해시 트릭(대카디널리티), 타겟 인코딩(누수방지 K-fold).  
- **텍스트/이미지/오디오**: 도메인 특화 특징추출(TF-IDF, CNN, MFCC).  
- **날짜/좌표/순환**: 순환 인코딩, 지리적 거리, 시차·롤링, 공휴일 특성.

### 4.2 거리/유사도
- 유클리드: 수치형 스케일 민감.  
- 코사인: 방향 중시(텍스트 TF-IDF와 궁합).  
- 하버사인: 위경도.  
- **Gower**: 혼합형(수치+범주)용.
$$
\text{Gower}(i,j)=\frac{1}{p}\sum_{k=1}^p s_{ijk},\quad s_{ijk}\in[0,1].
$$
(수치형은 정규화 거리, 범주형은 일치/불일치)

```python
import numpy as np

def gower_pairwise(X_num, X_cat):
    # X_num: 수치형 (n, p1), X_cat: 범주형 (n, p2)
    n = X_num.shape[0]
    # 수치형 범위 정규화
    num_min = X_num.min(axis=0); num_max = X_num.max(axis=0)
    num_rng = np.where(num_max>num_min, num_max-num_min, 1.0)
    Xn = (X_num - num_min)/num_rng
    D = np.zeros((n,n), dtype=float)
    for i in range(n):
        # 브로드캐스트로 한번에 계산
        num_dist = np.abs(Xn[i] - Xn).mean(axis=1) if Xn.shape[1]>0 else 0.0
        if X_cat.shape[1]>0:
            cat_match = (X_cat[i] == X_cat).mean(axis=1)
            cat_dist = 1 - cat_match
            D[i] = (num_dist + cat_dist) / ( (Xn.shape[1]>0) + (X_cat.shape[1]>0) )
        else:
            D[i] = num_dist
    return D
```

### 4.3 알고리즘 선택
- **거리 기반(KNN/K-Means)**: 스케일·거리 정의 중요(수치만/혼합형은 Gower).  
- **트리/앙상블**: 스케일 영향 작음, 범주형 처리 우수(고카디널리티는 주의).  
- **선형/커널**: 스케일링 필수, 고차원 희소(텍스트)에 강함(정규화 중요).  
- **딥러닝**: 도메인별 표준 전처리/증강 레시피.

---

## 5. 데이터 품질과 라벨 품질

### 5.1 품질 차원
- **정확성**(Accuracy), **완전성**(Completeness), **일관성**(Consistency), **고유성**(Uniqueness), **적시성**(Timeliness).

### 5.2 결측(Missingness)
- 유형: **MCAR/MAR/MNAR**.  
- 처치: 단순 대치(평균/최빈), **KNNImputer**, **MICE**(다중 대치), 모형 기반 대치.

```python
import numpy as np, pandas as pd
from sklearn.impute import SimpleImputer, KNNImputer

X = pd.DataFrame({"a":[1,2,np.nan,4], "b":[10,np.nan,30,40]})
imp_mean = SimpleImputer(strategy="mean").fit_transform(X)
imp_knn  = KNNImputer(n_neighbors=2).fit_transform(X)
```

### 5.3 이상치(Outlier)
- z-점수, IQR(사분위 범위)로 탐지.  
- 강건 모델(Huber, RANSAC), winsorizing, 트리 기반.

```python
import numpy as np
def iqr_winsorize(x, k=1.5):
    q1, q3 = np.percentile(x, [25, 75]); iqr = q3-q1
    lo, hi = q1 - k*iqr, q3 + k*iqr
    return np.clip(x, lo, hi)
```

### 5.4 중복/누수/편향
- 중복 제거(고유키/퍼지 매칭),  
- **누수(Leakage)**: 미래/라벨정보가 피처에 섞임(타겟 인코딩 전처리 시 특히 주의),  
- **표본편향**: 모집단 대비 샘플 분포 차이 → 리샘플링/가중치.

### 5.5 라벨 품질
- 하드/소프트 라벨, 다중 어노테이터 합의(플립 레이트/신뢰도 추정),  
- 노이즈 견고 손실/로버스트 학습 전략.

---

## 6. 데이터 분할/검증 전략

### 6.1 IID(일반 탭ular)
- KFold/StratifiedKFold.

### 6.2 그룹 의존(사용자/세션)
- **GroupKFold**: 그룹이 train/test에 동시에 등장하지 않도록.

### 6.3 시계열(Time Series)
- **TimeSeriesSplit**: 과거→미래 순서 유지, 누수 차단.  
- **Purged/Embargo CV**: 이벤트 윈도 겹침 제거(금융 등).

```python
from sklearn.model_selection import TimeSeriesSplit
X = list(range(10))
tscv = TimeSeriesSplit(n_splits=3)
for tr, te in tscv.split(X):
    print("train:", tr, "test:", te)
```

---

## 7. 실전 파이프라인(혼합형 탭ular 데이터)

### 7.1 데이터 예시/타입 지정
```python
import pandas as pd
df = pd.DataFrame({
    "age":[25, 32, 29, 41, 37],                 # 수치(비율)
    "gender":["M","M","F","F","M"],             # 명목
    "satisfaction":["high","mid","low","mid","high"],  # 서열
    "price":[100.0, 210.5, 150.2, 90.0, 300.0], # 비율
    "hour":[2, 6, 23, 0, 14],                   # 순환(24h)
    "country":["KR","US","KR","JP","US"],       # 명목(고카디널 가능)
    "target":[1,0,1,0,1]                        # 분류 라벨
})
```

### 7.2 전처리(수치/범주/순환/해시/타겟인코딩)
- **원-핫**: 저/중 카디널리티.  
- **해시**: 고카디널(메모리·과적합 억제).  
- **순환 인코딩**: hour.  
- **타겟 인코딩**: 누수 방지용 K-fold 평균 인코딩(간단 구현).

```python
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import KFold

class CyclicalHour(BaseEstimator, TransformerMixin):
    def __init__(self, col="hour", T=24):
        self.col=col; self.T=T
    def fit(self, X, y=None): return self
    def transform(self, X):
        t = X[self.col].astype(float).values
        return np.column_stack([np.sin(2*np.pi*t/self.T), np.cos(2*np.pi*t/self.T)])

class KFoldTargetEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, col, n_splits=5, smoothing=5.0, random_state=42):
        self.col=col; self.n_splits=n_splits; self.smoothing=smoothing; self.random_state=random_state
    def fit(self, X, y):
        self.global_mean_ = float(np.mean(y))
        self.cat_stats_ = X[[self.col]].copy()
        self.cat_stats_["y"]=y
        self.map_ = self.cat_stats_.groupby(self.col)["y"].mean().to_dict()
        return self
    def transform(self, X):
        # 단순 hold-out용: 학습 후 적용 시 사용(실전 CV에는 fold-wise 적용 필요)
        s = X[self.col].map(self.map_).fillna(self.global_mean_).astype(float).values.reshape(-1,1)
        return s
```

### 7.3 ColumnTransformer + 파이프라인
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction import FeatureHasher
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

num_cols = ["age","price"]
nominal_low = ["gender"]         # 원-핫
nominal_high = ["country"]       # 해시
ordinal_cols = ["satisfaction"]  # 순서 인코딩 매핑
hour_col = "hour"

# 순서 인코딩 매핑
ord_map = {"low":0, "mid":1, "high":2}
df["satisfaction_ord"] = df["satisfaction"].map(ord_map)

X = df[ num_cols + ["gender","country","satisfaction_ord","hour"] ]
y = df["target"].values

num_pipe = Pipeline([("sc", StandardScaler())])
cat_low_pipe = Pipeline([("oh", OneHotEncoder(handle_unknown="ignore"))])
cat_high_pipe = Pipeline([("fh", FeatureHasher(input_type="string", n_features=8))])
hour_pipe = Pipeline([("cyc", CyclicalHour(col=hour_col, T=24))])

pre = ColumnTransformer([
    ("num", num_pipe, num_cols),
    ("cat_low", cat_low_pipe, nominal_low),
    ("cat_high", cat_high_pipe, nominal_high),
    ("hour", hour_pipe, [hour_col]),
    ("ord", "passthrough", ["satisfaction_ord"]),
])

clf = Pipeline([
    ("pre", pre),
    ("lr", LogisticRegression(max_iter=1000))
])

Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)
clf.fit(Xtr, ytr)
print("Test acc:", clf.score(Xte, yte))
```

**설명**  
- 혼합형 데이터를 **하나의 파이프라인**으로 묶어 재현성 확보.  
- 고카디널 범주는 **해시**로 빠르게 축소.  
- 순환형은 sin/cos.  
- 순서형은 수치 매핑(단조 관계 가정).  
- 타겟 인코딩은 누수 우려 → 실제 프로젝트에서는 **KFold 내부에서 transform**.

---

## 8. 고급 토픽

### 8.1 고차·희소·다중공선성
- 텍스트/원-핫은 **희소 고차원** → L2/L1 정규화·선형모델 적합.  
- 다항/상호작용은 공선성↑ → 규제(릿지)·차원축소(PCA)·특성선택.

### 8.2 고유치수/내재 차원
- 매니폴드 가정(이미지/음성) → PCA/UMAP/오토인코더로 저차 임베딩.

### 8.3 데이터 드리프트/셰프트
- **공변량 시프트**: \(p(x)\) 변화, **라벨 시프트**: \(p(y)\) 변화, **개념 드리프트**: \(p(y\mid x)\) 변화.  
- 대응: 모니터링(KS/PSI), 재학습, 중요도 재추정, 도메인 적응.

---

## 9. 개인정보·거버넌스

- **PII**: 이름/전화/주민번호/위치/얼굴. 마스킹/가명화/권한통제.  
- **k-익명성**: 동일 준식별자 그룹 크기 ≥ k.  
- **데이터 시트/카드**: 수집원/스키마/라벨링/제한/편향 보고.  
- **버전관리**: DVC/MLflow로 데이터·모델 동시 버전.

---

## 10. 현업 체크리스트

- [ ] 데이터 구조: 정형/반정형/비정형 구분, 스키마 정의/검증  
- [ ] 척도 확인: 명목/서열/등간/비율에 맞는 인코딩/변환  
- [ ] 형식별 레시피: 순환/좌표/시계열/텍스트/이미지별 전처리  
- [ ] 품질: 결측/이상치/중복/라벨 노이즈 점검  
- [ ] 분할: IID vs 그룹 vs 시계열에 맞는 CV  
- [ ] 거리: KNN/KMeans 전에 스케일/거리 정의 점검(Gower/하버사인)  
- [ ] 카디널리티: 해시/타겟 인코딩, 누수 방지 K-fold 설계  
- [ ] 규제/희소성: L1/L2, 차원축소, 특성선택  
- [ ] 드리프트: 입력/성능 모니터링, 재학습 전략  
- [ ] PII/보안/감사: 접속·변경 로그, 데이터 시트 유지

---

## 11. 요약
- 데이터의 **구조·척도·형식**은 전처리/거리/알고리즘/검증 전략을 결정한다.  
- 혼합형 탭ular에선 **ColumnTransformer + 파이프라인**으로 재현성과 누수 방지를 달성한다.  
- 시계열/좌표/순환/텍스트/이미지 등 **도메인 특성**을 반영하면 성능과 견고성이 크게 올라간다.  
- 품질·라벨·드리프트·거버넌스까지 포함한 **데이터 중심** 접근이 실무 성공의 핵심이다.