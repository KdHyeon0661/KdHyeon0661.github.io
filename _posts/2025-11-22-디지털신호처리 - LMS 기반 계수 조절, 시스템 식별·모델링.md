---
layout: post
title: 디지털신호처리 - LMS 기반 계수 조절, 시스템 식별·모델링
date: 2025-11-22 16:25:23 +0900
category: 디지털신호처리
---
# 적응 필터 응용 — LMS 기반 계수 조절, 시스템 식별·모델링

다룰 범위는 다음 두 축이다.

1) **계수 조절을 위한 LMS 알고리즘**
   - 비용함수, 경사하강법, 업데이트 식 유도
   - 스텝 사이즈(μ), 수렴/안정 조건, 오차의 통계적 해석
   - NLMS/Leaky LMS/Sign-LMS 등 실무 확장
   - 유한정밀도에서의 주의점

2) **시스템 식별(system identification)과 시스템 모델링(system modeling)**
   - 미지의 LTI 시스템 식별(“블랙박스 경로 학습”)
   - 잡음이 있는 관측에서 모델을 뽑아내는 방법
   - 선형 예측(Adaptive Prediction)으로 AR 모델/스펙트럼 모델링
   - 대표 응용(에코 캔슬러, 채널 추정/등화, 능동 소음제거, 적응 라인 인핸서)

예제는 모두 **GNU Octave**로 바로 재현 가능하게 구성했다.

---

## 0) 왜 적응 필터가 필요한가

고정 FIR/IIR 필터는 계수 \(b_k, a_k\)가 **설계 시점에 고정**된다.
하지만 현실 문제는 종종 다음과 같다.

- **시스템이 시간에 따라 변한다**
  - 통신 채널이 이동/페이딩으로 변함
  - 스피커–마이크로폰 경로가 사람/물체 이동으로 변함(에코 경로)
- **시스템을 모른다**
  - 측정할 수 없는 경로/기계/환경의 동특성
  - Unknown plant, unknown channel
- **잡음/간섭 경로가 바뀐다**
  - ANC(Active Noise Control)에서 2차 경로가 조금씩 변함
  - 산업 현장 센서의 간섭 경로 변화

이때는 **데이터(입력·출력)를 보면서 필터 계수를 “학습”해야** 한다.
이게 적응 필터다.

---

## 1) 적응 FIR 필터의 기본 구조

적응 필터는 거의 항상 **FIR(탭 지연선)** 기반으로 표현한다.

- 입력 벡터
  $$
  \mathbf{x}[n]=
  \begin{bmatrix}
  x[n]\\ x[n-1]\\ \vdots\\ x[n-M]
  \end{bmatrix}
  $$
- 계수(가중치) 벡터
  $$
  \mathbf{w}[n]=
  \begin{bmatrix}
  w_0[n]\\ w_1[n]\\ \vdots\\ w_M[n]
  \end{bmatrix}
  $$

출력은 내적:

$$
y[n] = \mathbf{w}^T[n]\mathbf{x}[n].
$$

우리는 **원하는 목표 신호(desired)** \(d[n]\)이 있고,
필터 출력 \(y[n]\)가 그것과 같아지도록 계수를 업데이트한다.

오차:

$$
e[n] = d[n] - y[n]
     = d[n] - \mathbf{w}^T[n]\mathbf{x}[n].
$$

---

## 2) LMS 알고리즘의 유도

### 최소자승(MSE) 비용함수

적응 필터가 최소화하려는 대표 비용은 **평균제곱오차(MSE)**:

$$
J(\mathbf{w}) = \mathbb{E}\{e^2[n]\}
              = \mathbb{E}\{(d[n]-\mathbf{w}^T\mathbf{x}[n])^2\}.
$$

이때 최적 해는 **위너 해(Wiener solution)**:

$$
\mathbf{w}^\star = \mathbf{R}^{-1}\mathbf{p}
$$

여기서

- 입력 자기상관 행렬
  $$
  \mathbf{R} = \mathbb{E}\{\mathbf{x}[n]\mathbf{x}^T[n]\}
  $$
- 교차상관 벡터
  $$
  \mathbf{p} = \mathbb{E}\{d[n]\mathbf{x}[n]\}.
  $$

문제는 \(\mathbf{R}, \mathbf{p}\)를 **모른다**는 것.
대신 “데이터가 들어올 때마다” 근사해서 업데이트한다.

---

### 경사하강법(Gradient Descent)

비용함수의 기울기:

$$
\nabla J(\mathbf{w})
= \frac{\partial}{\partial \mathbf{w}} \mathbb{E}\{e^2[n]\}
= -2\,\mathbf{p} + 2\,\mathbf{R}\mathbf{w}.
$$

경사하강 업데이트:

$$
\mathbf{w}[n+1]
= \mathbf{w}[n] - \frac{\mu}{2}\nabla J(\mathbf{w}[n])
= \mathbf{w}[n] + \mu(\mathbf{p} - \mathbf{R}\mathbf{w}[n]).
$$

하지만 \(\mathbf{p}, \mathbf{R}\)는 기대값이라 직접 계산 못 함.
그래서 **순간값(instantaneous) 근사**를 쓴다.

---

### 순간기울기 근사 → LMS

순간 비용:

$$
\hat{J}[n]=e^2[n].
$$

순간 기울기:

$$
\nabla \hat{J}[n]
= \frac{\partial}{\partial \mathbf{w}} e^2[n]
= 2e[n]\frac{\partial e[n]}{\partial \mathbf{w}}
= -2e[n]\mathbf{x}[n].
$$

따라서

$$
\mathbf{w}[n+1]
= \mathbf{w}[n] - \frac{\mu}{2}(-2e[n]\mathbf{x}[n])
= \mathbf{w}[n] + \mu e[n]\mathbf{x}[n].
$$

이게 **LMS 업데이트 식**이다.

---

## 3) LMS의 해석: 스텝 사이즈 μ, 수렴과 안정

### 안정(Mean Stability) 조건

입력 상관행렬 \(\mathbf{R}\)의 최대 고유값 \(\lambda_{\max}\)에 대해:

$$
0 < \mu < \frac{2}{\lambda_{\max}}
$$

이 범위를 벗어나면 평균적으로 발산한다.

실무에서 직접 \(\lambda_{\max}\)를 모를 때는 입력 파워로 근사:

$$
\mu \lesssim \frac{1}{(M+1)\,P_x}
,\quad P_x=\mathbb{E}\{x^2[n]\}.
$$

**탭 수가 많을수록 μ를 더 작게 해야** 안정/수렴한다.

---

### 수렴 속도 vs 정상상태 오차

- μ가 크면: **빠르게 수렴**하지만 정상상태에서 오차가 큼(“미스어드저스트먼트”)
- μ가 작으면: **천천히 수렴**하지만 더 정확

정상상태 미스어드저스트먼트(Misadjustment) 근사:

$$
\mathcal{M} \approx \frac{\mu}{2}\,\mathrm{tr}(\mathbf{R}).
$$

즉 μ와 입력 분산이 커질수록 정상상태 잔차가 커진다.

---

### 입력이 컬러드(colored)하면 느려진다

\(\mathbf{R}\)의 고유값 분산(조건수):

$$
\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}
$$

가 크면(입력이 컬러드/상관이 큼) 수렴이 **훨씬 느려진다**.
이게 실무에서 NLMS나 RLS로 가는 이유다.

---

## 4) LMS의 실무 확장

### NLMS (Normalized LMS)

입력 크기가 변하면 LMS는 순간적으로 과도 업데이트를 해서 불안정해질 수 있다.
그래서 입력 에너지로 정규화한다.

$$
\mathbf{w}[n+1]
= \mathbf{w}[n] + \frac{\mu}{\delta + \|\mathbf{x}[n]\|^2}\,e[n]\mathbf{x}[n].
$$

- \(\delta\): 0 나눗셈 방지 작은 값
- **입력 파워에 무관하게 안정**
- 수렴 속도도 개선되는 경우가 많음

---

### Leaky LMS (계수 누설)

시간이 오래 지나면 계수가 과도하게 커지고 드리프트하는 문제를 방지:

$$
\mathbf{w}[n+1]
= (1-\mu\gamma)\mathbf{w}[n] + \mu e[n]\mathbf{x}[n].
$$

- \(\gamma>0\)가 작을수록 “천천히 누설”
- 잡음이 큰 환경에서 안정적

---

### Sign-LMS / Sign-Sign LMS (저연산 버전)

곱셈기 비용이 큰 임베디드에서 쓰는 근사형:

- Sign-LMS
  $$
  \mathbf{w}[n+1]
  = \mathbf{w}[n] + \mu\,\mathrm{sign}(e[n])\,\mathbf{x}[n].
  $$
- Sign-Sign
  $$
  \mathbf{w}[n+1]
  = \mathbf{w}[n] + \mu\,\mathrm{sign}(e[n])\,\mathrm{sign}(\mathbf{x}[n]).
  $$

연산은 줄지만 수렴이 느려지고 잔차가 커질 수 있다.

---

## 5) 시스템 식별(System Identification)

### 문제 정의

미지의 LTI 시스템 \(h[k]\)가 있고, 그 시스템을 통과한 출력이 관측된다.

- 미지 시스템 출력:
  $$
  d[n] = \sum_{k=0}^{M} h[k] x[n-k] + v[n]
       = \mathbf{h}^T\mathbf{x}[n] + v[n].
  $$

여기서 \(v[n]\)은 측정 잡음.

적응 필터가 \(\mathbf{w}[n]\)으로 \(d[n]\)을 추정하고,
결국

$$
\mathbf{w}[n]\to \mathbf{h}
$$

가 되도록 학습하는 것이 **시스템 식별**이다.

---

### 실무 예시

1. **에코 경로 추정(Acoustic Echo Cancellation)**
   - \(x[n]\): 스피커로 나가는 신호
   - \(d[n]\): 마이크에 들어온 에코 + 근단 화자 + 잡음
   - 목표: 에코 경로 \(h\)를 학습해 에코 제거

2. **통신 채널 추정(Channel Estimation)**
   - \(x[n]\): 파일럿/훈련 시퀀스
   - \(d[n]\): 채널 통과 + 잡음
   - \(\mathbf{w}\)가 채널 임펄스 응답을 추정

3. **기계/구조물 동특성 추정**
   - \(x[n]\): 입력(force/torque)
   - \(d[n]\): 센서 출력(accel/displacement)
   - 경로를 **온라인으로 추정**해 제어기 보정

---

### Octave 예제 1: 기본 LMS 시스템 식별

**상황**
- 미지 시스템은 8탭 FIR(임의 계수)
- 입력은 백색잡음
- LMS가 시스템을 학습

```octave
clear; close all; clc

N = 6000;                 % 데이터 길이
M = 7;                    % 적응 필터 차수(8탭)
mu = 0.02;                % LMS step

% 미지 시스템(ground truth)
h = [0.7, -0.4, 0.25, 0.1, -0.05, 0.03, 0.02, -0.01]';

% 입력
x = randn(N,1);

% 미지 시스템 출력 + 잡음
v = 0.01*randn(N,1);
d = filter(h,1,x) + v;

% LMS
w = zeros(M+1,1);
y = zeros(N,1);
e = zeros(N,1);

for n = M+1:N
  xvec = x(n:-1:n-M);
  y(n) = w'*xvec;
  e(n) = d(n) - y(n);
  w = w + mu*e(n)*xvec;
end

fprintf("True h:\n"); disp(h');
fprintf("Estimated w:\n"); disp(w');

figure; stem(h,'filled'); hold on; stem(w,'r'); grid on;
legend("true h","estimated w");
title("System Identification by LMS");
```

**해석**
- 충분한 데이터와 적당한 μ가 있으면 \(w\)가 \(h\)에 근접한다.
- 잡음이 커지면 최종 오차 바닥이 생긴다.

---

### Octave 예제 2: 컬러드 입력에서 LMS vs NLMS

컬러드 입력(상관 큰 입력)은 LMS가 느리다.
NLMS로 개선됨을 확인한다.

```octave
clear; close all; clc

N=12000; M=15;
h = fir1(M, 0.4)';             % 미지 시스템(저역형 경로)
u = randn(N,1);
x = filter([1 0.7 0.4],1,u);   % 컬러드 입력
d = filter(h,1,x) + 0.01*randn(N,1);

mu_lms = 0.01;
mu_nlms = 0.7; delta=1e-3;

w1=zeros(M+1,1); w2=zeros(M+1,1);
e1=zeros(N,1); e2=zeros(N,1);

for n=M+1:N
  xvec=x(n:-1:n-M);

  % LMS
  y1=w1'*xvec; e1(n)=d(n)-y1;
  w1 = w1 + mu_lms*e1(n)*xvec;

  % NLMS
  y2=w2'*xvec; e2(n)=d(n)-y2;
  normx = (xvec'*xvec);
  w2 = w2 + (mu_nlms/(delta+normx))*e2(n)*xvec;
end

% 학습곡선(제곱오차 이동평균)
L=200;
J1=filter(ones(L,1)/L,1,e1.^2);
J2=filter(ones(L,1)/L,1,e2.^2);

figure; plot(10*log10(J1+1e-12)); hold on;
plot(10*log10(J2+1e-12),'r'); grid on;
legend("LMS","NLMS");
title("Learning Curve: colored input");
ylabel("MSE (dB)");
xlabel("n");
```

**해석**
- NLMS는 입력 파워 변화를 자동 보상 → 수렴이 훨씬 빠르다.
- 컬러드 입력에서 LMS가 느린 이유는 \(\kappa\)가 큰 \(\mathbf{R}\) 때문.

---

## 6) 시스템 모델링(System Modeling)

시스템 식별이 “미지 LTI 경로 \(h\)를 찾는 문제”라면,
시스템 모델링은 더 넓게 다음을 뜻한다.

1) **관측 데이터로 동적 모델을 만들어 미래를 예측하거나**
2) **모델을 기반으로 스펙트럼/특성을 추정**하는 것.

가장 대표가 **선형 예측(linear prediction)**이다.
여기서 적응 FIR은 “예측기”로 동작한다.

---

### 선형 예측 문제

현재 샘플을 과거 샘플의 선형 결합으로 예측:

$$
\hat{x}[n] = \sum_{k=1}^{M} w_k\,x[n-k]
           = \mathbf{w}^T\mathbf{x}_p[n]
$$

여기서 예측 입력 벡터:

$$
\mathbf{x}_p[n]=
\begin{bmatrix}
x[n-1]\\ x[n-2]\\ \vdots\\ x[n-M]
\end{bmatrix}.
$$

오차:

$$
e[n] = x[n] - \hat{x}[n].
$$

이 오차의 MSE를 최소화하는 \(\mathbf{w}\)가
바로 **AR 모델의 계수**와 연결된다.

---

### AR 모델과의 연결

AR(M) 과정:

$$
x[n] = \sum_{k=1}^{M} a_k x[n-k] + u[n]
$$

\(u[n]\)은 백색 혁신(innovation).

최적 예측기 계수는

$$
w_k^\star = a_k
$$

와 같은 구조로 수렴한다.
즉 적응 예측기를 돌리면 **데이터의 AR 모델을 온라인으로 추정**할 수 있다.

---

### Octave 예제 3: LMS 예측기로 AR(2) 모델링

**상황**
- 실제 데이터는 AR(2)로 생성
- 적응 예측기가 AR 계수를 학습하는지 확인

```octave
clear; close all; clc

N=10000; M=2; mu=0.05;

% 실제 AR(2) 계수
a_true = [1.2, -0.7];  % x[n]=1.2 x[n-1]-0.7 x[n-2]+u[n]

u = randn(N,1);
x = zeros(N,1);
for n=3:N
  x(n) = a_true(1)*x(n-1) + a_true(2)*x(n-2) + u(n);
end
x = x / std(x);  % 정규화

% LMS 예측기
w=zeros(M,1);
e=zeros(N,1);

for n=M+1:N
  xp = x(n-1:-1:n-M);
  xhat = w'*xp;
  e(n) = x(n) - xhat;
  w = w + mu*e(n)*xp;
end

fprintf("True AR coeffs: "); disp(a_true);
fprintf("Estimated w:    "); disp(w');

% 예측 오차 분산
fprintf("Prediction MSE=%.3e\n", mean(e(M+1:end).^2));
```

**해석**
- 데이터가 AR 과정이면, LMS 예측기 계수는 AR 계수로 수렴한다.
- 이 방식이 음성 코딩(LPC), 스펙트럼 추정의 기본이다.

---

## 7) 대표 응용 시나리오 정리

### 시스템 식별 기반 응용

#### (1) 에코 캔슬러(AEC)

- 목표: 에코 경로 \(h\)를 LMS로 추정
- 에코 추정 \(\hat{d}_\text{echo}\)를 빼서 잔차만 남김

구조:
```text
x[n] ─▶(미지 에코 경로 h)──▶ echo ──┐
                                    ▼
                                  (+)──▶ d[n]
                                    ▲
x[n] ─▶(적응 필터 w)──▶ y[n]=echo^ ──┘
e[n]=d[n]-y[n] → w 업데이트
```

실무 팁:
- 입력이 음악/음성이라 컬러드 → NLMS가 사실상 표준
- 더블토크(근단 화자) 상황에서 업데이트 정지/완화 필요

---

#### (2) 채널 추정/등화

- 훈련 신호 \(x[n]\)를 보내고 수신 \(d[n]\)에서 채널 \(h\)를 추정
- 추정된 채널로 역필터/등화기를 설계

---

### 시스템 모델링 기반 응용

#### (1) 적응 라인 인핸서(ALE)

- 잡음 속에 묻힌 협대역 톤/주기 성분을 추출
- “지연된 자기 자신으로 예측” 구조

#### (2) 적응 잡음 제거(ANC)

- 참조 잡음 \(x[n]\)이 있고, 주 신호 \(d[n]=s[n]+v[n]\)
- 적응 필터가 \(v[n]\)를 모델링해서 제거

---

## 8) 유한정밀도(반올림)와 LMS의 주의점

앞에서 FIR/IIR 반올림 영향을 봤듯, LMS도 마찬가지다.

### 업데이트 자체가 양자화된다

고정소수점에서

$$
\mathbf{w}[n+1]=\mathbf{w}[n]+\mu e[n]\mathbf{x}[n]
$$

의 우변이 라운딩되면

- 아주 작은 \(e[n]\mathbf{x}[n]\)는 **업데이트가 0으로 소멸**
- 수렴이 느려지거나, 더 이상 미세 조정이 안 되는 바닥이 생김

실무 대응:
- μ를 너무 작게 잡으면 고정소수점에서 “멈춤” 발생
- 가드비트/스케일링으로 \(e, x\)의 동적범위를 확보

---

### limit cycle 유사 현상

IIR처럼 강하진 않지만,
고정소수점 LMS는 계수가 **작은 진동**을 하며 멈출 수 있다.

- Leaky LMS는 이런 진동을 줄이는 데 도움
- NLMS는 입력 파워 변동에 강해 수치적으로 더 안정

---

## 9) 전체 정리

1. **LMS의 본질**
   - MSE 비용을 순간기울기로 근사한 경사하강법
   - 업데이트:
     $$
     \mathbf{w}[n+1]=\mathbf{w}[n]+\mu e[n]\mathbf{x}[n]
     $$
2. **수렴/안정**
   - \(0<\mu<2/\lambda_{\max}\)
   - μ↑ → 빠르지만 잔차↑, μ↓ → 느리지만 정확
   - 컬러드 입력에서 느려짐(조건수 문제)
3. **실무 확장**
   - NLMS: 입력 파워 정규화로 안정/속도 개선
   - Leaky LMS: 드리프트/수치 문제 완화
   - Sign 계열: 저연산 대안
4. **시스템 식별**
   - 미지 LTI 경로를 FIR 적응필터로 학습
   - 에코 캔슬러/채널 추정/기계 경로 추정의 핵심
5. **시스템 모델링**
   - 적응 예측기로 AR 모델/스펙트럼 특성 온라인 추정
   - LPC, 음성/레이더/진동 신호 모델링의 기본
6. **유한정밀도**
   - 업데이트 양자화 바닥/미세조정 정지/잡음 증가에 주의
   - NLMS/Leaky + 스케일링/가드비트로 완화

---

## 10) 연습문제(짧게)

1. 위 예제 1에서 μ를 0.01, 0.05, 0.2로 바꿔 학습곡선을 비교하라.
   - 수렴 속도와 정상상태 MSE가 어떻게 변하는지 정리하라.

2. 예제 2에서 입력 필터 \([1 0.7 0.4]\)를 \([1 0.95 0.9]\)로 바꿔 조건수를 더 키워라.
   - LMS/NLMS의 수렴 차이가 얼마나 벌어지는가?

3. 예제 3에서 AR 계수를 \([0.9, -0.2]\), \([1.5, -0.9]\)로 바꿔
   - 예측기의 안정성과 수렴을 비교하라.
