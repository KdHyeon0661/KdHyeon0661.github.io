---
layout: post
title: 기계학습 - SVM 커널 기법의 수학적 정당성
date: 2025-08-18 22:25:23 +0900
category: 기계학습
---
# SVM 커널 기법의 수학적 정당성 (Mercer’s Theorem 기반)

서포트 벡터 머신(SVM)의 강력함은 **커널 트릭(Kernel Trick)**에서 비롯됩니다.  
커널 트릭은 원래 입력 공간에서 비선형으로 분리되지 않는 데이터를 **고차원 특징 공간**으로 사상(mapping)하여 **선형 분리 가능**하게 만드는 핵심 아이디어입니다.  
이 과정에서 **명시적인 변환** 없이 **내적만 계산**하는 방법이 바로 **Mercer’s Theorem**에 의해 정당화됩니다.

---

## 1. 커널 트릭(Kernel Trick) 기본 개념

### (1) 원리
- 어떤 매핑 \(\phi: \mathbb{R}^n \to \mathbb{R}^m\)를 통해 데이터 \(x\)를 고차원 공간으로 보냅니다.
- 고차원에서 내적:
  $$
  \langle \phi(x_i), \phi(x_j) \rangle
  $$
- **문제**: 실제 \(\phi(x)\)를 구하면 차원이 매우 커져 연산량 폭발.
- **해결**: 내적 결과만 필요하다면 **커널 함수** \(K(x_i, x_j)\)를 사용.
  $$
  K(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
  $$

---

## 2. Mercer’s Theorem

### (1) 정리 (비공식)
**Mercer’s Theorem**에 따르면,  
대칭 함수 \(K(x, y)\)가 모든 연속 함수 \(g\)에 대해 다음 조건을 만족하면:
$$
\iint g(x) K(x, y) g(y) \, dx \, dy \ge 0
$$
이 함수 \(K\)는 **내적 함수**로 표현 가능:
$$
K(x, y) = \langle \phi(x), \phi(y) \rangle
$$
즉, \(K\)는 어떤 고차원 공간 \(\mathcal{H}\)에서의 **양의 준정(definite)** 그램 행렬을 생성합니다.

---

### (2) 의미
- 모든 **양의 준정 커널(Positive Semi-Definite Kernel)**은 어떤 Hilbert 공간에서의 내적에 해당.
- 따라서 커널을 쓰는 것은 **"고차원 특징 공간에서의 선형 모델"**을 쓰는 것과 동치.

---

## 3. 양의 준정(PSD) 조건

### (1) 그램 행렬(Gram Matrix)
데이터 \(\{x_1, \dots, x_n\}\)에 대해:
$$
G_{ij} = K(x_i, x_j)
$$
가 **양의 준정(PSD)**이면 커널로 사용 가능.

### (2) PSD 조건
모든 실수 벡터 \(\mathbf{c} \in \mathbb{R}^n\)에 대해:
$$
\mathbf{c}^\top G \mathbf{c} \ge 0
$$
이 성립해야 함.

---

## 4. 대표적인 커널과 PSD 여부

| 커널 | 식 | PSD 여부 |
|------|----|----------|
| **선형 커널** | \(K(x, y) = x^\top y\) | ✅ |
| **다항 커널** | \(K(x, y) = (x^\top y + c)^d\) | ✅ |
| **RBF 커널** | \(K(x, y) = \exp(-\gamma \|x-y\|^2)\) | ✅ |
| **시그모이드 커널** | \(K(x, y) = \tanh(\kappa x^\top y + c)\) | 조건부 |

---

## 5. SVM 최적화에서 커널의 역할

SVM의 **이중 문제(Dual Form)**:
$$
\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j)
$$
- **원래**는 \(x_i^\top x_j\)였지만, 이를 \(K(x_i, x_j)\)로 치환.
- 고차원 공간에서의 내적을 직접 계산하지 않고도 비선형 분류 가능.

---

## 6. 커널 트릭의 장점

1. **고차원 변환 비용 절감**  
   \(\phi(x)\)를 직접 계산하지 않아도 됨.
2. **비선형 데이터 처리 가능**  
   RBF, 다항 커널 등 사용 가능.
3. **수학적 정당성**  
   Mercer’s Theorem에 의해, PSD 커널이면 SVM 최적화 문제의 해가 항상 존재.

---

## 7. 파이썬 예제 (RBF 커널 사용)
```python
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 로드
X, y = datasets.make_moons(noise=0.2, random_state=42)

# 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# RBF 커널 SVM
svm_rbf = SVC(kernel='rbf', C=1.0, gamma=0.5)
svm_rbf.fit(X_train, y_train)

# 예측
y_pred = svm_rbf.predict(X_test)

# 정확도
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

## 📌 정리
- **Mercer’s Theorem**은 "커널 = 고차원 공간의 내적"임을 수학적으로 보장
- 커널 함수가 **양의 준정**이면, SVM 최적화 문제에서 안정적으로 사용 가능
- 이로 인해 SVM은 **비선형 데이터 분류**에서도 강력한 성능을 발휘
- 대표 커널: 선형, 다항, RBF, 시그모이드
- 커널 트릭은 **계산 효율성**과 **표현력**을 동시에 확보하는 핵심 기법