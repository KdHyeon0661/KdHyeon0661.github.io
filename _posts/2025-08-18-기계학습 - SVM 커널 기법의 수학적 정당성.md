---
layout: post
title: 기계학습 - SVM 커널 기법의 수학적 정당성
date: 2025-08-18 22:25:23 +0900
category: 기계학습
---
# SVM 커널 기법의 수학적 정당성

## 1. 커널 트릭(Kernel Trick) 재정의

### (1) 핵심 아이디어
원공간 \(\mathcal{X}\subset\mathbb{R}^d\)에서 비선형으로 분리되지 않는 데이터를 특징공간 \(\mathcal{H}\)로 사상 \(\phi:\mathcal{X}\to\mathcal{H}\)하여 **선형 분리**를 시도한다. 분류기는
$$
f(x)=\operatorname{sign}\big(\langle w,\phi(x)\rangle_{\mathcal{H}} + b\big).
$$
직접 \(\phi\)를 구성하면 차원이 매우 크거나 무한대가 될 수 있다. **커널 함수** \(K\)를 사용하면
$$
K(x,z)=\langle \phi(x),\phi(z)\rangle_{\mathcal{H}}
$$
을 통해 \(\phi\)를 **명시하지 않고** 내적을 계산한다.

### (2) 선형 SVM의 이중(dual)에서 커널이 들어가는 자리
소프트 마진(슬랙 \(\xi_i\), 정규화 \(C>0\))의 원문제(primal):
$$
\min_{w,b,\xi}\ \frac12\|w\|_{\mathcal{H}}^2 + C\sum_{i=1}^n \xi_i
\quad\text{s.t.}\quad
y_i\big(\langle w,\phi(x_i)\rangle + b\big)\ge 1-\xi_i,\ \xi_i\ge 0.
$$
라그랑지안으로 이중화하면
$$
\max_{\alpha}\ \sum_{i=1}^n \alpha_i - \frac12\sum_{i,j} \alpha_i\alpha_j y_i y_j\, \underbrace{\langle \phi(x_i),\phi(x_j)\rangle}_{K(x_i,x_j)}
\quad
\text{s.t.}\ \sum_i \alpha_i y_i=0,\ 0\le \alpha_i\le C.
$$
의사결정 함수는
$$
f(x) = \operatorname{sign}\!\left(\sum_{i=1}^n \alpha_i^\* y_i K(x_i,x) + b^\*\right).
$$

---

## 2. Mercer 정리(Mercer’s Theorem)

### (1) 비공식 진술(핵심)
연속이고 대칭인 함수 \(K:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\)가 **양의 준정(PSD)** 조건
$$
\iint g(x)\, K(x,y)\, g(y)\,dx\,dy \ge 0\quad\text{for all continuous }g
$$
을 만족(컴팩트 \(\mathcal{X}\), 적분은 \(\mathcal{X}\) 위)하면, 어떤 힐베르트 공간 \(\mathcal{H}\)와 사상 \(\phi:\mathcal{X}\to\mathcal{H}\)가 존재하여
$$
K(x,y) = \langle \phi(x),\phi(y)\rangle_{\mathcal{H}}.
$$
즉 **커널 = 내적**임을 보장한다.

### (2) 보다 정밀한 형태(고유분해)
적절한 조건(연속, 대칭, \(\mathcal{X}\) 컴팩트)에서 적분연산자
$$
(T_K f)(x) = \int_{\mathcal{X}} K(x,y) f(y)\,dy
$$
는 컴팩트·자가수반이며 비음이 아닌 고유값 \(\{\lambda_m\}_{m=1}^\infty\)과 정규직교 고유함수 \(\{\psi_m\}\)가 존재:
$$
K(x,y) = \sum_{m=1}^\infty \lambda_m \psi_m(x)\psi_m(y),\quad \lambda_m\ge 0\ (\text{L}^2\text{-수렴}).
$$
이때 \(\phi(x) = (\sqrt{\lambda_1}\psi_1(x),\sqrt{\lambda_2}\psi_2(x),\ldots)\)로 두면
$$
\langle \phi(x),\phi(y)\rangle = \sum_m \lambda_m \psi_m(x)\psi_m(y) = K(x,y).
$$

---

## 3. RKHS와 재생(“reproducing”) 속성

### (1) Moore–Aronszajn 정리
**모든 PSD 커널 \(K\)** 에 대해, **유일한** 재생커널 힐베르트 공간(RKHS) \(\mathcal{H}_K\)가 존재한다. \(\mathcal{H}_K\)의 내적은
$$
\langle K(\cdot,x), K(\cdot,y)\rangle_{\mathcal{H}_K} = K(x,y).
$$

### (2) 재생 속성
모든 \(f\in\mathcal{H}_K\)와 \(x\)에 대해
$$
f(x) = \langle f,\ K(\cdot,x)\rangle_{\mathcal{H}_K}.
$$
이 속성은 **점 평가 연산**이 연속임을 보장하고, 커널 방법의 해석·최적화에 핵심적 역할을 한다.

### (3) Representer 정리
정규화된(예: \(\|f\|_{\mathcal{H}_K}^2\)) 목적을 최소화하는 모든 경험위험 최소화 문제에서 **해는 항상**
$$
f^\*(\cdot) = \sum_{i=1}^n \alpha_i K(\cdot, x_i)
$$
의 **유한 결합** 형태를 갖는다. (SVM, 커널 릿지, 가우시안 프로세스 등 공통 기반)

---

## 4. PSD(양의 준정) 조건과 그램(Gram) 행렬

### (1) 정의
샘플 \(\{x_i\}_{i=1}^n\)에 대한 그램 행렬 \(G\)의 원소 \(G_{ij}=K(x_i,x_j)\)가
$$
\forall c\in\mathbb{R}^n,\quad c^\top G c\ge 0
$$
이면 \(K\)는 PSD이다. 이산 데이터에서 PSD 검증은 \(G\succeq 0\) (모든 고윳값 \(\ge 0\)) 확인과 동치.

### (2) 커널의 **폐쇄성(closure) 규칙**
- \(K_1,K_2\)가 PSD, \(\alpha,\beta\ge 0\) ⇒ \(\alpha K_1 + \beta K_2\) PSD
- PSD \(K\), 임의의 \(\phi\): \(K'(x,y)=\phi(x)K(x,y)\phi(y)\) PSD
- 점별곱 \(K_1\cdot K_2\) PSD, 텐서곱도 PSD
- 선형 내적 \(K(x,y)=\langle \psi(x),\psi(y)\rangle\)는 자명하게 PSD

> 실전적 의미: **커널을 안전하게 합성**하여 도메인 지식을 주입할 수 있다.

---

## 5. 대표 커널과 성질 (확장)

| 커널 | 식 | 매개변수/설명 | PSD |
|---|---|---|---|
| 선형 | \(x^\top y\) | 표준화 후 자주 사용 | ✅ |
| 다항 | \((x^\top y + c)^d\) | \(c\ge 0, d\in\mathbb{N}\) | ✅ |
| RBF(가우시안) | \(\exp(-\gamma\|x-y\|^2)\) | \(\gamma>0\); **보편(universal)** | ✅ |
| 라플라스 | \(\exp(-\gamma\|x-y\|_1)\) | 맨해튼 기반 | ✅ |
| χ² | \(\sum_j \frac{2x_j y_j}{x_j+y_j}\) | 히스토그램 유사도 | ✅ |
| 히스토그램 교차 | \(\sum_j \min(x_j,y_j)\) | 비음수 히스토그램 | ✅ |
| 문자열(스펙트럼) | \(K(s,t)=\sum_{u\in\Sigma^k}\#_u(s)\#_u(t)\) | 바이오/NLP | ✅ |
| 시그모이드 | \(\tanh(\kappa x^\top y + c)\) | 특정 \((\kappa,c)\)에서만 PSD | 조건부 |

- **RBF의 보편성**: 연속함수 근사 가능 → 충분한 데이터와 적절한 정규화로 강력.
- **시그모이드 커널**은 신경망의 활성화와 유사하지만, PSD가 **항상** 보장되지 않는다(파라미터 제약 필요).

---

## 6. Bochner 정리와 RFF(랜덤 푸리에 특징)

### (1) Bochner 정리(Shift-invariant 커널)
\(K(x,y)=\kappa(x-y)\)가 연속이고 양의 준정 ⇔ \(\kappa(\delta)\)는 **비음수 측도** \(\mu\)의 푸리에 변환:
$$
\kappa(\delta)=\int_{\mathbb{R}^d} e^{i\omega^\top \delta}\, d\mu(\omega).
$$
RBF의 경우 \(\mu\)는 가우시안 스펙트럼.

### (2) 랜덤 푸리에 특징(RFF)
\(\omega\sim \mu\), \(b\sim\mathrm{Unif}[0,2\pi]\)로 샘플링하여
$$
\varphi(x)=\sqrt{\tfrac{2}{D}}\,[\cos(\omega_1^\top x+b_1),\ldots,\cos(\omega_D^\top x+b_D)]
$$
이면
$$
\langle \varphi(x),\varphi(y)\rangle \approx K(x,y).
$$
→ **무한차원 \(\phi\)**를 **유한차원 근사**로 대체, **대규모 학습**에 유용(선형 SVM/로지스틱과 결합).

---

## 7. 실전 관점: 전처리·중심화·정렬·정규화

- **스케일링**: 다항/가우시안 모두 특징 스케일에 민감. 표준화가 기본.
- **커널 중심화(kernel centering)**: 커널 PCA/정렬에서
  $$
  \tilde{K} = K - \mathbf{1}K/n - K\mathbf{1}/n + \mathbf{1}K\mathbf{1}/n^2.
  $$
- **커널 정렬(kernel alignment)**: 타깃 \(Y\)에 대한 커널 \(K_Y\)(예: \(Y Y^\top\) 이진라벨)과의 코사인 정렬로 커널 선택/튜닝 지표:
  $$
  A(K,K_Y)=\frac{\langle K, K_Y\rangle_F}{\|K\|_F\|K_Y\|_F}.
  $$

---

## 8. 일반화, 마진, KKT 요약

- SVM은 \(\|w\|_{\mathcal{H}}\) 최소화로 **마진 극대화** ⇒ 일반화 경계(마진, RKHS 노름, 슬랙 합)로 설명.
- **KKT**에서 \(w=\sum_i \alpha_i y_i \phi(x_i)\), 서포트 벡터의 \(\alpha_i>0\)만이 결정에 기여.
- \(C\)↑ → 마진↓, 훈련오차 패널티↑(과적합 위험). \(\gamma\)↑(RBF) → 결정경계 복잡 ↑.

---

## 9. 대규모·근사 커널: Nyström & RFF

### (1) 복잡도
정확 커널 SVM: **메모리 \(O(n^2)\)**, SMO류 시간 대략 \(O(n^2\sim n^3)\). 대규모에서는 병목.

### (2) Nyström 근사
컬럼 서브샘플링 \(m\ll n\):
- 샘플 인덱스 \(S\)로 \(W=K_{SS}\), \(C=K_{:S}\).
- 근사 \(K \approx C W^{\dagger} C^\top\).
- \(Z=C W^{-\frac12}\)를 특징으로 사용해 **선형 SVM** 학습.

### (3) RFF
위 §6.2. 고차원 데이터를 **선형 분류기**로 효율 학습.

---

## 10. 실전 코드

### (A) RBF SVM (moon 데이터) + 정확도
```python
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

X, y = datasets.make_moons(noise=0.2, random_state=42)
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

clf = Pipeline([
    ("scaler", StandardScaler()),
    ("svc", SVC(kernel="rbf", C=1.0, gamma=0.5))
])
clf.fit(Xtr, ytr)
yp = clf.predict(Xte)
print("Accuracy:", accuracy_score(yte, yp))
```

### (B) 그램 행렬의 PSD 수치 검사(고윳값)
```python
import numpy as np
from sklearn.metrics.pairwise import rbf_kernel

X = np.random.RandomState(0).randn(200, 5)
K = rbf_kernel(X, gamma=0.7)  # K_{ij} = exp(-gamma||xi-xj||^2)
eigvals = np.linalg.eigvalsh(K)  # 대칭행렬 전용
print("min eigenvalue:", eigvals.min())  # >= ~0이어야 함(수치적으로 약간 음수 가능)
```

### (C) 커널 중심화와 정렬(Alignment)
```python
import numpy as np
from sklearn.metrics.pairwise import rbf_kernel

def center_kernel(K):
    n = K.shape[0]
    H = np.eye(n) - np.ones((n,n))/n
    return H @ K @ H

def alignment(K, Ky):
    num = np.sum(K * Ky)
    den = np.linalg.norm(K, 'fro') * np.linalg.norm(Ky, 'fro')
    return num / den

X = np.random.RandomState(1).randn(150, 10)
y = (X[:,0] + 0.5*X[:,1] > 0).astype(int)*2 - 1  # {-1,+1}

K = rbf_kernel(X, gamma=0.5)
Ky = np.outer(y, y)  # 라벨 커널(이진)
Kc, Kyc = center_kernel(K), center_kernel(Ky)
print("alignment (centered):", alignment(Kc, Kyc))
```

### (D) RFF로 RBF 근사 + 선형 SVM
```python
import numpy as np
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from numpy.random import default_rng

def rff_transform(X, D=2000, gamma=0.5, rng=None):
    rng = default_rng(None) if rng is None else rng
    d = X.shape[1]
    W = rng.normal(0, np.sqrt(2*gamma), size=(d, D))  # RBF의 스펙트럼
    b = rng.uniform(0, 2*np.pi, size=D)
    Z = np.sqrt(2.0/D) * np.cos(X @ W + b)
    return Z

X = np.random.RandomState(0).randn(5000, 20)
y = (X[:,0] + 0.3*X[:,1] - 0.2*X[:,2] + 0.5*np.sin(X[:,3]) > 0).astype(int)

Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

Ztr = rff_transform(Xtr, D=3000, gamma=0.2)
Zte = rff_transform(Xte, D=3000, gamma=0.2)  # 주의: 동일 W,b를 써야 함(실전: 저장/재사용)

lin = LinearSVC(C=1.0, dual=True)  # 대규모 효율
lin.fit(Ztr, ytr)
print("RFF+LinearSVM acc:", lin.score(Zte, yte))
```

### (E) Nyström 근사(간단 구현)
```python
import numpy as np
from sklearn.metrics.pairwise import rbf_kernel
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split

rng = np.random.RandomState(0)
X = rng.randn(4000, 30)
y = (X[:,0] - X[:,1] + 0.2*X[:,2] > 0).astype(int)
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)

m = 800  # 랜덤 앵커 수
idx = rng.choice(len(Xtr), size=m, replace=False)
S = Xtr[idx]

C = rbf_kernel(Xtr, S, gamma=0.3)
W = rbf_kernel(S, S, gamma=0.3)
# 안정적 역제곱근
eigvals, eigvecs = np.linalg.eigh(W)
eps = 1e-8
Winvhalf = eigvecs @ np.diag(1.0/np.sqrt(eigvals+eps)) @ eigvecs.T

Ztr = C @ Winvhalf
C_te = rbf_kernel(Xte, S, gamma=0.3)
Zte = C_te @ Winvhalf

clf = LinearSVC(C=1.0)
clf.fit(Ztr, ytr)
print("Nyström+LinearSVM acc:", clf.score(Zte, yte))
```

---

## 11. 실무 체크리스트

- **커널 선택**: 선형(고차원 희소/선형 경계), RBF(기본값), 다항(특정 상호작용), χ²/HI(히스토그램), 문자열/그래프 커널(도메인 특화).
- **스케일링**: 반드시 표준화(다항/RBF 안정).
- **하이퍼파라미터**: RBF의 \(\gamma\), SVM의 \(C\)는 **교차검증**으로. \(\gamma\)는 ‘반경’ 역할(\(\gamma\)↑ ⇒ 경계 복잡).
- **중심화/정렬**: 커널 PCA/정렬 지표 활용 시 커널 중심화.
- **PSD 확인**: 수치적으로 음의 고윳값이 매우 작게 나올 수 있음(수치오차); 필요 시 **클리핑**.
- **대규모 데이터**: RFF/Nyström·미니배치·근사 NN(유사도 기반 전처리) 고려.
- **확률출력**: Platt scaling/Isotonic 회귀로 보정.
- **불균형**: `class_weight='balanced'` 또는 비용 민감 \(C\) 조정.
- **도메인 커널 합성**: 합/곱/스케일로 안전하게 구성(폐쇄성).

---

## 12. 자주 묻는 질문(FAQ)

**Q1. 시그모이드 커널은 왜 “조건부” PSD인가?**
하이퍼볼릭 탄젠트는 특정 \((\kappa,c)\)에서만 그램 행렬이 PSD를 보장한다. 일반적 파라미터에선 **부정정**일 수 있어 권장되지 않는다(RBF 권장).

**Q2. RBF의 \(\gamma\)를 어떻게 초기화?**
표준화 기준에서 보통 \(\gamma\in\{2^{-k}\}\) 그리드를 로그스케일로 탐색(예: \(10^{-3}\sim10^1\)). 또는 중간거리 통계(중앙 \(\ell_2\) 거리의 역수) 근처로 시작.

**Q3. 커널이 PSD가 아니면?**
(1) 파라미터 조정, (2) 수치오차면 고윳값 **클리핑**, (3) **Indefinite SVM**(Kreĭn space) 같은 특수기법. 일반적으로는 **PSD 커널 사용** 권장.

---

## 13. 요약

- **Mercer 정리**: 연속 대칭 PSD 커널은 **일정 조건 하에** 어떤 힐베르트 공간에서의 **내적**으로 표현된다 → 커널 트릭의 수학적 정당성.
- **RKHS/Representer 정리**: 커널 기반 학습의 해는 **샘플 중심 커널의 선형결합**.
- **Bochner**: 평행이동 불변 커널은 **스펙트럼(푸리에) 밀도**로 표현 ⇒ RFF로 효율적 근사.
- **실전**: 스케일링·CV 튜닝·중심화·정렬·PSD 확인·근사화(RFF/Nyström)·불균형 대응이 핵심.
- **권장 기본값**: 표준화 + RBF SVM, \((C,\gamma)\) 로그스케일 탐색 → 필요 시 도메인 커널 합성.

---

## 부록 A. 다항 커널의 명시적 특징맵(스케치)
\(K(x,y)=(x^\top y + c)^d\)는 멀티인덱스 \(\alpha\)에 대해
$$
\phi_\alpha(x) = \sqrt{\binom{d}{\alpha}}\, c^{\frac{d-|\alpha|}{2}}\, x^\alpha,\quad |\alpha|\le d,
$$
으로 구성된 유한차원 특징맵을 갖는다(조합 수가 급증).

## 부록 B. 소프트마진 KKT(요약)
라그랑주 승수 \(\alpha_i,\ \mu_i\)에 대해
- 정류성: \(w=\sum_i \alpha_i y_i \phi(x_i),\ \sum_i \alpha_i y_i = 0,\ 0\le \alpha_i\le C\)
- 보완슬랙: \(\alpha_i\big(y_i f(x_i)-1+\xi_i\big)=0,\ \mu_i\xi_i=0\)

## 부록 C. 커널 PCA 핵심식
고유문제 \(K v = \lambda n v\)를 풀고 투영은
$$
\text{PC}_k(x)=\sum_{i=1}^n v_{k,i}\, K(x_i,x).
$$

```python
# 커널 PCA 간단 구현 예시(학습 포인트만)
import numpy as np
from sklearn.metrics.pairwise import rbf_kernel
X = np.random.RandomState(0).randn(300, 5)
K = rbf_kernel(X, gamma=0.5)
n = len(X); one = np.ones((n,n))/n
Kc = K - one@K - K@one + one@K@one
eigvals, eigvecs = np.linalg.eigh(Kc)
idx = eigvals.argsort()[::-1]
eigvals, eigvecs = eigvals[idx], eigvecs[:,idx]
Z = eigvecs[:, :2] * np.sqrt(np.maximum(eigvals[:2], 0))  # 2D 투영
print(Z.shape)
```
