---
layout: post
title: AWS - 고가용성과 확장성
date: 2025-08-08 16:20:23 +0900
category: AWS
---
# 고가용성(HA)과 확장성(Scalability) 설계

## 초간단 요약

- **HA**: 장애를 빨리 감지하고 **서비스 중단 없이**(또는 거의 없이) 복구/전환하는 능력
- **Scalability**: 트래픽 증가에도 **응답시간/성능을 유지**하도록 용량을 자동으로 늘리고(줄이고) 유지하는 능력
- 함께 설계할 키워드: **중복성, 분리(Decoupling), 자동화, 관측성(Observability), 점진적 배포**

---

## 핵심 개념 및 수식

### 가용성(Availability) 기본 수식

- 구성요소의 **가용성**:
$$
A = \frac{\text{MTBF}}{\text{MTBF} + \text{MTTR}}
$$
- **직렬(Series)** 구성(한 요소라도 다운이면 전체 다운)의 총 가용성:
$$
A_{\text{series}} = \prod_{i=1}^{n} A_i
$$
- **병렬(Parallel, Active-Active)** 구성(한 요소가 다운되어도 서비스 지속)의 총 가용성:
$$
A_{\text{parallel}} = 1 - \prod_{i=1}^{n}(1 - A_i)
$$

> 실무 의미: 같은 99.9%(“3 nines”)라도 **병렬/중복**을 통해 99.99% 이상으로 올릴 수 있음. 반대로 **직렬 의존**이 많으면 계산상 가용성은 빠르게 떨어진다.

### 확장성(Scalability)와 지연(Throughput/Latency)

- 처리율 근사(큐가 없는 단순 서버 c대, 평균 서비스율 μ):
$$
\text{Throughput} \approx c \cdot \mu
$$
- 목표 응답시간 \(R\)을 만족하려면, 관측된 **도착율** \(\lambda\) 에 대해 **서버 수** \(c\) 와 **서비스율** \(\mu\) 를 조절:
$$
\lambda < c \cdot \mu \cdot \rho_{\text{target}}
$$
(여기서 \(\rho_{\text{target}}\)은 목표 이용률 상한, 예: 0.6~0.7)

---

## 설계 원칙(Design Principles)

1) **무상태(Stateless)** 우선
- 세션은 **외부 저장소**(ElastiCache/DynamoDB/EFS)에, 업로드 파일은 **S3**에, 임시 작업 상태는 **SQS**로 분리

2) **결합도 낮추기(Decouple)**
- 생산자-소비자 사이에 **SQS/Kinesis/SNS/EventBridge**를 두어 스파이크 흡수, 장애 전파 차단

3) **중복성과 고립(Failure Isolation)**
- 멀티-AZ/멀티-리전, 서킷브레이커, 격벽(Bulkhead)로 장애를 **국소화**

4) **자동화(Infra & Deploy)**
- IaC(CloudFormation/Terraform/CDK), **오토스케일링**, **헬스체크** + 자동 교체, **자동 롤백**

5) **관측성(Observability)**
- 메트릭(레이트·에러·지연·포화), 로그, 트레이싱(X-Ray/OpenTelemetry)로 **SLO/SLI** 운영

6) **점진적 배포(Progressive Delivery)**
- **Canary / Blue-Green / Rolling** 배포로 리스크 최소화

7) **한계/포화(Saturation) 모니터링**
- CPU·메모리뿐 아니라 **큐 길이**, **ALB 대상당 요청 수**, **EBS/IOPS**, **스레드 풀/DB 커넥션 풀** 등

---

## 참조 아키텍처 패턴

### 엣지 + CDN + 웹 방화

- **CloudFront**(캐시/HTTPS/TLS) + **AWS WAF**(OWASP 룰/Rate limit) → 오리진(ALB/EC2/S3) 보호

### L7 로드밸런싱 + 오토스케일링

- **ALB + Auto Scaling Group(EC2)** 또는 **ECS(Fargate)/EKS + HPA**
- 헬스체크 실패시 인스턴스 자동교체

### 비동기/버퍼링

- **SQS(표준/FIFO)**, **SNS Fan-out**, **Kinesis**(실시간 스트림)로 피크 흡수 및 의존성 분리

### 캐싱

- **ElastiCache Redis**(세션/키-값/분산락), **Memcached**(단순 캐시, 수평확장 용이)

### 데이터베이스 HA

- **RDS/Aurora**: Multi-AZ, Read Replica, Global DB(멀티-리전)
- **DynamoDB**: 온디맨드+오토스케일, **Global Tables**로 액티브-액티브 가능

### 멀티-리전 DR 전략

- **Backup & Restore** → **Pilot Light** → **Warm Standby** → **Active-Active**
- RTO/RPO 예산과 비용/복잡도 균형

---

## AWS 서비스별 구현 가이드

### 컴퓨팅(웹/백엔드)

#### (A) EC2 + ASG + ALB

- **ASG 헬스체크**를 **ALB**와 통합, 실패 시 자동 교체
- **TargetTracking**: `ALBRequestCountPerTarget` 혹은 `AverageCPUUtilization`
- **Warm Pools**: 급격한 스케일아웃 대비

```yaml
# CloudFormation: ALB + TG + ASG(요약)

AWSTemplateFormatVersion: '2010-09-09'
Resources:
  ALB:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Scheme: internet-facing
      Subnets: [subnet-a, subnet-b]
  TG:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Port: 80
      Protocol: HTTP
      VpcId: vpc-xxx
      HealthCheckPath: /health
  ASG:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      MinSize: "2"
      MaxSize: "10"
      DesiredCapacity: "2"
      VPCZoneIdentifier: [subnet-a, subnet-b]
      TargetGroupARNs: [!Ref TG]
```

#### (B) ECS(Fargate) / EKS

- **ECS**: Capacity Provider + Target Tracking(서비스 지표)
- **EKS**: HPA(리소스/사용자 정의 메트릭), **Cluster Autoscaler**로 노드 오토스케일

```yaml
# EKS HPA 예 (CPU 60% 목표)

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web
  minReplicas: 3
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
```

#### (C) Lambda

- 자동 수평 확장. **동시성 제한/예약**, **DLQ/SQS 이벤트 소스**로 안정성 강화

---

### 네트워크 & DNS

#### Route 53 — 장애 격리/페일오버

- **헬스체크** + **Failover/Weighted/Latency** 라우팅
- 다중 리전에 **Active-Passive** 혹은 **Active-Active** 구성

```json
# Failover 레코드 (요약, 콘솔/CF로 구성 권장)

{
  "Name": "api.example.com",
  "Type": "A",
  "SetIdentifier": "primary",
  "Failover": "PRIMARY",
  "AliasTarget": { "HostedZoneId": "Z2P70J7EXAMPLE", "DNSName": "dualstack.alb-primary.amazonaws.com" }
}
```

#### VPC 설계

- **다중 AZ 서브넷**(공개/비공개 분리), **NAT GW** 최소화(비용/HA 균형), **VPC Endpoint**로 내부 통신

---

### 스토리지

- **S3**: 11 9’s 내구성, 멀티-AZ. 정적 웹/아카이브/데이터 레이크
- **EFS**: 서버리스 멀티-AZ 파일시스템, **성능 모드** 선택 주의
- **EBS**: AZ 한정, 스냅샷 자동화

---

### 데이터베이스

#### RDS/Aurora

- **Multi-AZ**: 자동 페일오버. **읽기 복제본**으로 확장
- **Aurora Global DB**: **리전 간** 지연 최소 복제, 세컨더리는 초단위 Failover

```bash
# RDS 이벤트/상태 감시 예(간단 체크)

aws rds describe-db-instances --query 'DBInstances[].{id:DBInstanceIdentifier,status:DBInstanceStatus,az:AvailabilityZone}'
```

#### DynamoDB

- **On-Demand** 또는 **Auto Scaling**
- **Global Tables**: 다중 리전 Active-Active, 충돌 해결(최종 일관성) 정책 고려

```bash
# 테이블 쓰로틀 확인

aws cloudwatch get-metric-statistics \
  --namespace AWS/DynamoDB --metric-name ThrottledRequests \
  --dimensions Name=TableName,Value=my-table \
  --start-time 2025-11-09T00:00:00Z --end-time 2025-11-10T00:00:00Z \
  --period 300 --statistics Sum
```

---

### 메시징/이벤트/스트리밍

- **SQS**: 작업 큐잉/백프레셔 완충, **DLQ** 필수
- **SNS**: Pub/Sub Fan-out
- **EventBridge**: 느슨한 이벤트 통합
- **Kinesis/MSK**: 실시간 스트림 + 소비자 스케일

```yaml
# SQS Redrive (DLQ) 설정 예시 (CFN 약식)

Resources:
  MainQueue:
    Type: AWS::SQS::Queue
    Properties:
      RedrivePolicy:
        deadLetterTargetArn: !GetAtt DLQ.Arn
        maxReceiveCount: 5
  DLQ:
    Type: AWS::SQS::Queue
```

---

## 오토스케일링 전략

### 전략 유형

- **Reactive**: 임계치 초과 시 스케일아웃(간단, 지연 가능)
- **Predictive/Scheduled**: 트래픽 패턴 기반 사전 확장
- **혼합**: Target Tracking(평균화) + Step Scaling(급격한 상승 대응) + 스케줄

### 실전 정책

```yaml
# TargetTracking (ALB 대상당 요청 수 100 목표)

Type: AWS::ApplicationAutoScaling::ScalingPolicy
Properties:
  PolicyType: TargetTrackingScaling
  TargetTrackingScalingPolicyConfiguration:
    TargetValue: 100
    PredefinedMetricSpecification:
      PredefinedMetricType: ALBRequestCountPerTarget
    ScaleInCooldown: 300
    ScaleOutCooldown: 120
```

```yaml
# Step Scaling (CPU 기준)

Type: AWS::ApplicationAutoScaling::ScalingPolicy
Properties:
  PolicyType: StepScaling
  StepScalingPolicyConfiguration:
    AdjustmentType: ChangeInCapacity
    Cooldown: 180
    StepAdjustments:
      - MetricIntervalLowerBound: 0
        MetricIntervalUpperBound: 10
        ScalingAdjustment: 1
      - MetricIntervalLowerBound: 10
        ScalingAdjustment: 2
```

> **팁**: Scale-In은 **천천히**, Scale-Out은 **빠르게**. Stabilization Window로 출렁임 방지.

---

## 데이터 일관성과 DR

### RTO/RPO 선정

- **RTO**(복구시간), **RPO**(허용 데이터 손실)를 비즈니스와 합의
- 예: 결제 시스템 RPO≈0, RTO≤수분 → Multi-AZ + 동기복제, DR은 Warm/Active

### DR 패턴 요약

| 패턴 | 비용 | 복구속도 | 개요 |
|---|---|---|---|
| 백업/복원 | 낮음 | 느림 | S3/Glacier 백업에서 리소스 재생성 |
| Pilot Light | 중간-낮음 | 중간 | 핵심 데이터/서비스만 상시 유지 |
| Warm Standby | 중간 | 빠름 | 축소 규모로 항상 구동, 스케일업 |
| Active-Active | 높음 | 매우 빠름 | 다중 리전 동시 서비스 |

---

## 배포 전략과 가용성

- **Blue-Green**: 새 환경에 배포 후 **트래픽 스위치**. 롤백 즉시
- **Canary**: 일부 트래픽(예: 10%) → 메트릭 정상 확인 → 100% 전환
- **Rolling**: 점진 교체, 무중단에 가깝지만 검증 시간이 길 수 있음

```yaml
# CodeDeploy Lambda Canary 예 (요약)

TrafficRoutingConfig:
  Type: TimeBasedCanary
  TimeBasedCanary:
    Interval: 5
    Percentage: 10
```

---

## 내고장성(Resilience) 패턴

- **Circuit Breaker**: 실패율 증가 시 빠른 실패/대체 경로
- **Retry with Jitter**: 지수 백오프 + 지터, **멱등성 키**
- **Bulkhead**: 스레드/커넥션 풀 분리
- **Graceful Degradation**: 핵심 기능 위주, 비핵심 중단

```python
# Python 예: 지수 백오프 + 지터

import random, time, requests
def call_with_retry(url):
    delay = 0.2
    for attempt in range(6):
        try:
            return requests.get(url, timeout=2).json()
        except Exception:
            time.sleep(delay + random.random()*delay)
            delay *= 2
    raise RuntimeError("max retry")
```

---

## 관측성(Observability) & SLO

### 핵심 지표(RED + USE)

- **RED**(Rate, Errors, Duration): 요청률, 에러율, 지연
- **USE**(Utilization, Saturation, Errors): 리소스 이용/포화/에러

### 예시 SLO/SLI

- SLI: “p95 응답시간 < 500ms 비율”
- SLO: “월 99.9%의 요청이 500ms 미만”
- 에러 버짓:
$$
\text{ErrorBudget} = 1 - \text{SLO}
$$

```json
// CloudWatch Alarm (p95 Latency > 500ms 5분 지속)
{
  "AlarmName": "api-latency-p95",
  "Namespace": "AWS/ApplicationELB",
  "MetricName": "TargetResponseTime",
  "Statistic": "p95",
  "Period": 60,
  "EvaluationPeriods": 5,
  "Threshold": 0.5,
  "ComparisonOperator": "GreaterThanThreshold",
  "Dimensions": [{ "Name":"TargetGroup","Value":"targetgroup/api/xxx" }]
}
```

---

## 테스트/검증(GameDay & Chaos)

- **로드 테스트**: 목표 TPS/동시 사용자에서 Autoscaling latency 포함 검증
- **Chaos**: 인스턴스 강제 종료, 네트워크 지연/패킷 손실, AZ 차단 가정
- **DR Drill**: 정기적으로 리전 장애 시나리오 연습, RTO/RPO 측정

```bash
# 간단 헬스체크/교체 검증 스크립트(예)

alb_dns="my-alb-xxxx.ap-northeast-2.elb.amazonaws.com"
for i in {1..1000}; do
  curl -s -o /dev/null -w "%{http_code}\n" "https://${alb_dns}/health" || true
  sleep 0.2
done
```

---

## 운영(Incident → Rollback → RCA)

1) **탐지**: 알람/로그/트레이스
2) **완화**: 트래픽 가중치 조정(Route 53/ALB), 스케일아웃, 임시 기능 플래그
3) **복구/롤백**: 배포 롤백(Blue-Green 스위치, 이전 리비전 복귀)
4) **RCA**: 원인 분석/재발 방지, SLO/알람 튜닝, 런북 갱신

```yaml
# GitOps 스타일 Runbook(요약)

- alert: api-5xx-spike
  actions:
    - scale_out: ecs service api +50%
    - route53_weight: canary=0
    - rollback: codedeploy last-success
    - notify: oncall@sre
```

---

## 비용과 성능의 트레이드오프

- 멀티-AZ/리전, 복제본, 예비 인프라는 **비용**을 키움
- SLO를 명문화 → **필요 수준만의 가용성/복원력**을 구축
- 캐싱/서버리스/온디맨드/스팟/예약 인스턴스 조합으로 비용 튜닝

---

## 예제: 전형적 3티어(웹/앱/DB) 고가용 + 확장 풀스택

```
[Client]
  → CloudFront(+WAF)
  → ALB
  → (ECS Fargate 서비스 n개, HPA/오토스케일)
  ↔ ElastiCache Redis (세션/캐시)
  ↔ SQS(비동기 작업 큐)
  ↔ RDS(Aurora) Multi-AZ + Read Replicas
  ↔ S3(정적/백업) + Lifecycle
  ↔ CloudWatch(메트릭/로그) + X-Ray(트레이스)
  ↔ Route 53(LBR/Failover)
  ↔ DR: Aurora Global DB + Route 53 헬스체크 전환
```

**포인트**
- 세션 무상태화(쿠키-토큰 또는 Redis 세션 저장)
- 쓰기 경로 단순화, 읽기 캐시/리플리카로 분산
- 큐 기반 비동기로 피크 흡수
- IaC + Progressive Delivery + Alarms + Auto Rollback

---

## 코드/템플릿 모음

### ALB 리스너 + 고정 응답 헬스 체크(테스트용)

```yaml
Resources:
  Listener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      DefaultActions:
        - Type: fixed-response
          FixedResponseConfig:
            StatusCode: "200"
            MessageBody: "ok"
            ContentType: text/plain
```

### Aurora Global DB(요약, 실제 구성은 콘솔/CF 세부 파라미터 필요)

```bash
# 1차 리전에 클러스터 생성 후 2차 리전에 Global Database 추가

aws rds create-global-cluster --global-cluster-identifier prod-global
# 이후 각 리전에 클러스터/인스턴스 연결

```

### SQS 소비자(파이썬 워커, 멱등성 예시)

```python
import boto3, json, hashlib
sqs = boto3.client('sqs')
queue_url = 'https://sqs.ap-northeast-2.amazonaws.com/123/queue'
def idempotency_key(body):
    return hashlib.sha256(json.dumps(body, sort_keys=True).encode()).hexdigest()

while True:
    msgs = sqs.receive_message(QueueUrl=queue_url, MaxNumberOfMessages=10, WaitTimeSeconds=20).get('Messages',[])
    for m in msgs:
        body = json.loads(m['Body'])
        key = idempotency_key(body)
        # 키로 처리 여부 확인(예: Redis SETNX)
        # 처리...
        sqs.delete_message(QueueUrl=queue_url, ReceiptHandle=m['ReceiptHandle'])
```

---

## 체크리스트(복붙)

- [ ] **RTO/RPO** 문서화 및 합의
- [ ] **무상태** 설계(세션/파일 외부화)
- [ ] **멀티-AZ** 기본, 멀티-리전 필요 시 DR 패턴 선택
- [ ] **오토스케일링**: TargetTracking + Step + Scheduled
- [ ] **캐싱/큐잉**: Database 보호(핫 키/슬로우 쿼리 제거)
- [ ] **배포전략**: Canary/Blue-Green + Auto Rollback
- [ ] **관측성**: RED/USE 대시보드, SLO/SLI, 알람 임계
- [ ] **혼잡/포화** 지표: 큐 길이, ALB 요청/타겟, 커넥션 풀
- [ ] **Chaos/DR Drill** 정기 수행
- [ ] **보안**: WAF, 보안그룹 최소화, KMS 암호화, 비밀관리(Secrets Manager)

---

## 마무리 — 적용 로드맵

1) **목표 정의**: 가용성 목표(예: 99.9%), RTO/RPO, 성능 SLO
2) **설계**: 무상태/분리/중복/관측성 원칙 반영
3) **구현**: IaC + 오토스케일 + 헬스체크 + 배포전략
4) **관측성**: RED/USE + 트레이싱 + 알람/대시보드
5) **검증**: 로드/장애/DR Drill로 가설 검증
6) **운영**: 에러 버짓 기반 배포 주기/리스크 관리, 비용/성능 튜닝
