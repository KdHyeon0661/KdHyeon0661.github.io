---
layout: post
title: 기계학습 - 지도학습(분류)
date: 2025-08-18 19:25:23 +0900
category: 기계학습
---
# 지도학습(분류, Classification)

**지도학습(Supervised Learning)**은 입력 데이터와 그에 대응하는 **정답(Label)**을 바탕으로 모델을 학습시키는 방법입니다.  
그중 **분류(Classification)**는 입력 데이터를 **미리 정의된 클래스(Class)** 중 하나로 분류하는 문제를 다룹니다.

예시:
- 이메일 스팸 여부 예측 (스팸 / 정상)
- 환자 질병 진단 (양성 / 음성)
- 이미지 객체 인식 (고양이, 강아지, 자동차 등)

---

## 1. 분류의 개념

### (1) 정의
분류는 주어진 입력 \( \mathbf{x} \)를 특정 클래스 \( C_k \)로 할당하는 작업입니다.
수학적으로:
$$
\hat{y} = f(\mathbf{x}) \in \{ C_1, C_2, \dots, C_K \}
$$
- \(K\): 클래스 개수
- \(f\): 학습된 분류 함수(모델)

---

### (2) 회귀와 분류의 차이

| 구분 | 회귀(Regression) | 분류(Classification) |
|------|------------------|----------------------|
| 출력 값 | 연속형 수치 | 이산형 클래스 |
| 예시 | 주가 예측, 온도 예측 | 스팸 분류, 질병 진단 |
| 손실 함수 | MSE, MAE 등 | 크로스 엔트로피, 힌지 손실 |
| 평가 지표 | RMSE, R² | Accuracy, Precision, Recall, F1-score |

---

## 2. 분류 문제 유형

### (1) 이진 분류(Binary Classification)
- 클래스가 2개인 경우
- 예: 스팸(1) / 정상(0), 암 양성 / 음성
- 출력: 시그모이드 함수(Sigmoid)로 확률값 예측
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

---

### (2) 다중 분류(Multi-class Classification)
- 클래스가 3개 이상인 경우
- 예: 숫자 이미지(0~9)
- 출력: 소프트맥스 함수(Softmax)로 각 클래스 확률 예측
$$
P(y = k | \mathbf{x}) = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}
$$

---

### (3) 다중 레이블 분류(Multi-label Classification)
- 하나의 샘플이 여러 클래스를 동시에 가질 수 있음
- 예: 뉴스 기사(스포츠, 정치, 경제 중 여러 카테고리 포함 가능)
- 출력: 각 클래스별 확률을 독립적으로 예측 (시그모이드 사용)

---

## 3. 주요 분류 알고리즘

| 알고리즘 | 설명 | 장점 | 단점 |
|----------|------|------|------|
| 로지스틱 회귀 | 시그모이드 기반 확률 예측 | 단순, 해석 용이 | 비선형 데이터에 한계 |
| KNN | 가장 가까운 K개의 이웃 클래스 | 구현 간단 | 계산 비용 큼, 스케일링 필요 |
| SVM | 초평면(Hyperplane) 기반 마진 최대화 | 고차원에 강함 | 대규모 데이터 느림 |
| 결정트리 | 데이터 분할 규칙 학습 | 해석 용이 | 과적합 가능 |
| 랜덤 포레스트 | 다수의 결정트리 앙상블 | 높은 성능, 과적합 방지 | 모델 크기 큼 |
| XGBoost | Gradient Boosting 기반 | 높은 예측력 | 파라미터 많음 |
| 신경망 | 다층 퍼셉트론, CNN, RNN 등 | 복잡한 패턴 학습 | 데이터와 자원 많이 필요 |

---

## 4. 분류의 손실 함수

### (1) 이진 분류 — 이진 크로스 엔트로피(Binary Cross-Entropy)
$$
L = -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i) \right]
$$

### (2) 다중 분류 — 크로스 엔트로피(Cross-Entropy)
$$
L = -\frac{1}{n} \sum_{i=1}^n \sum_{c=1}^K y_{i,c} \log(\hat{y}_{i,c})
$$

### (3) SVM — 힌지 손실(Hinge Loss)
$$
L = \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i \hat{y}_i)
$$

---

## 5. 분류 성능 평가 지표

| 지표 | 설명 | 수식 |
|------|------|------|
| Accuracy | 전체 샘플 중 맞춘 비율 | \(\frac{TP + TN}{TP+FP+TN+FN}\) |
| Precision | 양성으로 예측한 것 중 실제 양성 비율 | \(\frac{TP}{TP+FP}\) |
| Recall(민감도) | 실제 양성 중 맞춘 비율 | \(\frac{TP}{TP+FN}\) |
| F1-score | Precision과 Recall의 조화평균 | \(2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}\) |
| ROC-AUC | TPR과 FPR의 관계 면적 | - |

---

## 6. 파이썬 예제 (이진 분류 — 로지스틱 회귀)
```python
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# 데이터 생성
X, y = make_classification(n_samples=1000, n_features=5, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# 모델 학습
model = LogisticRegression()
model.fit(X_train, y_train)

# 예측
y_pred = model.predict(X_test)

# 평가
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

---

## 7. 실제 활용 사례
- **이메일 필터링**: 스팸 / 정상 메일 분류
- **의료 진단**: 질병 양성/음성 판정
- **이미지 인식**: 얼굴 인식, 객체 탐지
- **금융 사기 탐지**: 거래 패턴 기반 이상 거래 탐지
- **문서 분류**: 뉴스, 리뷰, 소셜 미디어 글 분류

---

## 📌 정리
- 지도학습(분류)은 입력 데이터를 이산적인 클래스에 할당
- 이진, 다중, 다중 레이블 분류로 나눔
- 주요 알고리즘: 로지스틱 회귀, KNN, SVM, 결정트리, 앙상블, 신경망
- 손실 함수: 크로스 엔트로피, 힌지 손실
- 평가 지표: Accuracy, Precision, Recall, F1-score, ROC-AUC
- 다양한 산업 분야에서 활용 가능