---
layout: post
title: 기계학습 - 머신러닝 평가 지표
date: 2025-08-19 23:25:23 +0900
category: 기계학습
---
# 📊 머신러닝 평가 지표 (Evaluation Metrics)

머신러닝 모델은 단순히 **정확도(Accuracy)** 만으로는 제대로 평가할 수 없습니다.  
특히 데이터 불균형 문제나 예측 목적에 따라 다양한 지표가 필요합니다.  

본 글에서는 **회귀(Regression)** 와 **분류(Classification)** 모델의 주요 평가 지표를 정리합니다.  

---

## 1. 회귀 모델 평가 지표 (Regression Metrics)

회귀 문제는 연속적인 값을 예측하는 문제이므로, 예측값과 실제값 사이의 **오차(Error)** 를 측정합니다.

### (1) 평균제곱오차 (MSE, Mean Squared Error)
- 예측값과 실제값의 차이를 제곱한 뒤 평균
- 오차가 클수록 크게 벌어짐 → 이상치(outlier)에 민감
\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

### (2) 평균제곱근오차 (RMSE, Root Mean Squared Error)
- MSE의 제곱근
- 실제 데이터 단위와 같음 → 해석 용이
\[
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\]

### (3) 평균절대오차 (MAE, Mean Absolute Error)
- 예측값과 실제값 차이의 절댓값 평균
- 이상치에 덜 민감
\[
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]

### (4) 결정계수 (R², Coefficient of Determination)
- 모델이 데이터를 얼마나 설명하는가를 나타냄
- 1에 가까울수록 설명력이 높음
\[
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\]

👉 회귀 모델 요약:
- **MSE/RMSE**: 평균 오차 크기  
- **MAE**: 이상치에 강건한 오차 측정  
- **R²**: 모델 설명력  

---

## 2. 분류 모델 평가 지표 (Classification Metrics)

분류 문제에서는 단순히 "맞았다/틀렸다"를 넘어서, **클래스별 성능**을 평가하는 것이 중요합니다.  

### (1) 혼동 행렬 (Confusion Matrix)
| 실제/예측 | Positive | Negative |
|-----------|----------|----------|
| Positive  | TP (True Positive) | FN (False Negative) |
| Negative  | FP (False Positive) | TN (True Negative) |

👉 이 네 가지 값을 기반으로 다양한 지표 계산 가능  

---

### (2) 정확도 (Accuracy)
\[
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\]
- 전체 샘플 중 맞춘 비율
- 클래스 불균형이 크면 신뢰하기 어려움  
예: 95%가 Negative인 데이터에서 "무조건 Negative" 모델 → Accuracy 95%  

---

### (3) 정밀도 (Precision)
\[
Precision = \frac{TP}{TP + FP}
\]
- **예측을 Positive라 한 것 중 실제로 맞은 비율**  
- FP(거짓 긍정)를 줄이는 데 중요  
👉 스팸 메일 필터링에서 유용 (정상 메일을 스팸으로 분류하면 문제)

---

### (4) 재현율 (Recall, Sensitivity, TPR)
\[
Recall = \frac{TP}{TP + FN}
\]
- **실제 Positive 중 얼마나 잘 맞췄는가**  
- FN(거짓 부정)을 줄이는 데 중요  
👉 암 환자 진단에서 유용 (암인데 놓치면 안 됨)

---

### (5) F1 Score
\[
F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
\]
- Precision과 Recall의 조화평균
- 불균형 데이터에서 유용  

---

### (6) ROC 곡선 (Receiver Operating Characteristic Curve) & AUC
- ROC Curve: **FPR(False Positive Rate)** vs **TPR(Recall)**  
- AUC (Area Under Curve): ROC 곡선 밑 면적
  - 1에 가까울수록 좋은 모델
  - 0.5는 랜덤 추측 수준

\[
FPR = \frac{FP}{FP + TN}, \quad TPR = Recall
\]

---

### (7) PR Curve (Precision-Recall Curve)
- 클래스 불균형 데이터에서 ROC보다 더 적합
- AUC 대신 **AP (Average Precision)** 사용하기도 함

---

## 3. 다중 분류 (Multi-class) 평가 지표

이진 분류 외에도 다중 분류 문제에서 성능 측정이 필요함.  

- **Macro 평균**: 각 클래스별 지표를 구한 뒤 단순 평균  
- **Micro 평균**: 전체 샘플 기준으로 지표 계산  
- **Weighted 평균**: 각 클래스별 샘플 수에 비례하여 가중 평균  

---

## 4. 회귀 vs 분류 지표 요약

| 구분 | 주요 지표 |
|------|-----------|
| 회귀 | MSE, RMSE, MAE, R² |
| 분류 | Accuracy, Precision, Recall, F1, ROC-AUC, PR-AUC |

---

## 📌 최종 정리
- 단순 **정확도(Accuracy)** 는 데이터 불균형에서 신뢰하기 어려움  
- **Precision/Recall/F1** 은 분류의 본질적인 성능을 보여줌  
- **ROC-AUC / PR-AUC** 는 임계값 변화에도 강건한 성능 측정 가능  
- **회귀 모델** 은 MSE/RMSE/MAE로 오차 크기를 측정, R²로 설명력 평가  

---

👉 따라서 **문제 상황에 맞는 지표 선택**이 핵심:  
- 스팸 메일 → Precision  
- 질병 진단 → Recall  
- 불균형 데이터 → F1 / PR-AUC  
- 가격 예측 → RMSE / MAE  