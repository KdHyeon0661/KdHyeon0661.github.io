---
layout: post
title: 기계학습 - 머신러닝 평가 지표
date: 2025-08-19 23:25:23 +0900
category: 기계학습
---
# 머신러닝 평가 지표(Evaluation Metrics)

## 왜 다양한 지표가 필요한가?

- 한 모델의 **목적**(탐지/예측/랭킹/의사결정)과 **데이터 특성**(불균형·계절성·레이블 노이즈)에 따라 **옳은 지표가 달라진다**.
- **정확도(Accuracy)** 하나로는 **불균형·임계값 의존성·확률 보정**을 반영하기 어렵다.
- 따라서 **오차 기반(회귀)**, **혼동행렬 기반(분류)**, **랭킹 기반(ROC/PR)**, **확률 품질(캘리브레이션)**, **비용 민감(코스트 매트릭스)** 지표를 **상황에 맞게 조합**해야 한다.

---

## 회귀(Regression) 지표

### 기본 오차 지표

- **MSE (Mean Squared Error)** — 제곱 오차 평균(이상치 민감)
$$
\mathrm{MSE}=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2
$$

- **RMSE (Root MSE)** — 원 단위로 해석 용이
$$
\mathrm{RMSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i-\hat{y}_i)^2}
$$

- **MAE (Mean Absolute Error)** — 이상치에 강건
$$
\mathrm{MAE}=\frac{1}{n}\sum_{i=1}^n |y_i-\hat{y}_i|
$$

- **MedAE (Median Absolute Error)** — 중앙절대오차(매우 강건)

**선택 가이드**: 이상치가 많고 **중앙적 성능**이 중요하면 **MAE/MedAE**, 큰 오차를 크게 패널티해야 하면 **RMSE**.

---

### 상대/로그/계열 민감 지표

- **MAPE** (평균절대백분율오차, 0 포함 불가·작은 값 민감)
$$
\mathrm{MAPE}=\frac{100}{n}\sum_{i=1}^n \left|\frac{y_i-\hat{y}_i}{y_i}\right|
$$

- **SMAPE** (대칭 MAPE, 0 문제 완화)
$$
\mathrm{SMAPE}=\frac{100}{n}\sum_{i=1}^n \frac{|y_i-\hat{y}_i|}{(|y_i|+|\hat{y}_i|)/2}
$$

- **RMSLE** (로그 스케일 오차; 성장률/비율 데이터)
$$
\mathrm{RMSLE}=\sqrt{\frac{1}{n}\sum_{i=1}^n \left(\log(1+y_i)-\log(1+\hat{y}_i)\right)^2}
$$

- **MASE** (시계열 기준모델 대비 오차)
$$
\mathrm{MASE}=\frac{\frac{1}{n}\sum_{i=1}^n |y_i-\hat{y}_i|}{\frac{1}{n-1}\sum_{t=2}^{n}|y_t-y_{t-1}|}
$$

---

### 설명력 계수

- **\(R^2\)** (결정계수; 1에 가까울수록 설명력↑)
$$
R^2=1-\frac{\sum (y_i-\hat{y}_i)^2}{\sum(y_i-\bar{y})^2}
$$
> 주의: 테스트에서 **음수** 가능(평균보다 못한 모델).

- **조정 \(R^2\)** (특징 수 패널티)
$$
\bar{R}^2=1-\Big(1-R^2\Big)\frac{n-1}{n-p-1}
$$

---

### 분포·퀀타일 지표

- **핀볼 손실(Quantile Loss, τ-퀀타일)**
$$
L_\tau(y,\hat{y})=\max\{\tau(y-\hat{y}),(1-\tau)(\hat{y}-y)\}
$$
예: **상단 95% 예측구간**(τ=0.975) 등 **예측구간 추정**에 필수.

- **포아송/트위디 Deviance** — 카운트/보험손해액 등 **비정규 타깃**에 적합.

---

### 회귀 코드 예시

```python
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_true = np.array([3, 2.5, 4, 5, 6])
y_pred = np.array([2.7, 2.9, 3.6, 5.3, 5.8])

mse  = mean_squared_error(y_true, y_pred)
rmse = mean_squared_error(y_true, y_pred, squared=False)
mae  = mean_absolute_error(y_true, y_pred)
r2   = r2_score(y_true, y_pred)

print(mse, rmse, mae, r2)
```

---

## 분류(Classification) 지표

### 혼동행렬 기반

| 실제\\예측 | Positive | Negative |
|---|---:|---:|
| Positive | **TP** | **FN** |
| Negative | **FP** | **TN** |

- **Accuracy**
$$
\mathrm{Acc}=\frac{TP+TN}{TP+FP+TN+FN}
$$

- **Precision(정밀도)** — 양성 예측의 신뢰도
$$
\mathrm{Prec}=\frac{TP}{TP+FP}
$$

- **Recall(재현율/TPR)** — 양성 검출율
$$
\mathrm{Rec}=\frac{TP}{TP+FN}
$$

- **Specificity(TNR)** — 음성 정확
$$
\mathrm{Spec}=\frac{TN}{TN+FP}
$$

- **FPR/FNR**
$$
\mathrm{FPR}=\frac{FP}{FP+TN},\quad \mathrm{FNR}=\frac{FN}{TP+FN}
$$

- **F1 / \(F_\beta\)** — 정밀·재현 조화
$$
F_1 = 2\cdot\frac{\mathrm{Prec}\cdot\mathrm{Rec}}{\mathrm{Prec}+\mathrm{Rec}},\quad
F_\beta=(1+\beta^2)\frac{\mathrm{Prec}\cdot\mathrm{Rec}}{\beta^2\mathrm{Prec}+\mathrm{Rec}}
$$
> 예: **암 진단**처럼 재현율이 특히 중요하면 \(\beta>1\).

- **Balanced Accuracy** — 클래스 불균형 보정
$$
\mathrm{BAcc}=\frac{\mathrm{TPR}+\mathrm{TNR}}{2}
$$

- **MCC (Matthews Correlation Coef.)** — 클래스 불균형에 강함
$$
\mathrm{MCC}=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
$$

- **Cohen’s κ** — 우연 일치 보정 합의도
(다중분류 지원; 클래스 불균형에서 유용)

- **Log Loss(CE)** — 확률 품질
$$
\mathrm{CE}=-\frac{1}{n}\sum_{i=1}^n \Big[y_i\log \hat{p}_i + (1-y_i)\log(1-\hat{p}_i)\Big]
$$

- **Brier Score** — 확률 예측의 평균제곱오차
$$
\mathrm{Brier}=\frac{1}{n}\sum_{i=1}^n (\hat{p}_i-y_i)^2
$$

---

### 랭킹 곡선: ROC vs PR

- **ROC Curve**: \(x=\mathrm{FPR}\), \(y=\mathrm{TPR}\); **AUC**는 임계값 불변 **랭킹 성능**.
- **PR Curve**: \(x=\mathrm{Recall}\), \(y=\mathrm{Precision}\); **희소 양성(불균형)**에서 정보량↑.
- **AP (Average Precision)**: PR 곡선 면적.
- **언제 PR-AUC?** → **양성 희소·비용 민감**(탐지/알림) 문제.

---

### 캘리브레이션(확률의 “진짜다움”)

- **Calibration Curve/ Reliability Diagram**: 예측 확률 구간별 평균 예측 vs 실제 빈도.
- **ECE (Expected Calibration Error)**: 버킷 가중 평균 격차.
- **Brier 분해**(참고): **Calibration + Refinement**로 분리 가능.
- **보정 방법**: **Platt(로지스틱)**, **Isotonic**.

---

### 임계값(Threshold) 최적화

- **F1 최대**, **Youden’s J = TPR–FPR 최대**, 혹은 **비용 기대값** 최대화:
$$
\text{Expected Utility} = u_{TP}\cdot TP - c_{FP}\cdot FP - c_{FN}\cdot FN - c_{TN}\cdot 0
$$
- **Decision Curve Analysis**(순이익; 임상): **Net Benefit** 기반 임계값 선택.

---

### 다중분류·다중레이블

- 평균 방식
  - **Macro**: 클래스별 지표 단순 평균(희귀 클래스 반영 큼)
  - **Micro**: 전체 TP/FP/FN 합쳐서 계산(샘플 가중)
  - **Weighted**: 클래스 크기 가중 평균
- ROC-AUC/PR-AUC: **OvR** 또는 **OvO**로 클래스별 계산 후 **macro/micro** 집계
- 다중레이블: **서브셋 정확도**(엄격), **해밍 손실**(레이블 단위), **샘플 기반 F1** 등.

---

### 분류 코드 예시 (지표·곡선·캘리브레이션)

```python
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (confusion_matrix, classification_report, roc_auc_score,
                             average_precision_score, precision_recall_curve, roc_curve, f1_score,
                             matthews_corrcoef, brier_score_loss)
from sklearn.calibration import CalibratedClassifierCV

# 불균형 예시 데이터

X, y = make_classification(n_samples=6000, n_features=20, weights=[0.95, 0.05],
                           flip_y=0.01, class_sep=1.0, random_state=42)
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3,
                                          stratify=y, random_state=42)

# 모델 + 캘리브레이션

base = LogisticRegression(max_iter=2000)
clf  = CalibratedClassifierCV(base, method="isotonic", cv=5)
clf.fit(X_tr, y_tr)

proba = clf.predict_proba(X_te)[:,1]
yhat  = (proba >= 0.5).astype(int)

print("ROC-AUC:", roc_auc_score(y_te, proba))
print("PR-AUC(AP):", average_precision_score(y_te, proba))
print("Brier:", brier_score_loss(y_te, proba))
print("F1:", f1_score(y_te, yhat))
print("MCC:", matthews_corrcoef(y_te, yhat))
print(confusion_matrix(y_te, yhat))
print(classification_report(y_te, yhat, digits=3))

# 임계값 최적화 (F1)

p, r, th = precision_recall_curve(y_te, proba)
f1 = 2*p*r/(p+r+1e-12)
best_t = th[np.nanargmax(f1[:-1])]
print("Best-F1 threshold:", best_t)
```

---

## 불균형 데이터에서의 평가 전략

- **지표 선택**: **PR-AUC / F1 / MCC / Recall@Precision**(예: Precision≥0.95일 때 Recall)
- **분할**: 항상 **Stratified**(CV·Train/Test 모두)
- **리샘플링**: 오버/언더샘플링은 **CV 폴드 내부**에서만 적용(누수 방지)
- **임계값 튜닝**: 목적/비용에 맞춰 **사용자 정의 유틸리티**로 최적화
- **확률 보정**: 알림/의사결정 시스템에서 **캘리브레이션** 필수

---

## 모델 선택·검증 절차의 함정과 권장안

- **Train/Validation/Test** 3분할 또는 **CV + Holdout Test**
- **Nested CV**: 하이퍼파라미터 탐색의 **편향 제거**
- **GroupKFold / TimeSeriesSplit**: 그룹/시계열 누수 방지
- **CV에서의 ROC-AUC 집계**: **폴드별 확률**을 concat 후 **단일 AUC** 산출 권장
- **신뢰구간**: 부트스트랩(예: 1000회)로 지표의 **95% CI** 제공
- **통계 비교**
  - **McNemar**: 두 분류기 **짝지은 예측**의 Accuracy 차이 검정
  - **DeLong**: ROC-AUC의 유의 차이 검정
  - **부트스트랩 차이 CI**: 지표 차이의 불확실성 제시

---

## 비용 민감 평가(Cost-Sensitive)

실제 업무에서는 **FP/FN 비용이 대칭이 아니다**.

- **코스트 매트릭스** 예:
  - 스팸필터: FP(정상→스팸) 비용↑
  - 사기탐지/질병진단: FN 비용↑

**기대 비용 최소화** 또는 **기대 이익 최대화** 기준으로 **임계값**과 **모델**을 선택:

$$
\mathbb{E}[\text{Cost}]=c_{FP}\cdot FP + c_{FN}\cdot FN
$$

```python
def expected_cost(y_true, proba, threshold, c_fp=1.0, c_fn=10.0):
    y_pred = (proba >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return c_fp*fp + c_fn*fn
```

---

## 랭킹/탐색/추천에서의 보조 지표

- **Top-k Accuracy**, **Recall@k**, **Precision@k**, **Hit Rate**
- **NDCG**: 순위 품질(가중 로그 할인)
- **AUC**: 쌍대 랭킹 정확도(양성>음성 점수 확률)

> 분류 확률을 **랭킹**으로 사용한다면, **AUC·AP**와 함께 **Top-k** 지표를 병행.

---

## 종합 코드 스니펫 — 리포팅 유틸

```python
from sklearn.metrics import (confusion_matrix, roc_auc_score, average_precision_score,
                             f1_score, matthews_corrcoef, precision_score, recall_score,
                             balanced_accuracy_score, brier_score_loss, log_loss)

def classification_report_full(y_true, proba, threshold=0.5):
    y_pred = (proba >= threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    report = {
        "threshold": threshold,
        "TP": int(tp), "FP": int(fp), "FN": int(fn), "TN": int(tn),
        "Accuracy": (tp+tn)/(tp+tn+fp+fn),
        "Precision": precision_score(y_true, y_pred, zero_division=0),
        "Recall": recall_score(y_true, y_pred, zero_division=0),
        "F1": f1_score(y_true, y_pred, zero_division=0),
        "BalancedAcc": balanced_accuracy_score(y_true, y_pred),
        "MCC": matthews_corrcoef(y_true, y_pred),
        "ROC-AUC": roc_auc_score(y_true, proba),
        "PR-AUC": average_precision_score(y_true, proba),
        "Brier": brier_score_loss(y_true, proba),
        "LogLoss": log_loss(y_true, proba, eps=1e-15)
    }
    return report
```

---

## 요약 표 — 상황별 추천 지표

| 상황 | 1차 지표 | 2차 지표/보조 |
|---|---|---|
| 불균형 탐지(알림) | **PR-AUC, F1, Recall@Precision** | ROC-AUC, MCC, 임계값 최적화 |
| 의료 진단(민감도 우선) | **Recall, F\_\beta(\(\beta>1\))** | NPV, Spec, ROC-AUC, Decision Curve |
| 스팸 필터(과잉차단 부담) | **Precision, F\_\beta(\(\beta<1\))** | PR-AUC, FP 비용 반영 임계값 |
| 확률 예측 품질 | **LogLoss, Brier, ECE** | Calibration plot |
| 가격·수요 예측 | **RMSE/MAE** | RMSLE(성장률), MAPE/SMAPE, \(R^2\) |
| 시계열 | **MASE, s(M)APE** | RMSE/MAE, 예측구간 커버리지 |

---

## 자주 하는 실수와 체크리스트

- [ ] **데이터 누수**: 전처리/스케일/리샘플링은 **CV 폴드 내부**에서만 `fit`
- [ ] **불균형에서 Accuracy 보고**: **PR-AUC/MCC** 병행
- [ ] **임계값 0.5 고정**: **비용/업무 목적**으로 최적화
- [ ] **ROC-AUC만 보고**: **PR-AUC**도 확인(희귀 양성)
- [ ] **확률 사용**: **캘리브레이션** 후 의사결정
- [ ] **CV 평균만 보고**: **신뢰구간**(부트스트랩) 제공
- [ ] **다중레이블**: **Hamming/Subset/샘플 F1** 구분

---

## 소결

- **회귀**는 오차 크기(RMSE/MAE 등)와 **목표 분포 특성**(RMSLE/MASE/퀀타일)까지 고려해야 한다.
- **분류**는 혼동행렬 파생 지표(Precision/Recall/F1), **랭킹 곡선(ROC/PR)**, **캘리브레이션**, **임계값/비용**까지 함께 보자.
- **불균형·다중분류·다중레이블·시계열**은 **맞춤 분할·지표**가 핵심이다.
- 최종적으로는 **업무 목적(비용/리스크/정책)**에 맞춘 **지표 세트**로 모델을 선택·보고하는 것이 바람직하다.
