---
layout: post
title: 소켓프로그래밍 - 테스트 전략과 벤치마크
date: 2025-09-25 22:25:23 +0900
category: 소켓프로그래밍
---
## 18. 테스트 전략과 벤치마크

> 목표: **기능/내구/혼잡/장애/복구**까지 아우르는 **엔드투엔드 테스트 전략**을 잡고,  
> HTTP 레벨 도구(**`wrk`**, **`h2load`**)와 **자체 로더**(TCP/프레이밍 직접)로 **재현 가능한** 결과를 얻는다.  
> 네트워크 조건은 **`tc/netem`**으로 지연·손실·재정렬·대역폭 제약을 **실험적으로 주입**한다.  
> 마지막으로 **지표 기반 성능 리포트 템플릿**을 제시해 팀 공유 문서로 바로 쓸 수 있게 한다.

---

### 18.1 테스트 분류 — **무엇을 어떻게 검증할 것인가**

#### 18.1.1 테스트 축(Taxonomy)

| 범주 | 핵심 질문 | 지표(예시) | 도구/기법 |
|---|---|---|---|
| **기능(Functionality)** | 프로토콜(프레이밍/검증/에러) 그대로 동작? | 성공률, 에러 코드 분포, 프레이밍 오류 0 | 유닛/통합 테스트, 페이로드 변조/경계값 |
| **내구(Soak/Endurance)** | 6~24시간 이상 안정적? 메모리/FD 누수? | RSS/FD/소켓 상태 추이, GC/락 경합 | 장시간 로드 + 메트릭 스크래핑 |
| **혼잡(Congestion)** | RTT/손실/버퍼에 따른 처리량/지연 곡선? | p50/p95/p99, 재전송률, BBR/큐잉 | `tc/netem`, BDP 설계, 큐 관측 |
| **장애(Failure)** | SYN flood/포트 고갈/CLOSE_WAIT 누수 시? | 에러율/대기열/타임아웃, 복구 시간 | 시나리오 플레이북(13장) 재현 |
| **복구(Recovery)** | 실패 후 자동 복구/경고는 적시에? | MTTD/MTTR, 재시도 성공률, 알람 | 지표 알람/자동 조치/히스토리 |

> **원칙**: “**측정-가설-실험-결론**” 사이클을 문서화한다. 한 번의 성공보다 **재현 가능성**이 중요.

---

## 18.2 HTTP 레벨 벤치: `wrk`, `h2load` 개요

> HTTP 서버(혹은 TLS 종단) 성능이 필요한 경우, TCP 레벨 로더를 짜기 전에 **빠른 가늠샷**으로 사용.

### 18.2.1 `wrk` (HTTP/1.1, Lua 스크립트)

- **장점**: 매우 빠른 단일 바이너리, Lua로 **요청 모델**(경로/본문/헤더) 스크립팅.
- **단점**: HTTP/2 미지원, 커넥션 풀/파이프라인은 제한적.

```bash
# 2분 동안 8스레드, 256 커넥션
wrk -t8 -c256 -d120s --latency http://127.0.0.1:8080/

# 헤더/POST 바디/경로 랜덤화(Lua)
cat > post.lua <<'LUA'
wrk.method = "POST"
wrk.headers["Content-Type"] = "application/json"
math.randomseed(os.time())
function request()
  local n = math.random(16, 1024)
  local body = string.rep("x", n)
  return wrk.format(nil, "/echo", nil, body)
end
LUA

wrk -t4 -c200 -d60s -s post.lua --latency http://host:8080
```

> **해석 포인트**: `Requests/sec`, `Latency Distribution`(p50/p75/p90/p99).  
> p99가 **꼬리 두꺼움**(heavy-tail)을 보이면 **큐/락/디스크/GC** 신호일 수 있다.

### 18.2.2 `h2load` (HTTP/2/3, TLS)

- **장점**: HTTP/2 연결 N개·스트림 M개 동시성 모델, TLS 옵션 풍부.
- **단점**: HTTP/2/3 중심. HTTP/1.1은 별도 도구 추천.

```bash
# HTTP/2, 총 100만 요청, 연결 200, 최대 동시 스트림 100, 워커 8
h2load -n 1000000 -c 200 -m 100 -t 8 https://host:443/echo

# 지속 시간 기반 부하, 워밍업 포함
h2load --duration=120 --warm-up-time=10 -c 200 -m 50 -t 8 https://host/large
```

> **팁**: TLS 오버헤드/재개율/ALPN은 14장의 정책과 함께 본다. 핸드셰이크 p95 상승은 **세션재개 미스** 신호.

---

## 18.3 TCP/프레이밍 전용 — **자체 로더가 필요한 이유**

- 우리 책의 예제(길이-프리픽스/TLV/UDP 등)는 **HTTP가 아님** → HTTP 벤치로는 **모사 불가**.
- **자체 로더 설계 포인트**
  1) **프로토콜 인코더/디코더**(프레이밍/CRC/길이 cap) 내장
  2) **커넥션 풀**: `C`개 연결 × 각 연결 당 **동시 미해결 요청** 수(`pipeline`)
  3) **스케줄러**: **ramp-up → steady → ramp-down**(온·오프 가드)
  4) **지표**: **전송량**(Bytes/s), **RPS**, **지연 분포**(p50/p95/p99/p99.9), **에러율**, **소켓 에러 코드**
  5) **환경 메타**: 커밋/바이너리 버전/CPU/커널/옵션/`tc` 설정 등 **리포트에 자동 첨부**

> 아래 18.5에 **C++23 미니 로더**를 제공한다(길이-프리픽스 echo 측정).

---

## 18.4 재현 가능한 네트워크 조건 — `tc/netem` 레시피

> 동일 실험을 **일주일 뒤에도** 똑같이 만들려면, **인터페이스/큐디스크 상태를 스크립트화**해야 한다.

### 18.4.1 지연·지터·손실·재정렬

```bash
IF=eth0

# 깨끗한 상태로 초기화
sudo tc qdisc del dev $IF root 2>/dev/null || true

# (A) 지연 40ms ± 5ms (정규분포)
sudo tc qdisc add dev $IF root netem delay 40ms 5ms distribution normal

# (B) 손실 1%, 복구 상관 25%
sudo tc qdisc change dev $IF root netem loss 1% 25%

# (C) 재정렬 10% (gap 5 패킷)
sudo tc qdisc change dev $IF root netem reorder 10% 50% gap 5

# (D) 손상/복제(특수 시나리오)
sudo tc qdisc change dev $IF root netem corrupt 0.05%
sudo tc qdisc change dev $IF root netem duplicate 0.1%

# 원복
sudo tc qdisc del dev $IF root
```

### 18.4.2 대역폭 제한(TBF), BDP 매칭

- 토큰 버킷 필터(TBF)로 **전송률**을 제한해 **큐잉/혼잡**을 인위적으로 만든다.

```bash
IF=eth0
sudo tc qdisc del dev $IF root 2>/dev/null || true
sudo tc qdisc add dev $IF root handle 1: htb default 1
sudo tc class add dev $IF parent 1: classid 1:1 htb rate 20mbit ceil 20mbit
sudo tc qdisc add dev $IF parent 1:1 handle 10: netem delay 50ms 10ms
```

- **BDP 계산**:
  $$
  \text{BDP} = \text{Bandwidth} \times \text{RTT}
  $$
  예: 20 Mbps, RTT 100 ms →  
  $$
  \text{BDP} = 20\times10^6 \times 0.1 = 2\times10^6\ \text{bits} \approx 250\ \text{kB}
  $$
  → **소켓 버퍼(SNDBUF/RCVBUF)**가 최소 수백 kB가 되도록 조정해야 **윈도 제한**이 되지 않는다.

### 18.4.3 **네임스페이스 + veth**로 **로컬 샌드박스** 만들기

- 호스트 전체에 `netem`을 걸면 **다른 트래픽**까지 영향. **네임스페이스**로 **격리**!

```bash
# 클린업
sudo ip netns del bench 2>/dev/null || true

# 네임스페이스와 veth 쌍
sudo ip netns add bench
sudo ip link add veth0 type veth peer name veth1
sudo ip link set veth1 netns bench

# 주소
sudo ip addr add 10.10.0.1/24 dev veth0
sudo ip link set veth0 up
sudo ip netns exec bench ip addr add 10.10.0.2/24 dev veth1
sudo ip netns exec bench ip link set veth1 up

# bench NS에 서버 띄우기(예: dual_echo 9000)
sudo ip netns exec bench ./decho 9000

# 호스트→bench 경로에 netem 주입(호스트 veth0 쪽에)
sudo tc qdisc add dev veth0 root netem delay 30ms loss 0.5%

# 접속
nc 10.10.0.2 9000
```

> **장점**: 팀원이 그대로 스크립트만 실행하면 **동일 조건** 확보. CI에서도 가능(권한 필요).

---

## 18.5 실습 — **MiniBench**: 길이-프리픽스 에코 로더(C++23)

> **목표**: 5분만에 **우리 프로토콜**(15장 MiniRPC/1 또는 단순 길이-프리픽스)의 **지연/처리량**을 측정.  
> 특징: **epoll(ET)** 기반, **N 연결 × 파이프라인 K** 요청, **p50/p95/p99** 히스토그램.

### 18.5.1 설계 요점

- **프레이밍**: `[u32 BE length][payload bytes]` 를 **연속 전송**. 서버는 그대로 에코.  
- **파이프라인**: 한 연결에 **동시에 M개** 미해결 요청 가능(응답 순서는 보장).  
- **요청 페이로드**: 가변 길이 `L ∈ [min,max]`, 무작위.  
- **지연 측정**: 요청마다 **보낸 시각** 기록 → **첫 바이트 수신** 또는 **본문 완료**까지 RTT.  
- **이벤트 루프**: epoll(ET), **버퍼 소진 루프**(10장 패턴).  
- **출력**: 콘솔 요약 + **CSV/JSON**(옵션) 파일.

### 18.5.2 코드

> **주의**: 교육용으로 필요한 만큼만 담았다. 운영용은 **신호 처리/CPU 바인딩/NUMA/핫패스 최적화**를 덧붙이자.

```cpp
// minibench.cpp — C++23 epoll 기반 길이-프리픽스 에코 로더
// 빌드: g++ -std=c++23 -O2 -pthread minibench.cpp -o minibench
// 사용 예:
//   ./minibench 127.0.0.1 9000 \
//               --conns 200 --pipeline 32 \
//               --min 64 --max 1024 --qps 50000 \
//               --warmup 5 --duration 30 --cooldown 5

#include <arpa/inet.h>
#include <fcntl.h>
#include <netdb.h>
#include <sys/epoll.h>
#include <sys/socket.h>
#include <sys/types.h>
#include <unistd.h>

#include <algorithm>
#include <atomic>
#include <chrono>
#include <cstddef>
#include <cstdint>
#include <expected>
#include <map>
#include <numeric>
#include <optional>
#include <print>
#include <random>
#include <string>
#include <string_view>
#include <thread>
#include <vector>

using clock = std::chrono::steady_clock;

// ---- util ----
static int set_nonblock(int fd){ int fl=fcntl(fd,F_GETFL,0); return (fl<0)?-1:fcntl(fd,F_SETFL,fl|O_NONBLOCK); }
static int dial(const char* host, const char* serv){
    addrinfo hints{}, *res=nullptr;
    hints.ai_family=AF_UNSPEC; hints.ai_socktype=SOCK_STREAM; hints.ai_flags=AI_ADDRCONFIG|AI_NUMERICSERV;
    if (getaddrinfo(host, serv, &hints, &res)!=0) return -1;
    int s=-1;
    for (auto* ai=res; ai; ai=ai->ai_next){
        s=::socket(ai->ai_family, ai->ai_socktype, ai->ai_protocol);
        if (s<0) continue;
        set_nonblock(s);
        if (::connect(s, ai->ai_addr, ai->ai_addrlen)==0 || errno==EINPROGRESS) { freeaddrinfo(res); return s; }
        ::close(s); s=-1;
    }
    freeaddrinfo(res); return -1;
}

struct Hdr { uint32_t be_len; }; // network order

// ---- histogram ----
struct HdrHist {
    std::vector<uint64_t> buckets; // ns
    uint64_t min_ns=~0ull, max_ns=0, count=0, sum_ns=0;
    HdrHist(){ buckets.resize(64,0); } // log2 buckets
    void add(uint64_t ns){
        min_ns = std::min(min_ns, ns); max_ns = std::max(max_ns, ns);
        sum_ns += ns; ++count;
        int idx = std::min<int>(63, (ns==0?0: (63-__builtin_clzll(ns))));
        buckets[(size_t)idx]++; // rough log2 histogram
    }
    double p(double q) const { // naive linear scan (edu)
        if (count==0) return 0;
        uint64_t target = (uint64_t)std::ceil(q*count);
        uint64_t acc=0;
        for (size_t i=0;i<buckets.size();++i){
            acc += buckets[i];
            if (acc>=target){
                // bucket lower bound
                uint64_t lo = (i==0?0ull : (1ull<<(i)));
                return (double)lo;
            }
        }
        return (double)max_ns;
    }
};

// ---- connection state ----
struct Inflight {
    uint32_t id;
    uint32_t len;    // payload len
    uint32_t need;   // remaining to read in body (after header)
    clock::time_point t0;
};

struct Conn {
    int fd{-1};
    bool connected{false};
    std::vector<std::byte> out; // pending bytes to send (may contain multiple frames)
    std::vector<std::byte> in;  // recv buffer
    std::vector<Inflight> pipe; // in-flight requests (ordered)
    size_t in_off{0};           // parse offset
};

struct Options {
    std::string host="127.0.0.1", port="9000";
    int conns=100, pipeline=16;
    int min_len=64, max_len=1024;
    uint64_t target_qps=50000; // total across all conns
    int warmup_s=5, duration_s=30, cooldown_s=5;
};

static std::atomic<uint64_t> g_sent{0}, g_done{0}, g_err{0};
static HdrHist g_hist;

// ---- generators ----
struct PayloadGen {
    std::mt19937 rng{std::random_device{}()};
    std::uniform_int_distribution<int> dist;
    PayloadGen(int a, int b): dist(a,b){}
    std::vector<std::byte> next(){
        int n = dist(rng);
        std::vector<std::byte> buf(4+n);
        uint32_t be = htonl((uint32_t)n);
        std::memcpy(buf.data(), &be, 4);
        for (int i=0;i<n;i++) buf[4+i] = std::byte('a' + (i%26));
        return buf;
    }
};

static void arm_send(Conn& c, PayloadGen& gen, int how_many){
    for (int i=0;i<how_many;i++){
        auto frame = gen.next();
        // enqueue
        size_t old = c.out.size();
        c.out.resize(old + frame.size());
        std::memcpy(c.out.data()+old, frame.data(), frame.size());
        // track inflight
        uint32_t len=0; std::memcpy(&len, frame.data(), 4); len = ntohl(len);
        c.pipe.push_back(Inflight{.id=(uint32_t)g_sent.fetch_add(1)+1, .len=len, .need=len, .t0=clock::now()});
    }
}

// ---- event loop ----
int main(int argc, char** argv){
    Options opt;
    for (int i=1;i<argc;i++){
        std::string k=argv[i];
        auto need = [&](int i){ if (i+1>=argc){ std::print(stderr,"missing value for {}\n",k); std::exit(2);} return std::string(argv[i+1]); };
        if (k=="--conns")       opt.conns=std::stoi(need(i++));
        else if (k=="--pipeline") opt.pipeline=std::stoi(need(i++));
        else if (k=="--min")    opt.min_len=std::stoi(need(i++));
        else if (k=="--max")    opt.max_len=std::stoi(need(i++));
        else if (k=="--qps")    opt.target_qps=std::stoull(need(i++));
        else if (k=="--warmup") opt.warmup_s=std::stoi(need(i++));
        else if (k=="--duration")opt.duration_s=std::stoi(need(i++));
        else if (k=="--cooldown")opt.cooldown_s=std::stoi(need(i++));
        else if (k=="--host")   opt.host=need(i++);
        else if (k=="--port")   opt.port=need(i++);
    }

    std::vector<Conn> conns; conns.resize(opt.conns);
    for (int i=0;i<opt.conns;i++){
        int s = dial(opt.host.c_str(), opt.port.c_str());
        if (s<0){ perror("connect"); return 1; }
        conns[i].fd=s;
    }

    int ep = ::epoll_create1(EPOLL_CLOEXEC);
    for (auto& c: conns){
        epoll_event ev{.events=EPOLLOUT|EPOLLET|EPOLLRDHUP, .data={.fd=c.fd}};
        ::epoll_ctl(ep, EPOLL_CTL_ADD, c.fd, &ev);
    }

    PayloadGen gen(opt.min_len, opt.max_len);
    const uint64_t per_conn_rate = std::max<uint64_t>(1, opt.target_qps / std::max(1,opt.conns));
    const auto tick = std::chrono::nanoseconds(1'000'000'000ull / per_conn_rate); // naive pacing

    // 타임라인
    auto t0 = clock::now();
    auto t_warm_end = t0 + std::chrono::seconds(opt.warmup_s);
    auto t_steady_end = t_warm_end + std::chrono::seconds(opt.duration_s);
    auto t_end = t_steady_end + std::chrono::seconds(opt.cooldown_s);

    std::vector<clock::time_point> next_send(conns.size(), t0);

    std::vector<epoll_event> evs(4096);
    while (clock::now() < t_end){
        int to_ms = 10;
        int n = ::epoll_wait(ep, evs.data(), (int)evs.size(), to_ms);
        // (1) 이벤트 처리
        for (int i=0;i<n;i++){
            int fd = evs[i].data.fd;
            auto it = std::find_if(conns.begin(), conns.end(), [&](auto& x){return x.fd==fd;});
            if (it==conns.end()) continue;
            Conn& c = *it;

            // 연결 완료 확인(EPOLLOUT로 들어옴)
            if (!c.connected){
                int so=0; socklen_t sl=sizeof(so);
                getsockopt(fd,SOL_SOCKET,SO_ERROR,&so,&sl);
                if (so==0) c.connected=true;
                else { g_err++; ::close(fd); c.fd=-1; continue; }
            }

            // (1a) 쓰기 드레인
            if (evs[i].events & EPOLLOUT){
                while (!c.out.empty()){
                    ssize_t m = ::send(c.fd, c.out.data(), c.out.size(), 0);
                    if (m>0){ c.out.erase(c.out.begin(), c.out.begin()+m); }
                    else {
                        if (m<0 && (errno==EAGAIN||errno==EWOULDBLOCK||errno==EINTR)) break;
                        // fatal
                        g_err++; ::close(c.fd); c.fd=-1; break;
                    }
                }
            }

            // (1b) 읽기 드레인
            if (evs[i].events & (EPOLLIN|EPOLLRDHUP)){
                for(;;){
                    std::byte buf[8192];
                    ssize_t m = ::recv(c.fd, buf, sizeof(buf), 0);
                    if (m>0){
                        size_t old=c.in.size(); c.in.resize(old+m);
                        std::memcpy(c.in.data()+old, buf, m);
                        // 파싱: [len(4)][body]
                        while (true){
                            if (c.pipe.empty()) break;
                            if (c.in.size()-c.in_off < 4) break;
                            uint32_t be; std::memcpy(&be, c.in.data()+c.in_off, 4);
                            uint32_t need = ntohl(be);
                            if (c.in.size()-c.in_off < 4 + need) break;
                            // 완료
                            auto inf = c.pipe.front(); c.pipe.erase(c.pipe.begin());
                            auto t1 = clock::now();
                            if (t1>=t_warm_end && t1<=t_steady_end){
                                uint64_t ns = std::chrono::duration_cast<std::chrono::nanoseconds>(t1 - inf.t0).count();
                                g_hist.add(ns);
                            }
                            g_done++;
                            c.in_off += 4 + need;
                            if (c.in_off==c.in.size()){ c.in.clear(); c.in_off=0; } // reset
                        }
                    } else if (m==0){
                        g_err++; ::close(c.fd); c.fd=-1; break;
                    } else {
                        if (errno==EAGAIN||errno==EWOULDBLOCK||errno==EINTR) break;
                        g_err++; ::close(c.fd); c.fd=-1; break;
                    }
                }
            }
        }

        // (2) 페이싱: 각 연결별 타이밍에 맞춰 파이프 채우기
        auto now = clock::now();
        for (size_t i=0;i<conns.size();++i){
            Conn& c = conns[i];
            if (c.fd<0 || !c.connected) continue;
            // 파이프 유지
            int need = opt.pipeline - (int)c.pipe.size();
            if (need>0 && now >= next_send[i]){
                arm_send(c, gen, need);
                next_send[i] = now + tick;
                // EPOLLOUT 관심 유지
                epoll_event ev{.events=EPOLLIN|EPOLLOUT|EPOLLET|EPOLLRDHUP, .data={.fd=c.fd}};
                ::epoll_ctl(ep, EPOLL_CTL_MOD, c.fd, &ev);
            } else if (c.out.empty()){
                // 쓰기 관심 해제(불필요 인터럽트 감소)
                epoll_event ev{.events=EPOLLIN|EPOLLET|EPOLLRDHUP, .data={.fd=c.fd}};
                ::epoll_ctl(ep, EPOLL_CTL_MOD, c.fd, &ev);
            }
        }
    }

    // 마감
    for (auto& c: conns) if (c.fd>=0) ::close(c.fd);

    // 결과
    double dur = (double)opt.duration_s;
    double rps = g_done / dur;
    auto ns2ms = [](double ns){ return ns/1e6; };
    std::print("=== MiniBench Result ===\n");
    std::print("conns={} pipeline={} len=[{},{}] qps={} warm={}s run={}s\n",
               opt.conns, opt.pipeline, opt.min_len, opt.max_len, opt.target_qps, opt.warmup_s, opt.duration_s);
    std::print("sent={} done={} err={}\n", g_sent.load(), g_done.load(), g_err.load());
    std::print("throughput: {:.0f} req/s\n", rps);
    std::print("latency p50={:.3f}ms p95={:.3f}ms p99={:.3f}ms max≈{:.3f}ms\n",
               ns2ms(g_hist.p(0.50)), ns2ms(g_hist.p(0.95)), ns2ms(g_hist.p(0.99)), ns2ms((double)g_hist.max_ns));
}
```

**실행 예**  
1) 서버(에코): `./decho 9000` 또는 10장 epoll-에코 서버  
2) 로더:
```bash
./minibench 127.0.0.1 9000 --conns 200 --pipeline 32 --min 64 --max 1024 \
  --qps 100000 --warmup 5 --duration 30 --cooldown 5
```
3) 네트워크 제약:
```bash
sudo tc qdisc add dev lo root netem delay 2ms 0.5ms loss 0.2%
# 측정 후
sudo tc qdisc del dev lo root
```

> **관찰 포인트**  
> - **파이프라인**을 늘리면 RTT×대역폭 한계를 넘어서 **CPU/커널** 병목이 드러난다.  
> - p99가 급상승하면 **Send-Q/Recv-Q** 또는 **사용자 공간 큐/락**을 의심(13장 도구로 교차 확인).

---

## 18.6 테스트 시나리오 예시(기능/내구/혼잡/장애/복구)

### 18.6.1 기능(기본/경계/오류)

- **정상 프레임**: 1B, 64B, 1KiB, 1MiB-1B payload
- **경계**: 길이=0, 길이=MaxCap(+1은 **에러**)
- **CRC**: 임의 바이트 뒤집기 → `bad crc` 에러
- **프레이밍 붕괴**: 길이를 10배로 허위 선언 → 서버가 cap으로 거절
- **타임아웃**: READ_LEN/READ_BODY deadline(12장)으로 지연 공격 방어 확인

### 18.6.2 내구(Soak)

- **설정**: RPS=목표의 70%, 12시간
- **메트릭**: RSS/FD/소켓 상태 분포(CLOSE_WAIT/TIME_WAIT), 재전송률, 로그에러/분
- **패스 기준**: 메모리/FD **정상수렴**, 에러율 ≤ 0.01%, GC/락 히카프 없음

### 18.6.3 혼잡

- **RTT**: 1ms/10ms/100ms/200ms
- **손실**: 0%/0.5%/1%/2%
- **대역폭**: 10/50/100/500 Mbps
- **결과**: 처리량 vs 손실 근사
  $$
  \text{Throughput} \approx \frac{\text{MSS}}{\text{RTT}\sqrt{p}}
  $$
  → 손실 1%에서 처리량 급락 곡선 확인

### 18.6.4 장애

- **SYN flood**: SYN-RECV 급증 → `tcp_syncookies`/SYNPROXY 적용 전후 비교  
- **포트 고갈**: 초당 신규 연결 ↑ → TIME_WAIT/NAT 테이블 관측  
- **CLOSE_WAIT 누수**: 고의 close 누락 빌드로 누적 확인  
- **디스크 압박**: 로그 sync 옵션/회전 → tail latency 상승 여부

### 18.6.5 복구

- **프로세스 크래시**: 1분 간격으로 강제 종료 → **자동 재기동**/연결 재시도 성공률  
- **네트워크 절단**: 30초 동안 경로 불통 → **백오프** 및 **MTTR** 측정  
- **구성 롤백**: 옵션 바꿈(예: `TCP_NODELAY` on/off) → 리그레션 체크

---

## 18.7 보조 도구/메트릭 수집

- **ss**: `ss -ti 'sport = :9000'` → `rtt`, `retrans`, `cwnd`  
- **tcpdump/Wireshark**: 재전송/제로윈도우/Out-of-Order  
- **perf**: `perf top`, `perf record -g` → **핫스팟/락**  
- **/proc**: `sockstat`, `net/snmp`, `net/netstat`  
- **CPU/메모리/IO**: `pidstat`, `iostat`, `vmstat`, `numastat`  
- **컨테이너**: cgroup v2 `cpu.max`, `memory.current`, `io.stat`  
- **TCP_INFO**: 런타임에 RTT/재전송 카운트 로깅(12·13장 참고)

---

## 18.8 실습 — **지표 기반 성능 리포트 템플릿**

> 팀/리뷰어가 **환경·조건·결과**를 **한눈에** 보게 하는 **양식**.  
> 그대로 복사해 매 실험 **PR/위키**에 붙여 넣자.

### 18.8.1 템플릿(마크다운)

```markdown
# 성능 리포트 — <서비스/버전/커밋>

## 1. 요약
- 목적: <예: epoll ET + TLS 전환 후 p99 지연 영향 측정>
- 결론: <예: p99 -18%, RPS +12% @ 100ms RTT, 0.5% loss>

## 2. 환경
- 하드웨어: <CPU, 코어/스레드, RAM, NIC>
- OS/커널: <배포판, 커널 버전>
- 바이너리: <커밋 SHA, 빌드 플래그>
- 실행 옵션: <예: TCP_NODELAY=on, SNDBUF/RCVBUF=...>
- 네트워크: `tc netem` = <delay, jitter, loss, bw>

## 3. 트래픽 모델
- 프로토콜: <길이-프리픽스 / MiniRPC/1 / HTTP/2>
- 요청 페이로드: <분포/크기>
- 동시성: <연결 수, 파이프라인>
- 런타임: <warm/duration/cooldown>

## 4. 결과 (핵심 지표)
| Case | RPS | p50(ms) | p95(ms) | p99(ms) | Error(%) | CPU(%) |
|------|-----|---------|---------|---------|----------|--------|
| Baseline | 80k | 1.1 | 3.5 | 7.2 | 0.01 | 320 |
| New(ET+TLS) | 90k | 1.0 | 3.0 | 5.9 | 0.01 | 335 |

## 5. 보조 메트릭
- TCP: rtt ~ <>, retrans ~ <>, cwnd ~ <>
- 큐: Recv-Q 상위 5, Send-Q 상위 5
- GC/락: <핫스팟 함수, 경합 포인트>

## 6. 재현 절차
- 서버: `<cmd>`
- 로더: `<cmd>`
- netem: `<cmd>`

## 7. 해석 & 액션
- 원인 추정: <예: CORK로 헤더/바디 묶임 개선>
- 다음 액션: <예: SNDBUF 상향, TLS 레코드 크기 튜닝>

## 8. 원자료 링크
- 로그/CSV/JSON/pcap 경로
```

### 18.8.2 샘플 수치(예시)

- 조건: 20Mbps, RTT 100ms, 손실 0.5%, 200 conns × pipeline 32  
- 결과:
  - Baseline: RPS=78,500 / p99=7.4ms / Error 0.02%  
  - 튜닝(`sendfile`+`TCP_CORK`): RPS=88,900 / p99=6.0ms / Error 0.02%  
  - **개선폭**: RPS **+13.2%**, p99 **-18.9%**

---

## 18.9 자동화 스크립트 스니펫

### 18.9.1 러너(Bash)

```bash
#!/usr/bin/env bash
set -euo pipefail

IF=lo
run() { echo "+ $*"; "$@"; }

# 네트워크 설정
run sudo tc qdisc del dev $IF root || true
run sudo tc qdisc add dev $IF root netem delay 2ms 0.5ms loss 0.2%

# 서버
pkill -f decho || true
./decho 9000 >/tmp/srv.log 2>&1 &
SRV_PID=$!
sleep 1

# 벤치
./minibench 127.0.0.1 9000 --conns 200 --pipeline 32 --qps 100000 --warmup 5 --duration 30 --cooldown 5 \
  | tee /tmp/bench.txt

# 청소
kill $SRV_PID || true
run sudo tc qdisc del dev $IF root || true
```

### 18.9.2 Lua(wrk)로 **RPS 스윕**

```bash
for c in 64 128 256 512; do
  wrk -t4 -c$c -d30s --latency http://127.0.0.1:8080/ | tee result_c${c}.txt
done
```

---

## 18.10 해석 가이드 — **통계/꼬리/유의성**

- **평균**은 의미가 약하다. 항상 **p50/p95/p99**를 보고, 가능하면 **p99.9**도 본다.  
- **꼬리(heavy-tail)**는 대개 **큐잉/재전송/락 경합**에서 발생. **원인-메트릭**을 짝지어 본다.  
- 비교 시 **이분산/비정규**가 흔하다 → “±표준편차”보다 **부트스트랩** 또는 **중위수 차이** 보고를 권장.  
- **샘플 수**가 작으면 p99는 **노이즈**가 크다(최소 수십만 건 이상 권장).

---

## 18.11 체크리스트(요약)

- [ ] **테스트 분류**를 명확히(기능/내구/혼잡/장애/복구)  
- [ ] **조건 스크립트화**: `tc/netem`, veth+ns — **재현 가능**  
- [ ] **로더 선택**: HTTP → `wrk/h2load`, 자체 프로토콜 → **MiniBench**  
- [ ] **지표**: RPS/지연(p50/p95/p99)/에러율/TCP_INFO/큐  
- [ ] **리포트 템플릿**으로 문서화, PR/위키에 첨부  
- [ ] **원자료 보존**: 로그/CSV/pcap — 6개월 보관 권장  
- [ ] **액션 아이템**: 개선/회귀 기준선 업데이트

---

### 18.12 마무리

좋은 벤치마크는 “**빠른 수치**”가 아니라 “**반복해도 같은 결론**”이다.  
**네트워크 조건을 고정**하고, **도구의 제약**을 이해하며, **자체 로더**로 프로토콜의 본질을 측정하라.  
그 위에 13~17장에서 다진 **관측/옵션/IPv6/보안**을 얹으면, 결과는 **근거 있는 엔지니어링 선택**이 된다.
