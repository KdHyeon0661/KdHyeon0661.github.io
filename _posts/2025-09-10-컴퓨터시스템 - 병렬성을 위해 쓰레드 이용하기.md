---
layout: post
title: 컴퓨터시스템 - 병렬성을 위해 쓰레드 이용하기
date: 2025-09-10 22:20:23 +0900
category: 컴퓨터시스템
---
# 병렬성을 위해 **쓰레드** 이용하기

## 0. 동시성과 병렬성, 그리고 왜 “쓰레드”인가

- **동시성(Concurrency)**: 여러 작업을 *겹쳐서* 다룸(스케줄링/인터리빙).  
- **병렬성(Parallelism)**: 여러 코어가 **물리적으로 동시에** 수행.  
- **쓰레드 기반 병렬성**의 장점
  - **공유 주소공간** → 공유 데이터/결과 통합이 간단.
  - **생성·전환 비용**이 프로세스보다 작음.
  - **벡터화/NUMA/스케줄러** 및 라이브러리와 자연스럽게 연동.
- **주의**: 데이터 레이스, **메모리 모델**, **거짓 공유(false sharing)**, **락 경합**을 모르면 성능이 급락하고 정확성도 깨짐.

---

## 1. 무엇을, 어떻게 나눌 것인가 — 병렬화 전략

### 1.1 병렬화 형태
- **데이터 병렬**: 같은 연산을 큰 데이터에 반복. 예) 벡터 합/필터/행렬 연산.  
- **태스크 병렬**: 이질 작업을 병렬. 예) 다중 파일 인코딩/스테이지별 파이프라인.  
- **분할-정복**: 문제를 재귀 분할해 병렬 처리. 예) 병렬 머지/퀵소트.  
- **파이프라인**: (입력)→(전처리)→(연산)→(출력) 스테이지 병렬.

### 1.2 작업 크기(Granularity)
- 너무 작으면 스케줄/동기화 오버헤드가 이득을 잠식.  
- 너무 크면 **로드 불균형**.  
→ **블록/청크**로 나누고, 데이터 분포가 불균일하면 **동적 배분(작업 큐/워크 스틸링)** 고려.

---

## 2. 성능의 수학 — Amdahl & Gustafson

- **암달의 법칙** (직렬 비율 \\(\alpha\\), 코어 수 \\(p\\))
  $$
  S(p) = \frac{1}{\alpha + \frac{1-\alpha}{p}}
  $$
  → **직렬 구간 축소**가 최우선.
- **거스타프슨의 법칙**: 문제 크기를 키워 병렬 구간을 늘리면 **거의 선형**까지 확장 가능.

---

## 3. 메모리 모델 핵심 (C11 원자 + pthreads)

- **happens-before**를 만들면 앞의 쓰기가 뒤에서 **반드시 보임**.
  - 락 해제 ↔ 다음 획득, 조건변수 signal ↔ wait 반환,
  - `atomic_store(..., release)` ↔ `atomic_load(..., acquire)`.
- **`volatile`는 동기화가 아님**. MMIO 최적화 억제용일 뿐.  
- **원자 오더**: `seq_cst` > `acq_rel` > `relaxed`.  
  플래그/발행은 **release-store / acquire-load**로.

---

## 4. 공유 쓰기 줄이기 — 기본 원칙

1) **per-thread 로컬 상태** → 마지막에 **reduction**으로 합치기.  
2) **거짓 공유 방지**: 쓰기 변수는 **캐시라인(보통 64B)**가 겹치지 않게 **패딩/정렬**.  
3) **락 최소화**: 긴 루프에 락 금지, **배치(batch)** 처리 후 한 번에 반영.  
4) **지역성**: 블로킹/타일링, **first-touch**로 NUMA 로컬리티 확보.

---

## 5. 실전 ①: 벡터 합 — per-thread 청크 + 패딩된 reduction

### 5.1 코드
```c
// parallel_sum.c : gcc -O3 -march=native -pthread parallel_sum.c -o sum
#define _GNU_SOURCE
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <time.h>
#include <unistd.h>

#ifndef CL
#define CL 64
#endif

typedef struct { double value; char pad[CL - sizeof(double)]; } padded_double;

typedef struct {
  const double *a;
  size_t begin, end;
  padded_double *out_slot; // 거짓 공유 방지
} task_t;

static void* worker(void* arg){
  task_t* t=(task_t*)arg;
  double s=0.0;
  for(size_t i=t->begin;i<t->end;i++) s += t->a[i];
  t->out_slot->value = s;
  return NULL;
}

static double now_sec(void){
  struct timespec ts; clock_gettime(CLOCK_MONOTONIC, &ts);
  return ts.tv_sec + ts.tv_nsec*1e-9;
}

int main(int argc, char** argv){
  size_t n = (argc>1) ? strtoull(argv[1],NULL,10) : (size_t)1e8;
  int    P = (argc>2) ? atoi(argv[2]) : sysconf(_SC_NPROCESSORS_ONLN);

  double *a = aligned_alloc(CL, n*sizeof(double));
  for(size_t i=0;i<n;i++) a[i]=1.0; // 테스트 입력

  padded_double *partial = aligned_alloc(CL, P*sizeof(padded_double));
  pthread_t *th = malloc(P*sizeof(pthread_t));
  task_t    *tk = malloc(P*sizeof(task_t));

  size_t chunk=(n+P-1)/P;
  double t0=now_sec();

  for(int p=0;p<P;p++){
    tk[p].a=a;
    tk[p].begin = p*chunk;
    tk[p].end   = (tk[p].begin+chunk>n)? n : tk[p].begin+chunk;
    tk[p].out_slot = &partial[p];
    pthread_create(&th[p],NULL,worker,&tk[p]);
  }

  double sum=0.0;
  for(int p=0;p<P;p++){ pthread_join(th[p],NULL); sum+=partial[p].value; }
  double t1=now_sec();

  printf("sum=%.0f time=%.3f s (P=%d, n=%zu)\n", sum, t1-t0, P, n);
  free(a); free(partial); free(th); free(tk);
  return 0;
}
```

### 5.2 튜닝 포인트
- **NUMA first-touch**: 배열 초기화를 스레드별 청크에서 수행(§10).
- 큰 n에서는 **메모리 대역폭**이 병목 → 코어 증가 대비 이득 둔화(루프 내 연산 강도/프리페치 고려).

---

## 6. 실전 ②: 병렬 히스토그램 — 로컬화 후 병합

공유 히스토그램을 직접 갱신하면 **원자/락 경합** 발생 → 스레드별 **로컬 히스토그램** 생성 후 마지막에 병합.

```c
// parallel_hist.c : gcc -O3 -pthread parallel_hist.c -o hist
#include <pthread.h>
#include <stdlib.h>
#include <stdio.h>
#include <string.h>

#define BINS 256
typedef struct { const unsigned char *data; size_t begin, end; unsigned *local; } job_t;

void* worker(void* a){
  job_t* j=(job_t*)a;
  memset(j->local, 0, BINS*sizeof(unsigned));
  for(size_t i=j->begin;i<j->end;i++) j->local[j->data[i]]++;
  return NULL;
}

int main(int argc,char**argv){
  size_t n=(argc>1)?strtoull(argv[1],NULL,10):100000000;
  int P=(argc>2)?atoi(argv[2]):4;
  unsigned char *data=malloc(n);
  for(size_t i=0;i<n;i++) data[i]=(unsigned char)(i%BINS);

  pthread_t th[P]; job_t jb[P]; unsigned* locals[P];
  size_t chunk=(n+P-1)/P;
  for(int p=0;p<P;p++){
    locals[p]=aligned_alloc(64,BINS*sizeof(unsigned));
    jb[p]=(job_t){.data=data,.begin=p*chunk,.end=((p+1)*chunk>n)?n:(p+1)*chunk,.local=locals[p]};
    pthread_create(&th[p],NULL,worker,&jb[p]);
  }
  for(int p=0;p<P;p++) pthread_join(th[p],NULL);

  unsigned global[BINS]={0};
  for(int p=0;p<P;p++) for(int b=0;b<BINS;b++) global[b]+=locals[p][b];

  printf("global[0]=%u, global[255]=%u\n", global[0], global[255]);
  for(int p=0;p<P;p++) free(locals[p]); free(data);
}
```

---

## 7. 실전 ③: 블로킹(타일링) 기반 병렬 행렬곱

- 단순 i-j-k 루프는 **캐시 미스**/**대역폭 병목**.  
- **타일(블록)** 단위로 C의 서브블록을 각 스레드에 분배.

```c
// parallel_matmul_blocked.c : gcc -O3 -march=native -pthread -o mm parallel_matmul_blocked.c
#include <pthread.h>
#include <stdlib.h>
#include <string.h>
#include <stdio.h>

#define BS 64 // 타일 크기(머신에 맞게 조정)
typedef struct { int n,P,id; double* A; double* B; double* C; } job_t;

static inline void block_mm(double* C,double* A,double* B,int n,int i0,int j0,int k0){
  int i,j,k, i1=i0+BS, j1=j0+BS, k1=k0+BS;
  for(i=i0;i<i1;i++)
    for(k=k0;k<k1;k++){
      double aik=A[i*n+k];
      for(j=j0;j<j1;j++) C[i*n+j]+=aik*B[k*n+j];
    }
}

static void* worker(void* arg){
  job_t* t=(job_t*)arg; int n=t->n, P=t->P, id=t->id, nb=n/BS;
  for(int bi=id; bi<nb*nb; bi+=P){
    int i0=(bi/nb)*BS, j0=(bi%nb)*BS;
    for(int k0=0;k0<n;k0+=BS) block_mm(t->C,t->A,t->B,n,i0,j0,k0);
  }
  return NULL;
}

int main(int argc,char**argv){
  int n=(argc>1)?atoi(argv[1]):1024, P=(argc>2)?atoi(argv[2]):4;
  double *A=aligned_alloc(64,n*n*sizeof(double));
  double *B=aligned_alloc(64,n*n*sizeof(double));
  double *C=aligned_alloc(64,n*n*sizeof(double));
  for(int i=0;i<n*n;i++){ A[i]=1.0; B[i]=1.0; C[i]=0.0; }

  pthread_t th[P]; job_t jb[P];
  for(int p=0;p<P;p++){ jb[p]=(job_t){n,P,p,A,B,C}; pthread_create(&th[p],NULL,worker,&jb[p]); }
  for(int p=0;p<P;p++) pthread_join(th[p],NULL);

  printf("C[0]=%.0f, C[n*n-1]=%.0f\n", C[0], C[n*n-1]);
  free(A);free(B);free(C);
}
```

**팁**: 더 나가려면 커널 순서/타일 레이아웃/벡터화(컴파일러 지시문)·프리페치까지 조정.

---

## 8. 배리어(Barrier) — 표준/대체 구현

### 8.1 POSIX 배리어(있으면 사용)
```c
#include <pthread.h>
pthread_barrier_t bar;
pthread_barrier_init(&bar, NULL, P);
// 각 스레드에서
pthread_barrier_wait(&bar);
```
> macOS 등 일부 환경은 `pthread_barrier`가 없습니다.

### 8.2 센스-리버싱 배리어(휴대용)
```c
// sense_barrier.h
#include <stdatomic.h>
typedef struct {
  atomic_int count;
  int        n;
  atomic_int sense;
} sense_barrier_t;

static inline void sense_barrier_init(sense_barrier_t* b, int n){
  atomic_store(&b->count, n);
  b->n=n; atomic_store(&b->sense, 0);
}
static inline void sense_barrier_wait(sense_barrier_t* b, int* tls_sense){
  int s = *tls_sense;
  if (atomic_fetch_sub_explicit(&b->count, 1, memory_order_acq_rel) == 1){
    atomic_store_explicit(&b->count, b->n, memory_order_release);
    atomic_store_explicit(&b->sense, !s,   memory_order_release);
  } else {
    while (atomic_load_explicit(&b->sense, memory_order_acquire) == s) { /* spin */ }
  }
  *tls_sense = !s;
}
```
- 각 스레드는 TLS에 `int tls_sense`(초기 0)를 갖고 호출.

---

## 9. 스레드 풀(작업 큐) — 동적 분할 기초

### 9.1 단순 MPMC 큐(뮤텍스+조건변수)
```c
// thread_pool.c (핵심만)
#include <pthread.h>
#include <stdbool.h>
typedef void (*task_fn)(void*);

typedef struct job { task_fn fn; void* arg; struct job* next; } job_t;
typedef struct {
  pthread_mutex_t m; pthread_cond_t cv;
  job_t* head; job_t* tail; bool stop;
} queue_t;

static void q_init(queue_t* q){ pthread_mutex_init(&q->m,NULL); pthread_cond_init(&q->cv,NULL); q->head=q->tail=NULL; q->stop=false; }
static void q_push(queue_t* q, job_t* j){
  pthread_mutex_lock(&q->m);
  if (!q->tail) q->head=q->tail=j; else { q->tail->next=j; q->tail=j; }
  j->next=NULL; pthread_cond_signal(&q->cv); pthread_mutex_unlock(&q->m);
}
static job_t* q_pop(queue_t* q){
  pthread_mutex_lock(&q->m);
  while(!q->head && !q->stop) pthread_cond_wait(&q->cv,&q->m);
  job_t* j=q->head; if(j){ q->head=j->next; if(!q->head) q->tail=NULL; }
  pthread_mutex_unlock(&q->m); return j;
}
```

### 9.2 워커 스레드
```c
typedef struct { queue_t* q; } worker_arg_t;
static void* worker(void* a){
  worker_arg_t* wa=(worker_arg_t*)a;
  for(;;){
    job_t* j=q_pop(wa->q);
    if(!j){ // stop
      if (wa->q->stop) break;
      else continue;
    }
    j->fn(j->arg);
    free(j);
  }
  return NULL;
}
```

### 9.3 종료(포이즌) 처리
```c
void q_stop(queue_t* q){
  pthread_mutex_lock(&q->m); q->stop=true; pthread_cond_broadcast(&q->cv); pthread_mutex_unlock(&q->m);
}
```

> 고성능이 필요하면 **락-프리** 또는 **워크 스틸링 데크**를 검토(구현 난이도↑).

---

## 10. 코어 고정(affinity)·NUMA first-touch

### 10.1 코어 고정 (Linux)
```c
#define _GNU_SOURCE
#include <pthread.h>
#include <sched.h>

static void pin_to_core(int core){
  cpu_set_t cs; CPU_ZERO(&cs); CPU_SET(core, &cs);
  pthread_setaffinity_np(pthread_self(), sizeof(cs), &cs);
}
```
- 긴 작업/캐시 민감 워크로드에서 **캐시 지역성** 손실 방지.

### 10.2 NUMA **first-touch** 초기화
- 페이지는 **처음 접근한 코어의 노드**에 할당.  
- 각 스레드가 **자신의 청크를 초기화**하도록 하여 로컬 노드에 할당.

```c
// first-touch: sum 예제 초기화 루프를 스레드별 청크에서 수행
for(size_t i=tk[p].begin;i<tk[p].end;i++) a[i]=1.0;
```
- 고급: `libnuma`의 `numa_alloc_onnode`/`numa_run_on_node`로 정책을 더 정밀 제어.

---

## 11. 거짓 공유(False Sharing) — 데모와 해결

### 11.1 데모(느려지는 코드)
```c
// false_sharing.c : gcc -O3 -pthread false_sharing.c -o fs
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#define N (100000000)
typedef struct { long v; } bad_t;
bad_t arr[128]; // 같은 캐시라인에 여러 스레드가 v를 갱신할 수 있음

void* worker(void* a){
  int id=(int)(size_t)a;
  for(int i=0;i<N;i++) arr[id].v++; // 캐시라인 무효화 폭주
  return NULL;
}
int main(){ pthread_t t1,t2; pthread_create(&t1,NULL,worker,(void*)0); pthread_create(&t2,NULL,worker,(void*)1); pthread_join(t1,NULL); pthread_join(t2,NULL); printf("%ld %ld\n",arr[0].v,arr[1].v); }
```

### 11.2 해결(패딩/정렬)
```c
#include <stdalign.h>
#define CL 64
typedef struct { alignas(CL) long v; char pad[CL-sizeof(long)]; } good_t;
good_t arr2[128];
```

---

## 12. 원자·락·배리어의 비용 — 최소화 전략

- 원자적 `fetch_add`도 코어 간 **캐시 라인 이동** 비용 큼 → 빈도 ↓, **per-thread 누적 후 reduction**.  
- 락은 **핫 패스** 바깥에서 한 번에 처리.  
- 배리어는 꼭 필요한 타임스텝/스테이지에만 배치.

---

## 13. 분할-정복 예: 병렬 머지소트(스폰 임계값/풀)

- **스레드 과다 생성 방지**: 입력이 작거나 깊이가 얕아지면 **직렬** 처리.  
- 더 안정적으로는 **스레드 풀**에 태스크로 제출.

> (상세 구현은 §9 스레드 풀과 결합. 실무는 풀 기반이 예측 가능.)

---

## 14. 벤치마킹 — 신뢰 가능한 측정

- **컴파일러**: `-O3 -march=native -pthread` (+ 필요 시 `-ffast-math`).  
- **타이머**: `clock_gettime(CLOCK_MONOTONIC, ...)` 사용.  
- **워밍업**: 최초 실행/주파수 스케일링 안정화 후 본 측정.  
- **오버서브스크립션 방지**: OpenBLAS/MKL/numexpr 등 내부 스레딩을 **비활성**.
  - 예) `export OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 OPENBLAS_NUM_THREADS=1`.  
- **정확성**: 작은 입력에서 산술 오차/결과 검증 → 큰 입력으로 확장.  
- **레이스 검사**: **ThreadSanitizer** (`-fsanitize=thread`)로 먼저 확인.

---

## 15. 디버깅·관찰 도구

- **ThreadSanitizer(TSan)**: 데이터 레이스/락 순서 문제.  
- **perf / flamegraph / eBPF(bpftrace)**: 핫 함수/캐시/락 경합 가시화.  
- **numactl / numastat**: NUMA 배치/원격 접근 확인.  
- **hwloc/lstopo**: 코어·NUMA 토폴로지 파악.

---

## 16. 미니 요리책 (Map/Reduce/Scan/Stencils)

### 16.1 Parallel for (정적 분할) 매크로
```c
#define PAR_FOR(idx, begin, end, P, body) do{                    \
  pthread_t _th[P];                                              \
  struct { size_t b,e; } _seg[P];                                \
  size_t _N=(end)-(begin), _chunk=(_N+P-1)/P;                    \
  for(int _p=0;_p<P;_p++){                                       \
    _seg[_p].b=(begin)+_p*_chunk; _seg[_p].e=_seg[_p].b+_chunk;  \
    if(_seg[_p].e>(end)) _seg[_p].e=(end);                       \
    pthread_create(&_th[_p],NULL,                                \
      (void*(*)(void*))[](void* a)->void*{                       \
        size_t b=((typeof(_seg[0])*)a)->b, e=((typeof(_seg[0])*)a)->e; \
        for(size_t idx=b; idx<e; ++idx){ body; }                 \
        return (void*)0;                                         \
      }, &_seg[_p]);                                             \
  }                                                              \
  for(int _p=0;_p<P;_p++) pthread_join(_th[_p],NULL);            \
}while(0)
```
> C 표준에서 람다/익명 함수는 없으므로, 실제 사용은 함수 포인터로 바꾸거나 C++로 구현.

### 16.2 Reduce (per-thread → 최종 합)
- §5 벡터 합 패턴을 **템플릿**처럼 재사용.

### 16.3 Prefix-sum(Scan)
- 단계적 배리어가 필요 → **센스-리버싱 배리어** 결합.

### 16.4 Stencil(이미지/격자)
- 경계(halo) 교환은 **배리어** 후 인접 블록 공유. 가급적 **읽기 공유, 쓰기 로컬**.

---

## 17. 안전 체크리스트 (현업용)

- [ ] 공유 쓰기 **최소화**, per-thread 로컬 상태 + **마지막에 reduction**  
- [ ] **거짓 공유 방지**(패딩/정렬/데이터 배치)  
- [ ] 태스크 편차가 큰 입력은 **동적 분할(작업 큐/워크 스틸링)**  
- [ ] 스레드 수는 **물리 코어 수**±α에서 탐색, 내부 스레딩 비활성  
- [ ] **코어 고정** + **NUMA first-touch**로 메모리 로컬리티 확보  
- [ ] 락/원자/배리어 빈도 **최소화**, 핫 루프에 락 금지  
- [ ] 빌드/벤치 스크립트로 **재현 가능한 측정**(워밍업/고정 시드)  
- [ ] TSan/perf/eBPF로 **정확성+성능**을 함께 검증

---

## 18. 부록 A — 타 OS/런타임 메모

- **macOS**: `pthread_barrier` 미제공 → **센스-리버싱 배리어** 사용.  
- **Windows**: Win32 threads / SRWLocks / ConditionVariable / SetThreadAffinityMask.  
- **C++**: `std::thread`, `std::atomic`, `std::barrier`(C++20), `std::jthread`(C++20) 권장.  
- **OpenMP/TBB**: 동일 원칙(로컬화→reduction, 패딩, NUMA first-touch) 그대로 적용.

---

## 19. 부록 B — 간단 벤치 드라이버 스켈레톤

```c
// bench.h
#pragma once
double now_sec(void);
void  warmup(void (*fn)(void*), void* arg, int iters);
double bench(void (*fn)(void*), void* arg, int iters);

// bench.c
#include <time.h>
double now_sec(void){ struct timespec ts; clock_gettime(CLOCK_MONOTONIC,&ts); return ts.tv_sec+ts.tv_nsec*1e-9; }
void warmup(void (*fn)(void*), void* arg, int iters){ for(int i=0;i<iters;i++) fn(arg); }
double bench(void (*fn)(void*), void* arg, int iters){ double t0=now_sec(); for(int i=0;i<iters;i++) fn(arg); return now_sec()-t0; }
```

---

## 20. 한 줄 결론

**쓰레드로 병렬성**을 얻는 요체는  
(1) **공유 쓰기 회피**(per-thread 로컬 + reduction),  
(2) **캐시/NUMA 친화적 분할**(블로킹·first-touch·코어 고정),  
(3) **락/원자/배리어 최소화**와 **동적 스케줄링**입니다.  
이 원칙을 벡터 합·히스토그램·행렬곱·분할정복·작업 큐로 일관되게 적용하면, 실전에서도 **가까운 선형 스케일**을 안정적으로 끌어낼 수 있습니다.