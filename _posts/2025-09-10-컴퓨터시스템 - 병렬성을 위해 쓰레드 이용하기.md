---
layout: post
title: 컴퓨터시스템 - 병렬성을 위해 쓰레드 이용하기
date: 2025-09-10 22:20:23 +0900
category: 컴퓨터시스템
---
# 병렬성을 위해 **쓰레드** 이용하기 — 설계·구현·튜닝 완전 가이드 (CS:APP 스타일)

> 서술 전제: 이 문서는 전체를 `~~~markdown`으로 감싸고, 모든 코드는 ```로 감쌉니다.  
> 목표: **CPU 바운드 병렬화**를 쓰레드로 구현할 때 필요한 이론(분해/스케줄/메모리 모델/NUMA)과  
> 실전 구현(분할·축소(reduction)·배리어·거짓공유 방지·코어 고정) 및 벤치마킹까지 한 번에 정리합니다.

---

## 0) 동시성과 병렬성, 그리고 왜 “쓰레드”인가

- **동시성(Concurrency)**: 여러 작업을 *겹쳐서* 다루는 설계. (I/O 대기 중 다른 일 수행)
- **병렬성(Parallelism)**: **여러 코어가 물리적으로 동시에** 작업을 수행.
- **쓰레드 기반 병렬성**의 장점
  - 같은 프로세스 주소공간을 **공유** → 공유 데이터 접근/결과 통합이 간단
  - 스레드 생성/스위치 비용이 프로세스보다 작음
  - 라이브러리/런타임(벡터화·NUMA 정책·스케줄러)와 **자연스럽게 연동**
- 단점/주의
  - **데이터 레이스/메모리 모델** 숙지 필요
  - **거짓공유(false sharing)**, **락 경합** → 성능 급락

---

## 1) 병렬화 전략 고르기: 무엇을, 어떻게 나눌 것인가

### 1.1 병렬화 형태
- **데이터 병렬(Data Parallel)**: 동일 연산을 큰 데이터 집합에 반복 적용  
  예) 벡터 합, 이미지 필터, 행렬 연산
- **태스크 병렬(Task Parallel)**: 이질적 작업을 병렬 수행  
  예) 서로 다른 파일 인코딩, 서로 다른 신호 처리 단계
- **분할-정복(Divide & Conquer)**: 재귀적 분할 후 부분 문제 병렬 처리  
  예) 병렬 머지소트/퀵소트, 트리 탐색
- **파이프라인**: 단계별 스테이지를 병렬로 흐르게  
  예) (읽기)→(전처리)→(연산)→(쓰기)

### 1.2 작업 크기(Granularity)
- 너무 **작으면**: 스케줄/컨텍스트 스위치/동기화 오버헤드가 이득을 잠식
- 너무 **크면**: **로드 불균형**(한 스레드만 오래 일함)  
→ **블록/청크 단위**로 나눈 뒤, **동적 배분**(work stealing/작업 큐)을 고려

---

## 2) 병렬 성능의 수학 — Amdahl & Gustafson

- **암달의 법칙**: 직렬 비율 \( \alpha \), 코어 수 \( p \)
  \[
  S(p) = \frac{1}{\alpha + \frac{1-\alpha}{p}}
  \]
  직렬 구간을 줄이는 게 최우선!
- **거스타프슨의 법칙**: 문제 크기를 키워 병렬 구간을 늘리면 **선형에 가까운 확장**도 가능

---

## 3) 공유 데이터 설계 — 정확성과 성능을 동시에

- **원칙 1: 불필요한 공유를 피하라**  
  → **per-thread 로컬 상태**를 갖고, 마지막에 **reduction**으로 합치기
- **원칙 2: 거짓공유 방지**  
  → 각 스레드가 쓰는 변수는 **캐시라인(보통 64B)**을 **공유하지 않도록 패딩/정렬**
- **원칙 3: 락 최소화**  
  → 긴 루프 내부에서 락을 잡지 말고, **배치(batch)**로 처리 후 한 번에 합치기
- **원칙 4: 메모리 접근 지역성**  
  → **블로킹/타일링**(행렬 연산), **first-touch**(NUMA)로 메모리 친화

---

## 4) 실전 ①: 벡터 합(더블) — per-thread 청크 + reduction

### 4.1 핵심 포인트
- 입력 배열 `a[i]`를 **균등 청크**로 나눠 각 스레드가 **로컬 합**을 구함
- 마지막에 **원자/락 없이** 메인 스레드가 **합산**(join 이후)
- **거짓공유 방지**를 위해 per-thread 결과를 **패딩**하여 따로 저장

### 4.2 코드
```c
// parallel_sum.c : -O3 -pthread 로 빌드
#define _GNU_SOURCE
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <time.h>
#include <sched.h>

#ifndef CL
#define CL 64  // 캐시라인 크기 가정
#endif

typedef struct {
  double *a;
  size_t n, begin, end;
  // 거짓공유 방지: 각 스레드 결과는 별도 패딩 오브젝트에 기록
  struct { double value; char pad[CL - sizeof(double)]; } *out_slot;
} task_t;

static void* worker(void* arg){
  task_t* t = (task_t*)arg;
  double s = 0.0;
  for (size_t i = t->begin; i < t->end; i++) s += t->a[i];
  t->out_slot->value = s;
  return NULL;
}

static double now_sec(void){
  struct timespec ts; clock_gettime(CLOCK_MONOTONIC, &ts);
  return ts.tv_sec + ts.tv_nsec*1e-9;
}

int main(int argc, char** argv){
  size_t n = (argc>1)? strtoull(argv[1], NULL, 10) : (size_t)1e8;
  int    P = (argc>2)? atoi(argv[2]) : 0; // 0이면 HW 코어 수 추정(간단히 8 가정)
  if (P<=0) P = 8;

  double *a = aligned_alloc(CL, n*sizeof(double));
  for (size_t i=0;i<n;i++) a[i] = 1.0; // 테스트 입력

  // per-thread 결과 저장(패딩 포함)
  typedef struct { double value; char pad[CL - sizeof(double)]; } padded_t;
  padded_t *partial = aligned_alloc(CL, P*sizeof(padded_t));

  pthread_t *th = malloc(P*sizeof(pthread_t));
  task_t    *tk = malloc(P*sizeof(task_t));

  size_t chunk = (n + P - 1) / P;
  double t0 = now_sec();

  for (int p=0;p<P;p++){
    tk[p].a=a; tk[p].n=n;
    tk[p].begin = p * chunk;
    tk[p].end   = (tk[p].begin + chunk > n)? n : tk[p].begin + chunk;
    tk[p].out_slot = &partial[p];
    pthread_create(&th[p], NULL, worker, &tk[p]);
  }
  double sum = 0.0;
  for (int p=0;p<P;p++){
    pthread_join(th[p], NULL);
    sum += partial[p].value;
  }
  double t1 = now_sec();

  printf("sum=%.0f  time=%.3f s  (P=%d, n=%zu)\n", sum, t1-t0, P, n);

  free(a); free(partial); free(th); free(tk);
  return 0;
}
```

**튜닝 포인트**
- 입력 초기화(“first-touch”)를 **스레드별로** 수행하면 NUMA 로컬리티가 향상
- 큰 n에서 메모리 대역폭이 병목이면, **코어 증가 대비 이득이 둔화**(Roofline 생각)

---

## 5) 실전 ②: 병렬 히스토그램 — 로컬 버퍼 후 병합

- 공유 히스토그램 배열을 여러 스레드가 직접 갱신하면 **원자/락 경합**이 발생  
→ 스레드마다 **로컬 히스토그램**을 만든 뒤 마지막에 **병합**.

```c
// parallel_hist.c (요지)
#include <pthread.h>
#include <stdlib.h>
#include <string.h>
#define BINS 256
typedef struct { unsigned *local; const unsigned char *data; size_t begin,end; } job_t;

void* worker(void* a){
  job_t* j=(job_t*)a;
  unsigned *h=j->local; memset(h,0,BINS*sizeof(unsigned));
  for(size_t i=j->begin;i<j->end;i++) h[j->data[i]]++;
  return NULL;
}

int main(){
  size_t n=100000000; int P=8;
  unsigned char* data=malloc(n);
  for(size_t i=0;i<n;i++) data[i]=(unsigned char)(i%BINS);

  pthread_t th[P]; job_t jb[P]; unsigned* locals[P];
  for(int p=0;p<P;p++){ locals[p]=calloc(BINS,sizeof(unsigned)); }
  size_t chunk=(n+P-1)/P;
  for(int p=0;p<P;p++){
    jb[p]=(job_t){.local=locals[p],.data=data,.begin=p*chunk,.end=((p+1)*chunk>n)?n:(p+1)*chunk};
    pthread_create(&th[p],NULL,worker,&jb[p]);
  }
  for(int p=0;p<P;p++) pthread_join(th[p],NULL);

  unsigned global[BINS]={0};
  for(int p=0;p<P;p++) for(int b=0;b<BINS;b++) global[b]+=locals[p][b];
  // global 사용…
}
```

**핵심**: “쓰기 공유”를 피하고 **마지막에만 합치기(reduction)**.

---

## 6) 실전 ③: 병렬 행렬 곱 — 블로킹 + 타일 분배

- 단순 i-j-k 3중 루프는 **캐시 미스**와 **메모리 대역폭**으로 금방 병목
- **블로킹(타일링)**으로 L1/L2에 맞춰 재배열 + **블록 단위 병렬 분배**

```c
// parallel_matmul_blocked.c (핵심만)
#include <pthread.h>
#include <stdlib.h>
#include <string.h>
#define BS 64  // 타일 크기(머신에 맞게 조정)

typedef struct { int n, P, id; double *A,*B,*C; } job_t;

static void block_mm(double* C, double* A, double* B, int n, int i0,int j0,int k0){
  int i,j,k, i1=i0+BS, j1=j0+BS, k1=k0+BS;
  for (i=i0;i<i1;i++)
    for (k=k0;k<k1;k++){
      double aik = A[i*n+k];
      for (j=j0;j<j1;j++) C[i*n+j] += aik * B[k*n+j];
    }
}

void* worker(void* arg){
  job_t* t=(job_t*)arg; int n=t->n, P=t->P, id=t->id;
  for (int bi=id; bi < (n/BS)*(n/BS); bi += P) {
    int i0 = (bi / (n/BS)) * BS;
    int j0 = (bi % (n/BS)) * BS;
    for (int k0=0; k0<n; k0+=BS)
      block_mm(t->C, t->A, t->B, n, i0, j0, k0);
  }
  return NULL;
}
```

**아이디어**
- C의 타일( i0, j0 )을 **round-robin**으로 나누고, `k0` over tiles로 누적
- 더 고급: **동적 작업 큐**(남은 타일을 워커가 가져가는 방식)로 **로드 불균형 완화**

---

## 7) 분할-정복: 병렬 머지소트(스레드 스폰 임계값)

- **스레드 과다 생성**을 막기 위해, 입력 크기가 **임계값 이하이면 직렬**로 처리
- 부모는 두 하위 정렬을 *스레드 2개*로 보내고 **join** 후 병합

```c
// parallel_mergesort.c (스케치)
#include <pthread.h>
#include <stdlib.h>
#define THRESH 200000

typedef struct { int* A; int* T; int l, r; int depth; } job_t;

void merge(int* A,int* T,int l,int m,int r){
  int i=l,j=m,k=l;
  while(i<m && j<r) T[k++]= (A[i]<=A[j])? A[i++]:A[j++];
  while(i<m) T[k++]=A[i++]; while(j<r) T[k++]=A[j++];
  for(i=l;i<r;i++) A[i]=T[i];
}

void* sort_worker(void* arg){
  job_t* J=(job_t*)arg;
  int n=J->r - J->l;
  if (n <= THRESH || J->depth<=0){ /* 직렬 정렬(qsort 등)… */ qsort(J->A+J->l, n, sizeof(int), (__compar_fn_t) (int(*)(const void*,const void*))strcmp); return NULL; }
  int m=J->l + n/2;
  pthread_t t1,t2; job_t L={J->A,J->T,J->l,m,J->depth-1}, R={J->A,J->T,m,J->r,J->depth-1};
  pthread_create(&t1,NULL,sort_worker,&L);
  pthread_create(&t2,NULL,sort_worker,&R);
  pthread_join(t1,NULL); pthread_join(t2,NULL);
  merge(J->A,J->T,J->l,m,J->r);
  return NULL;
}
```

> 실제 비교 함수/정렬 루틴은 알맞게 교체하세요. **스레드 생성 비용**을 고려해 `depth` 또는 글로벌 스레드풀을 쓰면 안정적입니다.

---

## 8) 스케줄링·부하분산

- **정적 분할**: 등분 청크 → 간단·낮은 오버헤드. 단, 데이터 분포가 불균일하면 **꼬리(latency)** 발생
- **동적 분할**: 작업 큐에서 **스레드가 가져가기**(MPMC 큐) → **로드 균형 우수**, 약간의 큐 오버헤드
- **워크 스틸링**: 각 스레드의 데크(deque) + 빌 때 남의 데크를 훔침 → 태스크 기반 런타임(예: Cilk, TBB, Go 런타임)의 정석

---

## 9) 코어 고정(affinity)·NUMA·메모리 정책

### 9.1 코어 고정
- OS가 스레드를 이 코어 저 코어로 옮기면 **캐시 지역성 손실**  
→ 긴 작업은 **코어 고정**으로 편익이 큼

```c
// 리눅스 전용 예: 스레드 id를 특정 코어에 고정
#define _GNU_SOURCE
#include <pthread.h>
#include <sched.h>
void pin_to_core(int core){
  cpu_set_t cs; CPU_ZERO(&cs); CPU_SET(core, &cs);
  pthread_setaffinity_np(pthread_self(), sizeof(cs), &cs);
}
```

### 9.2 NUMA와 first-touch
- NUMA 시스템에서는 **처음에 접근한 노드**에 메모리 페이지가 할당  
→ **각 스레드가 자신이 쓸 데이터 초기화**(first-touch) → 로컬 메모리로 매핑되어 대역폭 향상

---

## 10) 락·원자·배리어의 비용: 최소화가 답

- **락 경합**은 병렬성의 **공적**  
  - 가능하면 **lock-free**(원자 카운터/사본 교체), **per-thread 로컬** 후 **reduction**
- **배리어**는 파이프라이닝/타임스텝에서 필요하지만, **빈도/위치 최소화**
- **원자 연산**도 코어 간 캐시 전파 비용이 큼(특히 fetch-add의 쓰기 무효화) → **빈도↓**

---

## 11) 벤치마킹과 검증

- **컴파일러 플래그**: `-O3 -march=native -pthread` (가능하면 `-ffast-math` 효과 검토)
- **타이머**: `clock_gettime(CLOCK_MONOTONIC, ...)`  
- **워밍업**: 캐시/주파수 스케일링 안정화 후 측정
- **오버서브스크립션 방지**: 라이브러리(OpenBLAS/numexpr 등)가 **추가 스레드**를 만들지 않도록 환경변수/설정 확인
- **정확성**: 작은 입력에서 결과 검증 → 큰 입력으로 확장  
- **레이스 탐지**: ThreadSanitizer(`-fsanitize=thread`)로 데이터 레이스 유무를 먼저 확인

---

## 12) 체크리스트(현업용 요약)

- [ ] **데이터 병렬**로 재구성: 공유 쓰기 최소화  
- [ ] per-thread 로컬 상태 + **마지막에 reduction**  
- [ ] **거짓공유 방지**(패딩/정렬/배치)  
- [ ] **정적 vs 동적 분할**을 데이터 특성에 맞게 선택  
- [ ] **스레드 수 = 물리 코어 수**±α에서 탐색(하이퍼스레딩 효과는 워크로드 따라 상이)  
- [ ] 장시간 작업은 **코어 고정** + **first-touch**  
- [ ] **락/원자 사용 빈도 최소화**, 배리어는 꼭 필요한 곳만  
- [ ] **Amdahl/Gustafson** 관점에서 직렬 구간을 꾸준히 제거  
- [ ] 결과 정확성과 성능 모두 **자동화 테스트**/벤치 스크립트로 추적

---

## 13) 한 줄 결론

쓰레드로 병렬성을 얻는 핵심은 **공유 쓰기를 피하고**, **데이터를 현명하게 나누고 배분**하며,  
**캐시·NUMA·락 비용**을 염두에 둔 설계를 하는 것입니다.  
이 원칙 위에 per-thread 로컬 + reduction, 블로킹/타일링, 작업 큐/워크 스틸링을 더하면  
병렬화로 **선형에 가까운 스케일**을 실전에서도 끌어내실 수 있습니다.