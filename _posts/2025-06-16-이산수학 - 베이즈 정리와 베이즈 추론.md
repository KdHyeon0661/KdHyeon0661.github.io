---
layout: post
title: 이산수학 - 베이즈 정리와 베이즈 추론
date: 2025-06-16 21:20:23 +0900
category: 이산수학
---
# 베이즈 정리와 베이즈 추론

## 0. 큰 그림: Posterior ∝ Likelihood × Prior

베이즈 추론은 **사전확률(믿음)**을 **데이터의 증거(가능도)**로 갱신해 **사후확률**을 얻습니다.

$$
\text{Posterior} \;\propto\; \text{Likelihood} \times \text{Prior}
\qquad\Longleftrightarrow\qquad
p(\theta\mid D) \;=\; \frac{p(D\mid \theta)\,p(\theta)}{p(D)} .
$$

여기서 정규화 상수
$$
p(D)=\int p(D\mid\theta)p(\theta)\,d\theta
$$
는 **모형 전체가 데이터를 설명하는 총 확률**(증거, 마지널 가능도)입니다.

---

## 1. 베이즈 정리 (Bayes’ Theorem)

### 1.1 이산 사건 버전
$$
P(H\mid E) \;=\; \frac{P(E\mid H)\,P(H)}{P(E)},
\quad
P(E)=\sum_h P(E\mid h)P(h).
$$

### 1.2 연속 매개변수 버전
$$
p(\theta\mid D) \;=\; \frac{p(D\mid \theta)\,p(\theta)}{\int p(D\mid \vartheta)\,p(\vartheta)\,d\vartheta}.
$$

### 1.3 **오즈(odds) 형태 & 베이즈 팩터**
두 가설 \(H_1,H_0\):
$$
\underbrace{\frac{P(H_1\mid E)}{P(H_0\mid E)}}_{\text{Posterior Odds}}
= 
\underbrace{\frac{P(E\mid H_1)}{P(E\mid H_0)}}_{\text{Bayes Factor } B_{10}}
\times
\underbrace{\frac{P(H_1)}{P(H_0)}}_{\text{Prior Odds}}.
$$
→ **데이터가 가설 사이의 오즈를 얼마나 “업데이트”하는가**를 분해해 보여줍니다.

---

## 2. 직관: “기저율(Base Rate) × 검사 성능 = 최종 신뢰도”

- **사전** \(P(H)\): 사전에 그럴듯함(유병률, 사전 지식).  
- **가능도** \(P(E\mid H)\): 가설이 맞을 때 데이터가 나올 가능성.  
- **사후** \(P(H\mid E)\): 관찰 후 갱신된 믿음.

> 경고: **기저율 무시의 오류**(Base-rate fallacy). 희귀한 사건일수록 양성이라도 PPV(양성예측도)가 낮을 수 있습니다.

---

## 3. 예제 1 — 의학 검사 (PPV/NPV)

**유병률** 1% (\(P(H)=0.01\)), **민감도** \(P(+\mid H)=0.99\), **위양성률** \(P(+\mid \overline{H})=0.05\).

분모:
$$
P(+)=0.99\times 0.01 + 0.05\times 0.99 = 0.0594.
$$

사후(양성일 때 실제 환자일 확률, PPV):
$$
P(H\mid +)=\frac{0.99\times 0.01}{0.0594}\approx 0.1667.
$$

> **해석**: 양성을 받아도 실제 환자일 확률은 **약 16.7%**.  
> 원인: 희귀(1%) + 위양성률 5% ⇒ **양성의 상당수가 비환자**.

### 3.1 혼합적으로 보기(10,000명 스케일)
- 환자 100명: 양성 \( \approx 99\).  
- 비환자 9,900명: 위양성 \( \approx 495\).  
- 양성 총 594명 중 진짜 환자 99명 ⇒ 99/594 ≈ 16.7%.

### 3.2 두 번째 독립 재검(같은 성능 가정)
두 번 연속 양성일 확률의 사후:
첫 양성 후 \(P(H\mid +)\approx 0.1667\). 두 번째 테스트 결과 \(+\)일 때:
$$
P(H\mid ++)=\frac{0.99\times 0.1667}{0.99\times 0.1667 + 0.05\times 0.8333}\approx 0.799.
$$
→ **재검을 통한 PPV의 비약적 상승**.

---

## 4. 베이즈 추론: 모수의 사후분포

### 4.1 결합, 주변화, 사후
$$
p(\theta, D)=p(D\mid \theta)p(\theta),\quad
p(\theta\mid D)=\frac{p(\theta, D)}{p(D)}.
$$

### 4.2 **결합적 사고**: 모형 선택(사후모형확률), 모수 추정(사후), 예측(사후예측)까지 일관된 확률 프레임으로 처리.

---

## 5. 공액 사전(Conjugate Priors) — 계산이 쉬운 조합들

### 5.1 베르누이/이항–베타 (성공확률 \(\theta\))
사전 \(\theta\sim\mathrm{Beta}(\alpha,\beta)\), 관측 \(x\)번 성공, \(n\!-\!x\) 실패:
$$
\theta\mid x \;\sim\; \mathrm{Beta}(\alpha+x,\;\beta+n-x).
$$
사후 평균:
$$
E[\theta\mid x]=\frac{\alpha+x}{\alpha+\beta+n}.
$$

**사후예측**(다음 1회 성공 확률):
$$
P(X_{\text{new}}=1\mid x) = E[\theta\mid x] = \frac{\alpha+x}{\alpha+\beta+n}.
$$

**베타–이항 사후예측 분포**(미래 \(m\)회 성공 수 \(Y\)):
$$
P(Y=y\mid x)=\binom{m}{y}\frac{B(\alpha+x+y,\;\beta+n-x+m-y)}{B(\alpha+x,\;\beta+n-x)}.
$$

### 5.2 포아송–감마 (평균률 \(\lambda\))
사전 \(\lambda\sim \mathrm{Gamma}(a,b)\) (밀도 \(\propto \lambda^{a-1}e^{-b\lambda}\)), 합계 관측 \(k=\sum y_i\), 기간 총합 \(T\):
$$
\lambda\mid \mathbf{y} \sim \mathrm{Gamma}(a+k,\; b+T).
$$
사후예측(미래 기간 \(\Delta\)의 카운트) → 음이항(Negative Binomial).

### 5.3 정규–정규(분산 알려짐 \(\sigma^2\))
사전 \(\theta\sim \mathcal{N}(\mu_0,\tau_0^2)\), 데이터 평균 \(\bar{y}\), 표본크기 \(n\):
$$
\theta\mid \mathbf{y} \sim \mathcal{N}\!\Big(\underbrace{\frac{\frac{\mu_0}{\tau_0^2}+\frac{n\bar{y}}{\sigma^2}}{\frac{1}{\tau_0^2}+\frac{n}{\sigma^2}}}_{\text{가중 평균}},\;
\underbrace{\Big(\tfrac{1}{\tau_0^2}+\tfrac{n}{\sigma^2}\Big)^{-1}}_{\text{가중 조화합}}\Big).
$$

### 5.4 정규(평균·분산 미지) — Normal–Inverse-Gamma
사후는 **Normal–Inverse-Gamma**가 되어 평균·분산을 동시에 갱신. (자세한 식은 참고 공식을 사용)

---

## 6. 사후예측(Posterior Predictive)

데이터 \(D\)를 본 뒤 미래 관측 \(x_{\text{new}}\)의 분포:
$$
p(x_{\text{new}}\mid D)=\int p(x_{\text{new}}\mid \theta)\,p(\theta\mid D)\,d\theta.
$$
> **모형 적합도 검증**·의사결정(기대 손실)·시뮬레이션(샘플링)·A/B 승산 평가 등에 핵심.

---

## 7. 의사결정: 손실, 임계값, 베이즈 리스크

- **0–1 손실**에서 “양성 선언” 임계값 \(t\)은 비용 비율로 조정:
$$
\text{Declare } H \text{ if } P(H\mid E) > t,
\qquad 
t=\frac{L_{\text{FP}}}{L_{\text{FP}}+L_{\text{FN}}}.
$$
FP(위양성)·FN(위음성) 비용을 반영해 **현실적인 Cutoff**를 설정.

---

## 8. 분류: 나이브 베이즈(문서/스팸)

가정: 특징 독립성(조건부). 클래스 \(C\), 단어 벡터 \(\mathbf{x}\):
$$
P(C\mid \mathbf{x}) \propto P(C)\prod_j P(x_j\mid C).
$$
문서 분류에선 라플라스/디리클레 스무딩으로 \(P(w\mid C)\)의 0확률 방지:
$$
\hat{P}(w\mid C)=\frac{\text{count}(w,C)+\alpha}{\sum_{w'} \text{count}(w',C)+\alpha V}.
$$

---

## 9. 실전: A/B 테스트(베타–이항) — 확률로 말하기

A: \(x_A/n_A\), B: \(x_B/n_B\), 사전 \(\mathrm{Beta}(1,1)\) (비정보적):
$$
\theta_A\mid D \sim \mathrm{Beta}(1+x_A, 1+n_A-x_A),\\
\theta_B\mid D \sim \mathrm{Beta}(1+x_B, 1+n_B-x_B).
$$
관심량: \(P(\theta_B>\theta_A\mid D)\), 기대 리프트 \(E[\theta_B-\theta_A\mid D]\), 95% 신뢰구간(정확히는 **신뢰구간이 아닌 신뢰 가능한 구간=credible interval**).

**결정**: 비용·가치 \(V\)를 고려, 기대 이득 \(V\times E[\theta_B-\theta_A]_+\) vs 전환 손실 비교.

---

## 10. 계층 베이즈 & 경험 베이즈

### 10.1 계층(Beta–Binomial) 예
다수 캠페인 \(i=1..m\), 각 전환률 \(\theta_i\sim \mathrm{Beta}(\alpha,\beta)\), 관측 \(x_i\sim \mathrm{Bin}(n_i,\theta_i)\).  
사후는 각 그룹의 관측에 따라 **수축(shrinkage)**: 데이터 적을수록 전체 평균으로 수축, 과대분산 완화.

### 10.2 경험 베이즈
상위 하이퍼파라미터 \((\alpha,\beta)\)를 데이터로 추정(최우추정 등) 후 하위 사후를 계산 — 실무에서 간단·강력.

---

## 11. 계산: 공액 외의 경우

- **MCMC**: Metropolis–Hastings, Gibbs, HMC/NUTS(연속모수 고차원에 강함).  
- **변분추론(VI)**: 최적화로 근사(빠름), ELBO 최대화.  
- **라플라스 근사**: 사후 모드 근처 정규 근사.  
- **BIC/WAIC/LOO**: 모델 비교/일반화 성능 근사.

> 체크: 수렴(R-hat), 유효샘플크기(ESS), 발산(diagnostics), 사전/사후/사후예측 체크(PPC).

---

## 12. 파이썬 실습 (NumPy/SciPy)

> **참고**: 실행 환경에 SciPy가 없다면 의사코드로 읽으시면 됩니다. 수식 자체는 동일합니다.

### 12.1 의학 검사 PPV/NPV 함수

```python
def ppv_npv(prior, sens, fpr):
    """
    prior = P(H), sens = P(+|H), fpr = P(+|~H)
    returns: PPV = P(H|+), NPV = P(~H|-)
    """
    Ppos = sens*prior + fpr*(1-prior)
    Pneg = (1-sens)*prior + (1-fpr)*(1-prior)
    ppv = sens*prior / Ppos
    npv = (1-fpr)*(1-prior) / Pneg
    return ppv, npv

print(ppv_npv(0.01, 0.99, 0.05))  # (≈0.1667, ≈0.9999)
```

### 12.2 베타–이항: 사후 & 신뢰가능구간 & \(P(\theta_B>\theta_A)\)

```python
import numpy as np
from scipy.stats import beta

def beta_posterior(alpha, beta_, x, n):
    return alpha + x, beta_ + (n - x)

def beta_ci(alpha_post, beta_post, level=0.95):
    lo = (1-level)/2
    hi = 1 - lo
    return beta.ppf(lo, alpha_post, beta_post), beta.ppf(hi, alpha_post, beta_post)

def prob_B_exceeds_A(aA,bA,aB,bB, draws=200000):
    A = beta.rvs(aA,bA,size=draws)
    B = beta.rvs(aB,bB,size=draws)
    return np.mean(B > A), np.mean(B - A), np.quantile(B-A, [0.025,0.975])

# Example: A: 480/10000, B: 520/10000
aA,bA = beta_posterior(1,1, 480,10000)
aB,bB = beta_posterior(1,1, 520,10000)
print("A CI:", beta_ci(aA,bA))
print("B CI:", beta_ci(aB,bB))
print("P(B>A), E[B-A], 95% CI:", prob_B_exceeds_A(aA,bA,aB,bB))
```

### 12.3 나이브 베이즈(멀티노미얼, 간단 구현)

```python
import numpy as np

class NaiveBayesMultinomial:
    def __init__(self, alpha=1.0):
        self.alpha = alpha
    def fit(self, X_counts, y, V):
        # X_counts: (N, V) 문서-단어 카운트, y: {0,1,...,K-1}
        self.K = np.max(y)+1
        self.V = V
        self.class_prior = np.bincount(y)/len(y)
        self.word_count = np.zeros((self.K, V))
        for xi, ci in zip(X_counts, y):
            self.word_count[ci] += xi
        self.word_prob = (self.word_count + self.alpha) / \
                         (self.word_count.sum(axis=1, keepdims=True) + self.alpha*V)
    def predict_logproba(self, X_counts):
        # log P(C) + sum_x count * log P(w|C)
        logp = np.log(self.class_prior + 1e-12)[None, :]
        loglik = X_counts @ np.log(self.word_prob.T + 1e-12)  # (N,K)
        return logp + loglik
    def predict(self, X_counts):
        return np.argmax(self.predict_logproba(X_counts), axis=1)

# 사용 예시: (X_counts, y)로 학습, 예측
```

### 12.4 코인 치우침 \(\theta\)에 대한 **간단 Metropolis** (공액 없이도 가능 예시)

```python
import numpy as np

def logpost(theta, a, b, x, n):
    if theta <= 0 or theta >= 1:
        return -np.inf
    # Beta(a,b) prior + Binomial likelihood
    return (a-1)*np.log(theta) + (b-1)*np.log(1-theta) + x*np.log(theta) + (n-x)*np.log(1-theta)

def metropolis(a=1,b=1, x=7,n=10, steps=20000, prop_sd=0.1):
    theta = 0.5
    samples=[]
    lp = logpost(theta,a,b,x,n)
    for _ in range(steps):
        cand = np.clip(theta + np.random.normal(0, prop_sd), 1e-6, 1-1e-6)
        lp_c = logpost(cand,a,b,x,n)
        if np.log(np.random.rand()) < lp_c - lp:
            theta, lp = cand, lp_c
        samples.append(theta)
    return np.array(samples[int(steps/2):])  # burn-in drop

samps = metropolis()
print(samps.mean(), np.quantile(samps,[0.025,0.975]))
```

---

## 13. 베이즈 vs 빈도주의 — 실전에서의 차이

| 관점 | 베이즈 | 빈도주의 |
|---|---|---|
| 확률 의미 | 믿음의 정도 | 장기 빈도 |
| 사전 지식 | **활용** | 사용 안 함 |
| 출력 | **사후 분포** (신뢰 가능한 구간) | 점추정 + 신뢰구간(반복샘플 빈도 해석) |
| 업데이트 | **순차적** 가능 | 새 데이터로 재추정 |
| 가설 비교 | Bayes Factor/사후모형확률 | p-value, AIC/BIC(근사) |

---

## 14. 신뢰 가능한 구간(credible interval) vs 신뢰구간(confidence interval)

- **Credible interval**:  
  $$P(\theta \in [L,U]\mid D)=0.95$$  
  데이터 관찰 후 \(\theta\)가 구간에 있을 **확률**.

- **Confidence interval**:  
  많은 반복 표본 중 **95%의 구간이** 진짜 \(\theta\)를 포함.

> 실무 커뮤니케이션에선 해석 차이를 명확히.

---

## 15. 진단·체크리스트

1) **사전 민감도 분석**: 다른 합리적 사전들로 결과 변하는지.  
2) **사전예측(Posterior가 아닌 Prior Predictive) 체크**: 관측 전 분포가 말이 되는가?  
3) **사후예측(PPC)**: 모형이 데이터를 재현하는가?  
4) **분해법칙 활용**: \(E[\theta\mid D]\), \(p(x_{\text{new}}\mid D)\)로 **직접적 질문**에 답하라.  
5) **비용 기반 의사결정**: FP/FN 비용, 가치 반영해 임계값을 정하라.  
6) **계산 진단**: MCMC면 R-hat≈1, ESS 충분, 발산 없음. VI면 ELBO 수렴과 근사 확인.  
7) **베이즈 팩터 남용 주의**: 사전 의존성 큼, 모델공간 민감. WAIC/LOO와 함께 보라.  
8) **독립성 가정**: 나이브 베이즈의 조건부 독립은 근사 — 성능과 해석을 분리.

---

## 16. 추가 예제: 사전→사후 업데이트 직관

### 코인 편향 \(\theta\), 사전 \(\mathrm{Beta}(2,2)\) (약간 “공정” 선호), 관측 10회 7성공
사후 \(\mathrm{Beta}(9,5)\).  
사후 평균:
$$
E[\theta\mid D]=\frac{9}{14}\approx 0.643.
$$
사전 대비 데이터가 충분하면 사후가 데이터 중심으로 이동(“사전 + 데이터 = 가중 평균”).

---

## 17. 종합 요약

- **베이즈 정리**: 사전 × 가능도 → 사후.  
- **오즈/베이즈 팩터**: 데이터가 가설 오즈를 얼마나 바꾸는가.  
- **공액 사전**: 베타–이항, 감마–포아송, 정규–정규(알려진 분산)로 **폐형식** 갱신.  
- **사후예측**와 **전분산 법칙**: 예측과 변동의 분해를 일관되게.  
- **의사결정**: 손실/가치 기반 임계값 설정.  
- **실전**: 진단검사, 스팸, A/B, 하이퍼파라미터, 계층모형.  
- **계산**: 공액 밖은 MCMC/VI/라플라스, 진단 필수.  
- **커뮤니케이션**: credible interval, PPV/NPV 등 “해석 가능한 확률”로 설명.

---

## 부록) 시각화 코드(사전 vs 사후, 베타–이항)

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta

def plot_prior_posterior(alpha,beta_, x,n):
    xs = np.linspace(0,1,400)
    a_post, b_post = alpha+x, beta_+(n-x)
    plt.plot(xs, beta.pdf(xs, alpha,beta_), label=f"Prior Beta({alpha},{beta_})")
    plt.plot(xs, beta.pdf(xs, a_post,b_post), label=f"Posterior Beta({a_post},{b_post})")
    plt.xlabel("θ")
    plt.ylabel("density")
    plt.title(f"{n} trials, {x} successes")
    plt.legend(); plt.grid(True); plt.show()

plot_prior_posterior(2,2, 7,10)
```

> 그래프에서 **사후가 사전보다 우측(큰 θ)으로 이동**하는 모습을 확인할 수 있습니다.