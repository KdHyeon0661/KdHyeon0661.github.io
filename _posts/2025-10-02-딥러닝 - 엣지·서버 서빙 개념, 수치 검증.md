---
layout: post
title: 딥러닝 - 엣지·서버 서빙 개념, 수치 검증
date: 2025-10-02 16:25:23 +0900
category: 딥러닝
---
# 엣지·서버 서빙 개념(배치 vs 실시간), 수치 검증(정확도 회귀 체크)
**Edge vs Server 아키텍처 · 배치/실시간 서빙 전략 · 정확도 회귀(Regression) 방지용 수치 검증 파이프라인**

## 0. 큰 그림: 어디서, 어떻게, 무엇을

### 0.1 엣지(Edge) vs 서버(Server)
| 구분 | 엣지(모바일·임베디드·브라우저·온프레) | 서버(클라우드·IDC) |
|---|---|---|
| 장점 | 지연↓(로컬), 네트워크 독립, 개인정보 로컬 처리, 대역폭 절약 | 탄력 확장/AutoScale, 강력한 HW(GPU/TPU), 중앙집중 관리/모니터링 |
| 단점 | HW 제약(메모리·전력·열), 업데이트/배포가 번거로움, 모델 크기 제한 | 네트워크 지연·불안정, 비용, 데이터 주권/개인정보 이슈 |
| 주된 최적화 | 경량화(프루닝·증류·양자화), 런타임(TFLite, ORT Mobile, TensorRT on Jetson), 오프라인 캐시 | 동적 배칭, 모델 캐시, 멀티모델 라우팅, GPU 멀티테넌시, A/B/캐나리 |

**선택 기준**  
- **지연(SLA)**: 수십 ms 요구 → 엣지/서버 중 **가까운 곳** + **동적 배칭 최소화**  
- **프라이버시/규제**: 데이터 외부 반출 불가 → 엣지/온프레  
- **모델 업데이트 빈도/규모**: 자주 바뀜·크다 → 서버 우선  
- **오프라인 필요**: 엣지(로컬 추론/캐시)  
- **원가**: 대규모 트래픽·고성능 → 서버 GPU 효율화(배치·텐서RT)로 단가↓, 혹은 엣지 분산

---

## 1. 배치(Batch) vs 실시간(Online/Realtime)

### 1.1 배치 추론
- **정의**: 대량 데이터를 시간 단위(매시/매일)로 모아 일괄 추론.  
- **사용**: 추천 후보 사전 생성, 임베딩 인덱스 업데이트, 대용량 로그 스코어링.  
- **장점**: 비용 효율적(스팟/저우선순위 인스턴스), 파이프라인 단순, 재현·리트라이 용이.  
- **단점**: 최신성↓(지연 크다), 개인화/상황 반영 약함.

**엔드 투 엔드 수식(스케줄 T 간격, 처리량 R, 데이터량 D)**  
$$\text{완료시간} \approx \frac{D}{R} + \text{큐 대기} + \text{스케줄 간격 }T$$

### 1.2 실시간 추론
- **정의**: 요청 당 즉시 추론. (HTTP/gRPC, Kafka 스트림 등)  
- **장점**: 지연↓, 컨텍스트 최신 반영.  
- **단점**: 인프라 복잡, 스케일링/장애 대응 필요, 단가↑(피크 대비).

**지연 예산**  
$$
\text{E2E Latency} = t_{\text{pre}} + t_{\text{model}} + t_{\text{post}} + t_{\text{network}} + t_{\text{queue}}
$$
- **SLO**는 P95/P99 기준으로 관리: $$P99 < \text{SLA}$$

### 1.3 마이크로 배칭(Micro-batching)
- 실시간에서도 **몇 ms** 대기해 요청을 모아 **배치로** 처리(GPU 활용↑).  
- 지연 예산이 **10~50ms** 허용이면 **동적 배칭**으로 TPS↑.  
- Triton Inference Server “dynamic batching”, 자체 큐에선 **N개 또는 Δt 경과** 조건.

---

## 2. 엣지 서빙: 설계 포인트

### 2.1 런타임/포맷 선택
- **모바일**: PyTorch Lite/TorchScript, TFLite, ONNX Runtime Mobile.  
- **임베디드(NVIDIA Jetson)**: TensorRT(엔진 .plan), DeepStream, ORT + TensorRT EP.  
- **브라우저**: WebGPU/WebGL, ONNX Runtime Web.

### 2.2 리소스 제약 고려
- **메모리**: 모델(수십~수백 MB) + 중간 텐서 버퍼. → **INT8/QAT**, **채널 축소**.  
- **전력/열**: 풀로드 지속 시 **쓰로틀링**. → **FPS 캡**, **스케줄링**, **저전력 코어**.  
- **오프라인**: 캐시/로컬 로그, 연결 복구 시 업로드.

### 2.3 OTA 업데이트 전략
- **버전 라벨**(semver) + **호환성 검사**(전처리 파이프 스키마 체크).  
- **점진 배포**: 1% → 10% → 100%, **롤백 스위치**.  
- **사전 수치 검증**: 기기 내부 골든셋으로 **로컬 A/B**.

---

## 3. 서버 서빙: 설계 포인트

### 3.1 요청 경로
1) **L7 로드밸런서**  
2) **API 게이트웨이**(인증·라벨 분기·A/B 라우팅)  
3) **서빙 노드**(FastAPI/gRPC/Triton/TF Serving/ORT)  
4) **특성 저장소(Feature Store)** / **캐시** / **모델 레포**  
5) **모니터링/로깅**(Prometheus/OpenTelemetry)

### 3.2 성능 전략
- **모델 Warmup**(입력 스펙별), **고정 스레드/핀드 메모리**, **CUDA Graphs**(고급).  
- **동적 배칭**: **대기 한도** + **최대 배치 크기**.  
- **멀티모델**: 엔진 캐시/LSRU(Latency-sensitive least recently used) 정책.  
- **오토스케일**: CPU/GPU 사용률·큐 길이·지연으로 스케일 아웃.

---

## 4. 배치 파이프라인 예시(파이썬/파이토치 + DuckDB/S3)

아래는 “매일 이미지 100만 장을 분류하고 결과/메트릭을 저장”하는 최소 예시입니다.

```python
import os, time, glob, json, duckdb
import torch, torch.nn.functional as F
from PIL import Image
from torchvision import transforms as T

# 1) 입력/출력 구성
DATA_DIR = "/mnt/ds/images/2025-10-27/"
OUT_PARQUET = "/mnt/out/preds_2025-10-27.parquet"
BATCH = 256
device = "cuda" if torch.cuda.is_available() else "cpu"

# 2) 모델 로드(ONNXRuntime로도 가능)
model = torch.jit.load("/models/m_script.pt", map_location=device).eval()

tfm = T.Compose([
    T.Resize(256), T.CenterCrop(224), T.ToTensor(),
    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
])

def load_batch(paths):
    xs=[]; ids=[]
    for p in paths:
        try:
            im = tfm(Image.open(p).convert("RGB"))
            xs.append(im); ids.append(os.path.basename(p))
        except:
            pass
    if not xs: return None, None
    x = torch.stack(xs,0).to(device, non_blocking=True)
    return x, ids

# 3) 추론 루프
files = glob.glob(os.path.join(DATA_DIR, "*.jpg"))
rows=[]
for i in range(0, len(files), BATCH):
    xb, ids = load_batch(files[i:i+BATCH])
    if xb is None: continue
    with torch.inference_mode():
        logits = model(xb)
        probs = F.softmax(logits, -1)
        score, pred = probs.max(-1)
    for k, id_ in enumerate(ids):
        rows.append((id_, int(pred[k]), float(score[k])))

# 4) 결과 저장(duckdb → parquet)
con = duckdb.connect()
con.execute("CREATE TABLE tmp(id VARCHAR, pred INTEGER, score DOUBLE)")
con.executemany("INSERT INTO tmp VALUES (?, ?, ?)", rows)
con.execute(f"COPY tmp TO '{OUT_PARQUET}' (FORMAT 'parquet')")
con.close()
```

- **스케줄링**: Airflow/Cron “매일 02:00” 실행 → 실패 시 리트라이/부분 재처리.  
- **검증**: 지난 7일 결과와 요약 지표 비교(정확도 변동, 클래스 분포, 스코어 평균).

---

## 5. 실시간 서빙 예시(FastAPI + 동적 마이크로 배칭)

### 5.1 서버(단일 모델, GPU 1장)
```python
# server.py
import asyncio, time, torch, torch.nn.functional as F
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List

app = FastAPI()
device = "cuda"
model = torch.jit.load("/models/m_script.pt", map_location=device).eval()
WARMUP_BATCHES = [(1,3,224,224), (8,3,224,224)]
for shape in WARMUP_BATCHES:
    with torch.inference_mode():
        _ = model(torch.randn(*shape, device=device))

# --- 마이크로 배칭 큐 ---
class Req(BaseModel):
    id: str
    x: List[float]  # 예시: 이미 전처리된 3*224*224 플랫텐 (실전은 base64 이미지 + 서버 전처리)

BATCH_MAX = 16
BATCH_WAIT_MS = 8

queue = asyncio.Queue()
results = {}

async def batch_worker():
    while True:
        start = time.time()
        items = [await queue.get()]
        while (len(items) < BATCH_MAX) and ((time.time()-start)*1000 < BATCH_WAIT_MS):
            try:
                items.append(await asyncio.wait_for(queue.get(), timeout=BATCH_WAIT_MS/1000))
            except asyncio.TimeoutError:
                break
        ids, xs = zip(*items)
        xb = torch.tensor(xs, device=device).view(len(xs),3,224,224)
        with torch.inference_mode():
            logits = model(xb)
            probs = F.softmax(logits, -1)
            score, pred = probs.max(-1)
        for i,id_ in enumerate(ids):
            results[id_] = {"pred": int(pred[i]), "score": float(score[i])}

@app.on_event("startup")
async def startup_event():
    asyncio.create_task(batch_worker())

@app.post("/infer")
async def infer(r: Req):
    # 요청 수신 → 큐 삽입 → 결과 대기
    fut = asyncio.get_event_loop().create_future()
    results[r.id] = fut  # 자리표시
    await queue.put((r.id, r.x))
    # poll: 워커가 dict에 최종 결과를 dict로 교체
    while True:
        v = results.get(r.id)
        if isinstance(v, dict): break
        await asyncio.sleep(0.001)
    return v
```

- **BATCH_WAIT_MS**=8ms → **지연+처리량** 절충. SLA 100ms면 충분.  
- 실전은 **전처리 서버 측**(base64 → 텐서), **CPU 병렬**, **에러 처리**, **헬스체크** 추가.

### 5.2 gRPC(개념)
- **Proto**: `Predict(Request{bytes image, map<string,string> ctx}) returns (Response{int pred, float score})`  
- 장점: 이진 프로토콜, 낮은 오버헤드, 스트리밍/RPC.  
- Python/Go/Java/C++ 클라이언트 생성 쉬움.

---

## 6. 정확도 회귀(Regression) 수치 검증: 원칙 → 코드

### 6.1 왜 필요한가?
- 엔진 변경(ONNX→TensorRT), 정밀도(FP32→FP16/INT8), 전처리/후처리 변경, 모델 교체(Teacher→Student) 때 **몇 % 성능이 달라도 되는가?**  
- **“허용 오차를 수치화하고 자동으로 막는다.”**

### 6.2 골든셋(Golden set) & 기준선(Baseline)
- **골든셋**: 크기는 작아도 **대표성** 있는 검증 데이터(수천~수만).  
- **기준선**: 현재 운영 모델의 예측(라벨/확률/로그잇)과 **메트릭**(ACC/F1/mAP/IoU/BLEU/ECE 등).

> **버전 전환 가드레일** 예(분류):  
> - ACC 감소 ≤ 0.3pp, F1 감소 ≤ 0.5pp,  
> - ECE 증가는 ≤ 0.02,  
> - Top-1 동일률 ≥ 98% **혹은** 로그잇 MSE ≤ 1e-3,  
> - 특정 **아키 트래픽 슬라이스**에서 악화 금지.

### 6.3 수치 비교 기준(예)
- **스칼라 메트릭**: $$|\Delta \text{ACC}| \le 0.003,\ |\Delta \text{F1}| \le 0.005$$  
- **확률/로그잇**: $$\text{MSE}(\hat{p}_\text{new}, \hat{p}_\text{base}) \le \epsilon$$, Top-1 일치율 ≥ θ  
- **검정/CI**: 부트스트랩 CI 겹침, McNemar 검정 p-value>0.05 (선택)

### 6.4 회귀 체크 파이썬 스크립트(분류; 라벨·확률 비교)
```python
# regress_check.py
import json, numpy as np
from sklearn.metrics import accuracy_score, f1_score, log_loss

THRESH = {
    "acc_drop": 0.003,
    "f1_drop": 0.005,
    "prob_mse": 1e-3,
    "top1_equal": 0.98
}

def load_preds(path):
    # JSON lines: {"id": "...", "y": 3, "prob": [..C..]}
    ids=[]; y=[]; P=[]
    with open(path) as f:
        for line in f:
            o=json.loads(line); ids.append(o["id"]); y.append(o["y"]); P.append(o["prob"])
    return np.array(ids), np.array(y), np.array(P)

def metrics(y, P):
    pred = P.argmax(1)
    return {
        "acc": float(accuracy_score(y, pred)),
        "f1": float(f1_score(y, pred, average="macro")),
        "nll": float(log_loss(y, P, labels=np.arange(P.shape[1])))
    }

def compare(base, new, y, Pb, Pn):
    mb = metrics(y, Pb)
    mn = metrics(y, Pn)
    mse = float(((Pb - Pn)**2).mean())
    top1_equal = float((Pb.argmax(1) == Pn.argmax(1)).mean())
    delta = {k: mn[k]-mb[k] for k in mb}
    ok = True
    if delta["acc"] < -THRESH["acc_drop"]: ok=False
    if delta["f1"]  < -THRESH["f1_drop"]: ok=False
    if mse > THRESH["prob_mse"]: ok=False
    if top1_equal < THRESH["top1_equal"]: ok=False
    report = {"base":mb, "new":mn, "delta":delta, "mse_prob":mse, "top1_equal":top1_equal, "pass":ok}
    return ok, report

if __name__=="__main__":
    ids_b, y_b, P_b = load_preds("golden_base.jsonl")
    ids_n, y_n, P_n = load_preds("golden_new.jsonl")
    assert np.all(ids_b==ids_n) and np.all(y_b==y_n)
    ok, rep = compare("base","new", y_b, P_b, P_n)
    print(json.dumps(rep, indent=2))
    exit(0 if ok else 1)
```

- **입력**: `golden_base.jsonl`(운영 모델 결과를 캐시), `golden_new.jsonl`(새 엔진/모델 결과).  
- **출력**: 통과/실패와 상세 변화량. CI에서 **exit code**로 게이트.

### 6.5 IoU/mAP 계열(Detection/Seg) 회귀 체크 핵심
- **검출**: IoU@0.5, AP@[.50:.95], **TP/FP 곡선** 비교.  
- **세그멘테이션**: mIoU/mDice, **경계 IoU**(thin object 주의), 클래스별 변화.  
- **허용 규칙**: AP drop ≤ 0.5pp, **소객체 AP_S** drop 금지 등 **업무별 규칙**.

### 6.6 캘리브레이션(ECE) 회귀 체크
- **INT8/PTQ** 후 **확률 스케일**이 변형될 수 있음 → ECE↑.  
- **Temperature Scaling**으로 보정 후 ECE 비교(Section 1.12 참조).  
- 회귀 체크에 **ECE Δ ≤ 0.02** 같은 가드 추가 권장.

---

## 7. 엔진/정밀도 변경(ONNX↔TensorRT/FP32↔FP16↔INT8) 수치 검증

### 7.1 로그잇 레벨 비교(수치 안정)
- **최선**: 소프트맥스 전 **로짓** 직접 비교(Top-1 불변성↑).  
- 분류: $$\mathrm{cos\_sim}(z_\text{base}, z_\text{new}) \ge 0.999$$ or MSE ≤ 1e-3.  
- 확률/라벨은 비선형이라 **작은 로짓 차이→확률 큰 차이** 가능(특히 큰 로짓에서).

### 7.2 허용오차 설계
- FP16/BF16: 로짓 **상대 오차** 1e-3~1e-2 허용.  
- INT8 PTQ: 모델/데이터에 따라 **메트릭 기준**(ACC/F1/mAP)으로 게이트 권장.

### 7.3 샘플 코드(로짓/확률 동시 비교)
```python
import numpy as np

def diff_report(logits_base, logits_new):
    # logits: (N,C)
    z1, z2 = logits_base, logits_new
    mse = ((z1 - z2)**2).mean()
    cos = (z1*z2).sum(1) / (np.linalg.norm(z1,axis=1)*np.linalg.norm(z2,axis=1)+1e-9)
    cos_mean = float(cos.mean())
    # 확률
    def softmax(z): 
        z = z - z.max(1, keepdims=True)
        ez = np.exp(z); return ez/ez.sum(1, keepdims=True)
    p1, p2 = softmax(z1), softmax(z2)
    kl = (p1 * (np.log(p1+1e-12)-np.log(p2+1e-12))).sum(1).mean()
    top1_eq = float((p1.argmax(1)==p2.argmax(1)).mean())
    return {"mse_logit":float(mse), "cos":cos_mean, "kl":float(kl), "top1_equal":top1_eq}
```

---

## 8. CI/CD에 회귀 체크 넣기

### 8.1 GitHub Actions 예시(YAML 스케치)
```yaml
name: model-regression-check
on: [push, pull_request]
jobs:
  regress:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.11" }
      - run: pip install -r requirements.txt  # numpy sklearn
      - run: python export_new_preds.py --onnx smallcnn.onnx --golden ids.jsonl --out golden_new.jsonl
      - run: python regress_check.py
```
- 실패 시 PR 차단 → **수동 Override**(승인자 코멘트) 절차를 운영 정책으로.

### 8.2 배포 파이프라인 게이트
- **Staging**에서 Shadow Traffic(실요청 복제)로 **오프라인 비교** → PASS 시 캐나리 5% → 50% → 100%.  
- 모든 단계에 **회귀 스크립트/슬라이스 메트릭** 보고.

---

## 9. 운영 중 모니터링·알람

### 9.1 핵심 지표
- **지연**: P50/P95/P99, 타임아웃율  
- **성능**: 온라인 레이블이 있으면 A/B 대조(CTR/LTV/정확도), 없으면 **프록시 지표**(스코어 분포/ECE proxy)  
- **시스템**: GPU/CPU/메모리/큐 길이/드롭률  
- **드리프트**: 입력 피처 분포(PSI/KL), 예측 분포, 에러 코드

### 9.2 엣지 텔레메트리
- **경량 로그**: 요청 샘플링(예: 1%) → 요약 통계만 업로드.  
- **프라이버시**: PII 마스킹/집계, k-익명성 고려.

---

## 10. 실전 시나리오 종합

### 10.1 “서버 실시간 + 배치 보강” 하이브리드
- **배치**: 매일 임베딩/후보 생성(오프라인)  
- **실시간**: 요청 시 재순위/개인화  
- **검증**: 후보 품질 분포, 온라인 CTR A/B, 회귀 체크로 “후보 생성기/재순위기” 각각 게이트

### 10.2 “엣지 오프라인 + 서버 보정”
- **엣지**: 실시간 오프라인 추론(지연 20ms), **버전 v1.8**  
- **서버**: 주기적으로 파라미터/온도 스케일 업데이트 제공  
- **검증**: OTA 전 로컬 골든셋으로 미니 회귀 체크 → OTA % 롤아웃

---

## 11. 부록: 수식/지침 퀵 레퍼런스

- **지연 예산**  
  $$P99(t_{\text{pre}}+t_{\text{model}}+t_{\text{post}}+t_{\text{net}}+t_{\text{queue}}) \le \text{SLA}$$
- **배치 처리량**  
  $$R \approx \frac{\text{GPU Util}\times \text{ops/sec}}{\text{avg ops/request}}$$
- **회귀 가드 예(분류)**  
  $$\Delta \mathrm{ACC} \ge -0.003,\ \Delta \mathrm{F1}\ge -0.005,\ \mathrm{Top1Equal}\ge 0.98,\ \mathrm{MSE}_{\text{prob}}\le 10^{-3}$$
- **검출/세그**  
  $$\Delta \mathrm{AP}_{[.50:.95]} \ge -0.005,\ \Delta \mathrm{mIoU}\ge -0.005$$

---

## 12. 체크리스트

### 설계
- [ ] SLA(지연/가용성)와 SLO(P95/P99) 정의  
- [ ] 엣지/서버 선택 근거(규제·원가·연결성·업데이트 빈도)  
- [ ] 배치/실시간 분리와 상호작용(하이브리드)  
- [ ] 캐시/재시도/백프레셔/서킷브레이커

### 구현
- [ ] 모델 **eval** 고정, 전처리/후처리 **동일성 보장**  
- [ ] 동적 배칭 파라미터(최대 N, 대기 Δt)  
- [ ] Warmup(입력 스펙별), 고정 스레드, 핀드 메모리  
- [ ] 헬스체크/레디니스/리로딩(무중단)

### 수치 검증
- [ ] 골든셋/베이스라인 고정, 주기적 갱신 규칙  
- [ ] 회귀 스크립트(CI) + 슬라이스 메트릭  
- [ ] 엔진/정밀도 변경 시 로짓/메트릭 이중 비교  
- [ ] 실패 시 롤백/오버라이드 정책 문서화

---

### 마무리
- **서빙 전략**은 **엣지↔서버**, **배치↔실시간**의 축에서 SLA/원가/규제를 맞추는 **공학적 최적화 문제**입니다.  
- **정확도 회귀 체크**는 “바꿨는데 좋아졌는가?”를 **숫자로 증명**하는 안전장치입니다.