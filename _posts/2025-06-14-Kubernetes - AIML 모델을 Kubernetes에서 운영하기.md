---
layout: post
title: Kubernetes - AI/ML ëª¨ë¸ì„ Kubernetesì—ì„œ ìš´ì˜í•˜ê¸°
date: 2025-06-14 22:20:23 +0900
category: Kubernetes
---
# ì‹¤ìŠµ: AI/ML ëª¨ë¸ì„ Kubernetesì—ì„œ ìš´ì˜í•˜ê¸° â€” í™•ì¥Â·ë³´ì•ˆÂ·ê´€ì¸¡ê¹Œì§€

## âœ… ëª©í‘œì™€ ë²”ìœ„

- **ëª¨ë¸ ì„œë¹™**: Flask ê¸°ë°˜ REST API(ì¶”ê°€ë¡œ FastAPI ëŒ€ì•ˆ ì œê³µ)
- **K8s ë¦¬ì†ŒìŠ¤**: Deployment/Service/Ingress/TLS, Config/Secret/PVC
- **ìŠ¤ì¼€ì¼ë§**: HPA(CPU) + KEDA(ìš”ì²­ ìˆ˜/í ê¸¸ì´) ì˜ˆì œ
- **ê´€ì¸¡**: Prometheus ì§€í‘œ, OpenTelemetry íŠ¸ë ˆì´ì‹±(ì„ íƒ)
- **ë³´ì•ˆ/ê²©ë¦¬**: Pod Security, SecurityContext, NetworkPolicy
- **ì „ëµ**: Canary/Blue-Green(Argo Rollouts/Ingress), ë²„ì „ ë¼ìš°íŒ…
- **ì„±ëŠ¥**: Gunicorn+Uvicorn, í”„ë¦¬ì›Œë°, ë©€í‹° í”„ë¡œì„¸ìŠ¤
- **GPU**: nvidia.com/gpu, nodeSelector, tolerations

---

## ğŸ§± ì „ì²´ ì•„í‚¤í…ì²˜

```plaintext
ì‚¬ìš©ì/í´ë¼ì´ì–¸íŠ¸
      â”‚
      â–¼
   Ingress (+TLS/Canary) â”€â”€> (ì˜µì…˜) WAF/RateLimit
      â”‚
      â–¼
 Service (ClusterIP)
      â”‚
      â–¼
 Deployment (Pod: Flask/FastAPI + Model)
      â”‚                   â”‚
      â”‚                   â”œâ”€ ConfigMap/Secret (í™˜ê²½/ë¯¼ê°ì •ë³´)
      â”‚                   â”œâ”€ PVC (ëŒ€í˜• ëª¨ë¸ íŒŒì¼, ìºì‹œ)
      â”‚                   â””â”€ Metrics/Tracing (Prometheus/OTel)
      â”‚
      â””â”€ HPA/KEDA (ì˜¤í† ìŠ¤ì¼€ì¼)
```

---

## ğŸ“¦ 1. ëª¨ë¸ ì¤€ë¹„ â€” í•™ìŠµ ë° ì§ë ¬í™”(Joblib)

```python
# train_model.py
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
import joblib

X, y = load_iris(return_X_y=True)
clf = RandomForestClassifier(n_estimators=200, random_state=42)
clf.fit(X, y)

joblib.dump(clf, 'model.pkl')
print("Saved: model.pkl")
```

```bash
python train_model.py
```

> ëŒ€í˜• ëª¨ë¸ì€ **PVC**ì— ì˜¬ë¦¬ê±°ë‚˜, **ì‹œì‘ ì‹œ ì™¸ë¶€ ì €ì¥ì†Œ(S3/GCS)**ì—ì„œ ê°€ì ¸ì™€ **InitContainer**ë¡œ ë§ˆìš´íŠ¸í•˜ëŠ” íŒ¨í„´ì„ ì¶”ì²œí•©ë‹ˆë‹¤(ì•„ë˜ ì˜ˆì œ í¬í•¨).

---

## ğŸ³ 2. ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ â€” Flask + í”„ë¡œë©”í…Œìš°ìŠ¤ ì§€í‘œ + ì˜ˆì¸¡ API

**requirements.txt**
```
flask
scikit-learn
joblib
prometheus_client
gunicorn
numpy
```

**app.py (Flask + /metrics + ì˜ˆì¸¡ API + í—¬ìŠ¤ì²´í¬)**

```python
from flask import Flask, request, jsonify
import joblib, numpy as np, os, time
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST

MODEL_PATH = os.getenv("MODEL_PATH", "model.pkl")
PORT = int(os.getenv("PORT", "5000"))

app = Flask(__name__)
model = joblib.load(MODEL_PATH)

REQ = Counter("inference_requests_total", "Total inference requests", ["endpoint"])
LAT = Histogram("inference_latency_seconds", "Latency per inference", buckets=(0.005,0.01,0.02,0.05,0.1,0.2,0.5,1,2,5))

@app.route("/healthz")
def healthz():
    return "ok", 200

@app.route("/readyz")
def readyz():
    # ê°„ë‹¨í•œ ëª¨ë¸ ë¡œë”© ê²€ì¦
    try:
        _ = model.n_features_in_
        return "ready", 200
    except Exception:
        return "not-ready", 500

@app.route("/metrics")
def metrics():
    return generate_latest(), 200, {"Content-Type": CONTENT_TYPE_LATEST}

@app.route("/predict", methods=["POST"])
def predict():
    REQ.labels(endpoint="predict").inc()
    start = time.time()
    body = request.get_json(force=True)
    inputs = np.array(body["inputs"]).reshape(1, -1)
    pred = model.predict(inputs).tolist()
    LAT.observe(time.time() - start)
    return jsonify({"prediction": pred})
    
if __name__ == "__main__":
    # ê°œë°œìš©: gunicornì„ ì“°ë©´ ë©€í‹°í”„ë¡œì„¸ìŠ¤/ì›Œì»¤ë¡œ ì´ë“
    app.run(host="0.0.0.0", port=PORT)
```

**Dockerfile (ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ ê¶Œì¥)**
```Dockerfile
FROM python:3.9-slim AS base
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

FROM base AS runtime
WORKDIR /app
COPY --from=base /usr/local /usr/local
COPY app.py model.pkl . 
ENV PORT=5000
# í”„ë¡œë•ì…˜: Gunicorn + gevent/uvicorn workers (Flaskâ†’FastAPI ì „í™˜ì‹œ uvicorn ì›Œì»¤)
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "2", "--threads", "2", "app:app"]
```

ì´ë¯¸ì§€ ë¹Œë“œÂ·í‘¸ì‹œ:
```bash
docker build -t <docker_id>/ml-api:0.1.0 .
docker push <docker_id>/ml-api:0.1.0
```

> **íŒ**: ë²„ì „ íƒœê·¸ë¥¼ Git SHAì™€ í•¨ê»˜ ê´€ë¦¬í•˜ì„¸ìš”. `ml-api:0.1.0-<SHA>`.

---

## ğŸš€ 3. Kubernetes ë°°í¬ â€” Deployment/Service/Config/Secret/Probe/Resource

### 3.1 ConfigMap/Secret (í™˜ê²½Â·ë¯¼ê°ì •ë³´ ë¶„ë¦¬)

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-api-config
data:
  PORT: "5000"
  MODEL_PATH: "/models/model.pkl"
---
apiVersion: v1
kind: Secret
metadata:
  name: ml-api-secrets
type: Opaque
stringData:
  API_KEY: "local-dev-only-change-me"
```

### 3.2 PVC(ì„ íƒ: ëŒ€í˜• ëª¨ë¸/ìºì‹œ)

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ml-model-pvc
spec:
  accessModes: ["ReadOnlyMany"]
  resources:
    requests:
      storage: 1Gi
  storageClassName: standard
```

> ì˜¤ë¸Œì íŠ¸ ìŠ¤í† ë¦¬ì§€ì—ì„œ **InitContainer**ë¡œ ëª¨ë¸ì„ ë‚´ë ¤ë°›ì•„ PVCì— ì €ì¥í•˜ëŠ” íŒ¨í„´ë„ ìœ ìš©í•©ë‹ˆë‹¤.

### 3.3 Deployment + Service

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-api
  labels: { app: ml-api }
spec:
  replicas: 2
  selector:
    matchLabels: { app: ml-api }
  template:
    metadata:
      labels: { app: ml-api }
    spec:
      # (ì˜µì…˜) InitContainer: ì™¸ë¶€ì—ì„œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
      initContainers:
      - name: fetch-model
        image: curlimages/curl:8.10.1
        command: ["sh","-c"]
        args:
          - |
            set -e
            echo "Downloading model..."
            curl -fSL "$MODEL_URL" -o /models/model.pkl
        env:
        - name: MODEL_URL
          valueFrom:
            secretKeyRef:
              name: ml-api-secrets
              key: MODEL_URL
        volumeMounts:
        - name: model-vol
          mountPath: /models

      containers:
      - name: api
        image: <docker_id>/ml-api:0.1.0
        ports: [{ containerPort: 5000 }]
        envFrom:
        - configMapRef: { name: ml-api-config }
        - secretRef: { name: ml-api-secrets }
        volumeMounts:
        - name: model-vol
          mountPath: /models
          readOnly: true
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
          limits:
            cpu: "1"
            memory: "1Gi"
        livenessProbe:
          httpGet: { path: /healthz, port: 5000 }
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet: { path: /readyz, port: 5000 }
          initialDelaySeconds: 5
          periodSeconds: 5
        startupProbe:
          httpGet: { path: /healthz, port: 5000 }
          failureThreshold: 30
          periodSeconds: 2
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 10001
          capabilities:
            drop: ["ALL"]
      volumes:
      - name: model-vol
        persistentVolumeClaim:
          claimName: ml-model-pvc
      securityContext:
        fsGroup: 10001
---
apiVersion: v1
kind: Service
metadata:
  name: ml-api-svc
  labels: { app: ml-api }
spec:
  selector: { app: ml-api }
  ports:
  - name: http
    port: 80
    targetPort: 5000
  type: ClusterIP
```

> Probeë¥¼ **ê´€ëŒ€í•˜ê²Œ** ì¡ê³ , `startupProbe`ë¥¼ í†µí•´ **ì´ˆê¸° ë¡œë”© ì§€ì—°**ì„ ê²¬ë”œ ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²ƒì´ ì•ˆì •í™”ì— ì¤‘ìš”í•©ë‹ˆë‹¤.

---

## ğŸŒ 4. Ingress + TLS â€” ì‹¤ì„œë¹„ìŠ¤ ë…¸ì¶œ

### 4.1 NGINX Ingress (ê¸°ë³¸)

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ml-api-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
spec:
  ingressClassName: nginx
  rules:
  - host: ml.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ml-api-svc
            port: { number: 80 }
```

í˜¸ìŠ¤íŠ¸ ë§µ:
```plaintext
127.0.0.1 ml.local
```

### 4.2 cert-managerë¥¼ ì´ìš©í•œ TLS(ì‹¤ì„œë¹„ìŠ¤)

```yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-http01
spec:
  acme:
    email: you@example.com
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef: { name: letsencrypt-key }
    solvers:
    - http01:
        ingress:
          class: nginx
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ml-api-ingress-tls
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-http01"
spec:
  ingressClassName: nginx
  tls:
  - hosts: ["ml.your-domain.com"]
    secretName: ml-api-tls
  rules:
  - host: ml.your-domain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ml-api-svc
            port: { number: 80 }
```

---

## ğŸ§ª 5. í˜¸ì¶œ í…ŒìŠ¤íŠ¸

```bash
curl -X POST http://ml.local/predict \
  -H "Content-Type: application/json" \
  -d '{"inputs":[5.1,3.5,1.4,0.2]}'
```

ì˜ˆìƒ:
```json
{"prediction":[0]}
```

---

## ğŸ“ˆ 6. ì˜¤í† ìŠ¤ì¼€ì¼ë§ â€” HPA(ê¸°ë³¸) + KEDA(ì´ë²¤íŠ¸)

### 6.1 HPA(CPU ê¸°ë°˜)
```bash
kubectl autoscale deployment ml-api \
  --cpu-percent=60 --min=2 --max=8
kubectl get hpa
```

> CPU/ë©”ëª¨ë¦¬ë§Œìœ¼ë¡œ ì˜ˆì¸¡ë¶€í•˜ë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ê¸´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. **KEDA**ë¡œ í ê¸¸ì´/ìš”ì²­ ìˆ˜ ê¸°ë°˜ í™•ì¥ë„ í•¨ê»˜ ê³ ë ¤í•˜ì„¸ìš”.

### 6.2 KEDA(ìš”ì²­ ìˆ˜Â·í ê¸°ë°˜, ì˜ˆ: Prometheus Scaler)

**KEDA ì„¤ì¹˜ í›„**, ì•„ë˜ì²˜ëŸ¼ `ScaledObject`ë¥¼ ì •ì˜í•˜ë©´ Prometheus ì§€í‘œ(ì´ˆë‹¹ ìš”ì²­ ìˆ˜ ë“±)ì— ë§ì¶° ìŠ¤ì¼€ì¼í•©ë‹ˆë‹¤.

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ml-api-keda
spec:
  scaleTargetRef:
    name: ml-api
  pollingInterval: 10
  cooldownPeriod: 60
  minReplicaCount: 2
  maxReplicaCount: 20
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-server.monitoring.svc.cluster.local
      metricName: inference_requests_total
      threshold: '50'            # ì´ˆë‹¹ ìš”ì²­ 50ê±´ ê¸°ì¤€
      query: |
        sum(rate(inference_requests_total{endpoint="predict"}[1m]))
```

---

## ğŸ“Š 7. ê´€ì¸¡(Observability) â€” Prometheus, Grafana, OpenTelemetry

- **/metrics** ì—”ë“œí¬ì¸íŠ¸ë¡œ Prometheus ìŠ¤í¬ë© (ìœ„ Flask ì˜ˆì œ í¬í•¨)
- Grafanaë¡œ ëŒ€ì‹œë³´ë“œ(Pod QPS, p95 ì§€ì—°, Error rate) êµ¬ì„±
- ë¶„ì‚° íŠ¸ë ˆì´ì‹±ì´ í•„ìš”í•˜ë©´ **OpenTelemetry**ë¡œ **Flask/FastAPI**ì— tracer ì„¤ì¹˜

**OpenTelemetry(ì„ íƒ, ê°„ë‹¨ ì˜ˆì‹œ)**
```bash
pip install opentelemetry-api opentelemetry-sdk opentelemetry-instrumentation-flask
```

```python
# app.py ìƒë‹¨
from opentelemetry.instrumentation.flask import FlaskInstrumentor
FlaskInstrumentor().instrument_app(app)
```

Collector/Jaeger/Tempo ì—°ë™ì€ í™˜ê²½ì— ë§ì¶° ë°°ì¹˜(Helm ì°¨íŠ¸ ì¶”ì²œ).

---

## ğŸ” 8. ë³´ì•ˆÂ·ê²©ë¦¬ â€” SecurityContext, Pod Security, NetworkPolicy

### 8.1 Pod Security(ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë ˆë²¨)
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ml-prod
  labels:
    pod-security.kubernetes.io/enforce: "restricted"
```

### 8.2 NetworkPolicy(ë‚´ë¶€ í†µì‹ ë§Œ í—ˆìš©)
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ml-api-allow-ingress
  namespace: ml-prod
spec:
  podSelector:
    matchLabels: { app: ml-api }
  ingress:
  - from:
    - namespaceSelector:
        matchLabels: { kubernetes.io/metadata.name: ingress-nginx }
    ports: [{ port: 5000, protocol: TCP }]
```

### 8.3 ì´ë¯¸ì§€ì„œëª…/ê²€ì¦(ê¶Œì¥)
- Cosignìœ¼ë¡œ ì„œëª… â†’ OPA/Kyvernoë¡œ ê²€ì¦ ì •ì±… ì ìš©

---

## ğŸ§ª 9. ì¹´ë‚˜ë¦¬/ë¸”ë£¨ê·¸ë¦° â€” ì ì§„ë°°í¬Â·ë¡¤ë°±

### 9.1 NGINX Ingress ê¸°ë°˜ ê°„ë‹¨ Canary(ê°€ì¤‘ì¹˜)
```yaml
metadata:
  name: ml-api-canary
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "20"  # 20% íŠ¸ë˜í”½
```

### 9.2 Argo Rollouts(ê¶Œì¥)
- Rollout CRë¡œ **Step(10%â†’30%â†’50%â†’100%)** ì •ì˜, ë©”íŠ¸ë¦­ ê²Œì´íŒ…(Prometheus), ìë™ ë¡¤ë°±

---

## âš¡ 10. ì„±ëŠ¥Â·ì‹ ë¢°ì„± íŠœë‹

- **Gunicorn ì›Œì»¤/ìŠ¤ë ˆë“œ** ì¡°ì •(ì½”ì–´Â·ë©”ëª¨ë¦¬ ëŒ€ë¹„): `--workers 2 --threads 2`
- **í”„ë¦¬ì›Œë°(warm-up)**: ì‹œì‘ í›„ ì¤€ë¹„ë‹¨ê³„ì—ì„œ ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ëª¨ë¸ ë¡œë”©Â·JIT ìºì‹œ
- **ë™ì‹œì„±**: FastAPI(+Uvicorn/ASGI) ì „í™˜ ì‹œ ê³ ë™ì‹œì„± HTTPì— ìœ ë¦¬
- **ëŒ€í˜• ëª¨ë¸**: ì„œë²„ í”„ë¡œì„¸ìŠ¤ **preload**, **shared memory**(ì½ê¸° ì „ìš©) ì „ëµ
- **ë¡œê¹…**: êµ¬ì¡°í™”(JSON) ë¡œê¹… + ìˆ˜ëª…ì£¼ê¸° ì´ë²¤íŠ¸ ê¸°ë¡
- **ë¦¬ì†ŒìŠ¤ ê°€ë“œ**: requests/limitsë¥¼ í™•ì‹¤íˆ(ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ëŒ€ë¹„), OOMKilled ëª¨ë‹ˆí„°

---

## ğŸ§® 11. ê°„ë‹¨ ìš©ëŸ‰Â·ì§€ì—° ì¶”ì •

ìš”ì²­ìœ¨ \(\lambda\) (req/s), í‰ê·  ì²˜ë¦¬ì‹œê°„ \(S\) (s), Podë‹¹ ë™ì‹œì„± \(c\) ì¼ ë•Œ í•„ìš”í•œ Pod ìˆ˜ \(N\) ê·¼ì‚¬:
$$
N \approx \left\lceil \frac{\lambda \cdot S}{c} \right\rceil
$$

ì½œë“œ ìŠ¤íƒ€íŠ¸ ë¹„ìœ¨ \(p_{cold}\), ì½œë“œ/ì›œ ì§€ì—° \(T_{cold}, T_{warm}\) ì¼ ë•Œ ê¸°ëŒ€ ì‘ë‹µì‹œê°„:
$$
ERT \approx p_{cold} \cdot T_{cold} + (1-p_{cold}) \cdot T_{warm}
$$

> **Warming/MinReplicas** ë¡œ \(p_{cold}\) ë‚®ì¶”ê³ , **ëŸ°íƒ€ì„ ìµœì í™”**ë¡œ \(T_{cold}\) ë¥¼ ì¤„ì…ë‹ˆë‹¤.

---

## ğŸ§² 12. GPU ì„œë¹™(ì„ íƒ)

NVIDIA í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜ í›„:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-gpu-api
spec:
  replicas: 1
  selector: { matchLabels: { app: ml-gpu } }
  template:
    metadata: { labels: { app: ml-gpu } }
    spec:
      nodeSelector: { "nvidia.com/gpu.present": "true" }
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: api
        image: <docker_id>/ml-gpu-api:0.1.0
        resources:
          limits:
            nvidia.com/gpu: 1
            cpu: "2"
            memory: "8Gi"
        env:
        - name: TORCH_CUDA_ARCH_LIST
          value: "8.0+PTX"
```

> GPUëŠ” **Pod ìˆ˜ë³´ë‹¤ ì„¸ì…˜ ê³ ì •/ë°°ì¹˜ ì²˜ë¦¬**ê°€ ì¤‘ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
> Triton Inference Server, TorchServe, TF Servingë¡œ ì „í™˜ ê²€í† .

---

## ğŸ§± 13. FastAPI ëŒ€ì•ˆ (é«˜ë™ì‹œì„±/ìŠ¤í‚¤ë§ˆ/ë¬¸ì„œí™”)

**requirements.txt (ëŒ€ì²´)**
```
fastapi
uvicorn[standard]
scikit-learn
joblib
prometheus_client
numpy
```

**main.py**
```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib, numpy as np
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
from starlette.responses import Response

class PredictIn(BaseModel):
    inputs: list[float]

REQ = Counter("inference_requests_total", "Total inference requests", ["endpoint"])
LAT = Histogram("inference_latency_seconds", "Latency per inference")

app = FastAPI()
model = joblib.load("model.pkl")

@app.get("/healthz")
def healthz(): return {"ok": True}

@app.get("/readyz")
def readyz(): return {"ready": hasattr(model, "n_features_in_")}

@app.get("/metrics")
def metrics(): return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

@app.post("/predict")
def predict(body: PredictIn):
    import time
    REQ.labels(endpoint="predict").inc()
    st = time.time()
    pred = model.predict(np.array(body.inputs).reshape(1,-1)).tolist()
    LAT.observe(time.time()-st)
    return {"prediction": pred}
```

**Docker CMD(ì˜ˆì‹œ)**:
```Dockerfile
CMD ["uvicorn","main:app","--host","0.0.0.0","--port","5000","--workers","2"]
```

---

## ğŸ§° 14. Kustomize/Helm, GitOps(ArgoCD)ë¡œ ë°°í¬ ê´€ë¦¬

- **Kustomize**: `overlays/dev`, `overlays/prod` ë¡œ ì´ë¯¸ì§€ íƒœê·¸Â·ReplicaÂ·ë¦¬ì†ŒìŠ¤ ì°¨ë“± ê´€ë¦¬
- **Helm**: valuesë¡œ HPA ì„ê³„ì¹˜, Ingress host, TLS, ë¦¬ì†ŒìŠ¤ ë“± íŒŒë¼ë¯¸í„°í™”
- **ArgoCD**: Git push â†’ ìë™ ë™ê¸°í™”, ë“œë¦¬í”„íŠ¸ ìë™ ë³µêµ¬, ë¡¤ë°± ì´ë ¥

---

## ğŸ§¼ 15. ì •ë¦¬Â·ì‚­ì œ

```bash
kubectl delete -f deployment_service_ingress.yaml
kubectl delete hpa ml-api
kubectl delete pvc ml-model-pvc
kubectl delete cm ml-api-config
kubectl delete secret ml-api-secrets
```

---

## ğŸ“ ì˜ˆì œ ë¦¬í¬ì§€í† ë¦¬ êµ¬ì¡°(ê¶Œì¥)

```plaintext
.
â”œâ”€ app/                 # ì•± ì†ŒìŠ¤(Flask/FastAPI)
â”‚  â”œâ”€ app.py / main.py
â”‚  â”œâ”€ requirements.txt
â”‚  â””â”€ Dockerfile
â”œâ”€ model/
â”‚  â””â”€ model.pkl
â”œâ”€ k8s/
â”‚  â”œâ”€ config-secret.yaml
â”‚  â”œâ”€ pvc.yaml
â”‚  â”œâ”€ deployment.yaml
â”‚  â”œâ”€ service.yaml
â”‚  â””â”€ ingress.yaml
â”œâ”€ kustomize/
â”‚  â”œâ”€ base/ ...
â”‚  â””â”€ overlays/ (dev/prod) ...
â””â”€ .github/workflows/deploy.yml (ì„ íƒ: CI/CD)
```

---

## âœ… ì‹¤ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸ (í˜„ì¥ ìš”ì•½)

- **ê°€ìš©ì„±**: replicasâ‰¥2, readiness/liveness/startup probe ì„¤ì •
- **ìŠ¤ì¼€ì¼ë§**: HPA(ê¸°ë³¸), KEDA(ì´ë²¤íŠ¸/ì§€í‘œ), minReplicasë¡œ ì½œë“œ ê°ì†Œ
- **ê´€ì¸¡**: /metrics(í”„ë¡œë©”í…Œìš°ìŠ¤), ë¡œê¹…(JSON), íŠ¸ë ˆì´ì‹±(OTel)
- **ë³´ì•ˆ**: Pod Security, SecurityContext(runAsNonRoot), NetworkPolicy, ì´ë¯¸ì§€ì„œëª…
- **ì„±ëŠ¥**: Gunicorn/uvicorn ì›Œì»¤, Pre-warm, ëŒ€í˜•ëª¨ë¸ ë¡œë”© ìµœì í™”
- **ë°°í¬ì „ëµ**: Canary/Blue-Green(Argo Rollouts), Helm/Kustomize/ArgoCD
- **ìŠ¤í† ë¦¬ì§€**: PVC/InitContainerë¡œ ëª¨ë¸ ê³µê¸‰(or ì™¸ë¶€ ì €ì¥ì†Œ)
- **GPU**: nvidia.com/gpu limits, ìŠ¤ì¼€ì¤„ë§ ì •ì±…, ë°°ì¹˜/ì„¸ì…˜ ì„¤ê³„

---

## ğŸ“š ì°¸ê³  ë§í¬

- Kubernetes â€” Workloads/Autoscaling/Ingress
- Prometheus & Grafana â€” Metrics & Dashboard
- KEDA â€” Event-driven Autoscaling
- cert-manager â€” TLS ìë™í™”
- ArgoCD & Argo Rollouts â€” GitOps & Progressive Delivery
- NVIDIA GPU Operator / Triton Inference Server
- TorchServe / TensorFlow Serving

---

## ğŸ”š ë§ˆë¬´ë¦¬

ì´ ê°€ì´ë“œëŠ” **í•™ìŠµâ†’ì„œë¹™â†’ìš´ì˜**ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ì‹¤ì „ í”Œë¡œìš°ë¥¼ â€œ**ë‹¨ì¼ ë¬¸ì„œ**â€ë¡œ ì—®ì—ˆìŠµë‹ˆë‹¤.  
ì—¬ê¸° ë‚˜ì˜¨ í…œí”Œë¦¿ì„ **ë³µì‚¬-ìˆ˜ì •**ë§Œ í•´ë„, **ì‘ì€ ëª¨ë¸ì€ ìˆ˜ ì‹œê°„ ë‚´** ì•ˆì •ì ìœ¼ë¡œ í”„ë¡œë•ì…˜ ëŠë‚Œì˜ í™˜ê²½ì— ì˜¬ë¦´ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.  
ê·¸ë‹¤ìŒ ë‹¨ê³„ë¡œëŠ” **GitOpsÂ·CanaryÂ·ê´€ì¸¡ ì§€í‘œ ê¸°ë°˜ SLO**ë¥¼ ë„ì…í•´, íŒ€ ì „ì²´ê°€ **ì¼ê´€ëœ ë°©ì‹**ìœ¼ë¡œ ëª¨ë¸ì„ ìš´ì˜í•˜ë„ë¡ í™•ì¥í•´ë³´ì„¸ìš”.