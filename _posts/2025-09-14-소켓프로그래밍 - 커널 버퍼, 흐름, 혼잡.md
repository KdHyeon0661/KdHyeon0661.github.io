---
layout: post
title: 소켓프로그래밍 - 커널 버퍼, 흐름, 혼잡
date: 2025-09-14 20:25:23 +0900
category: 소켓프로그래밍
---
## 6. 커널 버퍼/흐름/혼잡 — 성능 직관 만들기

> 목표: TCP의 **송·수신 커널 버퍼**, **흐름 제어(rwnd)**, **혼잡 제어(cwnd)** 가 **애플리케이션 성능**에 미치는 영향을 **직관**으로 잡는다.  
> 핵심 수식 두 가지로 사고한다:
>
> - **대역폭–지연곱(BDP)**  
>   $$
>   \text{BDP} = \text{Bandwidth} \times \text{RTT}
>   $$
> - **손실 환경 처리량 근사(직관식)**  
>   $$
>   \text{Throughput} \approx \frac{\text{MSS}}{\text{RTT}\sqrt{p}}
>   $$
>
> 그리고 **버퍼 크기/RTT/손실률**을 바꿔가며 **체감 실험**을 설계·구현한다.

---

### 6.1 큰 그림: 애플리케이션 ↔ 커널 버퍼 ↔ 네트워크

```
   [App send()]            [Kernel]             [Network]         [Kernel]            [App recv()]
   ─────────────→ (송신버퍼) → TCP 세그먼트 → ─────────→ (수신버퍼) ───────────→ ────────────────
                     ^                ^                     ^                ^
                     |                |                     |                |
                흐름제어(rwnd)   혼잡제어(cwnd)        혼잡제어(cwnd)   흐름제어(rwnd)

```

- **송신 커널 버퍼 (send buffer)**: `send()`가 먼저 여기에 기록된다. 여유가 없으면 `send()`는 **부분 쓰기** 또는 **블록**.
- **수신 커널 버퍼 (receive buffer)**: 네트워크에서 도착한 페이로드는 일단 여기에 저장. 응용이 `recv()`를 **빨리** 해주지 않으면 가득 차고, 원격지에 **작은 윈도우(rwnd)** 를 광고하여 **백프레셔**를 유발.
- **흐름 제어 (Flow control)**: **수신 측**이 광고하는 **rwnd**(Receive Window). “더 받을 공간”의 상한.
- **혼잡 제어 (Congestion control)**: **네트워크 상태**(손실/지연)를 반영해 송신 측이 유지하는 **cwnd**(Congestion Window).
- **실효 송신 윈도우**:
  $$
  \text{win}_\text{eff} = \min\{\text{cwnd},\ \text{rwnd}\}
  $$
  한 번에 “날아갈 수 있는 인플라이트 데이터”의 양은 이 값으로 제한된다.

---

### 6.2 송수신 커널 버퍼와 **백프레셔(backpressure)**

#### 6.2.1 송신 버퍼 고갈 → `send()`가 막힌다
- 송신 버퍼가 꽉 차는 이유:
  - 네트워크/상대가 느리다(작은 cwnd/rwnd, 긴 RTT).
  - 애플리케이션이 너무 빨리 쓴다.
- 결과:
  - **블로킹 소켓**: `send()`가 **대기** 또는 **부분만 쓰고 반환**.
  - **논블로킹 소켓**: `send()`가 **EAGAIN** 으로 실패 → **이벤트 루프**에서 **POLLOUT**(혹은 EPOLLOUT)까지 대기.

#### 6.2.2 수신 버퍼 고갈 → **수신 윈도우(rwnd) 축소**
- 앱이 `recv()`를 제때 호출하지 않으면 커널 수신 버퍼가 가득 참.
- TCP는 **광고 윈도우**를 낮춰 원격 송신자의 **실효 윈도우(win_eff)** 를 줄인다 → **전송률 하락 = 백프레셔**.
- 극단에서는 **제로 윈도우**가 되어 송신 측이 **Zero-Window Probe**(탐침)를 보내며 대기.

#### 6.2.3 커널 자동 튜닝(오토튜닝)과 상하한
- 현대 리눅스/FreeBSD는 **Rcv/Snd 버퍼 자동 확대**(오토튜닝)를 지원.
- 소켓 옵션으로 힌트를 줄 수 있다:
  - `SO_SNDBUF`, `SO_RCVBUF` (바이트 단위, 보통 커널이 **두 배** 내부 계수로 적용)
  - 최대치는 `net.ipv4.tcp_rmem`, `net.ipv4.tcp_wmem` 등의 **sysctl** 상한을 따름.

---

### 6.3 **RTT와 BDP**: 파이프를 채워야 처리량이 나온다

#### 6.3.1 BDP 정의와 의미
- **대역폭–지연곱(BDP)**:
  $$
  \text{BDP} = \text{Bandwidth} \times \text{RTT}
  $$
  단위 일치(예: 바이트/초 × 초 = 바이트)로 보면, “**한 시점에 파이프 속에 떠 있어야 하는 데이터 양**”.
- **직관**: BDP 만큼의 인플라이트가 없으면, ACK를 기다리는 동안 링크가 놀고 → **처리량 하락**.

#### 6.3.2 수치 예시
- 100 Mbps 링크, RTT = 50 ms
  $$
  \text{BDP} = 100\ \text{Mb/s} \times 0.05\ \text{s} = 5\ \text{Mb} \approx 625\ \text{kB}
  $$
  즉, **최소 수백 kB**의 유효 윈도우(= 인플라이트)가 있어야 **링크 포화**에 근접.
- 1 Gbps, RTT = 100 ms:
  $$
  \text{BDP} = 1{,}000\ \text{Mb/s} \times 0.1\ \text{s} = 100\ \text{Mb} \approx 12.5\ \text{MB}
  $$
  **수 MB~수십 MB**의 윈도우가 필요. 버퍼/윈도우가 작으면 1Gbps를 못 뽑음.

#### 6.3.3 실효 윈도우와 BDP
- 성능 조건(대략):
  $$
  \min\{\text{cwnd}, \text{rwnd}\} \gtrsim \text{BDP}
  $$
  + 소켓 **sndbuf/rcvbuf** 및 커널 **오토튜닝 상한**도 충분히 커야 한다.

---

### 6.4 손실과 처리량 직관

- 고전적(대략적) TCP Reno 근사:
  $$
  \text{Throughput} \approx \frac{\text{MSS}}{\text{RTT}\sqrt{p}}
  $$
  - \( \text{MSS} \): 세그먼트 페이로드 최대 크기 (보통 1448B 근처: 1500 MTU - 20(IP) - 32(TCP옵션 가정) 등 환경에 따라 다름)
  - \( p \): 손실 확률
- **메시지**:
  - 동일 RTT에서 **손실(\(p\))** 이 커질수록 처리량은 비선형적으로 **급락**.
  - 같은 손실률이라도 **RTT가 크면** 타격이 더 큼.
- 현대 혼잡제어(CUBIC/BBR)는 Reno와 **거동이 다르다**. 위 식은 “손실 많은 환경에서 RTT·손실이 성능에 어떻게 체감되는지” **직관**을 주는 용도.

---

### 6.5 백프레셔가 애플리케이션에 보이는 징후

- **송신 측**: `send()`가 평소보다 **느려지거나**(블로킹 지연), **짧게 쓰고** 반환(부분 쓰기 증가), 논블로킹에선 **EAGAIN** 빈발.
- **수신 측**: `recv()`가 제때 오지 않고 **대기 시간이 증가**. 응용 레벨 큐가 쌓임.
- **대응**:
  - **프레이밍**과 **백프레셔 전파**: 생산자(송신) 속도를 **소비자(수신)** 속도에 맞추는 정책(예: **윈도윙**, **크레딧 기반** 프로토콜).
  - **버퍼 조정**: 커널/소켓 버퍼를 **BDP**에 맞춰 키운다. 단, 과도한 버퍼는 **버퍼블로트**(지연 증가)를 유발 → **측정 기반**.

---

### 6.6 관측 도구로 실시간 감(感) 잡기

#### 6.6.1 `ss`로 큐/상태 확인
```bash
ss -tnpi '( sport = :9000 )'
# Recv-Q / Send-Q, cwnd(표시되는 시스템도 있음), rto, ato, pacing_rate 등 참고
```

#### 6.6.2 `getsockopt(TCP_INFO)` (리눅스)로 미세 관측
- C++에서 `struct tcp_info` 를 읽어 **RTT, rttvar, snd_cwnd, rcv_space, total_retrans** 등을 로깅.

```cpp
#include <netinet/tcp.h>
#include <sys/socket.h>
#include <print>

void log_tcp_info(int fd, std::string_view tag) {
    tcp_info info{};
    socklen_t len = sizeof(info);
    if (::getsockopt(fd, IPPROTO_TCP, TCP_INFO, &info, &len) == 0) {
        // 단위: rtt(마이크로초), snd_cwnd(세그먼트 단위)
        std::print("[tcp_info][{}] rtt={}us rttvar={}us cwnd={} rcv_space={} bytes retrans={}\n",
                   tag, info.tcpi_rtt, info.tcpi_rttvar, info.tcpi_snd_cwnd,
                   info.tcpi_rcv_space, info.tcpi_total_retrans);
    }
}
```

#### 6.6.3 `tcpdump`/Wireshark
- **ACK의 간격**, **재전송**, **윈도우 광고**(Window field) 변화 관찰.
- **Zero window** / **Window update** 패턴을 눈으로 확인.

---

### 6.7 코드: 버퍼/타임아웃/노드레이 옵션 편의 함수(C++23)

```cpp
#include <sys/socket.h>
#include <netinet/tcp.h>
#include <chrono>

inline void set_sndbuf(int fd, int bytes) {
    ::setsockopt(fd, SOL_SOCKET, SO_SNDBUF, &bytes, sizeof(bytes));
}
inline void set_rcvbuf(int fd, int bytes) {
    ::setsockopt(fd, SOL_SOCKET, SO_RCVBUF, &bytes, sizeof(bytes));
}
inline void set_nodelay(int fd, bool on=true) {
    int v = on ? 1 : 0;
    ::setsockopt(fd, IPPROTO_TCP, TCP_NODELAY, &v, sizeof(v));
}
inline timeval to_timeval(std::chrono::milliseconds ms) {
    timeval tv{};
    tv.tv_sec = (long)(ms.count()/1000);
    tv.tv_usec= (long)((ms.count()%1000)*1000);
    return tv;
}
inline void set_timeouts_ms(int fd, int recv_ms, int send_ms) {
    auto r = to_timeval(std::chrono::milliseconds(recv_ms));
    auto s = to_timeval(std::chrono::milliseconds(send_ms));
    ::setsockopt(fd, SOL_SOCKET, SO_RCVTIMEO, &r, sizeof(r));
    ::setsockopt(fd, SOL_SOCKET, SO_SNDTIMEO, &s, sizeof(s));
}
```

- **주의**: `SO_SNDBUF/SO_RCVBUF` 실제 적용값은 커널에서 **2배** 등으로 보정되는 경우가 있다(플랫폼 의존).
- `TCP_NODELAY`는 **작은 메시지 지연**을 줄이는 대신 **세그먼트 수 증가** 가능 → **측정 기반** 선택.

---

### 6.8 예제: **처리량 측정용 송수신 프로그램** (블로킹, C++23)

#### 6.8.1 서버 — 지정한 총량을 **읽고 버리기**(수신 스루풋 측정)
```cpp
// thr_server.cpp
// 빌드: g++ -std=c++23 -O2 thr_server.cpp -o thr_server
#include <arpa/inet.h>
#include <netdb.h>
#include <sys/socket.h>
#include <unistd.h>
#include <print>
#include <vector>
#include <string>
#include <chrono>

int main(int argc, char** argv) {
    const char* bind_host = (argc>1 ? argv[1] : nullptr); // null → any
    const char* port      = (argc>2 ? argv[2] : "9000");
    addrinfo hints{}, *res=nullptr;
    hints.ai_family=AF_UNSPEC; hints.ai_socktype=SOCK_STREAM; hints.ai_flags=AI_PASSIVE|AI_ADDRCONFIG;
    if (getaddrinfo(bind_host, port, &hints, &res)!=0) { perror("gai"); return 1; }

    int lfd=-1;
    for (auto* ai=res; ai; ai=ai->ai_next) {
        lfd = ::socket(ai->ai_family, ai->ai_socktype, ai->ai_protocol);
        if (lfd<0) continue;
        int yes=1; setsockopt(lfd, SOL_SOCKET, SO_REUSEADDR, &yes, sizeof(yes));
        if (::bind(lfd, ai->ai_addr, ai->ai_addrlen)==0 && ::listen(lfd, 128)==0) break;
        ::close(lfd); lfd=-1;
    }
    freeaddrinfo(res);
    if (lfd<0) { perror("listen"); return 1; }
    std::print("[server] listening on {}\n", port);

    sockaddr_storage ss{}; socklen_t slen=sizeof(ss);
    int cfd = ::accept(lfd, (sockaddr*)&ss, &slen);
    if (cfd<0) { perror("accept"); return 1; }
    ::close(lfd);

    std::vector<char> buf(1<<20); // 1MiB
    auto t0 = std::chrono::steady_clock::now();
    unsigned long long total=0;
    for(;;){
        ssize_t n = ::recv(cfd, buf.data(), buf.size(), 0);
        if (n>0) { total += (unsigned long long)n; continue; }
        if (n==0) break;
        if (errno==EINTR) continue;
        perror("recv"); break;
    }
    auto t1 = std::chrono::steady_clock::now();
    double sec = std::chrono::duration<double>(t1-t0).count();
    double gbps = (total*8.0)/(sec*1e9);
    std::print("[server] received={} bytes, time={:.3f}s, rate={:.3f} Gbps\n", total, sec, gbps);
    ::close(cfd);
}
```

#### 6.8.2 클라이언트 — **총 N 바이트**를 연속 전송(송신 스루풋 측정)
```cpp
// thr_client.cpp
// 빌드: g++ -std=c++23 -O2 thr_client.cpp -o thr_client
#include <arpa/inet.h>
#include <netdb.h>
#include <sys/socket.h>
#include <unistd.h>
#include <print>
#include <vector>
#include <string>
#include <chrono>
#include <cstring>

void set_sndbuf(int fd, int bytes){ ::setsockopt(fd, SOL_SOCKET, SO_SNDBUF, &bytes, sizeof(bytes)); }
void set_rcvbuf(int fd, int bytes){ ::setsockopt(fd, SOL_SOCKET, SO_RCVBUF, &bytes, sizeof(bytes)); }
void set_nodelay(int fd, bool on){ int v=on?1:0; ::setsockopt(fd, IPPROTO_TCP, TCP_NODELAY, &v, sizeof(v)); }

int main(int argc, char** argv) {
    if (argc < 5) {
        std::print(stderr, "usage: {} <host> <port> <gigabytes> <buf_kib> [nodelay=0|1]\n", argv[0]);
        return 1;
    }
    const char* host = argv[1];
    const char* port = argv[2];
    double gigabytes = std::stod(argv[3]);
    size_t buf_kib = std::stoul(argv[4]);
    bool nodelay = (argc>5 && std::stoi(argv[5])!=0);

    addrinfo hints{}, *res=nullptr;
    hints.ai_family=AF_UNSPEC; hints.ai_socktype=SOCK_STREAM; hints.ai_flags=AI_ADDRCONFIG;
    if (getaddrinfo(host, port, &hints, &res)!=0) { perror("gai"); return 1; }

    int s=-1;
    for (auto* ai=res; ai; ai=ai->ai_next) {
        s = ::socket(ai->ai_family, ai->ai_socktype, ai->ai_protocol);
        if (s<0) continue;
        set_sndbuf(s, 4<<20); // 힌트: 4MiB
        set_rcvbuf(s, 4<<20);
        set_nodelay(s, nodelay);
        if (::connect(s, ai->ai_addr, ai->ai_addrlen)==0) break;
        ::close(s); s=-1;
    }
    freeaddrinfo(res);
    if (s<0) { perror("connect"); return 1; }

    std::vector<char> buf(buf_kib*1024, 'x');
    unsigned long long target = (unsigned long long)(gigabytes*1024*1024*1024ULL);
    unsigned long long total=0;
    auto t0 = std::chrono::steady_clock::now();
    while (total < target) {
        size_t tosend = std::min<unsigned long long>(buf.size(), target-total);
        ssize_t n = ::send(s, buf.data(), tosend, 0);
        if (n>0) { total += (unsigned long long)n; continue; }
        if (n==0) break;
        if (errno==EINTR) continue;
        perror("send"); break;
    }
    ::shutdown(s, SHUT_WR); // 송신 종료 알림
    // 서버가 닫을 때까지 drain (옵션)
    char tmp[4096];
    while (::recv(s, tmp, sizeof(tmp), 0) > 0) {}
    auto t1 = std::chrono::steady_clock::now();
    double sec = std::chrono::duration<double>(t1-t0).count();
    double gbps = (total*8.0)/(sec*1e9);
    std::print("[client] sent={} bytes, time={:.3f}s, rate={:.3f} Gbps, nodelay={}\n", total, sec, gbps, nodelay);
    ::close(s);
}
```

> 이 두 프로그램은 **순수 송·수신 경로**의 처리량을 재기동 없이 바로 측정하는 **최소 실험기**다.  
> 이후 **RTT/손실**을 인위적으로 바꿔 **체감**할 수 있다.

---

### 6.9 실험 설계: **버퍼 크기/RTT/손실률**이 처리량에 미치는 영향

#### 6.9.1 실험 환경 선택
- **같은 머신**에서 루프백(매우 낮은 RTT, 손실 없음) → **버퍼 효과**를 보기엔 좋지만, **RTT/손실 실험은 제한**.
- **가상 네트워크**(Linux `tc netem`)로 RTT/손실을 **에뮬**:
  - 단일 머신의 **veth pair** 또는 **네임스페이스(netns)** 를 사용하면 깨끗한 실험 가능.

#### 6.9.2 최소 설정: `tc netem` 로 RTT/손실 설정
```bash
# 예) eth0 대신 vethX 같은 실험 인터페이스에 적용하는 것을 권장
# RTT 50ms 왕복 ~= one-way delay 25ms*2 로 근사 (보통 netem delay는 one-way 모델)
sudo tc qdisc add dev eth0 root netem delay 25ms
# 손실 0.5%
sudo tc qdisc change dev eth0 root netem delay 25ms loss 0.5%
# 제거
sudo tc qdisc del dev eth0 root
```

> **권장**: 실제 NIC에 직접 걸기보단 **네임스페이스 + veth pair** 를 만들어, **실험 트래픽**에만 영향 주도록 한다.

#### 6.9.3 단계별 실험 시나리오

**(A) RTT만 변화시키며 처리량 확인**
1. 서버: `./thr_server 0.0.0.0 9000`
2. 클라(동일 머신 또는 원격): `./thr_client <server_ip> 9000 2 1024`
   - 2 GiB 전송, 1 MiB 버퍼(앱 단 버퍼 사이즈)
3. `tc` 로 RTT 변경:
   - 10ms → 50ms → 100ms → 200ms
4. 각 RTT에서의 처리량(Gbps)을 기록하고, **BDP** 계산치와 비교:
   - 예: **100 Mbps** 링크 가정 시, `BDP = 100Mb/s × RTT`.  
     **sndbuf/rcvbuf** 및 **cwnd**가 **BDP 이상**이 되는지 **TCP_INFO** 로 확인.

**(B) 버퍼 크기(SO_SNDBUF/SO_RCVBUF) 변화**
1. 클라 코드에서 `set_sndbuf/rcvbuf` 값을 256KiB, 1MiB, 4MiB, 16MiB 등으로 바꿔가며 전송.
2. RTT가 큰(예: 100ms) 상황에서 **버퍼를 키울수록** **처리량 증가**가 관측되는지 확인.
3. 어느 시점부터는 **cwnd/rwnd** 또는 **링크/CPU 한계**로 증가가 **포화**.

**(C) 손실률 변화**
1. RTT 고정(예: 50ms), 손실률 0%, 0.1%, 0.5%, 1% 로 바꿈.
2. 처리량이 **\( \propto 1/(\text{RTT}\sqrt{p}) \)** 직관에 따라 **급락**하는지 관측.
3. 혼잡제어(예: CUBIC vs BBR) 변경 시 비교(시스템 전역 혼잡제어 변경이 필요. 루트 권한/커널 지원 필수).

**(D) Nagle/Delayed ACK 영향**
1. 작은 메시지를 자주 보내는 테스트(예: 64B 반복).
2. `TCP_NODELAY` ON/OFF 비교 → **지연/처리량** 변화를 기록.

---

### 6.10 실험 결과를 **수식과 함께** 해석하는 훈련

#### 6.10.1 BDP와 실효 윈도우
- 실험에서 **처리량이 낮다**면:
  1) `TCP_INFO` 로 **cwnd**(세그먼트 단위), **rtt**, **rcv_space** 확인.  
  2) `win_eff = min(cwnd*MSL, rwnd, sndbuf)` 가 **BDP** 에 미달인지 계산.
  3) 부족하면 **버퍼 확대** 또는 **cwnd 성장**에 영향 주는 요소(손실, pacing, RTT) 확인.

#### 6.10.2 손실률 \(p\) 증가에 따른 급락
- 실험 기록에서 \(p\) 증가 시 처리량이 \(1/\sqrt{p}\) 처럼 내려가는 경향이 보이면, 재전송 카운트 증가(TCP_INFO `total_retrans`)와 상관 관계를 본다.

#### 6.10.3 버퍼블로트 경계
- 버퍼를 크게 하면 처리량이 오르다가, 어느 지점부터 **지연/지터가 급증**: RTT(평균/분산)가 커지는지 `tcp_info.rtt/rttvar`로 모니터링.
- **목표**: **필요한 만큼만** 버퍼를 키운다(대략 **BDP** 수준 + 여유).

---

### 6.11 대역폭–지연곱 계산 **예제 연습**

1) 200 Mbps, RTT = 80 ms  
   $$
   \text{BDP} = 200\times10^6\ \text{b/s} \times 0.08\ \text{s} = 16\times10^6\ \text{b} \approx 2\ \text{MB}
   $$
   → 실효 윈도우가 **2MB 이상** 필요. **sndbuf/rcvbuf ≥ 2MB**, **cwnd ≥ 2MB/MSS** 세그먼트.

2) 40 Mbps, RTT = 5 ms  
   $$
   \text{BDP} \approx 40\times10^6\times0.005=200{,}000\ \text{b} \approx 25\ \text{kB}
   $$
   → 작은 버퍼로도 포화 가능. 대신 **작은 메시지 묶기**(coalescing)가 중요.

---

### 6.12 운영 팁(짧은 체크리스트)

- [ ] **목표 링크·RTT**를 알고 **BDP**를 먼저 계산한다.  
- [ ] **sndbuf/rcvbuf** 및 **sysctl 상한**을 BDP에 맞춰 조정(오토튜닝 확인).  
- [ ] **TCP_INFO** 로 **rtt, cwnd, retrans, rcv_space** 를 수시 관측.  
- [ ] **프레이밍**과 **백프레셔 전파**(프로토콜 레벨)로 생산/소비 균형 유지.  
- [ ] **Nagle/TCP_NODELAY** 는 **계측 기반**으로 선택.  
- [ ] **손실률**이 관측되면(재전송↑) 라우팅/큐/무선 구간을 점검, 필요 시 **FEC/재시도 정책** 검토.  
- [ ] **tc netem** 등으로 사전 성능·회복력 테스트(Chaos/Impairment Test).

---

### 6.13 보너스: **에코 서버**에 BDP 의식 적용(간단 예)

- 서버가 **큰 응답**을 보낼 때:
  - `SO_SNDBUF` 를 적절히 키우고,
  - 응답을 **청크 단위**로 보내되 **send_all** 루틴으로 **부분 쓰기**를 흡수.
- 클라이언트가 **느릴** 경우:
  - 서버 응답 스레드가 **send()에서 블록** → 상위 레이어 큐에 **백프레셔 전파**(생산 속도 제한).

```cpp
// 응답 전송 전 힌트 (예: 2MiB)
set_sndbuf(client_fd, 2<<20);
// 긴 응답을 128KiB씩 보내기 (coalescing + backpressure 흡수)
const size_t CHUNK = 128<<10;
for (size_t off=0; off<resp.size(); off+=CHUNK) {
    size_t n = std::min(CHUNK, resp.size()-off);
    auto ok = send_all(client_fd, std::as_bytes(std::span(resp.data()+off, n)));
    if (!ok) { /* 에러 처리(상대 종료/네트워크 문제) */ break; }
}
```

---

## 6.14 마무리

- **커널 버퍼/흐름/혼잡**은 별개가 아니다. **송신버퍼 ↔ rwnd ↔ cwnd** 의 **삼각관계**가 **처리량/지연**을 결정한다.
- **BDP**로 **필요 윈도우**를 빠르게 가늠하고, **TCP_INFO**로 실제 동작을 숫자로 확인하라.
- **손실률**이 조금만 높아도(특히 **RTT가 클수록**) 처리량은 **비선형적으로** 떨어진다.
- 실험은 어렵지 않다: 간단한 **스루풋 프로그램** + **tc netem** 으로 버퍼/RTT/손실 파라미터를 바꿔가며 **체감 학습**을 하자.  
  이 직관이 쌓이면, 실무에서 “왜 여기서 느려지는가?”를 **수식과 숫자**로 설명하고 **정확히 튜닝**할 수 있다.
