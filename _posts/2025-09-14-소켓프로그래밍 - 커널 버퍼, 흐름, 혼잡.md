---
layout: post
title: 소켓프로그래밍 - 커널 버퍼, 흐름, 혼잡
date: 2025-09-14 20:25:23 +0900
category: 소켓프로그래밍
---
# TCP 커널 버퍼·흐름 제어·혼잡 제어 — BDP 기반 성능 직관

## 0) 큰 그림 요약 — 애플리케이션 ↔ 커널 버퍼 ↔ 네트워크

먼저, 원래 정리해 둔 큰 그림을 다시 한 번 구조화해서 적어보자.

```text
   [App send()]            [Kernel]             [Network]         [Kernel]            [App recv()]
   ─────────────→ (송신버퍼) → TCP 세그먼트 → ─────────→ (수신버퍼) ───────────→ ────────────────
                     ^                ^                     ^                ^
                     |                |                     |                |
                흐름제어(rwnd)   혼잡제어(cwnd)        혼잡제어(cwnd)   흐름제어(rwnd)
```

- **송신 커널 버퍼 (send buffer)**  
  - 애플리케이션의 `send()`/`write()` 호출은 먼저 이 버퍼에 적힌다.
  - 이 버퍼에서 TCP가 **MSS 단위 세그먼트**로 잘라 네트워크로 흘려보낸다.
- **수신 커널 버퍼 (receive buffer)**  
  - 네트워크에서 들어온 페이로드는 일단 여기에 쌓인다.
  - 애플리케이션이 `recv()`를 늦게 호출하면 이 버퍼가 가득 차고, 리모트에 **작은 윈도우(rwnd)** 를 광고해 송신 속도를 줄인다.
- **흐름 제어 (Flow Control)**  
  - **수신 측**이 광고하는 **rwnd(Receive Window)** 로 구현.
  - “내가 지금 더 받을 수 있는 바이트가 이 정도야” 를 송신 측에 계속 알려주는 메커니즘.
- **혼잡 제어 (Congestion Control)**  
  - **네트워크 경로의 혼잡 상태**(손실·지연 등)를 반영해 송신 측이 잡는 **cwnd(Congestion Window)**.
  - cwnd는 “네트워크 상에 동시에 떠 있어도 괜찮은(손실 없이 버틸 수 있는) 데이터 양”에 대한 송신 측의 추정치.

이 둘을 합치면, 실제 한 시점에 날아갈 수 있는 **실효 송신 윈도우**는

$$
\text{win}_\text{eff} = \min\{\text{cwnd},\ \text{rwnd}\}
$$

이고, 이 값이 충분히 크지 않으면 링크는 놀고, 처리량은 떨어진다.

---

## 1) 송수신 커널 버퍼와 백프레셔(backpressure)

### 1.1 송신 버퍼 고갈 → `send()`가 막히는 모습

**송신 버퍼가 꽉 차는 이유**

1. 네트워크/상대가 느리다  
   - cwnd가 작다(혼잡 제어가 아직 충분히 윈도우를 키우지 못했거나 손실 때문에 줄었다).
   - 상대의 rwnd가 작다(수신 버퍼가 비워지지 않는다).
   - RTT가 길다(ACK가 늦게 돌아온다).
2. 애플리케이션이 너무 빠르게 쓴다  
   - 고속 생산자(producer)가 저속 소비자(consumer)보다 훨씬 빠르게 데이터를 생성.

**결과**

- **블로킹 소켓**  
  - `send()` 호출이 **블록**된다(커널 버퍼에 빈자리가 날 때까지 기다림).
  - 혹은 **일부만 쓰고** 바로 반환(부분 쓰기).
- **논블로킹 소켓**  
  - `send()` 가 **-1/EAGAIN** 을 반환.
  - 이벤트 루프에서는 **POLLOUT/EPOLLOUT** 이벤트를 기다렸다가 다시 시도해야 한다.

이 모든 현상은 결국 “**백프레셔(backpressure)** 가 위로 전파되고 있다”는 신호이다.  
프로듀서 쪽에서 “이 이상 빨리 보내지 말라”는 압력이 걸리고 있다고 보면 된다.

### 1.2 수신 버퍼 고갈 → rwnd 축소와 Zero Window

수신 쪽에서도 비슷한 일이 일어난다.

- 애플리케이션이 `recv()`를 자주 호출하지 않으면, 커널 **수신 버퍼가 가득 찬다.**
- 이때 TCP는 송신 측에 광고하는 **rwnd**를 점점 줄인다.
- 극단적으로는 **0** 이 되어, 송신 측은 더 이상 보낼 수 없고, 주기적으로 **Zero-Window Probe(ZWP)** 를 보내며 “이제 좀 열렸냐?” 를 묻는다.

그래서 수신 측에서

- `recv()`를 **너무 큰 버퍼로 너무 자주** 호출하는 것도 문제지만,
- 반대로 **오랫동안 호출하지 않는 것**도 곧바로 **백프레셔 → 처리량 하락**으로 이어진다.

특히 **서버 애플리케이션**에서

- 상위 계층 큐(예: job queue)만 보고 “아, 아직 버틸만 하네”라고 느슨하게 처리하면,
- 커널 수신 버퍼가 이미 꽉 차 있고, 그 영향이 네트워크를 타고 **클라이언트까지 전파**될 수 있다.

---

## 2) 커널 버퍼의 자동 튜닝과 한계

### 2.1 오토튜닝 개념

현대 리눅스/FreeBSD는 소켓 버퍼에 대해 **자동 튜닝(auto tuning)** 을 수행한다.

- `SO_SNDBUF`, `SO_RCVBUF`를 설정하지 않아도, 커널은 RTT와 트래픽 패턴을 보면서 **버퍼 크기를 점차 늘려** 준다.
- 다만, 그 상한은 시스템 전역 설정에 의해 제한된다.
  - 리눅스 예: `net.ipv4.tcp_rmem`, `net.ipv4.tcp_wmem` (최소/기본/최대)

그래서 실무에서는

- 오토튜닝이 동작하고 있는지,
- 상한값이 충분히 큰지,

를 먼저 확인하고 필요할 때 **힌트(hint)** 를 줘서 **초기값/최대값을 끌어올리는 방식**을 많이 사용한다.

### 2.2 소켓 옵션: `SO_SNDBUF`, `SO_RCVBUF`

C++23 코드에서 도우미 함수를 만들어 두면 좋다.

```cpp
#include <sys/socket.h>

inline void set_sndbuf(int fd, int bytes) {
    ::setsockopt(fd, SOL_SOCKET, SO_SNDBUF, &bytes, sizeof(bytes));
}

inline void set_rcvbuf(int fd, int bytes) {
    ::setsockopt(fd, SOL_SOCKET, SO_RCVBUF, &bytes, sizeof(bytes));
}
```

주의할 점:

- 실제 적용되는 값은 커널 구현에 따라 **2배** 등으로 조정되어 표시될 수 있다.
- 너무 크게 잡으면 윈도우는 충분히 나오지만, **버퍼블로트(bufferbloat)** 로 지연·지터가 커질 수 있다.
  - “필요한 만큼만” 키우고, RTT/지연을 반드시 함께 관측해야 한다.

---

## 3) RTT와 BDP — “파이프를 채워야” 처리량이 나온다

원래 적어 둔 핵심 수식:

$$
\text{BDP} = \text{Bandwidth} \times \text{RTT}
$$

단위로 풀어보면:

- 대역폭(Bandwidth) [비트/초 또는 바이트/초]
- RTT [초]
- BDP [비트 또는 바이트]

즉, **어떤 순간에 네트워크 파이프 안에 떠 있어야 하는 데이터 양**이다.

### 3.1 직관: BDP가 작은데 윈도우만 키우면?

반대로 표현하면,

- **유효 윈도우**가 BDP보다 작으면:
  - 송신자는 일정량을 보내고 ACK를 기다리느라 그 사이에 링크가 비게 된다.
  - 처리량은 링크 이론 최대치보다 훨씬 낮게 나온다.
- **유효 윈도우**가 BDP보다 충분히 크면:
  - 파이프가 항상 채워져 있고, 새로운 세그먼트가 나갈 때마다 다른 세그먼트가 도착해 ACK를 보내주므로 링크에 가까운 처리량을 얻을 수 있다.

이를 식으로 적으면, 대략적인 성능 조건은

$$
\min\{\text{cwnd}, \text{rwnd}\} \gtrsim \text{BDP}
$$

에 더해,

- 소켓 **sndbuf/rcvbuf** 및 커널 오토튜닝 상한도 BDP보다 충분히 커야 한다.

### 3.2 수치 예시 1 — 100 Mbps, RTT 50 ms

- 대역폭: 100 Mbps = \(100\times 10^6\) 비트/초
- RTT: 0.05 초

$$
\text{BDP} = 100\times 10^6\ \text{b/s} \times 0.05\ \text{s}
           = 5\times 10^6\ \text{b}
           \approx 625\ \text{kB}
$$

즉, 이 링크를 포화시키려면 **인플라이트 데이터**가 적어도 **수백 kB(예: 600~800 kB)** 정도는 되어야 한다는 뜻이다.

- cwnd, rwnd, sndbuf, rcvbuf의 최소값이 **이 정도 이상** 나오도록 잡아주지 않으면, 100 Mbps를 다 쓰지 못한다.

### 3.3 수치 예시 2 — 1 Gbps, RTT 100 ms

- 대역폭: 1 Gbps = \(10^9\) 비트/초
- RTT: 0.1 초

$$
\text{BDP} = 10^9\ \text{b/s} \times 0.1\ \text{s}
           = 10^8\ \text{b}
           \approx 12.5\ \text{MB}
$$

여기서는 **수 MB~수십 MB** 급의 윈도우가 필요하다.

- 이 수준의 링크(예: 장거리 1 Gbps WAN)에서 소켓 버퍼를 몇백 kB 정도로만 잡아두면,
  - cwnd가 아무리 커져도 **sndbuf/rcvbuf가 병목**이 되어 버린다.
- 반대로, 버퍼를 충분히 키우지 못한 채 “성능이 안 나와요”라고 고민하는 패턴이 실제로 굉장히 많다.

---

## 4) 손실과 처리량: Reno 근사식으로 보는 직관

고전적인 TCP Reno 모델에서 손실 기반 처리량 근사식은 다음과 같다.

$$
\text{Throughput} \approx \frac{\text{MSS}}{\text{RTT}\sqrt{p}}
$$

여기서

- \( \text{MSS} \): Maximum Segment Size (페이로드 최대 크기, 보통 약 1448B 근처)
- \( \text{RTT} \): 왕복 지연 시간
- \( p \): 패킷 손실 확률

이 식은 엄밀하게는 여러 가정 아래에서 나온 근사식이지만, **직관을 주는 데는 매우 유용**하다.

- 동일한 RTT에서, 손실률 \(p\)가 증가하면 처리량은 \(1/\sqrt{p}\) 비율로 **비선형적으로 급락**한다.
- 동일한 손실률이라도, RTT가 두 배가 되면 처리량은 거의 두 배 떨어진다.

현대 혼잡 제어들(CUBIC, BBR 등)은 Reno와는 다른 공식을 쓰지만,

- “**손실이 조금만 있어도, 특히 RTT가 클수록 대역폭 손실이 심하다**”는 방향성은 그대로 유효하다.
- 실험에서 손실률을 0%, 0.1%, 0.5%, 1%로 바꿔보면 이 감을 금방 얻을 수 있다.

---

## 5) 백프레셔가 애플리케이션에 드러나는 패턴

### 5.1 송신 측에서 보이는 현상

- `send()` 호출이 **평소보다 길게 블록**하거나, 한 번에 쓰는 바이트 수가 줄어든다.
- 논블로킹 소켓에서는 EAGAIN 발생 빈도가 눈에 띄게 증가한다.
- 이벤트 루프(예: epoll)에서 **EPOLLOUT** 이벤트가 예전보다 자주/늦게 온다.

이를 관측하는 간단한 방법:

- 송신 측에서 `send()`가 반환하는 바이트 수와 소요 시간을 로그에 찍는다.
- 일정 주기로 `TCP_INFO`를 읽어서 snd_cwnd, rtt, rcv_space 등의 변화를 함께 관찰한다.

### 5.2 수신 측에서 보이는 현상

- `recv()` 호출이 평소보다 오래 블록된다.
- 애플리케이션 내부 큐(예: 메시지 큐, 작업 큐)가 눈에 띄게 쌓인다.
- 서버라면 **스레드/코루틴 수를 늘려도 생각만큼 스루풋이 늘지 않는다.** (네트워크가 이미 병목)

### 5.3 설계 대응: 백프레셔 전파

- 프레이밍 레벨에서 **프로듀서-컨슈머 모델**을 명확히 두고, **크레딧 기반** 또는 **윈도우 기반**으로 조절한다.
  - 예: “한 번에 최대 N개의 프레임만 outstanding하게 두고, ACK/응답이 오면 그만큼 크레딧을 회복하는 구조”.
- 서버에서
  - “요청을 무조건 다 받아서 큐에 쌓아두는 것”보다,
  - **큐 길이 기준으로 바로 접속 제한**(accept throttle) 혹은 **프로토콜 레벨 backpressure**를 걸어주는 편이 전체 레이턴시와 안정성 측면에서 낫다.

---

## 6) `TCP_INFO`로 RTT·cwnd·손실률 관측하기

원래 정리했던 C++ 코드에, 약간 더 설명을 붙여보자.

```cpp
#include <netinet/tcp.h>
#include <sys/socket.h>
#include <print>
#include <string_view>

void log_tcp_info(int fd, std::string_view tag) {
    tcp_info info{};
    socklen_t len = sizeof(info);
    if (::getsockopt(fd, IPPROTO_TCP, TCP_INFO, &info, &len) == 0) {
        // 리눅스 기준: rtt/rttvar는 마이크로초 단위
        std::print("[tcp_info][{}] rtt={}us rttvar={}us cwnd={} "
                   "rcv_space={} bytes snd_ssthresh={} retrans={}\n",
                   tag,
                   info.tcpi_rtt,
                   info.tcpi_rttvar,
                   info.tcpi_snd_cwnd,
                   info.tcpi_rcv_space,
                   info.tcpi_snd_ssthresh,
                   info.tcpi_total_retrans);
    }
}
```

여기서 중요한 필드:

- `tcpi_rtt` / `tcpi_rttvar`: RTT 평균과 분산(마이크로초 단위)
- `tcpi_snd_cwnd`: 현재 송신 cwnd (세그먼트 개수 단위, MSS 기준)
- `tcpi_rcv_space`: 수신 측이 광고 중인 윈도우 크기(바이트)
- `tcpi_snd_ssthresh`: slow-start에서 congestion avoidance로 넘어가는 경계 값
- `tcpi_total_retrans`: 지금까지 재전송된 세그먼트 수

이 값을 로그와 함께 저장해 두면,

- 특정 시점의 처리량 문제를 디버깅할 때
  - RTT가 갑자기 폭증했는지,
  - cwnd가 줄어들어 있는지,
  - rcv_space가 갑자기 줄어들었는지,
  - 재전송이 급격히 증가했는지

를 숫자로 확인할 수 있다.

---

## 7) 소켓 옵션 도우미 (버퍼·타임아웃·TCP_NODELAY)

원래 정리했던 간단한 도우미 코드에 약간 더 붙여서, C++23 스타일로 정리해보자.

```cpp
#include <sys/socket.h>
#include <netinet/tcp.h>
#include <chrono>

inline void set_sndbuf(int fd, int bytes) {
    ::setsockopt(fd, SOL_SOCKET, SO_SNDBUF, &bytes, sizeof(bytes));
}

inline void set_rcvbuf(int fd, int bytes) {
    ::setsockopt(fd, SOL_SOCKET, SO_RCVBUF, &bytes, sizeof(bytes));
}

inline void set_nodelay(int fd, bool on = true) {
    int v = on ? 1 : 0;
    ::setsockopt(fd, IPPROTO_TCP, TCP_NODELAY, &v, sizeof(v));
}

inline timeval to_timeval(std::chrono::milliseconds ms) {
    timeval tv{};
    tv.tv_sec  = static_cast<long>(ms.count() / 1000);
    tv.tv_usec = static_cast<long>((ms.count() % 1000) * 1000);
    return tv;
}

inline void set_timeouts_ms(int fd, int recv_ms, int send_ms) {
    auto r = to_timeval(std::chrono::milliseconds(recv_ms));
    auto s = to_timeval(std::chrono::milliseconds(send_ms));
    ::setsockopt(fd, SOL_SOCKET, SO_RCVTIMEO, &r, sizeof(r));
    ::setsockopt(fd, SOL_SOCKET, SO_SNDTIMEO, &s, sizeof(s));
}
```

실무 팁:

- `SO_RCVTIMEO`, `SO_SNDTIMEO`는
  - “이 시간 동안 읽기/쓰기 진행이 없으면 시스템 콜을 실패로 돌려라”는 의미에 가깝다.
  - 논블로킹 + poll/epoll 기반 데드라인 루프와는 성격이 다르므로, 둘 중 하나를 채택해서 일관되게 쓰는 편이 좋다.
- `TCP_NODELAY`:
  - 작은 메시지를 빈번하게 보내는 RPC, 명령/응답 패턴에서 레이턴시를 줄이는 데 유리.
  - 대신 세그먼트 수가 늘어날 수 있어 CPU와 네트워크에 부담이 될 수 있으므로 **계측 기반 선택**이 필요하다.

---

## 8) 처리량 측정을 위한 C++23 스루풋 테스트 (서버/클라이언트)

이미 작성해 둔 코드에 **설명과 옵션**을 더해 “실제 실험에 바로 쓸 수 있는 형태”로 정리한다.

### 8.1 수신 전용 서버 — 받은 데이터를 버리고 처리량만 측정

```cpp
// thr_server.cpp
// 빌드: g++ -std=c++23 -O2 thr_server.cpp -o thr_server

#include <arpa/inet.h>
#include <netdb.h>
#include <sys/socket.h>
#include <unistd.h>

#include <print>
#include <vector>
#include <string>
#include <chrono>
#include <cstring>

int main(int argc, char** argv) {
    const char* bind_host = (argc > 1 ? argv[1] : nullptr); // null → any
    const char* port      = (argc > 2 ? argv[2] : "9000");

    addrinfo hints{}, *res = nullptr;
    hints.ai_family   = AF_UNSPEC;
    hints.ai_socktype = SOCK_STREAM;
    hints.ai_flags    = AI_PASSIVE | AI_ADDRCONFIG;

    if (int rc = ::getaddrinfo(bind_host, port, &hints, &res); rc != 0) {
        std::print(stderr, "[server] getaddrinfo: {}\n", gai_strerror(rc));
        return 1;
    }

    int lfd = -1;
    for (auto* ai = res; ai; ai = ai->ai_next) {
        lfd = ::socket(ai->ai_family, ai->ai_socktype, ai->ai_protocol);
        if (lfd < 0) continue;

        int yes = 1;
        ::setsockopt(lfd, SOL_SOCKET, SO_REUSEADDR, &yes, sizeof(yes));

        if (::bind(lfd, ai->ai_addr, ai->ai_addrlen) == 0 &&
            ::listen(lfd, 128) == 0) {
            break;
        }
        ::close(lfd);
        lfd = -1;
    }
    ::freeaddrinfo(res);

    if (lfd < 0) {
        std::print(stderr, "[server] listen failed\n");
        return 1;
    }

    std::print("[server] listening on port {}\n", port);

    sockaddr_storage ss{};
    socklen_t slen = sizeof(ss);
    int cfd = ::accept(lfd, (sockaddr*)&ss, &slen);
    if (cfd < 0) {
        std::print(stderr, "[server] accept failed: {}\n", std::strerror(errno));
        return 1;
    }
    ::close(lfd);

    // 필요하다면 여기서 set_rcvbuf(cfd, ...), set_sndbuf(cfd, ...) 가능
    std::vector<char> buf(1 << 20); // 1MiB 버퍼

    auto t0 = std::chrono::steady_clock::now();
    unsigned long long total = 0;

    for (;;) {
        ssize_t n = ::recv(cfd, buf.data(), buf.size(), 0);
        if (n > 0) {
            total += static_cast<unsigned long long>(n);
            continue;
        }
        if (n == 0) {
            // 클라이언트가 정상 종료
            break;
        }
        if (errno == EINTR) {
            continue;
        }
        std::print(stderr, "[server] recv error: {}\n", std::strerror(errno));
        break;
    }

    auto t1 = std::chrono::steady_clock::now();
    double sec = std::chrono::duration<double>(t1 - t0).count();
    double gbps = (total * 8.0) / (sec * 1e9);

    std::print("[server] received={} bytes, time={:.3f}s, rate={:.3f} Gbps\n",
               total, sec, gbps);

    ::close(cfd);
    return 0;
}
```

### 8.2 송신 전용 클라이언트 — 총 N GiB를 전송해 처리량 측정

```cpp
// thr_client.cpp
// 빌드: g++ -std=c++23 -O2 thr_client.cpp -o thr_client

#include <arpa/inet.h>
#include <netdb.h>
#include <sys/socket.h>
#include <unistd.h>

#include <print>
#include <vector>
#include <string>
#include <chrono>
#include <cstring>
#include <cstdlib>

void set_sndbuf(int fd, int bytes){
    ::setsockopt(fd, SOL_SOCKET, SO_SNDBUF, &bytes, sizeof(bytes));
}
void set_rcvbuf(int fd, int bytes){
    ::setsockopt(fd, SOL_SOCKET, SO_RCVBUF, &bytes, sizeof(bytes));
}
void set_nodelay(int fd, bool on){
    int v = on ? 1 : 0;
    ::setsockopt(fd, IPPROTO_TCP, TCP_NODELAY, &v, sizeof(v));
}

int main(int argc, char** argv) {
    if (argc < 5) {
        std::print(stderr,
            "usage: {} <host> <port> <gigabytes> <buf_kib> [nodelay=0|1] [sndbuf_kib] [rcvbuf_kib]\n",
            argv[0]);
        return 1;
    }

    const char* host = argv[1];
    const char* port = argv[2];
    double gigabytes = std::stod(argv[3]);
    size_t buf_kib   = std::stoul(argv[4]);
    bool nodelay     = (argc > 5 && std::stoi(argv[5]) != 0);

    int sndbuf_kib = (argc > 6 ? std::stoi(argv[6]) : 4 * 1024);   // default 4MiB
    int rcvbuf_kib = (argc > 7 ? std::stoi(argv[7]) : 4 * 1024);

    addrinfo hints{}, *res = nullptr;
    hints.ai_family   = AF_UNSPEC;
    hints.ai_socktype = SOCK_STREAM;
    hints.ai_flags    = AI_ADDRCONFIG;

    if (int rc = ::getaddrinfo(host, port, &hints, &res); rc != 0) {
        std::print(stderr, "[client] getaddrinfo: {}\n", gai_strerror(rc));
        return 1;
    }

    int s = -1;
    for (auto* ai = res; ai; ai = ai->ai_next) {
        s = ::socket(ai->ai_family, ai->ai_socktype, ai->ai_protocol);
        if (s < 0) continue;

        set_sndbuf(s, sndbuf_kib * 1024);
        set_rcvbuf(s, rcvbuf_kib * 1024);
        set_nodelay(s, nodelay);

        if (::connect(s, ai->ai_addr, ai->ai_addrlen) == 0) {
            break;
        }
        ::close(s);
        s = -1;
    }
    ::freeaddrinfo(res);

    if (s < 0) {
        std::print(stderr, "[client] connect failed: {}\n", std::strerror(errno));
        return 1;
    }

    std::vector<char> buf(buf_kib * 1024, 'x');
    unsigned long long target = static_cast<unsigned long long>(gigabytes * 1024 * 1024 * 1024ULL);
    unsigned long long total  = 0;

    auto t0 = std::chrono::steady_clock::now();
    while (total < target) {
        size_t tosend = std::min<unsigned long long>(buf.size(), target - total);
        ssize_t n = ::send(s, buf.data(), tosend, 0);
        if (n > 0) {
            total += static_cast<unsigned long long>(n);
            continue;
        }
        if (n == 0) {
            break;
        }
        if (errno == EINTR) {
            continue;
        }
        std::print(stderr, "[client] send error: {}\n", std::strerror(errno));
        break;
    }

    ::shutdown(s, SHUT_WR);

    char tmp[4096];
    while (::recv(s, tmp, sizeof(tmp), 0) > 0) {
        // drain
    }

    auto t1 = std::chrono::steady_clock::now();
    double sec = std::chrono::duration<double>(t1 - t0).count();
    double gbps = (total * 8.0) / (sec * 1e9);

    std::print("[client] sent={} bytes, time={:.3f}s, rate={:.3f} Gbps, nodelay={}, sndbuf={}KiB, rcvbuf={}KiB\n",
               total, sec, gbps, nodelay, sndbuf_kib, rcvbuf_kib);

    ::close(s);
    return 0;
}
```

이 두 프로그램은 “**순수 송·수신 경로 처리량**을 측정하는 최소 실험기”다.

- RTT/손실이 거의 없는 루프백 환경에서는
  - CPU·메모리·NIC 성능이 주로 보인다.
- `tc netem` 등으로 인위적인 RTT/손실을 추가하면
  - BDP, cwnd, rwnd, sndbuf/rcvbuf, 손실률이 처리량에 미치는 영향을 눈으로 확인할 수 있다.

---

## 9) `tc netem`으로 RTT·손실·지터 실험 만들기

### 9.1 기본적인 `tc netem` 설정

실험용 인터페이스(예: veth pair 혹은 특정 NIC)에 다음과 같이 적용할 수 있다.

```bash
# RTT 50ms 정도를 만들고 싶을 때 (one-way delay 25ms 모델)
sudo tc qdisc add dev eth0 root netem delay 25ms

# 손실 0.5% 추가
sudo tc qdisc change dev eth0 root netem delay 25ms loss 0.5%

# 지터 5ms 정도 추가 (정규분포)
sudo tc qdisc change dev eth0 root netem delay 25ms 5ms distribution normal

# 설정 제거
sudo tc qdisc del dev eth0 root
```

실무에서는 실제 NIC에 걸기보다는

- **네임스페이스(netns)** 를 두 개 만들고
- 그 사이를 **veth pair**로 연결한 뒤
- 그 veth에 netem을 걸어주는 방식을 추천한다.

이렇게 하면

- 실험 트래픽만 환경을 변경할 수 있고,
- 호스트 전체 네트워크에 영향을 주지 않는다.

### 9.2 실험 시나리오 예시

#### (A) RTT만 변경하며 처리량 측정

1. 서버 실행

   ```bash
   ./thr_server 0.0.0.0 9000
   ```

2. 클라이언트 실행 (동일 머신 혹은 다른 머신에서)

   ```bash
   ./thr_client <server_ip> 9000 2 1024 1
   # 2 GiB 전송, 앱 버퍼 1MiB, TCP_NODELAY=1
   ```

3. `tc` 로 RTT를 단계적으로 변경

   - 10ms → 50ms → 100ms → 200ms …

4. 각 단계에서

   - thr_client, thr_server 출력의 처리량(Gbps)을 비교
   - 동시에 `TCP_INFO` 로그를 찍어
     - RTT가 잘 설정되었는지
     - cwnd가 어느 정도까지 성장하는지
     - rcv_space가 충분한지
     - 재전송이 발생하는지

   를 관찰한다.

#### (B) 소켓 버퍼 크기 변화의 영향

- RTT를 크게 잡은 상태에서(예: 100ms)
- `sndbuf_kib` / `rcvbuf_kib` 를 256KiB → 1MiB → 4MiB → 16MiB 등으로 바꿔가며 실행:

```bash
./thr_client <server_ip> 9000 2 1024 1 256 256
./thr_client <server_ip> 9000 2 1024 1 1024 1024
./thr_client <server_ip> 9000 2 1024 1 4096 4096
./thr_client <server_ip> 9000 2 1024 1 16384 16384
```

- 처리량이 어느 정도까지 늘다가 포화되는지,
- 그 포화 지점이 BDP에 대략 맞는지,
- 그 이상에서는 RTT가 더 늘어나지 않는지/늘어나는지

를 함께 확인한다.

#### (C) 손실률 변화

- RTT를 적당히 고정(예: 50ms)
- 손실률 0%, 0.1%, 0.5%, 1% 등으로 바꾸며 실험:

```bash
sudo tc qdisc change dev eth0 root netem delay 25ms loss 0%
sudo tc qdisc change dev eth0 root netem delay 25ms loss 0.1%
sudo tc qdisc change dev eth0 root netem delay 25ms loss 0.5%
sudo tc qdisc change dev eth0 root netem delay 25ms loss 1%
```

각 손실률에서

- 처리량이 어떻게 떨어지는지,
- `TCP_INFO` 의 `tcpi_total_retrans` 가 어떻게 증가하는지,
- cwnd가 얼마나 자주/얼마나 크게 떨어지는지

를 관측하면, 앞에서 본

$$
\text{Throughput} \approx \frac{\text{MSS}}{\text{RTT}\sqrt{p}}
$$

직관과 잘 맞아떨어지는 것을 확인할 수 있다.

#### (D) Nagle / Delayed ACK의 영향

- 작은 메시지(예: 64바이트)를 일정 간격으로 전송하는 코드를 하나 더 만들고
  - `TCP_NODELAY` ON/OFF
  - OS의 Delayed ACK 정책 유무
- 두 경우의 레이턴시/처리량을 비교하면

“작은 메시지를 많이 보낼 때는 어떤 튜닝을 해야 하는지” 감을 쌓을 수 있다.

---

## 10) BDP 기반 계산 연습

### 10.1 예제 1 — 200 Mbps, RTT 80 ms

대역폭: 200 Mbps  
RTT: 80 ms

$$
\text{BDP}
= 200 \times 10^6\ \text{b/s} \times 0.08\ \text{s}
= 16 \times 10^6\ \text{b}
\approx 2\ \text{MB}
$$

따라서

- 최소한 **2MB 정도**의 유효 윈도우가 필요하다.
- 이를 만족하려면

  - `sndbuf`, `rcvbuf` 각각 2MB 이상 (여유를 봐서 4MB 정도)
  - `cwnd * MSS` 가 2MB 이상 (MSS = 1448B라면 cwnd ≈ 1500 세그먼트 수준)

정도가 필요하다.

### 10.2 예제 2 — 40 Mbps, RTT 5 ms

대역폭: 40 Mbps  
RTT: 5 ms

$$
\text{BDP}
\approx 40\times10^6\ \text{b/s} \times 0.005\ \text{s}
= 200{,}000\ \text{b}
\approx 25\ \text{kB}
$$

- BDP가 25kB 정도에 불과하므로, 소켓 버퍼를 수백 kB로 잡아도 **링크 포화에는 영향이 거의 없다**.
- 이 경우 중요한 것은
  - **작은 메시지 묶기(coalescing)**,
  - **시스템 호출 횟수 감소**,
  - **에코 패턴에서 Nagle/Delayed ACK의 영향을 줄이는 것**

에 더 가깝다.

---

## 11) 운영 관점 체크리스트 (정리)

- [ ] **목표 링크·RTT** 를 먼저 파악하고, 해당 환경의 **BDP** 를 계산했는가?
- [ ] **sndbuf/rcvbuf** 및 시스템 전역 상한(`tcp_rmem`, `tcp_wmem` 등)이 BDP에 비해 터무니없이 작지 않은가?
- [ ] `TCP_INFO` 로 `rtt`, `cwnd`, `rcv_space`, `total_retrans` 를 주기적으로 관측하고 있는가?
- [ ] 애플리케이션 프로토콜 레벨에서 **프레이밍**과 **백프레셔 전파**(크레딧·윈도우)를 설계했는가?
- [ ] 작은 메시지 중심 워크로드에서 **Nagle/TCP_NODELAY** 를 측정 기반으로 적절히 선택했는가?
- [ ] 손실률이 무시할 수 없을 정도로 관측된다면, 단순 소켓 튜닝이 아니라
  - 라우터/스위치 큐,
  - 무선 구간,
  - 과부하 링크
  를 점검하고 **혼잡 원인**을 제거하려는 노력을 하고 있는가?
- [ ] 배포 전에 `tc netem` 등을 이용한 **성능·회복력 테스트(impairment test)** 를 통해, RTT/손실/지터 변화에 대한 애플리케이션의 행동을 미리 확인했는가?

---

## 12) 응용 예 — 에코 서버에 BDP 의식 적용

마지막으로, 원래 예시로 들었던 “에코 서버가 큰 응답을 보낼 때”의 튜닝을 다시 정리해 보자.

```cpp
// 응답 전송 전 힌트 (예: 2MiB)
set_sndbuf(client_fd, 2 << 20);

// 긴 응답을 128KiB씩 보내기
const size_t CHUNK = 128 << 10;

for (size_t off = 0; off < resp.size(); off += CHUNK) {
    size_t n = std::min(CHUNK, resp.size() - off);
    auto ok = send_all(client_fd, std::as_bytes(std::span(resp.data() + off, n)));
    if (!ok) {
        // 상대 종료나 네트워크 문제 등 에러 처리
        break;
    }
}
```

여기서 핵심은 세 가지다.

1. **BDP를 의식한 sndbuf 설정**  
   - 링크·RTT를 알고 있으면 대략 필요한 BDP를 계산하고, 그 값 이상으로 sndbuf를 키운다.
2. **청크 단위 전송**  
   - 응답이 매우 클 때 한 번에 벡터 전체를 `send()` 하더라도 결국 커널 내부에서 청크로 나뉘지만,
   - 응용 단계에서 일정한 크기로 잘라 `send_all`을 사용하면, **부분 쓰기/블로킹**을 더 잘 흡수하고, 백프레셔를 상위 레이어로 전파하기 쉽다.
3. **에러 및 백프레셔 처리**  
   - `send_all` 실패 시, 클라이언트 종료나 네트워크 장애를 상위 레이어에 즉시 전달하기 좋다.
   - 만약 응답 생산자(예: 다른 스레드)가 너무 빨리 데이터를 밀어 넣고 있다면, 이 루프에서 느려지는 `send_all` 을 감지해 **상위 큐 길이 제한** 같은 조치로 연결할 수 있다.

---

## 13) 마무리

정리하면,

- **커널 송수신 버퍼**, **흐름 제어(rwnd)**, **혼잡 제어(cwnd)** 는 서로 분리된 개념이 아니라, 항상 **서로 얽혀서 실효 윈도우(win_eff)** 를 결정한다.
- 간단한 두 수식

  $$
  \text{BDP} = \text{Bandwidth} \times \text{RTT}
  $$

  $$
  \text{Throughput} \approx \frac{\text{MSS}}{\text{RTT}\sqrt{p}}
  $$

  만 머리에 넣고 있어도, 현장에서 “어디가 병목인지”를 추측하는 데 큰 도움이 된다.

- 여기에 `TCP_INFO`, 소켓 버퍼 튜닝(`SO_SNDBUF`/`SO_RCVBUF`), `tc netem` 기반 실험, 간단한 C++23 스루풋 측정 프로그램을 결합하면

  - “왜 여기서 느려지는지”
  - “버퍼를 더 키우는 게 맞는지”
  - “손실·RTT 어느 쪽이 더 큰 영향을 주는지”

  를 **수식과 숫자, 로그**로 확인하면서 설명할 수 있게 된다.

이 글의 내용은 이후에 만들고 있는 **C++23 스타일 TCP 프레이밍/RAII/`std::expected` 기반 네트워킹 유틸**과도 그대로 연결된다.  
프레이밍·백프레셔 설계(애플리케이션 레벨)와 BDP·cwnd·rwnd(커널·네트워크 레벨)를 한꺼번에 묶어서 보는 시야를 갖추면, 고속·고지연·손실 환경에서 “왜 이 시스템이 이렇게 동작하는지”를 설득력 있게 설명할 수 있다.