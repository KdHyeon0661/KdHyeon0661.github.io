---
layout: post
title: 데이터 통신 - Multicast Routing (1)
date: 2024-08-14 21:20:23 +0900
category: DataCommunication
---
# Multicast Routing (1)

## 21.1 Introduction

### Unicasting

**유니캐스팅(unicasting)** 은 우리가 가장 익숙한 통신 방식이다.

> 한 **송신자(Source)** 가 **한 수신자(Destination)** 에게 패킷을 보낸다. → **1 대 1(one-to-one)**

- IP 관점: **목적지 IP 주소 = 하나의 호스트 주소**
- 라우터는 **목적지 주소만** 보고 **단일 인터페이스**로 포워딩
- TCP 연결 대부분이 유니캐스트 (웹, SSH, 이메일 등)

#### 예제 1: 서버가 3명에게 동영상을 보낼 때

동영상 서버 S가 4Mbps 스트림을 3명의 클라이언트 A, B, C에게 전송한다고 하자.

- 유니캐스트 방식: S는 **3개의 TCP/UDP 세션**을 각각 생성

$$
\text{총 전송률} = 3 \times 4\,\text{Mbps} = 12\,\text{Mbps}
$$

중간 라우터의 링크가 다음과 같다고 해보자.

```text
S ---[100 Mbps 링크 L1]--- R ---[10 Mbps 링크 L2]--- (A,B,C 쪽)
```

- L1: 12 Mbps 사용 (괜찮음)
- L2: 12 Mbps가 흘러야 함 → **10 Mbps 링크에 과부하**

→ **같은 콘텐츠를 여러 번 복제**하기 때문에, 수신자가 많아질수록  
중간 링크에 심각한 부하가 걸린다.

---

### Multicasting

**멀티캐스팅(multicasting)** 은 **하나의 소스가 “그룹”에게** 데이터를 보내는 1대N 통신이다.

> 한 소스 → 관심 있는 수신자 그룹 (one-to-many 또는 many-to-many)

- IP 관점:  
  - **소스 IP**: 보통 유니캐스트 주소  
  - **목적지 IP**: **멀티캐스트 그룹 주소** (예: 239.1.1.1)
- 같은 패킷이 **라우터에서 복제**되어 여러 방향으로 분기  
- 네트워크는 **“최소 중복”이 되도록 트리 구조**(멀티캐스트 트리)를 만든다.

#### 예제 2: 위 예제를 멀티캐스트로 바꾸면

동일한 상황에서, S가 239.1.1.1 그룹으로 4Mbps 스트림을 보낸다고 하자.  
A, B, C는 모두 239.1.1.1 그룹에 가입(IGMP/MLD)해 있다.

- S → R 방향(링크 L1): **4 Mbps** (한 번만 전송)
- R → A/B/C로 분기되는 링크 L2에서는  
  - R이 패킷을 **로컬에서 복제**해서 A/B/C 각 포트로 내보낸다.
  - L2의 총 사용률은 여전히 4 Mbps (단, 스위치 내부 포트별 복제는 별도)

따라서

- 유니캐스트 3개: L1 = 12 Mbps, L2 = 12 Mbps  
- 멀티캐스트 1개: L1 = 4 Mbps, L2 = 4 Mbps

**네트워크 내부 복제 지점이 “수신자 근처”로 끌려 내려온다는 점**이 핵심이다.  
이는 IETF/운영자 문서들에서 IPTV, 주식 시세 방송, 대규모 소프트웨어 배포 등에서  
멀티캐스트를 선호하는 이유로 반복해서 언급된다.  

---

### Broadcasting

**브로드캐스팅(broadcasting)** 은 **한 소스 → 같은 브로드캐스트 도메인의 모든 노드**에게 보내는 통신이다.

- IPv4 예:
  - **255.255.255.255**: 제한 브로드캐스트 (로컬 네트워크 전체)
  - **192.168.1.255**: 서브넷 브로드캐스트 (마스크 기반)
- 이더넷 2계층:
  - MAC 주소 **ff:ff:ff:ff:ff:ff**

브로드캐스트의 특징:

- 수신 여부와 관계없이 **해당 도메인 전체가 패킷을 받는다** → 불필요한 부하
- 라우터는 일반적으로 브로드캐스트 프레임을 **다른 서브넷으로 포워딩하지 않는다**  
  (IP 브로드캐스트는 **라우터 경계를 넘지 않는** 것이 보통 정책)  

IPv6에서는 **브로드캐스트가 아예 제거되고, 멀티캐스트로 대체**되었다.  
예를 들어, “모든 노드” 대상 패킷은 ff02::1 멀티캐스트 주소로 전송한다.  

---

### Unicast vs Multicast vs Broadcast 정리

| 방식 | 관계 | 대상 | 네트워크 사용 |
|------|------|------|----------------|
| Unicast | 1 → 1 | 단일 호스트 | 수신자마다 별도 스트림 |
| Multicast | 1 → N | 그룹 가입 노드 | **트리 기반 복제**, 효율적 |
| Broadcast | 1 → 모든 노드 | 브로드캐스트 도메인 전체 | 가장 많은 부하, IPv6에서 제거 |

멀티캐스트는 **“필요한 노드만” 브로드캐스트 수준으로 효율적으로** 받도록 하는 방법이라고 보면 된다.

---

## 21.2 Multicasting Basics

이제 멀티캐스트 라우팅의 구체적인 빌딩 블록들을 본다.

1. **Multicast addresses** — 그룹을 어떻게 표현하는가  
2. **Delivery at data link layer** — L2에서 어떤 MAC 주소로 보내는가  
3. **Collecting information about groups** — 호스트가 어떻게 “나 이 그룹에 관심 있어요”를 알리는가  
4. **Multicast forwarding** — 라우터가 패킷을 어떤 인터페이스로 복제해서 내보낼지  
5. **Two approaches to multicasting** — 트리를 어떻게 구성할지 (소스 기반 / 그룹 공유)

---

## 21.2.1 Multicast Addresses

### IPv4 Multicast (Class D)

IPv4에서 멀티캐스트 주소는 **224.0.0.0 ~ 239.255.255.255** 범위로 정의된다.  

- 예전 Classful 표기에서는 “Class D 주소”라고 불렀던 범위
- 이 전체 범위는 **IANA IPv4 Multicast Address Space** 로 관리되며,  
  RFC 5771 등의 지침에 따라 여러 블록으로 나뉜다.  

대표적인 블록:

| 범위 | 이름 | 용도 (예시) |
|------|------|------------|
| 224.0.0.0/24 | Local Network Control Block | OSPF(224.0.0.5/6), RIPv2(224.0.0.9) 등 라우팅 프로토콜, 링크 로컬 제어 트래픽 |
| 224.0.1.0/24 | Internetwork Control Block | NTP 등 네트워크 제어 트래픽 가능 |
| 232.0.0.0/8 | SSM(Source-Specific Multicast) Block | IGMPv3 + SSM용 주소 범위 |
| 233.0.0.0/8 | GLOP Block | 각 AS가 자체 멀티캐스트 그룹을 만들 때 사용 |
| 239.0.0.0/8 | Administratively Scoped | 사설 멀티캐스트 (회사 내부 등) |

→ 실습, 사내 IPTV, 사설 테스트용 멀티캐스트는 보통 **239.x.x.x** 대역에서 많이 사용한다.

#### 예제: IPTV 서버용 그룹 주소 선택

회사 내부 전용 IPTV 스트림을 만들고 싶다고 하자.

- 회사 내부에서만 사용 → **관리 범위(Admin-scoped)** 사용  
- 예를 들어, 239.10.1.1 을 **“사내 공지 방송 채널”** 로 정할 수 있다.

이 경우, 라우터/스위치는 “239.10.1.1로 향하는 멀티캐스트 트래픽은 회사 외부로 나가지 않도록”  
경계 라우터에서 필터를 걸 수 있다 (admin-scoped 규칙).

---

### IPv6 Multicast

IPv6에서는 **FF00::/8** 이 멀티캐스트 주소 범위이다.  

형식(단순화):

```text
1111 1111 | flgs | scope | group ID
    FF      x      x        ...
```

- **scope 필드**: 멀티캐스트 범위 (링크 로컬, 사이트 로컬, 글로벌 등)를 정의
  - ff02::/16: 링크 로컬 (예: ff02::1 = 모든 노드, ff02::2 = 모든 라우터)
- IPv6는 브로드캐스트가 제거되었기 때문에,  
  **ARP 대체인 NDP, 라우터 광고 등도 모두 멀티캐스트 주소**를 사용한다.

---

## 21.2.2 Delivery at Data Link Layer

IP 멀티캐스트 주소만으로는 부족하다.  
실제 프레임을 전송할 때는 **2계층(이더넷, Wi-Fi 등)의 목적지 MAC 주소**가 필요하다.

### IPv4 Multicast → Ethernet MAC 매핑

이더넷에서 멀티캐스트 MAC 주소의 형태는 다음과 같다.  

```text
01:00:5E:0x:xx:xx
```

- 상위 25비트: **고정값** 01-00-5E-0 (이더넷 멀티캐스트 블록)
- **하위 23비트**: IPv4 멀티캐스트 주소의 하위 23비트를 그대로 사용

> 즉, IP 멀티캐스트 주소의 마지막 23비트를  
> 이더넷 멀티캐스트 MAC의 마지막 23비트에 매핑.

#### 간단한 예제

IPv4 멀티캐스트 주소: **224.0.1.1**

1. 10진수 → 16진수  
   - 224 = E0  
   - 0 = 00  
   - 1 = 01  
   - 1 = 01 → `E0 00 01 01`  

2. 하위 23비트만 사용 → 결과적으로 `00 01 01` 부분에서 23비트 추출  

3. MAC 주소:

```text
IP:  224.0.1.1
MAC: 01:00:5E:00:01:01
```

실제 벤더 문서에서도 같은 예를 사용해 설명한다.  

> **중요**: 32비트 IP 중 23비트만 사용되기 때문에, **최대 32개 서로 다른 멀티캐스트 그룹이 동일한 MAC으로 매핑**될 수 있다.  
> (32:1 충돌 가능성) — 스위치는 상위 계층 정보(IGMP Snooping 등)를 함께 사용해 불필요한 flooding을 줄이기도 한다.

#### 예제: 스위치에서의 멀티캐스트 전달

토폴로지:

```text
       +---------+
       | Switch  |
       +---------+
       |   |   |
      H1  H2  H3
```

- H1, H2는 239.10.1.1 그룹에 가입  
- H3는 가입하지 않음

패킷 흐름:

1. 서버가 239.10.1.1로 프레임 전송 (MAC: 01:00:5E:0A:01:01 같은 값)
2. 스위치는 처음엔 이 MAC에 대한 포트 정보를 모름 → **플러딩**(모든 포트)
3. IGMP Snooping이 활성화된 스위치라면:
   - H1, H2가 보내는 **IGMP Report**를 관찰해  
     “이 포트들은 239.10.1.1 그룹에 관심 있음”을 기록
   - 이후 239.10.1.1로 오는 프레임은 **H1/H2 포트로만 포워딩**, H3는 제외

이 과정은 Cisco/Juniper/노키아 등의 장비 설명서에서 동일한 메커니즘으로 설명된다.  

---

### 무선(802.11)에서의 멀티캐스트

Wi-Fi(802.11)에서는 멀티캐스트/브로드캐스트 프레임에 **ACK가 없고**,  
보통 **가장 낮은 기본 전송 속도**로 전송되기 때문에 성능 문제가 잘 알려져 있다.  

- AP는 멀티캐스트 프레임을 브로드캐스트처럼 처리 ⇒ 재전송 없음  
- 속도를 낮게 잡기 때문에 많은 수신자가 있으면 전체 Wi-Fi 용량이 크게 감소

그래서 일부 구현에서는:

- 멀티캐스트를 내부에서 **복수의 유니캐스트로 변환**해서 전송하거나
- 트래픽 양이 많을 때는 애초에 **유니캐스트로 구성(예: N개의 TCP/QUIC 세션)** 하기도 한다.

---

### 작은 실습 코드: 유니캐스트 vs 멀티캐스트 부하 계산

멀티캐스트의 이점을 정량적으로 감각만 잡기 위한 간단한 파이썬 코드(개념용)이다.

```python
def link_load_unicast(num_receivers, stream_mbps):
    return num_receivers * stream_mbps

def link_load_multicast(num_receivers, stream_mbps):
    # 네트워크 내부 링크 하나 기준: 콘텐츠는 한 번만 지나감
    return stream_mbps

for n in [1, 5, 50, 500]:
    print(f"{n}명 수신자:")
    print(f"  유니캐스트: {link_load_unicast(n, 4)} Mbps")
    print(f"  멀티캐스트: {link_load_multicast(n, 4)} Mbps")
```

- N이 커질수록 **유니캐스트는 선형 증가**, 멀티캐스트는 **상수 유지**한다는 점만 보면 된다.

---

## 21.2.3 Collecting Information About Groups

멀티캐스트 라우터가 제대로 동작하려면 **“어떤 인터페이스 뒤에 어떤 그룹 멤버가 있는지”** 알아야 한다.

- IPv4: **IGMP (Internet Group Management Protocol)**  
- IPv6: **MLD (Multicast Listener Discovery)**

둘 다 **호스트–인접 라우터 사이에서 그룹 멤버십을 관리**하는 프로토콜이다.  

### IGMP(IPv4) 기본 동작

- **호스트 → 라우터**
  - “나 이 그룹 G를 받고 싶다/더 이상 받고 싶지 않다”를 보고
- **라우터 → 호스트**
  - “이 서브넷에 아직 G를 받고 싶은 호스트가 있냐?”를 주기적으로 질의(Query)

#### 메시지 유형 (버전 2/3 기준)

- **Membership Query** (Generic / Group-specific)
- **Membership Report** (Join 뜻)
- **Leave Group**

IGMPv3는 **소스 필터링(source filtering)** 을 추가해  
“이 그룹을 **특정 소스 집합 S에서만** 받고 싶다”와 같은 표현을 가능하게 한다.  

#### 예제: 호스트가 멀티캐스트 그룹에 가입하는 과정

토폴로지:

```text
Host H --- Router R --- (멀티캐스트 네트워크)
```

1. H의 애플리케이션이 239.10.1.1 그룹 수신을 시작  
2. H의 OS는 **IGMP Membership Report(Join)** 를 R에게 전송
3. R은 자신의 인터페이스 테이블에  
   - 인터페이스 X: 그룹 239.10.1.1 멤버 존재  
   라는 정보를 기록
4. 일정 주기마다 R은 IGMP Query를 보내고,  
   해당 그룹을 계속 받고 싶은 호스트들은 Report로 응답  
   - 응답이 없으면 **타이머 만료 후 멤버 삭제**

이 정보를 기반으로 R은 “어떤 멀티캐스트 패킷을 어떤 인터페이스로 내보낼지” 결정한다.

---

### MLD(IPv6) 개요

IPv6에서는 **MLD (ICMPv6 확장)** 가 IGMP와 같은 역할을 한다.  

- 메시지:
  - Multicast Listener Query
  - Multicast Listener Report
  - Multicast Listener Done
- IGMPv3와 마찬가지로 **소스 필터링 기능**이 있는 버전(MLDv2) 존재

구조와 개념은 IGMP와 거의 동일하므로, 실무에서는  
“IPv4면 IGMP, IPv6면 MLD” 정도로 기억해도 크게 틀리지 않는다.

---

## 21.2.4 Multicast Forwarding

이제 라우터 입장에서의 **멀티캐스트 포워딩**을 본다.

### Unicast vs Multicast Forwarding의 차이

**유니캐스트 포워딩:**

- 입력: (목적지 IP D)
- 출력: **하나의 출력 인터페이스**

```text
F_unicast(D) → out_if
```

**멀티캐스트 포워딩:**

- 입력: (소스 S, 그룹 G) — (S,G) 또는 (*,G) 형태
- 출력: **여러 개의 출력 인터페이스 집합**

```text
F_multicast(S, G) → {out_if1, out_if2, ...}
```

차이점 요약:  

1. 목적지 주소가 **그룹**을 나타내며, 그룹에 속한 멤버는 여러 개일 수 있다.
2. 포워딩 결정이 **소스 주소 + 그룹 주소 둘 다**에 의존할 수 있다.

---

### Reverse Path Forwarding (RPF)

대부분의 멀티캐스트 라우팅 프로토콜은 **RPF(Reverse Path Forwarding)** 를 사용한다.

아이디어:

> “멀티캐스트 패킷이 들어온 인터페이스가,  
> 소스 S까지 가는 **유니캐스트 최단 경로 상에서 “올 법한” 방향인지** 확인한다.”

- 라우터는 **유니캐스트 라우팅 테이블**을 이미 갖고 있으므로,
  - “S까지 가려면 어느 인터페이스로 나가야 하는가?” 를 알고 있음
- 패킷이 그 인터페이스의 **반대 방향**으로 들어왔을 때만 유효로 인정

#### 예제: RPF 체크

```text
      R2
     /  \
   R1    R3
    \    /
      S
```

- 유니캐스트 라우팅 테이블상:
  - R2에서 S로 가려면 → R3 방향
- 멀티캐스트 패킷이 **R3에서 R2로** 들어오면:
  - “아, 이건 S까지의 최단 경로에서 오는 방향이군” → **RPF 통과**
- 반대로, R1에서 R2로 들어왔으면:
  - “유니캐스트 경로상 S는 R3 쪽인데, 이 패킷은 R1에서 왔다 → 루프 가능성” → **드롭**

이렇게 RPF는 **루프 방지** 및 **최단 경로 기반 트리 형성**에 사용된다.

---

### Multicast Distribution Tree

RPF 검사를 통과한 패킷에 대해, 라우터는  
“**어떤 하위 인터페이스로 복제해서 내보낼지**”를 결정해야 한다.

멀티캐스트 포워딩 테이블은 보통 다음과 같은 형태를 가진다.  

| (Source S, Group G) | RPF 인터페이스 | Outgoing Interfaces |
|---------------------|----------------|---------------------|
| (S1, 239.1.1.1) | if0 | {if1, if2} |
| (*,  239.1.1.1) | if0 | {if1} |

- **(S,G)** 항목: S에서 온 G 그룹 패킷에 대한 트리
- **(*,G)** 항목: “어떤 S이든 G 그룹에 대한 기본 트리” (PIM-SM에서 공유 트리 등)

#### 간단한 포워딩 의사코드

```pseudo
on multicast_packet_received(packet, in_if):
    S = packet.src_ip
    G = packet.dst_ip

    # 1. RPF 체크
    if in_if != rpf_interface(S):
        drop(packet)
        return

    # 2. 포워딩 인터페이스 집합 조회
    out_set = lookup_multicast_tree(S, G)
    if out_set is empty:
        drop(packet)  # 이 그룹을 위한 하위 멤버가 없음
        return

    # 3. 복제 및 포워딩
    for out_if in out_set:
        if out_if != in_if:
            send(packet, out_if)
```

실제 구현은 PIM, DVMRP, MOSPF 등 프로토콜에 따라 훨씬 복잡하지만,  
**RPF + 트리 기반 복제**라는 본질은 동일하다.

---

## 21.2.5 Two Approaches to Multicasting

멀티캐스트 트리를 만드는 방식은 크게 두 가지로 정리된다.  

1. **Source-based Tree (소스 기반 트리)**  
2. **Group-shared Tree (그룹 공유 트리)**

이 두 접근법은 이후 장에서 다룰 **DVMRP, MOSPF, PIM-DM, PIM-SM** 같은  
실제 프로토콜의 설계 철학을 이해하는 핵심이다.

---

### 1) Source-based Tree

**각 소스 S마다, 각 그룹 G에 대해 별도의 트리**를 만드는 방식.

> (S,G)마다 **SPT(Shortest Path Tree)** 를 만든다.

- 예: DVMRP, PIM-DM에서 **소스 기반 트리 + RPF + Pruning** 사용
- 장점:
  - 각 수신자는 **소스까지의 최단 경로**를 따라 데이터 수신 → 지연 최소
- 단점:
  - 그룹이 많고 소스가 많으면,  
    **트리 수 = m(그룹 수) × n(소스 수)** 로 폭증

#### 예제: 2개의 소스, 3개의 그룹

- 소스: S1, S2 (2개)
- 그룹: G1, G2, G3 (3개)

Source-based tree일 때, 이론적으로는 최대:

- (S1,G1), (S1,G2), (S1,G3),
- (S2,G1), (S2,G2), (S2,G3)
→ **6개의 트리**가 존재

실제로는 일부 조합에서 트리가 비어 있을 수 있지만,  
대규모 환경에서 소스/그룹 수가 많으면 관리 부담이 커진다.

---

### 2) Group-shared Tree

**그룹 G당 하나의 트리**만 만든다. (소스와 관계 없이)

> “이 그룹 G에 대한 공용 트리”  

대표 예: **PIM-SM의 RP(Rendezvous Point) 기반 트리**

- 각 라우터는 “이 그룹 G를 위해 **특정 RP(센터)** 쪽으로 join”  
- RP를 root로 하는 **공유 트리(*,G)** 형성
- 장점:
  - 그룹당 **트리 1개** → 상태 수 감소
  - 매우 드문 그룹(Many receivers but few groups)에서 스케일링 유리
- 단점:
  - 모든 소스 트래픽이 **RP를 경유**할 수 있어 경로가 최단이 아닐 수 있음  
  - 일부 구현에서는 나중에 **SPT로 전환**하여 지연을 줄인다.

#### 예제: Center-based Tree

```text
       (RP)
        R2
       /  \
     R1    R3
     |     |
    H1    H2
```

- H1, H2가 G1 그룹에 가입했다고 하자.
- R1, R3는 각각 **RP(R2)를 향해 Join 메시지**를 전송.
- 그 결과:

```text
H1 - R1 \
          R2(RP) - R3 - H2
```

이 트리가 **G1에 대한 group-shared tree**가 된다.

S가 H1에서 H2로 바뀌어도, G1의 트리는 동일하게 유지된다  
(필요시 SPT로 스위칭할 수 있음).

---

### 두 접근법 비교 요약

| 특징 | Source-based Tree | Group-shared Tree |
|------|-------------------|-------------------|
| 트리 개수 | 최대 m × n (그룹 × 소스) | 최대 m (그룹 수) |
| 경로 | 소스–수신자 **최단 경로** | 경유 지점(RP) 때문에 비최단 경로 가능 |
| 상태 규모 | 큰 편 (소스 많을수록 증가) | 상대적으로 작음 |
| 대표 프로토콜 | DVMRP, PIM-DM (Dense Mode) | PIM-SM (Sparse Mode) |
| 적합 환경 | 멤버가 “많은” 밀집(dense) 환경 | 멤버가 “적은” 희소(sparse) 환경 |

실제 인터넷 멀티캐스트에서는 **PIM-SM의 group-shared tree + SPT 전환** 방식이  
주류로 사용되고, ISP/장비 벤더 문서에서도 이 조합을 기본 패턴으로 설명한다.  

---

### 간단한 의사코드: 두 방식의 트리 상태 수 비교

```python
def state_source_based(num_groups, num_sources):
    return num_groups * num_sources

def state_group_shared(num_groups):
    return num_groups

print("그룹 100개, 소스 10개일 때:")
print("  Source-based: ", state_source_based(100, 10), "개의 (S,G) 상태")
print("  Group-shared: ", state_group_shared(100), "개의 (*,G) 상태")
```

출력 개념:

- Source-based: 1000 개 상태
- Group-shared: 100 개 상태

→ **트리 상태 수가 10배 차이**가 난다는 단순한 예시다.

---

## 마무리 정리

이 장(21.1–21.2)의 핵심을 요약하면:

1. **통신 모드**
   - Unicast: 1→1, 단순하지만 다수 수신자에 비효율적
   - Broadcast: 1→All, IPv4 로컬 도메인에는 있지만 IPv6에서는 제거
   - Multicast: 1→Interested Group, **효율적인 다수 수신자 전달**

2. **멀티캐스트 주소**
   - IPv4: 224.0.0.0 ~ 239.255.255.255, IANA가 블록별 용도를 정의
   - IPv6: FF00::/8, scope 필드로 범위 제어

3. **2계층 전달**
   - IPv4 멀티캐스트 IP → 이더넷 멀티캐스트 MAC으로 **23비트 매핑**
   - 여러 IP 그룹이 하나의 MAC으로 매핑될 수 있어 IGMP Snooping 등으로 보완

4. **그룹 정보 수집**
   - IPv4: IGMP, IPv6: MLD
   - 호스트와 인접 라우터 간 멤버십 관리 (Join/Leave, Query/Report)

5. **포워딩**
   - RPF 기반, (S,G) 또는 (*,G) 엔트리와 트리를 사용
   - 소스 방향의 유니캐스트 라우팅 정보를 활용해 루프 없이 트리 구성

6. **두 가지 트리 접근법**
   - Source-based Tree: (S,G)마다 트리, 최단 경로지만 상태 폭증
   - Group-shared Tree: 그룹당 하나의 트리, 상태 적지만 경로는 비최단 가능

이 기본기를 토대로, 다음 절(21.3 이후)에서 나오는  
**DVMRP, MOSPF, PIM-DM, PIM-SM, MSDP** 등의 실제 멀티캐스트 라우팅 프로토콜을  
각각 “어떤 트리 접근법을 쓰는지, 어떤 환경(dense/sparse)에 맞는지” 관점에서  
정리해 나가면 전체 그림이 자연스럽게 연결될 것이다.