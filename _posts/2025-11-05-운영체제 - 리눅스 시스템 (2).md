---
layout: post
title: 운영체제 - 리눅스 시스템 (2)
date: 2025-11-05 15:25:23 +0900
category: 운영체제
---
# Chapter 20 — The Linux System (2)

## 20.5 Scheduling

리눅스 스케줄러는 **CFS(Completely Fair Scheduler)** 를 기본으로, **RT(실시간: FIFO/RR)**, **SCHED_DEADLINE(EDF 기반)** 을 함께 제공한다. CPU 격리는 `cpuset`/affinity, 공정성/자원 배분은 **cgroups v2** 로 통제한다.

### 20.5.1 CFS 핵심 개념

- **목표**: 각 태스크에게 “공평한” CPU 시간을 나눠준다.  
- **데이터 구조**: 런큐(runqueue)는 **레드블랙트리**로, **가상시간(vruntime)** 이 가장 작은 태스크가 선택된다.  
- **가중치(weight)**: `nice` 값으로부터 결정. `nice` 가 낮을수록 가중치↑, 더 많은 시간 할당.  
- **핵심 식(정성)**  
  $$
  \Delta \mathrm{vruntime} = \Delta t \cdot \frac{W_{0}}{W_{\text{task}}}
  $$
  `W_task` 가 큰 태스크(우선순위↑)는 같은 실제 시간 `Δt` 동안 `vruntime` 이 적게 증가한다 → 다음에도 더 빨리 스케줄됨.

- **timeslice?** CFS는 고정 타임슬라이스가 아니라 **target latency**(예: 6~24ms) 안에 모든 태스크가 한 번씩 돌도록 슬라이스를 동적으로 결정.

#### 실습 A — nice→CPU 점유 비율 근사
```python
# cfs_share.py — nice 값에 따른 상대 점유(근사)
weights = { -5:3350, 0:1024, 5:335, 10:110 }  # 예시 값(대략적)
tot = sum(weights.values())
for nice, w in sorted(weights.items()):
    print(f"nice {nice:+}: share ≈ {w/tot:.3f}")
```

#### 실습 B — CFS 파라미터 관찰/조정
```bash
# 시스템 전체(기본값은 배포판/커널에 따라 다름)
cat /proc/sys/kernel/sched_latency_ns
cat /proc/sys/kernel/sched_min_granularity_ns
# 일시 조정(데스크톱 인터랙티브 향상 등 실험용)
echo 12000000 | sudo tee /proc/sys/kernel/sched_latency_ns
```

### 20.5.2 RT 스케줄러 (SCHED_FIFO / SCHED_RR)

- **정수 우선순위(1~99)**. 숫자 클수록 우선순위 높음.  
- `SCHED_FIFO`: 같은 우선순위에서는 선점 없음(명시적 yield/블록까지 실행).  
- `SCHED_RR`: time quantum 을 둔 라운드로빈.

> 주의: 잘못 쓰면 CPU를 독점해 시스템을 “먹통”으로 만들 수 있음. WDT/감시 쓰레드/한계치가 중요.

#### 예제 — RT FIFO 태스크
```c
// rt_fifo.c (root 필요): 우선순위 70의 FIFO 태스크
#define _GNU_SOURCE
#include <sched.h>
#include <unistd.h>
#include <stdio.h>
#include <sys/mman.h>

int main(){
    struct sched_param sp = { .sched_priority = 70 };
    mlockall(MCL_CURRENT|MCL_FUTURE);             // page fault 방지
    if (sched_setscheduler(0, SCHED_FIFO, &sp)) perror("sched_setscheduler");
    for(;;){
        write(1, ".", 1);                          // 짧은 작업
        sched_yield();                             // 같은 prio 작업에게 양보
        usleep(1000);                              // 시스템 압박 완화
    }
}
```

### 20.5.3 SCHED_DEADLINE (EDF)

- **Earliest Deadline First** 기반. 파라미터: **Runtime/Deadline/Period**.  
- 이론상 단일 CPU에서 **부하율**  
  $$
  U=\sum_{i=1}^{n}\frac{C_i}{T_i} \le 1
  $$
  이면 스케줄 가능.

#### 예제 — deadline 설정(개념 스니펫)
```c
// deadline_set.c — sched_setattr(커널 헤더 필요, root 권장)
#include <linux/sched.h>
#include <linux/types.h>
#include <sys/syscall.h>
#include <unistd.h>
#include <string.h>
#include <stdio.h>

struct sched_attr {
  __u32 size;
  __u32 sched_policy;
  __u64 sched_flags;
  __s32 sched_nice;
  __u32 sched_priority;
  __u64 sched_runtime;
  __u64 sched_deadline;
  __u64 sched_period;
};

int sched_setattr(pid_t pid, const struct sched_attr *attr, unsigned int flags){
  return syscall(314, pid, attr, flags); // 아키/커널 버전에 따라 번호 상이
}

int main(){
  struct sched_attr a; memset(&a,0,sizeof(a)); a.size=sizeof(a);
  a.sched_policy = 6; /* SCHED_DEADLINE */
  a.sched_runtime  = 10*1000*1000;   // 10ms
  a.sched_deadline = 50*1000*1000;   // 50ms
  a.sched_period   = 50*1000*1000;   // 50ms
  if (sched_setattr(0, &a, 0)) perror("sched_setattr");
  for(;;) { /* 주기 작업 */ }
}
```

### 20.5.4 CPU affinity / NUMA / cgroups

- **affinity**: 스레드를 특정 CPU에 고정, 캐시 친화/간섭 감소.
```bash
taskset -c 2-3 ./server    # CPU2-3에 고정
```

- **NUMA 정책**: 메모리 locality를 위해 프로세스/스레드별 정책 설정.
```bash
numactl --cpunodebind=0 --membind=0 ./job
```

- **cgroups v2**: CPU/메모리/IO/PIDs 등 **격리·할당**.
```bash
cd /sys/fs/cgroup; sudo mkdir cg && cd cg
echo $$ | sudo tee cgroup.procs      # 현재 쉘 이동(주의!)
echo "200000 100000" | sudo tee cpu.max  # runtime/period(ns)
```

### 20.5.5 관측/디버깅

- `top/htop`, `ps -T`, `/proc/sched_debug`, `perf sched`, `bpftrace`.
```bash
sudo bpftrace -e 'tracepoint:sched:sched_switch { @[prev_comm, next_comm] = count(); }'
```

---

## 20.6 Memory Management

리눅스 메모리는 **가상 메모리**를 기반으로 **Demand Paging**, **페이지 캐시**, **Swap**, **메모리 압축**, **HugePages**, **KSM**, **Compaction**, **NUMA** 등을 활용한다.

### 20.6.1 가상 메모리 · 페이지 폴트

- **페이지 폴트 흐름**: 미매핑 주소 접근 → 커널이 PTE 생성/디스크에서 페이지 읽음 → 재시작.  
- **COW**: `fork()` 후 쓰기 시점에만 물리 페이지 복제.

#### 예제 — mmap + COW 관찰
```c
// cow_mmap.c — 페이지 공유 후 쓰기
#define _GNU_SOURCE
#include <sys/mman.h>
#include <unistd.h>
#include <string.h>
#include <stdio.h>

int main(){
    size_t sz=4096;
    char* p = mmap(NULL, sz, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
    strcpy(p,"hello");
    if(fork()==0){        // 자식
        p[0]='H';         // COW 발생
        printf("child:%s\n", p);
    } else {
        sleep(1);
        printf("parent:%s\n", p);
    }
}
```

### 20.6.2 페이지 캐시 / 버퍼드 I/O

- 파일 읽기는 **페이지 캐시**에 적재, 재사용 가능.  
- 쓰기는 **dirty page** 로 남고 **writeback** 스레드가 비동기 플러시.  
- 튜닝: `/proc/sys/vm/dirty_*`, `drop_caches`, `readahead`.

```bash
# 캐시 드롭(테스트용, 신중히)
echo 3 | sudo tee /proc/sys/vm/drop_caches
# readahead(파일/블록) 설정
sudo blockdev --setra 1024 /dev/nvme0n1
```

### 20.6.3 Swap / zswap / zram

- **Swap**: 메모리를 디스크로 쫓아내 공간 확보.  
- **zswap**: 스왑 전에 **압축된 캐시**(메모리 내)로 저장.  
- **zram**: RAM 위에 **압축 블록 디바이스**를 만들고 스왑으로 사용.

```bash
# zram 예(테스트 환경)
sudo modprobe zram
echo 1024M | sudo tee /sys/block/zram0/disksize
sudo mkswap /dev/zram0 && sudo swapon /dev/zram0
```

### 20.6.4 THP(Transparent Huge Pages) / HugeTLB

- **THP**: 2MiB 큰 페이지를 자동 사용(익명/파일 지원 상황 다름).  
- **이점**: TLB miss 감소. **주의**: compaction/latency 영향을 줄 수 있음.

```bash
cat /sys/kernel/mm/transparent_hugepage/enabled
echo never | sudo tee /sys/kernel/mm/transparent_hugepage/enabled  # 실험용
```

- **HugeTLB(hugetlbfs)**: 명시적으로 큰 페이지를 할당.
```bash
sudo mount -t hugetlbfs nodev /mnt/huge
```

### 20.6.5 KSM(공유 페이지 병합) / Compaction

- **KSM**: 동일 내용의 익명 페이지를 병합(COW). VM/컨테이너 밀도↑.  
- **Compaction**: 큰 연속 페이지 확보 위해 페이지 재배치.

### 20.6.6 OOM Killer

- 메모리 부족 시 **희생자 선정**. `oom_score_adj` 로 영향 조정.
```bash
cat /proc/$$/oom_score /proc/$$/oom_score_adj
echo 500 | sudo tee /proc/$$/oom_score_adj
```

### 20.6.7 NUMA

- **정책**: local, interleave, preferred.  
- 관측: `numastat`, `perf c2c`, `/sys/devices/system/node/`.

### 20.6.8 파일 I/O 힌트

- `posix_fadvise()`, `madvise()` 로 커널 힌트를 제공.
```c
// fadv.c — 순차 읽기 힌트
#include <fcntl.h>
#include <stdio.h>
#include <unistd.h>
int main(){
    int fd=open("bigfile",O_RDONLY);
    posix_fadvise(fd, 0, 0, POSIX_FADV_SEQUENTIAL);
    char buf[1<<16]; while (read(fd, buf, sizeof(buf))>0) {}
}
```

---

## 20.7 File Systems

VFS(가상 파일시스템) 위에 ext4, XFS, Btrfs, F2FS, tmpfs, NFS/SMB 같은 다양한 구현이 올라간다.

### 20.7.1 VFS 계층

- **inode**: 파일 메타데이터(권한, 소유, 타임스탬프, 포인터).  
- **dentry**: 이름→inode 매핑(경로 캐시).  
- **superblock**: 파일시스템 전반 상태.  
- **address_space/page cache**: 파일 페이지 캐시.

### 20.7.2 Journaling / Barrier

- ext4/XFS 등은 **저널링**으로 메타데이터 일관성 보장.  
- **모드(ext4)**:  
  - `data=ordered`(기본): 데이터 먼저, 저널링은 메타데이터.  
  - `data=writeback`: 성능↑, 최신성 위험↑.  
  - `data=journal`: 데이터까지 저널(가장 안전, 성능↓).

- **Barrier/Flush(FUA)**: 디스크 캐시 순서 보장. **fsync()** 는 강력한 내구성 보장.

### 20.7.3 ext4 / XFS / Btrfs / F2FS 간략 비교

| 항목 | ext4 | XFS | Btrfs | F2FS |
|---|---|---|---|---|
| 주 타겟 | 범용 | 대용량 | 스냅샷/RAID | 플래시 |
| 특징 | 안정/광범위 | 대용량 디렉토리/병렬 | CoW, 스냅샷, 서브볼륨 | 로그 구조, NAND 수명 |
| 주의 | 수많은 옵션 | fsck 비용 | 조각화, 튜닝 | HDD 비권장 |

### 20.7.4 마운트/옵션 최적화

```bash
# ext4: 성능 실험 예시(신중히)
sudo mount -t ext4 -o noatime,discard /dev/nvme0n1p2 /data
# noatime: atime 갱신 방지
# discard: TRIM(실시간) — 대신 주기적 fstrim -av 도 대안
```

### 20.7.5 디렉토리/메타데이터 핫스팟 완화

- 해시 기반 디렉토리 분산, 다단 경로(`a/b/cd/...`), 시간/해시 프리픽스.  
- **dentry cache** hit율 개선: `openat2()`/상대경로 사용, `statx()`.

### 20.7.6 FUSE / OverlayFS

- **FUSE**: 유저 공간 파일시스템(편의↑, 성능↓ 가능).  
- **OverlayFS**: 컨테이너 계층 이미지(upperdir/lowerdir)로 **Copy-on-Write** 제공.

### 20.7.7 inotify/fanotify

- 파일 변경 이벤트 감시로 캐시/동기화/인덱싱.
```c
// inot.c — inotify로 디렉토리 감시
#include <sys/inotify.h>
#include <limits.h>
#include <unistd.h>
#include <stdio.h>
int main(){
    int fd=inotify_init1(IN_NONBLOCK);
    int wd=inotify_add_watch(fd,".",IN_CREATE|IN_MODIFY|IN_DELETE);
    char buf[4096] __attribute__ ((aligned(__alignof__(struct inotify_event))));
    for(;;){
        int n=read(fd,buf,sizeof(buf));
        for(char* p=buf; p<buf+n; ){
            struct inotify_event* e=(struct inotify_event*)p;
            printf("mask=0x%x name=%s\n", e->mask, e->len? e->name:"");
            p += sizeof(*e)+e->len;
        }
        usleep(200000);
    }
}
```

### 20.7.8 파일 잠금

- **advisory lock**: `flock`, `fcntl` 레코드 락.  
- 분산에서는 NFS lock, SMB oplock/lease, DB 레벨 락 권장.

---

## 20.8 Input and Output

I/O 경로는 **유저→VFS→파일시스템→페이지캐시/직접I/O→블록층(blk-mq)→드라이버→장치**. 네트워크는 소켓 계층을 통해 스택으로 흘러간다. 최신 리눅스는 **blk-mq**, **io_uring**, **NVMe** 최적화가 핵심.

### 20.8.1 blk-mq & I/O 스케줄러

- **blk-mq**: 다중 큐로 병렬 처리. CPU 당 SW 큐, HBA/NVMe 당 HW 큐.  
- **스케줄러**: `none`(NVMe 권장), `mq-deadline`, `bfq`(데스크톱 상호작용).

```bash
# 장치 스케줄러 확인/설정
cat /sys/block/nvme0n1/queue/scheduler
echo mq-deadline | sudo tee /sys/block/sda/queue/scheduler
```

### 20.8.2 Buffered I/O vs Direct I/O

- **Buffered**: 페이지 캐시 통해 읽기/쓰기(일반적, 재사용/리드어헤드).  
- **Direct O_DIRECT**: 페이지 캐시 우회, 자체 버퍼 정렬 필요(4096 정렬). DB/로그 등 이점.

```c
// direct_read.c — O_DIRECT 읽기(4096 정렬 버퍼 필요)
#define _GNU_SOURCE
#include <fcntl.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

int main(){
    int fd=open("bigfile", O_RDONLY|O_DIRECT);
    void* buf; posix_memalign(&buf, 4096, 4096);
    ssize_t n=read(fd, buf, 4096);
    printf("read=%zd\n", n);
    free(buf); close(fd);
}
```

### 20.8.3 readahead / posix_fadvise

- 순차 워크로드에서 대역폭↑, 지연↓.  
- 랜덤 워크로드에서는 과도한 프리페치 방지.

```c
// readahead.c — 커널 readahead 호출
#define _GNU_SOURCE
#include <fcntl.h>
#include <stdio.h>
#include <sys/syscall.h>
#include <unistd.h>
int main(){
    int fd=open("bigfile",O_RDONLY);
    syscall(SYS_readahead, fd, 0, 1<<20); // 1MiB
}
```

### 20.8.4 io_uring — 저오버헤드 비동기 I/O

- **특징**: 커널/유저 링 공유, 시스템콜 감소, 파일/네트워크까지 확장.  
- **패턴**: SQE 준비 → submit → CQE 수거.

```c
// uring_cat.c — liburing로 파일 => stdout 복사 (간단 스니펫)
#include <liburing.h>
#include <fcntl.h>
#include <unistd.h>
#include <stdio.h>
int main(int argc, char**argv){
    if(argc<2){fprintf(stderr,"usage: %s file\n",argv[0]); return 1;}
    int fd=open(argv[1],O_RDONLY);
    struct io_uring ring; io_uring_queue_init(64,&ring,0);
    char buf[1<<16];
    off_t off=0; for(;;){
        struct io_uring_sqe* r=io_uring_get_sqe(&ring);
        io_uring_prep_read(r, fd, buf, sizeof(buf), off);
        io_uring_submit(&ring);
        struct io_uring_cqe* cqe; if(io_uring_wait_cqe(&ring,&cqe)) break;
        if(cqe->res<=0) break;
        write(1, buf, cqe->res); off+=cqe->res; io_uring_cqe_seen(&ring,cqe);
    }
    io_uring_queue_exit(&ring); close(fd);
}
```

### 20.8.5 네트워크 I/O 요점

- **소켓 옵션**: `TCP_NODELAY`, `SO_REUSEPORT`, `SO_RCVBUF/SO_SNDBUF`.  
- **멀티큐 NIC**: RSS/irq affinity로 큐와 CPU를 매칭 → 스케일.  
- **sendfile/splice**: **zero-copy** 전송(커널 내 복사 최소화).

```c
// sendfile_copy.c — 파일 -> 소켓 zero-copy
#include <sys/sendfile.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <fcntl.h>
#include <unistd.h>
int main(){
    int s=socket(AF_INET,SOCK_STREAM,0);
    struct sockaddr_in a={.sin_family=AF_INET,.sin_port=htons(9000),.sin_addr={0}};
    bind(s,(void*)&a,sizeof(a)); listen(s,1);
    int c=accept(s,NULL,NULL);
    int fd=open("bigfile",O_RDONLY);
    off_t off=0; for(;;){
        ssize_t n=sendfile(c,fd,&off,1<<20);
        if(n<=0) break;
    }
}
```

### 20.8.6 fsync/오류 모델

- **쓰기 내구성 보장**: `fsync()`/`fdatasync()`/`sync_file_range()` 조합.  
- 전원 장애/크래시 시 **저널 순서/장치 캐시 flush** 가 관건.  
- 데이터베이스·거래 로그는 **WAL + fsync** 가 표준.

### 20.8.7 I/O 관측

- **iostat**, **pidstat -d**, **blktrace/btt**, **perf iostat**, **eBPF**.  
```bash
sudo bpftrace -e 'tracepoint:block:block_rq_issue { @[args->rwbs] = count(); }'
```

---

## 종합 실습 — “로그 수집기 성능 튜닝” 시나리오

**목표**: 초당 수천 라인의 로그 파일을 안정적으로 수집·전송.

1) 파일 입력: `O_DIRECT` 대신 **Buffered + posix_fadvise(SEQUENTIAL)** → 페이지 캐시 프리페치 이점.  
2) 네트워크 출력: **io_uring send/recv** 또는 **sendfile/splice** 활용.  
3) 디스크 대상일 때: `mq-deadline` (HDD), `none` (NVMe) 선택.  
4) fsync 전략: 배치로 묶어 `fdatasync()` 호출(트랜잭션 경계 시).  
5) cgroup으로 CPU/IO 제한을 분리, RT/Deadline 금지(시스템 과점 방지).  
6) 관측: p95/p99 지연, 재시도/드롭, blk 큐 길이, CPU steal/IRQ 시간을 모니터링.

---

## 체크리스트 요약

- **Scheduling**: CFS(가중치/vruntime), RT(FIFO/RR), Deadline(EDF), affinity/NUMA, cgroups.  
- **Memory**: Demand paging/COW, Page cache/writeback, Swap/zswap/zram, THP/HugeTLB, OOM.  
- **File Systems**: VFS(inode/dentry), journaling/barrier, ext4/XFS/Btrfs, FUSE/Overlay, inotify.  
- **I/O**: blk-mq, I/O 스케줄러, Buffered vs O_DIRECT, readahead/fadvise, io_uring, zero-copy.

> 실전 팁: “**읽기는 캐시를 믿고, 쓰기는 내구성 경계를 명확히**.” 스케줄러/NUMA/IO 큐/캐시 정책을 **워크로드 특성**(순차/랜덤, 파일 크기, 내구성 요구)에 맞춰 선택하라.
