---
layout: post
title: 컴퓨터시스템 - 병렬성 높이기
date: 2025-08-01 22:20:23 +0900
category: 컴퓨터시스템
---
# 병렬성 높이기

## 1. 병렬성의 4가지 축 — 무엇을, 어디서 키울 것인가

- **ILP (Instruction-Level Parallelism)**: 단일 코어 내부 파이프라인/슈퍼스칼라/OOO가 독립 명령을 중첩 실행. 소스 수준에서는 **의존 체인 단축, 다중 축적기**, 함수 인라인 등으로 도움을 준다.
- **DLP (Data-Level Parallelism, SIMD/벡터화)**: 동일 연산을 여러 데이터에 병렬 적용. AVX2/AVX-512/SVE, GPU의 워프/서브그룹도 DLP.
- **TLP (Thread-Level Parallelism)**: 여러 스레드/프로세스가 독립 작업을 병행. OpenMP/Pthreads/C++ Threads/TBB.
- **MLP (Memory-Level Parallelism)**: 동시 다중 메모리 접근(여러 outstanding miss)을 허용해 대기 겹치기. **타일링, 소프트웨어 프리패치, 비동기 파이프라인, NUMA 인식**이 핵심.

> 실전 성능은 **DLP + TLP + MLP**를 함께 끌어올릴 때 크게 오른다. ILP는 컴파일러/μarch가 많이 챙겨주나, **의존 길이/분기 감소**로 간접 지원 가능.

---

## 2. 병목을 수식으로 본다 — 작은 모델이 큰 판단을 만든다

- **CPU 시간 근사**
$$
\text{CPU time} \approx \frac{\text{Instructions}\times \text{CPI}}{f}
$$
  - CPI가 높은 이유: 캐시 미스(MLP/타일링), 분기 미스(분기 제거/예측), 자원 경합(포트/레지스터 압력).
- **Amdahl의 법칙(강 스케일링)**
$$
\text{Speedup}=\frac{1}{(1-p)+\frac{p}{s}},\quad 0\le p\le 1
$$
  - \(p\): 병렬화 가능한 비율, \(s\): 병렬 부분의 가속 배수.
- **Gustafson(약 스케일링)** — 문제 크기를 늘리며 스케일
$$
S(N)=N-(1-N)\alpha
$$
  - \(\alpha\): 직렬 비중.
- **병렬 효율**
$$
E = \frac{S(P)}{P},\qquad \text{Karp–Flatt}:\ \ e=\frac{\frac{1}{S(P)}-\frac{1}{P}}{1-\frac{1}{P}}
$$
- **Roofline 요약**
$$
\text{Attainable}=\min\left(\text{Peak FLOPs},\ \text{AI}\times \text{Peak BW}\right),\quad
\text{AI}=\frac{\text{FLOPs}}{\text{Bytes}}
$$

---

## 3. DLP — 벡터화가 잘 걸리는 코드로 바꾸는 법

### 3.1 별칭 제거, 정렬/연속성, 분기 최소화
```c
// before: 별칭 가능, 벡터화 방해
void saxpy(int n, float *x, float *y, float a){
  for (int i=0;i<n;i++) y[i] += a * x[i];
}

// after: 별칭 제거 + 벡터화 힌트
void saxpy(int n, float *restrict x, float *restrict y, float a){
  #pragma omp simd
  for (int i=0;i<n;i++) y[i] += a * x[i];
}
```
- `restrict`로 **별칭(alias) 금지**를 컴파일러에 약속.
- SoA(Structure of Arrays) 전환, `aligned_alloc(64, ...)` 등으로 **정렬/연속성** 확보.
- 조건은 **마스크/선택**으로 치환하여 분기 제거.

### 3.2 AoS → SoA 변환(벡터 친화)
```c
// AoS (비권장: x,y,z가 서로 멀리 떨어짐)
typedef struct { float x,y,z; } Vec3;
void norm_aos(Vec3* v, int n){
  for(int i=0;i<n;i++){
    float s = v[i].x*v[i].x + v[i].y*v[i].y + v[i].z*v[i].z;
    float inv = 1.0f / sqrtf(s);
    v[i].x *= inv; v[i].y *= inv; v[i].z *= inv;
  }
}

// SoA (권장: 각 필드가 연속)
typedef struct { float* x; float* y; float* z; } Vec3SoA;
void norm_soa(Vec3SoA V, int n){
  #pragma omp simd
  for(int i=0;i<n;i++){
    float s = V.x[i]*V.x[i] + V.y[i]*V.y[i] + V.z[i]*V.z[i];
    float inv = 1.0f / sqrtf(s);
    V.x[i]*=inv; V.y[i]*=inv; V.z[i]*=inv;
  }
}
```

### 3.3 소프트웨어 프리패치 + 언롤
```c
void sum_prefetch(const float *a, int n, float *res) {
  float acc=0.0f;
  for (int i=0;i<n;i+=16) {
    __builtin_prefetch(&a[i+64], 0, 1); // 읽기 힌트, next lines
    acc += a[i] + a[i+1] + a[i+2] + a[i+3]
         + a[i+4] + a[i+5] + a[i+6] + a[i+7]
         + a[i+8] + a[i+9] + a[i+10]+ a[i+11]
         + a[i+12]+ a[i+13]+ a[i+14]+ a[i+15];
  }
  *res = acc;
}
```
- 프리패치 거리와 지역성은 **실험으로 튜닝**.

---

## 4. TLP — 스레드 병렬화: for, reduction, task, 그리고 그레인

### 4.1 데이터 병렬 for
```c
#pragma omp parallel for schedule(dynamic,256)
for (int i=0;i<n;i++){
  y[i] = f(x[i]);
}
```
- 균일 작업: `static`, 편차 큼: `dynamic/guided`.  
- **그레인 크기**를 너무 작게 잡으면 스케줄 오버헤드 증가.

### 4.2 안정적 Reduce/Scan
```c
double sum = 0.0;
#pragma omp parallel for reduction(+:sum) schedule(static)
for (int i=0;i<n;i++) sum += a[i];
```
- 부동소수점은 **결합 법칙이 완벽히 성립하지 않음** → 약간의 차이를 허용하도록 테스트 설계(또는 Kahan 등 보정).

### 4.3 태스크 그래프(DAG) — 의존성으로 스케줄
```c
#pragma omp parallel
#pragma omp single
{
  #pragma omp task depend(out:A) workA();
  #pragma omp task depend(out:B) workB();
  #pragma omp task depend(in:A,B) workC();
  #pragma omp taskwait
}
```
- 태스크 폭이 충분하고 **work-stealing 런타임**이면 스케일이 잘 난다.

### 4.4 공유 상태 줄이기(샤딩/TLS/리듀스-그라뉼)
```c
// 샤딩된 카운터: 각 스레드 전용 카운터(패딩으로 false sharing 제거)
#define P 64
typedef struct { _Alignas(64) long v; } slot;
slot cnt[P];

void add(int tid, long v){ cnt[tid].v += v; }
long read_all(){ long s=0; for (int i=0;i<P;i++) s+=cnt[i].v; return s; }
```

---

## 5. 동시성 제어 — 락을 최소로, 정확성은 확실히

- **잠금 범위 축소**: 락 구간을 정확히 보호할 최소 범위로.
- **락 스트라이핑**: 큰 맵을 여러 락으로 분할하여 경합 분산.
- **RW 락**: 읽기 다수/쓰기 소수 상황.
- **원자 연산**: 카운터/플래그는 `atomic`으로 대체.
- **락 없는 구조**는 마지막 선택지. ABA/메모리 재활용(HP/epoch)까지 설계되지 않으면 위험.

```c
// 개념적 lock-free push (메모리 재활용 전략 필수)
typedef struct Node{ int v; struct Node* next; } Node;
_Atomic(Node*) head = NULL;

void push(int x){
  Node* n=malloc(sizeof *n); n->v=x;
  Node* old;
  do{
    old = atomic_load_explicit(&head, memory_order_relaxed);
    n->next = old;
  } while(!atomic_compare_exchange_weak_explicit(
          &head, &old, n, memory_order_release, memory_order_relaxed));
}
```

---

## 6. MLP — 메모리 병렬성: 타일링, 프리패치, NUMA

- **타일링/블로킹**: 워킹셋을 L1/L2에 맞춰 분할하여 재사용 극대화.
- **프리패치**: 예측 가능한 순차/스트라이드 접근에 효과.
- **동시 outstanding miss 증가**: 태스크/스레드로 대기를 겹치기.
- **NUMA 인식**: first-touch 규칙. 초기화 루프를 **병렬로** 돌려 페이지를 해당 노드에 할당.
```c
#pragma omp parallel for
for (int i=0;i<n;i++) a[i]=0.0; // first-touch: NUMA 로컬
```

---

## 7. 파이프라인 병렬성 — 생산자→가공→소비자

- **스테이지 분할**: parse → transform → encode.  
- **링 버퍼**: 크기 제한으로 배압(backpressure) 구현.  
- **균형 맞추기**: 가장 느린 스테이지가 전체 처리량을 결정.

```c
// 단일 생산자/소비자(SPSC) 링버퍼 개념(간략)
typedef struct { size_t N, r, w; _Alignas(64) int buf[]; } Ring;
int rb_push(Ring* q, int v){
  size_t n = q->N, w=q->w, r=q->r;
  if (((w+1)%n)==r) return 0; // full
  q->buf[w]=v; __atomic_store_n(&q->w,(w+1)%n,__ATOMIC_RELEASE);
  return 1;
}
int rb_pop(Ring* q, int* out){
  size_t n = q->N, w=q->w, r=q->r;
  if (r==w) return 0; // empty
  *out=q->buf[r]; __atomic_store_n(&q->r,(r+1)%n,__ATOMIC_ACQUIRE);
  return 1;
}
```

---

## 8. GPU/NPU 오프로딩 — 언제, 어떻게 이득인가

- **조건**: 전송/준비 오버헤드 < 커널 가속 이득. **배치(batch)** 로 전송비율 감소.  
- **핵심**: coalesced 접근, 분기 수렴, occupancy(레지스터/공유메모리), 병렬 블록 크기.

### 8.1 CUDA SAXPY 예
```c
// nvcc saxpy.cu -O3
__global__ void saxpy(int n, float a, const float* __restrict__ x, float* __restrict__ y){
  int i = blockIdx.x*blockDim.x + threadIdx.x;
  if (i<n) y[i] += a * x[i];
}
void run(int n, float a, float* x, float* y){
  float *dx,*dy; cudaMalloc(&dx,n*sizeof(float)); cudaMalloc(&dy,n*sizeof(float));
  cudaMemcpy(dx,x,n*sizeof(float),cudaMemcpyHostToDevice);
  cudaMemcpy(dy,y,n*sizeof(float),cudaMemcpyHostToDevice);
  int tb=256, nb=(n+tb-1)/tb;
  saxpy<<<nb,tb>>>(n,a,dx,dy);
  cudaMemcpy(y,dy,n*sizeof(float),cudaMemcpyDeviceToHost);
  cudaFree(dx); cudaFree(dy);
}
```
- **coalescing**: 스레드 \(i\)가 **연속 주소** \(x[i]\)를 접근.
- **오버랩**: `cudaMemcpyAsync` + **streams** + **pinned host memory**로 전송/계산 겹치기.

---

## 9. 정확성·재현성 — 병렬화의 함정과 방어

- **결합/교환 법칙**: 병렬 reduce는 순서가 바뀌어 FP 결과가 미세하게 달라진다. 허용 오차(ULP/상대오차) 정의. 필요하면 **Kahan, pairwise sum** 채택.
```c
double kahan_sum(const double* a,int n){
  double s=0.0,c=0.0;
  for(int i=0;i<n;i++){
    double y=a[i]-c, t=s+y; c=(t-s)-y; s=t;
  }
  return s;
}
```
- **데이터 레이스/데드락**: TSAN로 탐지, 락 순서 규칙, 타임아웃/try-lock.
- **재현성**: 랜덤 시드 고정, 스케줄 고정 옵션, 로그/트레이스.

---

## 10. 스케일링과 모델로 의사결정하기

- **강 스케일링**: 문제 크기 고정 → 스레드↑. 효율이 빨리 떨어지면 직렬/경합/메모리 바운드.  
- **약 스케일링**: 스레드↑에 비례하여 문제 크기↑. 효율 곡선이 평탄하면 좋은 구조.  
- **Isoefficiency**: 효율 일정 유지에 필요한 문제 크기의 함수. 데이터/알고리즘/메모리 구조를 다시 설계.

---

## 11. 측정·분석 워크플로(필수 루틴)

1) **속도**: 벽시계 시간(여러 번 실행, 중앙값/표준편차). 워밍업 반복으로 캐시/예측 안정화.  
2) **하드웨어 카운터**:  
   - `perf stat -e cycles,instructions,branches,branch-misses,cache-misses`  
   - `stalled-cycles-frontend/backend`, `L1-dcache-load-misses`, `LLC-load-misses`.  
3) **스케일링 곡선**: 스레드 수 vs 처리량. 포화/역전점 확인.  
4) **Roofline**: FLOPs/Byte 추산, 병목(메모리/연산)을 분리.

---

## 12. 바로 쓰는 레시피 모음

### 12.1 벡터화 + 병렬화(saxpy)
```c
void saxpy(int n, float *restrict x, float *restrict y, float a){
  #pragma omp parallel for schedule(static)
  for (int i=0;i<n;i++){
    #pragma omp simd
    for (int j=i;j<i+1;j++) // 단일 반복이더라도 simd 강제
      y[j] += a * x[j];
  }
}
```

### 12.2 병렬 reduce(정확성 주의)
```c
double sum_par(const double *a, int n){
  double s=0.0;
  #pragma omp parallel for reduction(+:s) schedule(static)
  for(int i=0;i<n;i++) s+=a[i];
  return s;
}
```

### 12.3 타일링(행렬 곱, 개념형식)
```c
void gemm_blocked(int N, const float* A, const float* B, float* C, int Bsz){
  for (int ii=0; ii<N; ii+=Bsz)
    for (int jj=0; jj<N; jj+=Bsz)
      for (int kk=0; kk<N; kk+=Bsz)
        for (int i=ii; i<ii+Bsz && i<N; ++i)
          for (int j=jj; j<jj+Bsz && j<N; ++j){
            float s=C[i*N+j];
            #pragma omp simd
            for (int k=kk; k<kk+Bsz && k<N; ++k)
              s += A[i*N+k]*B[k*N+j];
            C[i*N+j]=s;
          }
}
```

### 12.4 false sharing 피하기
```c
typedef struct { _Alignas(64) long v; } padded_long;
padded_long cnt[64]; // 각 스레드 전용
```

### 12.5 NUMA first-touch 초기화
```c
#pragma omp parallel for
for (int i=0;i<n;i++) a[i]=0; // 각 스레드가 자기 노드에 할당
```

### 12.6 AVX2 벡터 합 + 언롤
```c
#include <immintrin.h>
int sum_avx2_unroll(const int *a, int n){
  __m256i s0=_mm256_setzero_si256(), s1=_mm256_setzero_si256();
  int i=0; for(; i+15<n; i+=16){
    __m256i v0=_mm256_loadu_si256((const __m256i*)&a[i]);
    __m256i v1=_mm256_loadu_si256((const __m256i*)&a[i+8]);
    s0=_mm256_add_epi32(s0,v0); s1=_mm256_add_epi32(s1,v1);
  }
  s0=_mm256_add_epi32(s0,s1);
  alignas(32) int t[8]; _mm256_store_si256((__m256i*)t,s0);
  int s=t[0]+t[1]+t[2]+t[3]+t[4]+t[5]+t[6]+t[7];
  for(; i<n; ++i) s+=a[i];
  return s;
}
```

### 12.7 CUDA 오프로딩 골격(전송/계산 오버랩은 Async+Streams 사용)
```c
__global__ void addv(int n, const float* x, const float* y, float* z){
  int i = blockIdx.x*blockDim.x + threadIdx.x;
  if (i<n) z[i] = x[i] + y[i];
}
```

---

## 13. 안티패턴과 회피법

- **과도 언롤링** → I-cache/레지스터 압력으로 오히려 악화. **팩터를 측정으로 결정**.
- **스레드 과구독(oversubscription)** → 컨텍스트 스위치 폭증. 코어 수에 맞추고, I/O 대기라면 io_uring/async I/O 고려.
- **전역 락** → 샤딩/스트라이핑/RW락/락없는 자료구조로 대체.  
- **AoS 고집** → SoA/속성별 배열로 전환.  
- **NUMA 무시** → 원격 메모리 지연 급증. first-touch/스레드 바인딩 사용.
- **분기 남발** → 마스크/선택(cmov)/테이블 룩업 등 **branchless** 전환.

---

## 14. 체크리스트(실행 전·후)

- [ ] 문제를 **Map/Reduce/Scan/Stencil/Task**로 재구성했는가?  
- [ ] **별칭 제거(restrict)**, **정렬/연속성**, **분기 최소화**로 벡터화가 걸리는가?  
- [ ] 루프 **의존 체인**을 다중 축적기/언롤링으로 단축했는가?  
- [ ] **그레인 크기**가 적절한가(오버헤드 vs 로드 밸런스)?  
- [ ] 공유 상태를 **샤딩/TLS/리듀스**로 줄였는가?  
- [ ] **NUMA/first-touch/바인딩**을 적용했는가?  
- [ ] **Roofline**으로 메모리/연산 병목을 구분했는가?  
- [ ] **스케일링 곡선**과 **하드웨어 카운터**로 개선을 검증했는가?  
- [ ] FP 오차/재현성/레이스/데드락을 검증했는가(TSAN/테스트)?  

---

## 15. 결론 — 조합의 예술

1) **DLP**로 바탕(SoA/정렬/분기 최소화/벡터화),  
2) **TLP**로 코어를 채우고(그레인/스케줄/샤딩),  
3) **MLP**로 메모리 병목을 풀며(타일링/프리패치/NUMA),  
4) 필요하면 **GPU/NPU**로 대용량 DLP를 오프로딩하라.  

항상 **프로파일 → 변환 → 재프로파일**의 루프를 돌려라. 작은 수식과 체크리스트, 그리고 짧은 벤치 스캐폴딩이 **정량적 의사결정**을 가능하게 한다. 병렬성은 한 번의 비법이 아니라, **데이터 레이아웃·알고리즘·스케줄링·정확성**의 **균형 설계**다.