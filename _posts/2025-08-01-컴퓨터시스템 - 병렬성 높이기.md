---
layout: post
title: 컴퓨터시스템 - 병렬성 높이기
date: 2025-08-01 22:20:23 +0900
category: 컴퓨터시스템
---
# 병렬성 높이기 — ILP·SIMD·스레드·GPU까지 한 번에 정리

> 목표: 단일 스레드 코드부터 멀티코어·GPU·가속기까지 **병렬성(parallelism)** 을 끌어올리는 실전 가이드.  
> 구성: 병렬성의 종류 → 병목 진단 → 코드·데이터 변환 기법 → 동시성 제어 전략 → 스케일링 법칙 → 툴과 체크리스트.  
> 코드 예시는 C(일부 OpenMP) 기준. 수식은 MathJax 사용.

---

## 1) 병렬성의 4가지 축

1. **ILP (Instruction-Level Parallelism)**: 한 코어 내부에서 독립 명령을 동시에 처리(파이프라인/Out-of-Order/슈퍼스칼라).  
2. **DLP (Data-Level Parallelism, SIMD/벡터화)**: 같은 연산을 여러 데이터에 병렬 적용(AVX/SVE 등).  
3. **TLP (Thread-Level Parallelism)**: 다중 스레드/프로세스가 서로 독립 작업을 동시에 수행.  
4. **MLP (Memory-Level Parallelism)**: 여러 메모리 접근을 겹치기(프리패치, 다중 outstanding miss).

> 실제 성능은 **DLP+TLP+MLP**를 함께 끌어올릴 때 가장 크게 오른다. ILP는 주로 컴파일러/μarch 자동 혜택.

---

## 2) 어디서 병렬성을 찾나? (패턴)

- **Map/Transform**: 독립 요소 처리(`y[i]=f(x[i])`).  
- **Reduce/Scan**: 합/최댓값/누적합(연관·결합 법칙 중요).  
- **Stencil/Convolution**: 이웃 참조가 작고 규칙적이면 타일링+SIMD.  
- **Embarrassingly Parallel**: 이미지 배치 처리, Monte Carlo.  
- **Pipeline**: 생산자→가공→소비자 단계가 명확할 때.  
- **Task Graph(DAG)**: 의존 관계가 희박한 잡(job)을 스케줄.

---

## 3) 먼저 병목을 진단하자 (간단 모델)

- **CPU 시간**  
  \[
  \text{CPU time}=\frac{\text{Instructions}\times \text{CPI}}{f}
  \]
  높은 CPI의 원인: 캐시미스, 분기미스, 자원경쟁.
- **Amdahl**  
  \[
  \text{Speedup}=\frac{1}{(1-p)+\frac{p}{s}}
  \]
  \(p\): 병렬화 가능한 비율, \(s\): 그 부분 가속비. 직렬 구간이 크면 한계가 빠르게 온다.
- **Gustafson** (문제 크기 확대 시)  
  \[
  S(N)=N-(1-N)\cdot \alpha
  \]
  \(\alpha\): 직렬 비중. 데이터/해상도를 늘려 코어를 먹여 살리는 전략.

---

## 4) DLP — 벡터화로 병렬성 키우기

### 4.1 자동 벡터화가 잘 되게 쓰는 법
- **별칭 제거**: 포인터는 `restrict`로 겹치지 않음을 약속.
- **메모리 연속/정렬**: SoA(Structure of Arrays) 검토, `aligned_alloc`.
- **분기 최소화**: 루프 안 조건은 마스크 연산으로 치환(가능하면).
- **루프 캐리 의존 제거**: 누적·참조가 다음 반복에 영향 주지 않게 변환.

```c
// before
void saxpy(int n, float *x, float *y, float a) {
  for (int i=0;i<n;i++) y[i] += a * x[i];
}

// after: 별칭 제거 + 벡터화 힌트
void saxpy(int n, float *restrict x, float *restrict y, float a) {
  #pragma omp simd
  for (int i=0;i<n;i++) y[i] += a * x[i];
}
```

### 4.2 소프트웨어 프리패치 + 언롤
```c
void sum(float *a, int n, float *res) {
  float acc=0.0f;
  for (int i=0;i<n;i+=16) {
    __builtin_prefetch(&a[i+64], 0, 1);
    acc += a[i] + a[i+1] + a[i+2] + a[i+3]
         + a[i+4] + a[i+5] + a[i+6] + a[i+7]
         + a[i+8] + a[i+9] + a[i+10]+ a[i+11]
         + a[i+12]+ a[i+13]+ a[i+14]+ a[i+15];
  }
  *res = acc;
}
```

---

## 5) TLP — 스레드 병렬성 쌓기

### 5.1 데이터 병렬(For 병렬화)
```c
#pragma omp parallel for schedule(dynamic,256)
for (int i=0;i<n;i++) {
  y[i] = f(x[i]);
}
```
- **스케줄링**: `static`은 균일 작업에, `dynamic`/`guided`는 편차 큰 작업에.  
- **그레인 크기**: 너무 잘게 쪼개면 스케줄 오버헤드↑.

### 5.2 Reduce/Scan의 안정적 병렬화
```c
double sum = 0.0;
#pragma omp parallel for reduction(+:sum)
for (int i=0;i<n;i++) sum += a[i];
```
- 부동소수점은 결합 법칙이 완벽히 성립하지 않아 **합이 약간 달라질 수 있음**(허용 오차 설계).

### 5.3 작업(태스크) 병렬
```c
#pragma omp parallel
#pragma omp single
{
  #pragma omp task depend(out:A) workA();
  #pragma omp task depend(out:B) workB();
  #pragma omp task depend(in:A,B) workC();
  #pragma omp taskwait
}
```
- 의존이 적은 DAG라면 **작업 훔치기(work-stealing)** 런타임이 좋은 스케일을 낸다.

### 5.4 공유 상태를 줄이는 패턴
- **샤딩(Sharding)**: 전역 맵/카운터를 코어 수만큼 분리 → 마지막에 병합.
- **TLS(스레드 로컬)**: per-thread 버퍼/카운터.
- **리두스-그라뉼**: `local += ...;` → 마지막에 `atomic`/합치기.

```c
// 샤딩된 카운터
#define P 64
_Alignas(64) long cnt[P]; // 캐시라인 패딩

void add(int tid, long v){ cnt[tid] += v; } // contention 없음
long read_all(){ long s=0; for(int i=0;i<P;i++) s+=cnt[i]; return s; }
```

---

## 6) 동시성 제어 — 락을 똑똑하게

- **잠금 범위 축소**: 락이 보호하는 공유 구간을 최소화.  
- **락 스트라이핑**: 큰 맵을 여러 락으로 분할.  
- **RW 락**: 읽기 다수/쓰기 소수면 `rwlock`.  
- **락 없는 알고리즘**: CAS로 구현(주의: ABA, 메모리 재활용).  
- **원자 연산**: 카운터/플래그는 `atomic`으로 대체.

```c
// lock-free stack (개념적 예시, 안전한 메모리 재활용 필요)
typedef struct Node{ int v; struct Node* next; } Node;
_Atomic(Node*) head = NULL;

void push(int x){
  Node* n = malloc(sizeof *n); n->v=x;
  Node* old;
  do { old = atomic_load_explicit(&head, memory_order_relaxed);
       n->next = old;
  } while(!atomic_compare_exchange_weak_explicit(
          &head, &old, n, memory_order_release, memory_order_relaxed));
}
```

> **메모리 모델**을 모르면 잠시라도 무리하지 말고, 먼저 coarse-grained 락으로 정확성 확보 → 프로파일 → 정말 필요한 병목만 lock-free로.

---

## 7) MLP — 메모리 병렬성 올리기

- **타일링/블로킹**: 워킹셋을 캐시에 맞춰 재사용 극대화.  
- **프리패치**: 접근 패턴이 예측 가능하면 소프트웨어 프리패치.  
- **동시 outstanding miss 증가**: 스레드·태스크로 “기다림을 겹치기”.  
- **NUMA 인식**: 스레드가 접근하는 데이터는 **같은 노드**에 할당(`numactl`, first-touch).

---

## 8) 파이프라인 병렬성

- **스테이지 분할**: parse → transform → encode 같은 단계 나누고 **락-프리 큐**로 연결.  
- **배압(backpressure)**: 소비자가 느리면 큐가 무한히 커지지 않게 크기 제한+스핀 대신 슬립/양보.  
- **스테이지 균형**: 가장 느린 단계가 전체 처리량을 결정 → 폭을 맞추기(스레드 수 조정).

---

## 9) GPU/NPU 오프로딩(개요)

- **GPU**: 대용량 DLP에 최적. 병목은 **메모리 접근(coalescing)** 과 **데이터 전송(PCIe)**.  
- **NPU/행렬 유닛**: 딥러닝 추론/행렬 연산에 특화. 프레임워크(ONNX, CUDA/cuBLAS, oneDNN)를 활용.

> 오프로딩은 “전송/준비 오버헤드 < 커널 가속” 조건에서만 이득. **배치(batch)** 로 효율 극대화.

---

## 10) 정확성과 디버깅

- **결합/교환 법칙**: 병렬 reduce는 순서가 달라져 결과가 미세 차이. 허용 오차 정의.  
- **데이터 레이스**: TSAN(ThreadSanitizer)로 탐지.  
- **데드락 회피**: 락 순서 규칙, 타임아웃·try-lock 도입.  
- **재현성**: 스케줄 의존 버그는 로그·시드·고정 스케줄 옵션으로 좁힌다.

---

## 11) 성능 측정 루틴(요지)

1) **속도**: 벽시계/중앙값/표준편차.  
2) **하드웨어 카운터**: `cycles, instructions, cache-misses, branch-misses, stalled-cycles-frontend/backend`.  
3) **스케일링**: 스레드 수 vs 처리량 그래프(과적용 시 포화).  
4) **Roofline**: 메모리 바운드인지 연산 바운드인지 판단.

---

## 12) 바로 쓸 수 있는 레시피

### 12.1 벡터화 + 병렬화 (saxpy)
```c
void saxpy(int n, float *restrict x, float *restrict y, float a) {
  #pragma omp parallel for schedule(static)
  for (int i=0;i<n;i++) {
    #pragma omp simd
    for (int j=i;j<i+1;j++) // 단일 반복에도 simd 강제
      y[j] += a * x[j];
  }
}
```

### 12.2 병렬 Reduce (정확성 주의)
```c
double sum(const double *a, int n) {
  double s = 0.0;
  #pragma omp parallel for reduction(+:s) schedule(static)
  for (int i=0;i<n;i++) s += a[i];
  return s;
}
```

### 12.3 파이프라인(생산자→소비자)
```c
// 개념: SPSC lock-free ring (헤더 라이브러리 추천 사용)
```

### 12.4 false sharing 피하기 (패딩)
```c
typedef struct { _Alignas(64) long v; } padded_long;
padded_long counters[64]; // 각 스레드 전용 카운터
```

---

## 13) 체크리스트 (실행 전·후)

- [ ] 문제를 **Map/Reduce/Tile/Task**로 재구성했는가?  
- [ ] **별칭 제거(restrict)**, **정렬/연속성**, **분기 최소화**로 벡터화가 걸리는가?  
- [ ] 스레드 **그레인**이 적절한가? (너무 잘게 쪼개지 않았는가)  
- [ ] **공유 상태**를 샤딩·TLS·리듀스로 줄였는가?  
- [ ] **NUMA**를 고려했는가? 데이터와 스레드의 노드 일치.  
- [ ] 측정에서 **IPC/미스율/스레드 스케일링**이 개선되었는가?  
- [ ] 정확성: 레이스/데드락/부동소수점 오차를 검증했는가?

---

## 14) 결론

병렬성은 **알고리즘(패턴화) + 데이터 레이아웃 + 동시성 제어 + 런타임 스케줄링**의 종합 예술이다.  
작은 규칙부터 적용하자:
1) **DLP**(벡터화)로 바탕을 깔고,  
2) **TLP**(스레드/태스크)로 코어를 채우며,  
3) **MLP**(타일링/프리패치/NUMA)로 메모리 병목을 풀어라.  
항상 **프로파일→개선→재측정**의 루프를 돌리는 것이 왕도다.
