---
layout: post
title: Spring - 배치 처리
date: 2025-10-16 23:25:23 +0900
category: Spring
---
# 17. 배치 처리(Spring Batch) — 잡/스텝/청크, 리스타트/스키ップ, 스케줄링, 메타데이터, ETL 패턴

> 목표: **Spring Batch**의 핵심인 **잡/스텝/청크 모델**을 완전 이해하고, **리스타트/스키ップ/리트라이**를 안정적으로 설계하며, **스케줄링·운영 메타데이터**를 다루고, **대량 마이그레이션/ETL** 패턴을 실무 수준으로 정리한다.
> 환경: Spring Boot 3.3+, Spring Batch 5.x, Java 21, JPA/JDBC 혼용.

---

## A. Spring Batch 큰 그림 — Job / Step / Chunk

### A-1. 구성요소 요약
- **Job**: 배치의 최상위 논리 단위(여러 Step의 흐름). 파라미터로 **인스턴스** 식별.
- **Step**: 작업의 단계.
  - **Chunk-Oriented Step**(가장 일반): `ItemReader` → `ItemProcessor` → `ItemWriter` 의 **청크 단위 트랜잭션**.
  - **Tasklet Step**: 단일 태스크(스크립트, API 호출 등) 수행.
- **Execution**: Job/Step의 **실행 기록**(메타데이터에 저장). 실패/재시작의 기준.
- **Repository**: 메타 저장소(JDBC 기반 **BATCH_*** 테이블). 스키마 제공.
- **Launcher/Operator/Explorer**: 실행/관리/조회 API.

### A-2. 청크 트랜잭션
- `commit-interval=N` 또는 `chunk(N)`로 **N개 아이템을 읽고/처리/쓰기**까지 한 트랜잭션으로 커밋.
- 실패 시 **현재 청크** 전체 롤백 후 재시도/스킵 정책 적용.

---

## B. 스타터/스키마/기본 설정

### B-1. 의존성
```kotlin
dependencies {
  implementation("org.springframework.boot:spring-boot-starter-batch")
  implementation("org.springframework.boot:spring-boot-starter-jdbc")
  runtimeOnly("org.postgresql:postgresql")
  // (선택) JPA 사용 시
  implementation("org.springframework.boot:spring-boot-starter-data-jpa")
}
```

### B-2. 메타데이터 스키마
- DB 타입별 SQL 제공(예: `org/springframework/batch/core/schema-postgresql.sql`).
- Boot는 기본적으로 **자동 생성**(임베디드 DB)만, **운영 DB**는 **수동 실행** 권장.

`application.yml`
```yaml
spring:
  batch:
    jdbc:
      initialize-schema: never   # 운영 DB에는 직접 스키마 실행
    job:
      enabled: false             # 애플리케이션 시작 시 자동 실행 방지(운영에서 권장)
```

---

## C. Hello, Chunk — CSV → DB 예제

### C-1. 도메인/테이블
```sql
create table member (
  id bigserial primary key,
  email varchar(255) unique not null,
  name varchar(100) not null,
  joined_at timestamp not null
);
```

### C-2. Reader/Processor/Writer
```java
// DTO (CSV 한 줄)
public record MemberCsv(String email, String name, LocalDateTime joinedAt){}

// Reader: FlatFileItemReader
@Bean
@StepScope
public FlatFileItemReader<MemberCsv> memberCsvReader(
    @Value("#{jobParameters['csv']}") Resource csv) {
  var reader = new FlatFileItemReader<MemberCsv>();
  reader.setResource(csv);
  reader.setLinesToSkip(1);
  reader.setLineMapper((line, lineNumber) -> {
    String[] t = line.split(",");
    return new MemberCsv(t[0].trim(), t[1].trim(), LocalDateTime.parse(t[2].trim()));
  });
  return reader;
}

// Processor: 정규화/검증
@Bean
@StepScope
public ItemProcessor<MemberCsv, Member> memberProcessor() {
  return in -> {
    if (!in.email().contains("@")) return null; // null 반환 == "filter" (skip)
    var name = in.name().strip();
    return new Member(null, in.email().toLowerCase(), name, in.joinedAt());
  };
}

// Writer: JDBC batch upsert
@Bean
public JdbcBatchItemWriter<Member> memberWriter(DataSource ds) {
  var w = new JdbcBatchItemWriterBuilder<Member>()
      .dataSource(ds)
      .sql("""
          insert into member(email,name,joined_at)
          values (:email,:name,:joinedAt)
          on conflict (email) do update set name = excluded.name, joined_at = excluded.joined_at
          """)
      .beanMapped()
      .build();
  w.afterPropertiesSet();
  return w;
}
```

### C-3. Step/Job 정의
```java
@Bean
public Step importStep(JobRepository repo, PlatformTransactionManager tm,
                       FlatFileItemReader<MemberCsv> reader,
                       ItemProcessor<MemberCsv, Member> processor,
                       JdbcBatchItemWriter<Member> writer) {
  return new StepBuilder("importStep", repo)
      .<MemberCsv, Member>chunk(100, tm)   // 청크 100, 트랜잭션 경계
      .reader(reader)
      .processor(processor)
      .writer(writer)
      .faultTolerant()
      .skip(IllegalArgumentException.class) // 잘못된 이메일 등
      .skipLimit(100)                       // 최대 100건 스킵 허용
      .build();
}

@Bean
public Job importJob(JobRepository repo, Step importStep) {
  return new JobBuilder("importJob", repo)
      .incrementer(new RunIdIncrementer())  // 동일 파라미터 반복 실행 시도 시 run.id 증가
      .start(importStep)
      .build();
}
```

### C-4. 실행
```bash
java -jar app.jar \
  --spring.batch.job.enabled=true \
  --job.name=importJob csv=file:/data/members_202510.csv
```

> **포인트**
> - `@StepScope` 빈은 **실행 시점**에 생성되어 **JobParameters**를 주입받을 수 있음.
> - Processor에서 **null 반환은 필터(스킵)**. 예외는 **스킵/리트라이 정책**에 따라 처리.
> - Writer는 JDBC batch를 사용해 **대량 삽입** 효율↑.

---

## D. 리스타트(재시작)/스킵/리트라이 — 견고함의 핵심

### D-1. JobParameters로 **인스턴스 식별**
- 같은 `jobName + JobParameters` 조합은 **동일 JobInstance**.
- 재시작은 **실패한 JobExecution**을 이어감(읽은 위치/체크포인트 유지).

> 파라미터 예: `csv=file:/.../members_202510.csv date=2025-10-22`
> 날짜/버전/파일경로처럼 **재현 가능한 키**를 넣어 인스턴스를 구분.

### D-2. 체크포인트(ExecutionContext)
- `ItemStream` 구현 Reader/Writer는 **현재 위치**/상태를 `ExecutionContext`에 저장.
- `FlatFileItemReader`/`JdbcCursorItemReader` 등은 자동 관리. **커스텀 Reader**는 스스로 저장해야 함.

### D-3. 스킵/리트라이 정책
{% raw %}
```java
.faultTolerant()
.skip(ConstraintViolationException.class)
.skipLimit(1000)
.retry(SocketTimeoutException.class)
.retryLimit(3)
.backOffPolicy(new ExponentialBackOffPolicy() {{
  setInitialInterval(200L);
  setMultiplier(2.0);
  setMaxInterval(5000L);
}})
```
{% endraw %}
- **스킵**: 데이터 품질 이슈(잘못된 포맷/중복) 허용. `SkipListener`로 로깅/보조 테이블 기록.
- **리트라이**: 일시적 장애(IO/네트워크) 대비. **지터**(랜덤)로 떼빙 방지.

### D-4. Skip/Retry 리스너
```java
@Bean
public SkipListener<MemberCsv, Member> skipLogger() {
  return new SkipListener<>() {
    public void onSkipInRead(Throwable t) { log.warn("skip read: {}", t.getMessage()); }
    public void onSkipInProcess(MemberCsv item, Throwable t) { log.warn("skip proc {}:{}", item, t.getMessage()); }
    public void onSkipInWrite(Member item, Throwable t) { log.warn("skip write {}:{}", item, t.getMessage()); }
  };
}
```

### D-5. 재시작 시 주의
- **입력 소스의 불변성**: 같은 파라미터로 재시작하면 **같은 데이터**여야 함(파일 해시 검증/스냅샷 사용).
- **멱등 Writer**: upsert/merge 또는 **staging→스위치** 전략으로 **중복 반영** 방지.

---

## E. 스케줄링 — Cron, 병렬/순차, 동시 실행 잠금

### E-1. @Scheduled로 Cron 실행
```java
@Configuration
@EnableScheduling
class BatchSchedule {

  private final JobLauncher launcher;
  private final Job importJob;

  @Scheduled(cron = "0 0 2 * * *", zone = "Asia/Seoul") // 매일 02:00
  public void nightly() throws Exception {
    var p = new JobParametersBuilder()
        .addString("date", LocalDate.now().toString())
        .addString("csv", "file:/data/members_" + LocalDate.now().minusDays(1) + ".csv")
        .toJobParameters();
    launcher.run(importJob, p);
  }
}
```

### E-2. 동시 실행 방지(잠금)
- 간단: **DB 락 테이블** 사용. 실행 전에 `tryLock(jobName)` → 실패하면 return.
- Spring Batch 5의 **JobRepository**는 메타테이블 기반 락을 사용하지만, **외부 스케줄러**(Airflow, Argo, Quartz)로 **단일성 보장**이 더 확실.

### E-3. 외부 스케줄러
- **Quartz** 통합 또는 **K8s CronJob**, **Airflow**.
- 실패/성공 알림, 재시도, **잡 의존성**을 외부에서 관리하기 쉬움.

---

## F. 메타데이터 운영 — 모니터링/정리/운영 툴

### F-1. 핵심 테이블
- `BATCH_JOB_INSTANCE`(jobName + key), `BATCH_JOB_EXECUTION`(시작/종료/상태),
  `BATCH_STEP_EXECUTION`(read/write/commit/skip 카운터), `BATCH_JOB_EXECUTION_PARAMS`, `BATCH_*_CONTEXT`.

### F-2. 상태 코드
- `COMPLETED`, `FAILED`, `STOPPED`, `ABANDONED`, `UNKNOWN`.
- `ExitStatus`: `COMPLETED("code=...")`처럼 비즈니스 코드 포함 가능 → 다음 스텝 플로우 분기.

### F-3. JobExplorer/Operator
```java
@RestController
@RequiredArgsConstructor
class BatchOpsApi {
  private final JobExplorer explorer;
  private final JobOperator operator;

  @GetMapping("/batch/last/{job}")
  public Map<String,Object> last(@PathVariable String job) {
    var execs = explorer.findRunningJobExecutions(job);
    return Map.of("running", execs.size());
  }

  @PostMapping("/batch/restart/{execId}")
  public long restart(@PathVariable long execId) throws Exception {
    return operator.restart(execId);
  }

  @PostMapping("/batch/stop/{execId}")
  public void stop(@PathVariable long execId) throws Exception {
    operator.stop(execId);
  }
}
```

### F-4. 메타데이터 정리(보관 주기)
- 오래된 실행 기록을 **보관/삭제**(규정에 따라 90~365일).
- `BATCH_*` 삭제는 **외래키 순서**를 지켜야 함. 별도 **아카이브 스키마**로 이동 권장.

### F-5. 관측 지표
- Step별 `readCount/writeCount/skipCount/filterCount`, 처리율(건/초), 소요시간.
- Prometheus/Micrometer로 **메트릭** 노출, 알람(실패/장시간 실행).

---

## G. 고급 성능 — 병렬 처리, 파티셔닝, Remote Chunking

### G-1. 멀티스레드 Step (TaskExecutor)
```java
@Bean
public Step multiThreadedStep(JobRepository repo, PlatformTransactionManager tm,
                              ItemReader<Foo> reader, ItemProcessor<Foo, Bar> proc, ItemWriter<Bar> writer) {
  var te = new SimpleAsyncTaskExecutor("mt-");
  te.setConcurrencyLimit(8); // 동시성
  return new StepBuilder("multiThreadedStep", repo)
      .<Foo, Bar>chunk(500, tm)
      .reader(reader)
      .processor(proc)
      .writer(writer)
      .taskExecutor(te)
      .throttleLimit(8)
      .build();
}
```
> Reader가 **스레드 안전**해야 함. `JpaPagingItemReader`는 기본적으로 안전하지 않음 → **Thread-safe Cursor/Paging Reader** 또는 **분할(파티셔닝)** 사용.

### G-2. 파티셔닝(Partitioner)
- 한 Step을 **키 범위별**로 분할하여 **워커 스텝**을 병렬 실행.
```java
@Bean
public Partitioner rangePartitioner(JdbcTemplate jdbc) {
  return gridSize -> {
    long min = jdbc.queryForObject("select min(id) from orders", Long.class);
    long max = jdbc.queryForObject("select max(id) from orders", Long.class);
    long target = (max - min + 1) / gridSize;

    Map<String, ExecutionContext> map = new HashMap<>();
    long start = min;
    for (int i=0;i<gridSize;i++) {
      long end = (i == gridSize-1) ? max : start + target - 1;
      ExecutionContext ctx = new ExecutionContext();
      ctx.putLong("minId", start);
      ctx.putLong("maxId", end);
      map.put("partition"+i, ctx);
      start += target;
    }
    return map;
  };
}

@Bean
public Step workerStep(JobRepository repo, PlatformTransactionManager tm, ItemReader<Order> reader, ItemWriter<Order> writer) {
  return new StepBuilder("workerStep", repo)
    .<Order,Order>chunk(1000, tm)
    .reader(reader)   // reader는 minId~maxId 범위만 읽도록 @StepScope 파라미터 반영
    .writer(writer)
    .build();
}

@Bean
public Step masterStep(JobRepository repo, Partitioner partitioner, Step workerStep) {
  return new StepBuilder("masterStep", repo)
    .partitioner(workerStep.getName(), partitioner)
    .step(workerStep)
    .gridSize(8)
    .taskExecutor(new SimpleAsyncTaskExecutor("pt-"))
    .build();
}
```

### G-3. Remote Chunking / Remote Partitioning
- **메시지 브로커(Kafka/RabbitMQ)**를 사용해 **마스터**가 읽은 청크를 **워커**에게 분산(원격 청크).
- 고도 확장. 대신 **정합성/멱등성/관측성** 설계가 필요. (Spring Cloud Task/Stream 참고)

---

## H. 대량 마이그레이션 / ETL 패턴

### H-1. 파일 → DB (CSV/JSON/Parquet)
- `FlatFileItemReader`(CSV), **Jackson** 기반 JSON 라인 파서, Parquet는 별도 라이브러리.
- **검증/정규화** → **staging 테이블** → **MERGE**(upsert)로 본 테이블 반영.

### H-2. DB → DB (대량 이동)
- Source: `JdbcCursorItemReader`(fetchSize, read-only, auto-commit=false).
- Sink: `JdbcBatchItemWriter` 또는 **대용량 COPY**(Postgres `COPY`, MySQL `LOAD DATA`) → **Tasklet**으로 수행.
```java
@Bean
@StepScope
public JdbcCursorItemReader<SourceRow> cursor(DataSource srcDs,
    @Value("#{jobParameters['fromId']}") Long fromId) {
  return new JdbcCursorItemReaderBuilder<SourceRow>()
      .dataSource(srcDs)
      .name("srcReader")
      .sql("select id, name, updated_at from src where id >= ? order by id")
      .preparedStatementSetter(ps -> ps.setLong(1, fromId))
      .rowMapper((rs, i) -> new SourceRow(rs.getLong(1), rs.getString(2), rs.getTimestamp(3).toInstant()))
      .fetchSize(5_000)
      .saveState(true)
      .build();
}
```
> **대량 SELECT는 Paging보다 Cursor가 유리**(중간 insert로 인한 갭/중복 회피).

### H-3. CDC(Change Data Capture)
- Debezium/Kafka Connect로 변경 이벤트를 토픽으로 수집 → **배치 Step**이 DLT/보정용으로 소비.
- Type 2 SCD(이력 관리)는 **유효기간 컬럼**(valid_from/valid_to)와 **현재 플래그**로 모델링.

### H-4. SCD Type 2 간단 예
```sql
create table dim_customer (
  sk bigserial primary key,
  natural_key varchar(100) not null,
  name varchar(100),
  valid_from timestamp not null,
  valid_to   timestamp,
  current boolean not null default true
);
create unique index on dim_customer(natural_key, current) where current = true;
```
Writer에서:
- natural_key로 현재 row 조회 → 값 변화 있으면 **current=false, valid_to=now()**로 닫고 **신규 행 insert**.

### H-5. 파일 생성(보고서/S3 업로드)
- `FlatFileItemWriter`로 CSV 생성 → Step 종료 후 **Tasklet**로 S3 업로드.

---

## I. 테스트 — @SpringBatchTest / 슬라이스 / 리더·라이터 단위

### I-1. 의존성
```kotlin
testImplementation("org.springframework.batch:spring-batch-test")
testImplementation("org.springframework.boot:spring-boot-starter-test")
```

### I-2. 예제
```java
@SpringBatchTest
@SpringBootTest
class ImportJobTest {

  @Autowired JobLauncherTestUtils utils;
  @Autowired JobRepositoryTestUtils repoUtils;

  @BeforeEach void clean() { repoUtils.removeJobExecutions(); }

  @Test
  void job_ok() throws Exception {
    var params = new JobParametersBuilder()
        .addString("csv", "file:src/test/resources/members.csv")
        .addString("date", "2025-10-22")
        .toJobParameters();

    var exec = utils.launchJob(params);
    assertThat(exec.getStatus()).isEqualTo(BatchStatus.COMPLETED);

    var step = exec.getStepExecutions().iterator().next();
    assertThat(step.getReadCount()).isEqualTo(1000);
    assertThat(step.getWriteCount()).isEqualTo(995); // 5개 필터
  }
}
```

---

## J. 운영/튜닝 체크리스트

### J-1. Reader/Writer 튜닝
- JDBC: **fetchSize**(수천 단위), auto-commit off, read-only, 적절한 인덱스.
- Writer: **batch size**(JDBC/드라이버 옵션), upsert SQL 튜닝.
- JPA 사용 시: `JpaPagingItemReader`는 대량에 **비추천**(1차 캐시·더티체크 오버헤드). **JDBC**로 전환.

### J-2. 청크 크기/메모리
- 청크 너무 작으면 **오버헤드↑**, 너무 크면 **롤백 비용↑**. 100~1000 사이에서 벤치마크.
- Processor에서 큰 객체 생성/누수 금지. 스트림/버퍼 즉시 해제.

### J-3. 트랜잭션/락
- 대량 업데이트 시 **락 경합** 주의. 파티셔닝으로 **키 범위 분리**.
- 인덱스/외래키 상태 점검. **스테이징→스위치** 전략이 안전한 경우 많음.

### J-4. 실패 대응/알림
- Step 실패 시 **Slack/메일** 알림.
- 스킵 로그/파일 따로 보관, 리플레이 도구 제공.
- **재처리 가능한 단위**로 설계(청크/파티션/세그먼트).

### J-5. 리소스/풀
- **Hikari** 커넥션 풀 제한(동시성 대비), Statement 캐시.
- 스레드풀(멀티스텝/멀티스레드) 모니터링.
- GC 튜닝(대량 객체 생성 피하기, G1/Region 사이즈 확인).

---

## K. 실전 템플릿 — “대량 CSV → 정합성 있는 Upsert” 한 장 요약

1) **Reader**: `FlatFileItemReader`(라인 파싱, 라인넘버/오류 기록).
2) **Processor**: 정규화·검증·필터링(null). **비즈니스 룰 실패는 스킵**.
3) **Writer**: `JdbcBatchItemWriter`로 **MERGE/UPSERT**. 유니크 키 기준 **멱등**.
4) **faultTolerant**: `skipLimit`, `retryLimit`, `ExponentialBackOff`.
5) **리스타트**: 파일 **불변/해시 확인**, Reader 상태 저장.
6) **스케줄링**: Cron + **동시 실행 방지**.
7) **메타 운영**: JobExplorer/Operator API, 메트릭/알람.
8) **성능**: 청크 500~2000, fetchSize 5k, 파티셔닝 4~16, Writer batch 1k 벤치.

---

## L. 한 페이지 요약
- Spring Batch는 **청크 트랜잭션**으로 “대량 데이터를 안전하게” 처리한다.
- **JobParameters**로 인스턴스를 식별하고, **체크포인트/리스타트**로 중단 후 재개를 보장한다.
- **스킵/리트라이**를 분리해 데이터 문제와 일시 장애를 각각 다룬다.
- 운영은 **스케줄링/메타데이터/모니터링/알림**이 핵심이며, **파티셔닝/멀티스레드/원격 청크**로 확장한다.
- ETL/마이그레이션은 **staging→merge**와 **멱등성**을 중심으로, 파일/DB/CDC 등 다양한 소스를 안전하게 흡수한다.
