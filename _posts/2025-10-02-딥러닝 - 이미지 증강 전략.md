---
layout: post
title: 딥러닝 - 이미지 증강 전략
date: 2025-10-02 18:25:23 +0900
category: 딥러닝
---
# 이미지 증강 전략 총정리  
**기하(Geometric) · 컬러(Photometric) · AutoAugment/RandAugment · CutMix/MixUp**

## 0) 왜 증강인가? (Bias–Variance·데이터 증대 관점)

- **데이터 다양화**로 **분산(Variance)** 을 낮추고, 오버피팅을 줄입니다.  
- **도메인 쉬프트**(조명/각도/해상도/압축 변화)에 견고해집니다.  
- 하드 클래스에서 **결정 경계**를 매끄럽게(regularization) 하여 **칼리브레이션**도 좋아질 수 있습니다.

**확률 기하학 관점**: 원본 분포 \(p(\mathbf{x},y)\)에서 증강 변환 \(T\) 를 샘플하면  
증강 데이터 분포는  
$$
p_T(\tilde{\mathbf{x}}, y) = \int p(\mathbf{x},y)\,\delta(\tilde{\mathbf{x}}=T(\mathbf{x}))\,p(T)\,d\mathbf{x}\,dT
$$
즉, **작은 연산으로 거대한 가상 데이터셋**을 만든다고 볼 수 있습니다.

---

## 1) 기하 변환(Geometric) — 위치·형태를 바꾸는 증강

### 1.1 기본 변환
- **수평/수직 뒤집기**: `RandomHorizontalFlip`, `RandomVerticalFlip`  
  - 사람/자연물은 안정, **문자/화살표/숫자**는 라벨이 뒤집혀 **의미가 바뀔 수 있음** → 신중히 사용.
- **회전/기울임/아핀**: `RandomRotation`, `RandomAffine`  
  - 각도 범위가 과하면 †객체가 잘려나가거나 의미 손상.
- **크롭/리사이즈**: `RandomResizedCrop` (scale, ratio)  
  - **포커스 이동** 효과로 부분 특징 학습을 장려.  
- **패딩/리플렉트 패드**: 작은 이미지에서 유용.  
- **퍼스펙티브 변환**: 뷰포인트 변화에 견고.

#### 코드: 기초 파이프라인
```python
from torchvision import transforms as T
from torchvision.transforms import InterpolationMode

IMG = 224
geo_train = T.Compose([
    T.RandomResizedCrop(IMG, scale=(0.6, 1.0), ratio=(3/4, 4/3), interpolation=InterpolationMode.BICUBIC, antialias=True),
    T.RandomHorizontalFlip(p=0.5),
    T.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.95,1.05), shear=5, interpolation=InterpolationMode.BILINEAR),
    T.ToTensor(),
])
geo_val = T.Compose([
    T.Resize(int(IMG*1.14), interpolation=InterpolationMode.BICUBIC, antialias=True),
    T.CenterCrop(IMG),
    T.ToTensor(),
])
```

**팁(실무)**  
- **antialias=True**: 리사이즈 시 링잉·모아레 감소.  
- **Interpolation**: 작은 글자/라인 아트 → `BILINEAR`/`BICUBIC` 비교, 고주파 손실 주의.  
- **비율 유지**: 장신/와이드 객체가 많은 도메인은 ratio 범위 좁게.

### 1.2 영역 마스킹(Erasing/Cutout)
- **Cutout/RandomErasing**: 이미지의 일부를 지워 occlusion에 견고.  
  - 너무 큰 영역을 지우면 **라벨 오염** 가능(객체 전체 삭제).

```python
from torchvision.transforms import RandomErasing
geo_train_plus = T.Compose([
    T.RandomResizedCrop(IMG, scale=(0.6,1.0), ratio=(3/4,4/3)),
    T.RandomHorizontalFlip(),
    T.ToTensor(),
    RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random'),  # Cutout류
])
```

---

## 2) 컬러/광학 변환(Photometric) — 색/밝기/노이즈

### 2.1 기본 컬러 증강
- **ColorJitter**: 밝기(B), 대비(C), 채도(S), 색상(H)  
- **Grayscale**: 흑백화로 색상 의존성 완화  
- **GaussianBlur**: 모션/디포커스 대비  
- **JPEG 압축/포스터라이즈/솔러라이즈**: 압축 환경/센서 특성 대비

```python
from torchvision import transforms as T

color_train = T.Compose([
    T.RandomResizedCrop(IMG),
    T.RandomHorizontalFlip(),
    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),
    T.RandomGrayscale(p=0.1),
    T.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),
    T.ToTensor(),
])
```

**주의**  
- **의미 보존** 필수: 과한 Hue 변화는 클래스 바꿈(예: 과일/신호등/유니폼 색) → 범위 점진적 튜닝.  
- **정규화 순서**: `ToTensor` → `Normalize(mean,std)` 는 항상 **마지막** 부분.

---

## 3) 정책 기반 증강 — AutoAugment / RandAugment / TrivialAugment

### 3.1 개념
- **AutoAugment**: **정책(policy)**(여러 변환의 조합, 확률, 강도)를 **탐색**하여 가장 좋은 성능을 주는 세트를 사용.  
- **RandAugment**: 탐색 없이 **N개의 연산을 랜덤**으로 택하고 **강도 M**만 조정 → 간단/강력/범용.  
- **TrivialAugmentWide**: **단일 강도 없이** 넓게 무작위 → 소데이터·빠른 베이스라인.

### 3.2 torchvision 내장 사용법
```python
from torchvision.transforms import AutoAugment, AutoAugmentPolicy
from torchvision.transforms import RandAugment, TrivialAugmentWide, InterpolationMode

IMG=224
auto_train = T.Compose([
    T.RandomResizedCrop(IMG, interpolation=InterpolationMode.BICUBIC, antialias=True),
    T.RandomHorizontalFlip(),
    AutoAugment(policy=AutoAugmentPolicy.IMAGENET),   # CIFAR10, SVHN 정책도 있음
    T.ToTensor(),
    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

rand_train = T.Compose([
    T.RandomResizedCrop(IMG),
    T.RandomHorizontalFlip(),
    RandAugment(num_ops=2, magnitude=9),  # N=2~3, M=7~12에서 탐색
    T.ToTensor(),
    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

trivial_train = T.Compose([
    T.RandomResizedCrop(IMG),
    T.RandomHorizontalFlip(),
    TrivialAugmentWide(),  # 빠른 스타터
    T.ToTensor(),
    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
```

**튜닝 가이드(경험칙)**  
- **RandAugment**: `num_ops=2~3`, `magnitude=7~12`를 그리드 소규모 탐색.  
- **소데이터**: TrivialAugmentWide → RandAugment → AutoAugment 순으로 시도.  
- **고난도/대규모**: AutoAugment(IMAGENET policy) + CutMix/MixUp 결합이 강력.

---

## 4) MixUp / CutMix — 샘플 간 **혼합** 증강

### 4.1 수학적 정의

#### MixUp
- 두 샘플 \((\mathbf{x}_i,y_i)\), \((\mathbf{x}_j,y_j)\) 를 베타 분포 \(\lambda \sim \mathrm{Beta}(\alpha,\alpha)\) 로 섞어  
$$
\tilde{\mathbf{x}} = \lambda \mathbf{x}_i + (1-\lambda)\mathbf{x}_j,\quad
\tilde{\mathbf{y}} = \lambda \mathbf{y}_i + (1-\lambda)\mathbf{y}_j
$$
- **효과**: 결정 경계 **선형화**, **라벨 부드러움** → 과신 완화, 일반화↑

#### CutMix
- 이미지의 **사각 패치**를 잘라 **다른 이미지**로 대체:  
$$
\tilde{\mathbf{x}} = \mathbf{M}\odot \mathbf{x}_i + (1-\mathbf{M})\odot \mathbf{x}_j,
\quad \lambda = \frac{\|\mathbf{M}\|_1}{HW},\quad
\tilde{\mathbf{y}} = \lambda \mathbf{y}_i + (1-\lambda)\mathbf{y}_j
$$
- **효과**: MixUp보다 **국소 특징 보존**, occlusion/부분 관측에 강함.

### 4.2 PyTorch 구현(분류, one-hot 라벨 기준)

#### 준비: one-hot 유틸
```python
import torch

def to_onehot(target, num_classes):
    y = torch.zeros(target.size(0), num_classes, device=target.device)
    y.scatter_(1, target.view(-1,1), 1.)
    return y
```

#### MixUp
```python
import numpy as np

def mixup_data(x, y, alpha=0.2):
    if alpha <= 0:
        return x, y.float(), 1.0, torch.arange(x.size(0), device=x.device)
    lam = np.random.beta(alpha, alpha)
    idx = torch.randperm(x.size(0), device=x.device)
    x_mix = lam * x + (1 - lam) * x[idx]
    y1 = to_onehot(y, num_classes=x.size(1) if y.ndim>1 else num_classes)
    y2 = to_onehot(y[idx], num_classes=y1.size(1))
    y_mix = lam * y1 + (1 - lam) * y2
    return x_mix, y_mix, lam, idx
```

#### CutMix
```python
import random

def rand_bbox(W, H, lam):
    # lam에서 bbox 크기 도출(정확한 면적 비율)
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)
    cx = np.random.randint(W)
    cy = np.random.randint(H)
    x1 = np.clip(cx - cut_w // 2, 0, W)
    y1 = np.clip(cy - cut_h // 2, 0, H)
    x2 = np.clip(cx + cut_w // 2, 0, W)
    y2 = np.clip(cy + cut_h // 2, 0, H)
    return x1, y1, x2, y2

def cutmix_data(x, y, alpha=1.0):
    # alpha=1.0 권장. 0이면 미사용.
    if alpha <= 0:
        return x, y.float(), 1.0, torch.arange(x.size(0), device=x.device)
    lam = np.random.beta(alpha, alpha)
    idx = torch.randperm(x.size(0), device=x.device)
    B, C, H, W = x.size()
    x_cut = x.clone()
    x1,y1,x2,y2 = rand_bbox(W, H, lam)
    x_cut[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]
    lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))  # 정확한 라벨 람다 재계산
    y1_oh = to_onehot(y, num_classes=num_classes)
    y2_oh = to_onehot(y[idx], num_classes=num_classes)
    y_mix = lam * y1_oh + (1 - lam) * y2_oh
    return x_cut, y_mix, lam, idx
```

#### 손실 계산(CE with soft target)
```python
import torch.nn.functional as F

def ce_soft(logits, y_soft):
    logp = F.log_softmax(logits, dim=-1)
    return -(y_soft * logp).sum(dim=-1).mean()
```

#### 학습 루프에서 혼합 사용(확률로 MixUp/CutMix/None)
```python
MIXUP_ALPHA = 0.2
CUTMIX_ALPHA = 1.0
P_MIXUP, P_CUTMIX = 0.5, 0.5  # 동시에 둘 다 켜지지 않게 조절 권장

model.train()
for xb, yb in train_loader:
    xb, yb = xb.to(device), yb.to(device)
    use = random.random()
    if use < P_MIXUP:
        x_aug, y_soft, lam, idx = mixup_data(xb, yb, alpha=MIXUP_ALPHA)
    elif use < P_MIXUP + P_CUTMIX:
        x_aug, y_soft, lam, idx = cutmix_data(xb, yb, alpha=CUTMIX_ALPHA)
    else:
        x_aug = xb
        y_soft = to_onehot(yb, num_classes=num_classes).float()

    logits = model(x_aug)
    loss = ce_soft(logits, y_soft)
    # backward/step ...
```

**팁**  
- **Label Smoothing**와 중복 적용 시 강도가 과해질 수 있음 → 둘 중 하나를 약하게.  
- **멀티라벨(BCE)** 에서는 one-hot 대신 **원래 멀티-핫 벡터**를 같은 방식으로 섞으면 됩니다.

---

## 5) 증강 레시피: 상황별 모범안

### 5.1 소규모 데이터(클래스당 50장 미만)
- **강한 기하+컬러** + **TrivialAugmentWide**  
- **MixUp(α=0.2)** 자주, **CutMix** 낮은 확률  
- Early stopping/적은 epoch, **Linear probe → 점진적 unfreeze**

### 5.2 일반 규모(수천~수만 장)
- **RandAugment(N=2~3, M=7~12)** + 기본 기하 변환  
- **MixUp 0.5, CutMix 0.5**, 둘의 비율은 검증으로 조정  
- Cosine 스케줄 + AMP + Label Smoothing(0.05)

### 5.3 고난도/대규모(수십만 장 이상)
- **AutoAugment(ImageNet)** + **CutMix/MixUp** 조합  
- **Dropout/DropPath**(아키텍처에 따라), **Stochastic Depth**  
- 장시간 학습, 강한 정규화(Weight Decay↑)

---

## 6) 검증/테스트에서의 증강 — 반드시 구분!

- **검증/테스트**: 일반적으로 **CenterCrop + Normalize** **만** 사용 (데이터 분포 대표).  
- **TTA(Test-Time Augment)**: HFlip/ResizedCrop 몇 가지로 평균 → P95 리스크 줄이기.  
  - **SLA 엄격** 시 TTA는 지연/비용 증가 → 오프라인/배치용.

```python
def tta_predict(model, x):
    # x: (B,3,H,W) normalized tensor
    xs = [x, torch.flip(x, dims=[-1])]
    with torch.no_grad():
        ps = [torch.softmax(model(z), -1) for z in xs]
    return torch.stack(ps,0).mean(0)
```

---

## 7) 탐색과 평가 — 증강 강도 튜닝 절차

1) **베이스라인**(가벼운 기하+컬러) 확보  
2) **RandAugment**: N, M 그리드(작게) → 베스트 선택  
3) **MixUp/CutMix 확률/α** 그리드 → **Macro-F1/PR-AUC** 기준 비교(불균형 시)  
4) **라벨 품질 의심**: MixUp↑, Label Smoothing↑ → 고손실 샘플 검수  
5) **Over-regularization 감지**: 학습 손실은 낮은데 검증 정확도 **정체/하락** → 강도/확률 하향

**주의 신호**  
- 과도한 증강으로 **훈련 수렴 느림/불안정**  
- 클래스별 **리콜 급락**(특히 미소/텍스트/세밀 패턴 의존 클래스)

---

## 8) 성능 및 재현성

- **DataLoader 성능**: `num_workers`(코어수/2 ~ 코어수), `pin_memory=True`, `persistent_workers=True`  
- **시드/재현성**: `torch.Generator().manual_seed(seed)` + worker init에서 넘기기  
- **GPU 전처리**(고급): Kornia/torchvision v2 transforms(텐서형)로 **GPU 상에서** 일부 색/기하 연산 → 파이프라인 가속

```python
def worker_init_fn(worker_id):
    seed = 42 + worker_id
    np.random.seed(seed); random.seed(seed)

g = torch.Generator().manual_seed(42)
train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4,
                          pin_memory=True, persistent_workers=True,
                          worker_init_fn=worker_init_fn, generator=g)
```

---

## 9) 세부 도메인 주의

- **문자/문서 OCR**: 수평 뒤집기 금지, 회전 범위 제한, 콘트라스트/노이즈 조절  
- **의료영상**: 임상의 **해석 일관성** 유지(좌/우, 상/하 의미 보존), 강도 보수적  
- **원격탐사/위성**: 스펙트럼 밴드 별 **정규화/밝기** 일관, 지리적 회전은 의미가 다를 수 있음  
- **제품/패션**: 색이 레이블 핵심이면 Hue 증강 최소화

---

## 10) 통합 예제: 전이학습 + RandAugment + MixUp/CutMix

```python
import torch, torch.nn as nn, torch.nn.functional as F, random, numpy as np
import torchvision.models as M
from torchvision import transforms as T
from torchvision.transforms import RandAugment, InterpolationMode
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder

device="cuda" if torch.cuda.is_available() else "cpu"
IMG, BATCH = 224, 64

train_tf = T.Compose([
    T.RandomResizedCrop(IMG, scale=(0.6,1.0), ratio=(3/4,4/3), interpolation=InterpolationMode.BICUBIC, antialias=True),
    T.RandomHorizontalFlip(),
    RandAugment(num_ops=2, magnitude=9),
    T.ToTensor(),
    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

val_tf = T.Compose([
    T.Resize(int(IMG*1.14), interpolation=InterpolationMode.BICUBIC, antialias=True),
    T.CenterCrop(IMG),
    T.ToTensor(),
    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

train_ds = ImageFolder("dataset/train", transform=train_tf)
val_ds   = ImageFolder("dataset/val",   transform=val_tf)
num_classes = len(train_ds.classes)

train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True)
val_loader   = DataLoader(val_ds,   batch_size=BATCH*2, shuffle=False, num_workers=4)

# 모델
m = M.resnet50(weights=M.ResNet50_Weights.IMAGENET1K_V2)
m.fc = nn.Linear(m.fc.in_features, num_classes)
m = m.to(device)

opt = torch.optim.AdamW(m.parameters(), lr=3e-4, weight_decay=1e-4)
scaler = GradScaler()
sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=20)

def to_onehot(y):
    z = torch.zeros(y.size(0), num_classes, device=y.device)
    z.scatter_(1, y.view(-1,1), 1.)
    return z

def ce_soft(logits, y_soft):
    return -(y_soft * F.log_softmax(logits, -1)).sum(-1).mean()

def mixup(x, y, a=0.2):
    if a<=0: return x, to_onehot(y), 1.0
    lam = np.random.beta(a,a)
    idx = torch.randperm(x.size(0), device=x.device)
    xm = lam*x + (1-lam)*x[idx]
    ym = lam*to_onehot(y) + (1-lam)*to_onehot(y[idx])
    return xm, ym, lam

def cutmix(x, y, a=1.0):
    if a<=0: return x, to_onehot(y), 1.0
    lam = np.random.beta(a,a)
    B,C,H,W = x.size()
    idx = torch.randperm(B, device=x.device)
    cut_rat = np.sqrt(1. - lam)
    cw, ch = int(W*cut_rat), int(H*cut_rat)
    cx, cy = np.random.randint(W), np.random.randint(H)
    x1, y1 = np.clip(cx - cw//2, 0, W), np.clip(cy - ch//2, 0, H)
    x2, y2 = np.clip(cx + cw//2, 0, W), np.clip(cy + ch//2, 0, H)
    xm = x.clone()
    xm[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]
    lam = 1 - ((x2-x1)*(y2-y1)/(W*H))
    ym = lam*to_onehot(y) + (1-lam)*to_onehot(y[idx])
    return xm, ym, lam

MIXUP_A, CUTMIX_A = 0.2, 1.0
P_MIXUP, P_CUTMIX = 0.4, 0.4

best = 0.0
for ep in range(20):
    m.train()
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        r = random.random()
        if r < P_MIXUP:
            xa, ysoft, _ = mixup(xb, yb, MIXUP_A)
        elif r < P_MIXUP + P_CUTMIX:
            xa, ysoft, _ = cutmix(xb, yb, CUTMIX_A)
        else:
            xa, ysoft = xb, to_onehot(yb)

        opt.zero_grad(set_to_none=True)
        with autocast(dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32):
            logits = m(xa)
            loss = ce_soft(logits, ysoft)
        scaler.scale(loss).backward()
        scaler.unscale_(opt)
        torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)
        scaler.step(opt); scaler.update()
    sched.step()

    # 평가
    m.eval(); corr=tot=0
    with torch.no_grad():
        for xb, yb in val_loader:
            p = m(xb.to(device)).argmax(1).cpu()
            corr += (p==yb).sum().item(); tot += yb.size(0)
    acc = corr/tot
    print(f"[{ep+1:02d}] val acc={acc:.4f}")
    if acc>best:
        best=acc; torch.save(m.state_dict(),"best.pt")
```

---

## 11) 에러·함정 체크리스트

- [ ] **검증/테스트에 학습 증강 넣지 않기**(데이터 누수).  
- [ ] 좌우반전/회전이 **라벨 의미를 바꾸지 않는지** 확인(텍스트/숫자/로고 등).  
- [ ] 강한 Hue/ColorJitter로 **클래스 결정 신호**를 파괴하지 않았는지.  
- [ ] MixUp/CutMix 확률이 **너무 높아** 학습이 느려지지 않는지(초기 수렴 저하).  
- [ ] Normalize 순서/mean,std가 **백본 사전학습과 일치**하는지.  
- [ ] 재현성(시드/Generator/worker_init_fn)과 **도커/OS 버전** 간 차이 여부.

---

## 12) 요약(퀵 가이드)

- **기하**: RandomResizedCrop + HFlip + (Affine/Rotate 약하게)  
- **컬러**: ColorJitter 소량 + Grayscale/Blur 가끔  
- **정책**: RandAugment(N=2~3, M=7~12) → 필요시 AutoAugment  
- **혼합**: MixUp(α≈0.2) + CutMix(α≈1.0), 각 p≈0.4 전후  
- **평가**: 증강은 train 전용, 검증은 표준화만. TTA는 선택.  
- **튜닝**: 베이스라인 → RA 강도 → MixUp/CutMix 확률/α → 결과 슬라이스 분석

이 레시피를 스타터로 삼고, **도메인 지식**(라벨 불변성)과 **업무 메트릭**(Macro-F1/PR-AUC)을 기준으로 강도/확률을 조정하면,  
작은 데이터나 노이즈 많은 상황에서도 **견고하고 일반화 잘 되는 분류기**를 빠르게 얻을 수 있습니다.