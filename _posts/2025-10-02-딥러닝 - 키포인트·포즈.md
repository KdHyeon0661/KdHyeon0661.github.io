---
layout: post
title: 딥러닝 - 키포인트·포즈
date: 2025-10-02 21:25:23 +0900
category: 딥러닝
---
# 키포인트·포즈 추정 총정리  
**히트맵(Heatmap) & 토포로지(Topology) 개념부터 Top-down/Bottom-up 파이프라인, 손실/디코딩/평가, PyTorch 예제 코드까지**

## 0) 문제 정의 & 메트릭

### 0.1 키포인트·포즈란?
- 입력 이미지 \(\mathbf{I}\in\mathbb{R}^{H\times W\times 3}\)에서 **관절/랜드마크** 좌표 집합 \(\mathcal{K}=\{(x_j,y_j)\}_{j=1}^J\) 를 예측.  
- 사람 포즈의 경우 **토포로지(Topology)** = “관절 간 연결 그래프(스켈레톤)” 정의가 함께 쓰임. (예: COCO 17키 + 19개 엣지)

### 0.2 대표 메트릭
- **PCK(Percentage of Correct Keypoints)**: 정답 대비 오차가 **기준 길이**(머리/토르소/박스 크기 등)의 비율 \( \alpha \) 이하인 키포인트 비율.
  $$
  \mathrm{PCK}@\alpha = \frac{1}{NJ}\sum_{n=1}^N \sum_{j=1}^J \mathbb{1}\!\left(\frac{\lVert \hat{\mathbf{k}}_{n,j}-\mathbf{k}_{n,j}\rVert_2}{s_n} \le \alpha\right).
  $$
- **OKS(Object Keypoint Similarity)**: IoU의 키포인트 버전. 키포인트별 공차 \( \sigma_j \)와 객체 스케일 \(s\)을 반영.
  $$
  \mathrm{OKS} = \frac{\sum_j \exp\!\left(-\frac{d_j^2}{2 s^2 \sigma_j^2}\right)\,\delta(v_j>0)}{\sum_j \delta(v_j>0)},
  $$
  \(d_j\): 예측-GT 거리, \(v_j\): 가시성, \(s\): 객체 스케일(예: 박스 면적\(^\frac12\)).  
  COCO는 **OKS-mAP**(AP@[.50:.95] 평균)을 표준으로 사용.

---

## 1) 표현(Representation): 히트맵·오프셋·PAF

### 1.1 히트맵(Heatmap) — 가장 널리 쓰이는 키포인트 표현
- 각 키포인트 \(j\)에 대해 2D 히트맵 \(H_j\in[0,1]^{h\times w}\)을 예측.  
- 학습 타깃은 보통 **2D 가우시안**:
  $$
  H_j(u,v) = \exp\!\left(-\frac{(u-x_j^\*)^2 + (v-y_j^\*)^2}{2\sigma_j^2}\right).
  $$
  \(\sigma_j\)는 객체 크기/부위 크기에 비례해 설정(작은 관절은 더 작은 \(\sigma\)).

- **장점**: 미분 가능, 지역 최대값 탐색으로 디코딩 용이, 하이리졸루션 특징 이용에 유리.  
- **보완**: 서브픽셀 정밀도 향상을 위해 **소프트-아그맥스(Integral Pose)** 또는 **로컬 가우시안 피팅**을 사용.

### 1.2 오프셋/리파인(Offset/Refine) 지도(선택)
- 히트맵 해상도 한계 보완: 각 픽셀에서 키포인트까지의 **오프셋 벡터** \(\Delta x,\Delta y\)를 추가 회귀하여 정밀도 향상.

### 1.3 PAF(Part Affinity Field) — 연결 토포로지(Topology) 표현
- **엣지(관절쌍)** \(e=(a,b)\)마다 벡터 필드 \(\mathbf{F}_e(u,v) \in \mathbb{R}^2\) 를 예측.
- GT 생성: GT 관절 \(a,b\)를 잇는 선분 근처 픽셀에 **정규화된 방향 벡터** \(\hat{\mathbf{d}}=\frac{\mathbf{k}_b-\mathbf{k}_a}{\|\cdot\|}\)를 채움.
  - 선분 폭(띠) \(\tau\) 안에만 벡터를 두고, 바깥은 \(\mathbf{0}\).

- **쓰임**: Bottom-up에서 **키포인트를 사람별로 묶는(그룹핑)** 점수로 사용.

---

## 2) 파이프라인 개관: Top-down vs Bottom-up

| 구분 | Top-down (검출→포즈) | Bottom-up (전역 키포인트→그룹핑) |
|---|---|---|
| 흐름 | 1) 사람 박스 검출 → 2) 박스마다 포즈 | 1) 모든 키포인트 히트맵/PAF 예측 → 2) 연결·그룹핑 |
| 장점 | 단일 인스턴스에서 정밀도↑, 학습·튜닝 단순 | 다중 인원/밀집 장면에서 확장성↑, 한 번 추론 |
| 단점 | 박스 누락 시 포즈 실패, 인원수 ∝ 비용 | 그룹핑 구현 복잡, 훈련 타깃(히트맵+PAF) 제작 필요 |
| 대표 | SimpleBaseline/HRNet-W32/DeiT-Pose 등 | OpenPose/AE(Associative Embedding)/HigherHRNet/YOLOv8-pose(AF) 등 |

> **토포로지(Topology)** = 관절 그래프 정의(예: COCO 17관절, 19엣지). Bottom-up은 이 **엣지 목록**에 따라 PAF/연결을 계산합니다.

---

## 3) 데이터: 어노테이션·증강·타깃 생성

### 3.1 어노테이션 포맷(간단 COCO 스타일)
- `keypoints`: \([x_1,y_1,v_1,\dots,x_J,y_J,v_J]\) (가시성 \(v\in\{0,1,2\}\), 0=미표시, 1=표시·가림, 2=표시·보임)  
- `bbox`: 사람 박스 \([x,y,w,h]\)

### 3.2 증강(라벨 불변 조건)
- 기하(스케일/회전/이동/플립), 컬러(밝기/대비) — **키포인트 좌표**를 함께 변환.  
- 플립 시 **좌/우 관절 스왑**(예: Left-Wrist ↔ Right-Wrist).

### 3.3 타깃 생성(히트맵/PAF)
- **히트맵**: 각 키포인트 위치에 \(\sigma\)반경 가우시안.  
- **PAF**: 엣지마다 두 관절을 잇는 **띠 영역**에 단위 방향 벡터를 채움(나머지는 \(\mathbf{0}\)).  
- **마스킹**: \(v_j=0\) (미표시)면 해당 키포인트/엣지는 손실에서 제외.

---

## 4) 손실 함수(Heatmap/PAF/Offset)

### 4.1 히트맵 손실
- **MSE** (가우시안 타깃):  
  $$
  \mathcal{L}_{\text{hm}}=\frac{1}{J}\sum_{j}\frac{1}{hw}\lVert \hat{H}_j - H_j\rVert_2^2.
  $$
- **Focal-style(희소 피크 강화)**:  
  $$
  \mathcal{L}=\!-\!\frac{1}{Jhw}\sum_{j,u,v}\!
  \begin{cases}
  (1-\hat{H})^\gamma \log(\hat{H}) & H=1 \\
  (1-H)^\beta \hat{H}^\gamma \log(1-\hat{H}) & H<1
  \end{cases}
  $$
  (실무에선 보통 MSE로 시작 → 필요 시 Focal로 강화)

### 4.2 오프셋/리파인 손실
- 히트맵 최대점 근방 픽셀에 대해 \(\ell_1/\ell_2\) 손실.

### 4.3 PAF 손실
- **L2 회귀**:  
  $$
  \mathcal{L}_{\text{paf}}=\frac{1}{Ehw}\sum_{e}\sum_{u,v} m_{e,u,v}\,\lVert \hat{\mathbf{F}}_e(u,v)-\mathbf{F}_e(u,v)\rVert_2^2,
  $$
  \(m\): 띠 영역 마스크.

### 4.4 총손실
- \( \mathcal{L} = \lambda_{\text{hm}}\mathcal{L}_{\text{hm}} + \lambda_{\text{paf}}\mathcal{L}_{\text{paf}} + \lambda_{\text{off}}\mathcal{L}_{\text{off}} \).  
  (Top-down은 보통 \(\mathcal{L}_{\text{hm}}\)만으로도 강력)

---

## 5) 디코딩(Decoding): 히트맵→좌표, PAF→그룹핑

### 5.1 히트맵 피크 검출
- **NMS(맥스풀 기반)** 로 로컬 피크 후보 찾기 → 스코어 기준 상위 K.  
- **서브픽셀**: (a) 로컬 \(3\times3\) 가우시안 피팅, 혹은 (b) **소프트-아그맥스(Integral Pose)**:
  $$
  \hat{x}_j=\sum_{u}u\cdot \mathrm{softmax}(H_j)_{u,\cdot},\quad
  \hat{y}_j=\sum_{v}v\cdot \mathrm{softmax}(H_j)_{\cdot,v}.
  $$

### 5.2 Bottom-up 그룹핑(간단 OpenPose식)
1) 모든 관절 타입별 **피크 후보** 추출  
2) 엣지 \(e=(a,b)\)마다, a-피크와 b-피크 **쌍**에 대해 **PAF 선적분 점수** 계산:
   $$
   s(\mathbf{p}_a,\mathbf{p}_b) = \frac{1}{T}\sum_{t=1}^T \hat{\mathbf{F}}_e(\mathbf{p}_a + \frac{t}{T}(\mathbf{p}_b-\mathbf{p}_a)) \cdot \hat{\mathbf{d}}_{ab}.
   $$
3) 점수 높은 순으로 **이분 매칭/그리디 매칭** → **스켈레톤** 조립.

> 다른 방법: **Associative Embedding**(관절마다 태그 임베딩을 회귀)로 같은 사람에 속한 관절만 비슷한 태그로 만들고, **클러스터링**으로 그룹핑.

---

## 6) 코드: 핵심 유틸(히트맵·PAF 생성/피크 검출)

```python
import math, torch, torch.nn.functional as F

def draw_gaussian(heatmap, x, y, sigma):
    """heatmap: (H,W), x,y: float 좌표. sigma: 가우시안 표준편차(픽셀)"""
    H, W = heatmap.shape
    radius = int(3*sigma)
    ulx, uly = max(0, int(x)-radius), max(0, int(y)-radius)
    brx, bry = min(W-1, int(x)+radius), min(H-1, int(y)+radius)
    if ulx>brx or uly>bry: return heatmap
    xs = torch.arange(ulx, brx+1, device=heatmap.device)
    ys = torch.arange(uly, bry+1, device=heatmap.device)
    yy, xx = torch.meshgrid(ys, xs, indexing="ij")
    g = torch.exp(-((xx-x)**2+(yy-y)**2)/(2*sigma**2))
    heatmap[uly:bry+1, ulx:brx+1] = torch.maximum(heatmap[uly:bry+1, ulx:brx+1], g)
    return heatmap

def generate_heatmaps(keypoints, vis, H, W, sigmas):
    """
    keypoints: (J,2) [x,y], vis: (J,) {0,1,2}, sigmas: (J,)
    return: heatmaps (J,H,W)
    """
    J = keypoints.shape[0]
    hms = torch.zeros(J,H,W)
    for j in range(J):
        if vis[j] > 0:
            hms[j] = draw_gaussian(hms[j], keypoints[j,0], keypoints[j,1], sigmas[j])
    return hms

def generate_paf(edges, keypoints, vis, H, W, thickness=4):
    """
    edges: list of (a,b) 인덱스
    returns: paf (2*E, H, W)  # [ux,uy] concat per edge
    """
    E = len(edges)
    paf = torch.zeros(2*E, H, W)
    for ei,(a,b) in enumerate(edges):
        if vis[a]==0 or vis[b]==0: continue
        ax, ay = keypoints[a]; bx, by = keypoints[b]
        vx, vy = bx-ax, by-ay
        norm = math.sqrt(vx*vx+vy*vy) + 1e-8
        dx, dy = vx/norm, vy/norm
        # 선분 띠 영역에 벡터 채우기
        minx, maxx = int(max(0,min(ax,bx)-thickness)), int(min(W-1,max(ax,bx)+thickness))
        miny, maxy = int(max(0,min(ay,by)-thickness)), int(min(H-1,max(ay,by)+thickness))
        ys = torch.arange(miny, maxy+1); xs = torch.arange(minx, maxx+1)
        yy, xx = torch.meshgrid(ys, xs, indexing="ij")
        # 점에서 선분까지 거리: |(p-a) x d| (2D 외적 크기)
        pa_x, pa_y = xx-ax, yy-ay
        dist = torch.abs(pa_x*dy - pa_y*dx)
        # 투영이 [0, norm] 범위 안인지 체크
        t = (pa_x*dx + pa_y*dy)
        mask = (dist <= thickness) & (t >= 0) & (t <= norm)
        paf[2*ei,   yy[mask], xx[mask]] = dx
        paf[2*ei+1, yy[mask], xx[mask]] = dy
    return paf

def heatmap_nms(heatmaps, pool=3, thresh=0.1, topk=20):
    """
    heatmaps: (J,H,W)
    return: peaks list for each joint: [(x,y,score), ...]
    """
    J,H,W = heatmaps.shape
    pooled = F.max_pool2d(heatmaps.unsqueeze(0), kernel_size=pool, stride=1, padding=pool//2)[0]
    keep = (heatmaps == pooled) & (heatmaps > thresh)
    peaks = []
    for j in range(J):
        ys, xs = torch.where(keep[j])
        scores = heatmaps[j, ys, xs]
        if scores.numel() > topk:
            v, idx = torch.topk(scores, topk)
            xs, ys, scores = xs[idx], ys[idx], v
        peaks.append(list(zip(xs.tolist(), ys.tolist(), scores.tolist())))
    return peaks
```

---

## 7) 간단 네트워크: Top-down SimpleBaseline 헤드(Pytorch)

> **검출 박스**가 있다고 가정하고, 박스 크롭을 정규화 입력으로 만들어 **히트맵 \(J\times h\times w\)** 를 예측합니다.

```python
import torch, torch.nn as nn

class DeconvHead(nn.Module):
    """ResNet 백본 뒤에 3×(deconv+bn+relu) → conv로 J개 히트맵"""
    def __init__(self, in_ch=2048, num_joints=17, head_ch=256):
        super().__init__()
        self.deconvs = nn.Sequential(
            nn.ConvTranspose2d(in_ch, head_ch, 4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(head_ch), nn.ReLU(inplace=True),
            nn.ConvTranspose2d(head_ch, head_ch, 4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(head_ch), nn.ReLU(inplace=True),
            nn.ConvTranspose2d(head_ch, head_ch, 4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(head_ch), nn.ReLU(inplace=True),
        )
        self.out = nn.Conv2d(head_ch, num_joints, 1)

    def forward(self, x):
        x = self.deconvs(x)
        return self.out(x)

# 예: torchvision.models.resnet50(backbone) + DeconvHead
```

- 손실: 히트맵 **MSE**(관절 가중 가능).  
- 디코딩: `heatmap_nms` → 서브픽셀 보정(소프트-아그맥스/로컬 피팅).

---

## 8) 간단 네트워크: Bottom-up(히트맵+PAF) 미니 헤드

```python
class BottomUpHead(nn.Module):
    def __init__(self, in_ch=256, num_joints=17, num_edges=19, head_ch=128):
        super().__init__()
        self.body = nn.Sequential(
            nn.Conv2d(in_ch, head_ch, 3, padding=1), nn.BatchNorm2d(head_ch), nn.ReLU(True),
            nn.Conv2d(head_ch, head_ch, 3, padding=1), nn.BatchNorm2d(head_ch), nn.ReLU(True),
        )
        self.hm = nn.Conv2d(head_ch, num_joints, 1)
        self.paf = nn.Conv2d(head_ch, 2*num_edges, 1)

    def forward(self, x):
        x = self.body(x)
        return self.hm(x), self.paf(x)
```

- 손실: \(\mathcal{L}_{\text{hm}} + \lambda \mathcal{L}_{\text{paf}}\).  
- 디코딩: **joint별 피크** → **엣지별 매칭**(PAF 선적분 점수) → **사람 그룹핑**.

---

## 9) Bottom-up 디코딩(간단 그리디 매칭) 예시

```python
import numpy as np, torch

def paf_score(paf, a, b, T=10):
    # paf: (2,H,W) for one edge, a=(x,y,score), b=(x,y,score)
    ax, ay, _ = a; bx, by, _ = b
    vx, vy = bx-ax, by-ay
    norm = math.sqrt(vx*vx+vy*vy) + 1e-6
    dx, dy = vx/norm, vy/norm
    xs = np.linspace(ax, bx, T); ys = np.linspace(ay, by, T)
    vals = []
    H,W = paf.shape[1:]
    for x,y in zip(xs,ys):
        xi, yi = int(round(x)), int(round(y))
        if xi<0 or yi<0 or xi>=W or yi>=H: continue
        vx_ = paf[0, yi, xi]; vy_ = paf[1, yi, xi]
        vals.append(vx_*dx + vy_*dy)
    if len(vals)==0: return -1e9
    return float(np.mean(vals))

def greedy_group(peaks, pafs, edges, topk_conn=10, min_score=0.05):
    """
    peaks: list of list per joint type -> [(x,y,score), ...]
    pafs: (2E,H,W), edges: [(a,b),...]
    """
    persons = []  # each: dict {joint_id: (x,y,score)}
    for ei,(a,b) in enumerate(edges):
        cand_a, cand_b = peaks[a], peaks[b]
        conn = []
        for pa in cand_a:
            for pb in cand_b:
                s = paf_score(pafs[2*ei:2*ei+2], pa, pb)
                # 연결 신뢰도 = PAF + 끝점 점수의 평균
                conn.append((s + 0.5*(pa[2]+pb[2]), pa, pb))
        conn.sort(key=lambda x:-x[0])
        used_a, used_b = set(), set()
        for s,pa,pb in conn[:topk_conn]:
            if s < min_score: break
            if (pa in used_a) or (pb in used_b): continue
            # 기존 사람에 병합 또는 새 사람 생성
            match_idx = -1
            for i,p in enumerate(persons):
                has_a = (a in p); has_b = (b in p)
                if has_a ^ has_b:
                    match_idx = i; break
            if match_idx==-1:
                p={}
                p[a]=pa; p[b]=pb
                persons.append(p)
            else:
                persons[match_idx][a]=pa; persons[match_idx][b]=pb
            used_a.add(pa); used_b.add(pb)
    return persons
```

> 실제 구현은 **더 정교한 매칭/병합**과 **다중 엣지 일관성**을 봅니다(연결 점수 합/평균, 사이클 처리 등).

---

## 10) 학습 루프(Top-down, 히트맵 MSE) 요약 예제

```python
import torch, torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler

def mse_loss(pred, target, mask=None):
    # pred/target: (N,J,h,w), mask: (N,J,1,1) 관절 가중(가시성 등)
    if mask is None:
        return F.mse_loss(pred, target)
    else:
        return ((pred-target)**2 * mask).mean()

scaler = GradScaler()
for epoch in range(50):
    model.train()
    for imgs, hms, mask in train_loader:  # hms:(B,J,h,w), mask:(B,J,1,1)
        imgs, hms, mask = imgs.to(dev), hms.to(dev), mask.to(dev)
        opt.zero_grad(set_to_none=True)
        with autocast(dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32):
            feats = backbone(imgs)
            pred = head(feats)  # (B,J,h,w)
            loss = mse_loss(torch.sigmoid(pred), hms, mask)
        scaler.scale(loss).backward()
        scaler.unscale_(opt)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(opt); scaler.update()
    # validate…(OKS/PCK)
```

---

## 11) 평가 코드: OKS & PCK (간략)

```python
import numpy as np

def oks_score(pred_kpts, gt_kpts, gt_vis, scale, sigmas):
    # pred_kpts, gt_kpts: (J,2), gt_vis:(J,), scale: float, sigmas:(J,)
    d2 = ((pred_kpts - gt_kpts)**2).sum(axis=1)  # (J,)
    oks = 0.0; Z = 0
    for j in range(len(sigmas)):
        if gt_vis[j] > 0:
            oks += np.exp(-d2[j] / (2*(scale**2)*(sigmas[j]**2) + 1e-12))
            Z += 1
    return oks / max(Z,1)

def pck_score(pred_kpts, gt_kpts, thresh, ref_len):
    d = np.linalg.norm(pred_kpts-gt_kpts, axis=1)
    return (d <= thresh*ref_len).mean()
```

- **OKS-NMS**(인스턴스 억제): 박스 IoU 대신 **OKS**로 중복 포즈 억제.

---

## 12) 실무 팁(토포로지/히트맵/훈련)

1) **히트맵 해상도**: 입력 \(256\!\sim\!512\) → 히트맵 \(64\!\sim\!128\) (stride=4). 너무 낮으면 소수 픽셀에 피크가 몰림.  
2) **\(\sigma\) 스케일링**: 사람 박스 높이 \(h\)에 대해 \(\sigma = \alpha\cdot h\) (예: \(\alpha=0.02\))로 동적 설정.  
3) **가시성 마스킹**: \(v=0\) 키포인트의 히트맵/PAF 손실 제외.  
4) **좌우 플립**: 학습 증강 + **플립 테스트**(예측 뒤 좌우 스왑 평균)로 +0.3~0.6 AP.  
5) **Top-down 크롭 네트워크**: 크롭의 **패딩/스케일**을 일관되게 유지(예측 좌표 복원 시 오차 방지).  
6) **Bottom-up**: PAF 폭 \(\tau\)·연결 임계값을 데이터 특성(밀집/원근)에 맞춰 조정.  
7) **Integral Pose(소프트-아그맥스)**: 히트맵 MSE와 함께 쓰면 **서브픽셀** 향상.  
8) **멀티스케일 테스트**: Bottom-up에서 유효하나 지연↑ → 배치/리소스에 맞춰 선택.

---

## 13) 배포(지연/스루풋/정확도) 고려

- **Top-down**: 사람 수 \(N\)에 따라 비용 ∝ \(N\). 짧은 지연이 중요하면 **검출기와 포즈를 파이프라인 최적화**(동적 배칭·텐서RT, FP16).  
- **Bottom-up**: 한 번의 전역 추론으로 다인 포즈 → **그룹핑(후처리)** 최적화가 핵심(CUDA 커널/벡터화).  
- **정밀도**: FP16/BF16은 대체로 안전. INT8은 **히트맵 피크/PAF 방향**이 민감 → QAT 또는 넉넉한 교정.  
- **좌표 복원**: 크롭/패딩/리사이즈의 **역변환**을 정확히(오프셋 포함).  
- **OKS-NMS**: 다중 감지 중복 억제 시 필수.

---

## 14) 디버깅 체크리스트

- [ ] 히트맵이 **0 또는 1로 포화**? → \(\sigma\) 과소/과대, 출력 활성(sigmoid/identity)·손실 스케일 확인  
- [ ] 플립 후 **좌우 관절 스왑** 적용?  
- [ ] 크롭 좌표 복원 시 **스케일/패딩** 역변환 정확?  
- [ ] Bottom-up에서 **PAF 폭/임계**로 연결이 과하거나 부족하지 않은가?  
- [ ] 가시성 미표시 \(v=0\)를 손실에서 **무시**했는가?  
- [ ] 평가 참조 길이(PCK)·스케일(OKS) 정의가 **데이터셋 규약**과 일치?  
- [ ] 멀티인원에서 **OKS-NMS**를 적용했는가?

---

## 15) 소형 E2E 스니펫(Top-down: 검출→크롭→히트맵→좌표 복원, 개념형)

```python
# 1) 사람 박스 detector로 얻었다고 가정: bboxes = [[x,y,w,h], ...]
# 2) 각 박스 크롭(+패딩) → 정규화 → model → heatmap
# 3) heatmap_nms → 키포인트 좌표 (크롭 좌표계) → 원본 복원

def crop_and_resize(img, box, out_size=256, pad=0.25):
    x,y,w,h = box
    cx, cy = x+w/2, y+h/2
    s = max(w,h)*(1+pad)
    x1, y1 = cx - s/2, cy - s/2
    x2, y2 = cx + s/2, cy + s/2
    # affine transform to (out_size,out_size)
    src = torch.tensor([[x1,y1],[x2,y1],[x1,y2]], dtype=torch.float32)
    dst = torch.tensor([[0,0],[out_size-1,0],[0,out_size-1]], dtype=torch.float32)
    T = kornia.geometry.get_affine_matrix2d(src.unsqueeze(0), dst.unsqueeze(0))  # or custom
    crop = kornia.geometry.warp_affine(img.unsqueeze(0), T, dsize=(out_size,out_size))[0]
    return crop, T

def invert_coords(kpts, T):
    # kpts: (J,2) in crop
    # apply inverse affine to map back
    Ti = torch.linalg.inv(torch.cat([T, torch.tensor([0,0,1]).view(1,3)], dim=0))[:2,:]
    k = torch.cat([kpts, torch.ones(len(kpts),1)], dim=1).T  # (3,J)
    xy = (Ti @ k).T[:, :2]
    return xy
```

*(위 코드는 개념용. 실전에서는 Kornia/torchvision ops 또는 직접 구현으로 Affine을 안정적으로 처리하세요.)*

---

## 16) COCO 17키 토포로지(예시)

- **키 인덱스**(예): 0:鼻, 1:왼눈, 2:오른눈, 3:왼귀, 4:오른귀, 5:왼어깨, 6:오른어깨, 7:왼팔꿈치, 8:오른팔꿈치, 9:왼손목, 10:오른손목, 11:왼엉덩이, 12:오른엉덩이, 13:왼무릎, 14:오른무릎, 15:왼발목, 16:오른발목  
- **엣지**(PAF용 예):  
  ```
  [(5,7),(7,9),(6,8),(8,10),(5,6),(5,11),(6,12),
   (11,12),(11,13),(13,15),(12,14),(14,16),(0,5),(0,6)]
  ```
  (데이터셋 정의와 **정확히 일치**해야 합니다.)

---

## 17) 확장: 손/얼굴 랜드마크, 동물 포즈, 비디오 포즈

- **손/얼굴**: J가 커지고(예: 손 21, 얼굴 68/98/468), **디테일·경계 정확**이 중요 → **히트맵 해상도/HRNet** 계열 유리.  
- **동물**: 토포로지를 새로 정의(등뼈·다리·꼬리). 스케일 다양성↑ → \(\sigma\) 동적 스케일.  
- **비디오**: 시간 차원 연결(Temporal PAF/트래킹), 칼만/딥 소트 기반 포즈 트랙.

---

## 18) 요약(퀵 가이드)

- **히트맵**: 키포인트 위치를 **가우시안 피크**로 모델링 → MSE 학습, 소프트-아그맥스/로컬 피팅으로 서브픽셀 보정.  
- **토포로지**: 관절 간 **엣지 정의**가 Bottom-up 그룹핑의 핵심. PAF/AE 등으로 **사람별 연결성**을 추론.  
- **Top-down**: 구현 쉬움·정밀↑, **검출 품질**에 좌우. **Bottom-up**: 혼잡 장면·다중 인원에 유리, 후처리 중요.  
- **평가**: COCO는 **OKS-mAP**, 고전 PCK도 병행. **OKS-NMS**로 중복 억제.  
- **배포**: 좌표 복원, FP16, 동적 배칭, NMS/OKS-NMS 최적화, INT8 시 교정 주의.

위 스니펫을 **데이터셋 로더(히트맵/PAF 생성) + 간단 헤드 + 손실 + 디코딩**과 결합하면, **단시간에 동작하는 포즈 베이스라인**을 만들 수 있습니다.  
이후에는 **백본(ResNet/HRNet/ViT), 해상도, 손실 가중, PAF/AE 선택, 플립/멀티스케일** 등 한 가지씩 바꿔 **OKS-mAP/PCK** 개선을 체계적으로 시도하세요.