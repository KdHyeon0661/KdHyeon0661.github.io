---
layout: post
title: 운영체제 - 네트워크와 분산시스템 (3)
date: 2025-11-02 23:25:23 +0900
category: 운영체제
---
# Chapter 19 — Networks and Distributed Systems (3)

## DFS Naming and Transparency

### Naming의 목표

분산 파일시스템(DFS)에서 **이름(Name)** 은 “파일에 접근하기 위한 안정적 식별자”다. 핵심 목표:

- **접근 투명성**: 지역/원격 구분 없이 동일 인터페이스(`open/read/write/close`).
- **위치(Location) 투명성**: 물리 위치(서버/랙/리전) 변화가 경로명에 드러나지 않음.
- **이동(Migration) 투명성**: 파일/디렉터리를 옮겨도 핸들이 깨지지 않음.
- **복제(Replication) 투명성**: 사본이 많아도 하나처럼 보임.
- **동시성 투명성**: 여러 클라이언트의 접근에서 일관된 의미.
- **장애 투명성**: 서버 재시작/네트워크 단절 후에도 안정적으로 복구.

> **식별자 층위**
> - **이름(name)**: 사람이 읽는 경로 `/home/alice/data.csv`
> - **핸들(handle)**: 서버 내부 식별자(예: inode# + generation)
> - **UUID**: 전역 고유 ID(리밸런싱·마이그레이션 중에도 불변)

---

### 네임스페이스 디자인 패턴

1) **단일 전역 네임스페이스**
   - 모든 노드가 같은 이름공간을 본다(Plan 9, NFS 공용 export 루트 + autofs).
   - *장점*: 운영 간결. *단점*: 메타데이터 핫스팟.

2) **클라이언트-특화 조립형 네임스페이스**
   - 클라이언트가 mount table로 여러 DFS를 **논리적으로 합성**(Plan 9의 per-process name space).
   - *장점*: 유연성. *단점*: 관리 복잡성.

3) **메타데이터 샤딩**
   - 루트/상위 디렉터리는 **MDS(메타데이터 서버)** 군으로 파티셔닝.
   - 디렉터리 단위 리더 선출(락), 히트 디렉터리는 **분할(split)**.

> **경로 해석 비용(근사)**
> 디렉터리 깊이 \(L\), 컴포넌트당 RPC 평균 지연 \(d\) 일 때,
> $$T_{\text{lookup}} \approx L \cdot d \quad(\text{negative/attr cache로 감소 가능})$$

---

### 이름→위치 해석(Indirection)

- 경로 → **(디렉터리 엔트리, inode/UUID)** → **청크/오브젝트 위치 목록**.
- 위치는 **변동 가능**: 리밸런싱/복구/복제 재배치.
- 클라이언트는 **UUID 고정**을 들고 다니며, 위치는 **캐시** + **만료(TTL)/콜백**으로 갱신.

```python
# ns_core.py — 교육용 네임서버/마운트/경로 해석(간단화)

from dataclasses import dataclass, field
import time, hashlib, random

@dataclass
class DirEnt:   # 디렉터리 엔트리
    name: str
    uuid: str   # 파일/디렉터리 고유 ID

@dataclass
class Inode:
    uuid: str
    is_dir: bool
    children: dict = field(default_factory=dict)  # name -> DirEnt
    chunks: list = field(default_factory=list)    # 파일이면 청크ID 나열

class NameServer:
    def __init__(self):
        self.inodes = {}      # uuid -> Inode
        self.loc = {}         # chunk -> [server...]
        self.root = self.mk_dir("/")  # 전역 루트
    def mk_uuid(self, s): return hashlib.sha1(s.encode()).hexdigest()
    def mk_dir(self, name, parent=None):
        u=self.mk_uuid(f"D:{name}:{time.time()}:{random.random()}")
        nd=Inode(u, True); self.inodes[u]=nd
        if parent:
            parent.children[name]=DirEnt(name,u)
        return nd
    def mk_file(self, parent, name, chunks):
        u=self.mk_uuid(f"F:{name}:{time.time()}:{random.random()}")
        nf=Inode(u, False, chunks=chunks); self.inodes[u]=nf
        parent.children[name]=DirEnt(name,u)
        return nf
    def set_chunk_locs(self, chunk, servers):
        self.loc[chunk]=servers
    # 경로 해석
    def lookup(self, path):
        cur=self.root
        parts=[p for p in path.split("/") if p]
        for p in parts:
            if p not in cur.children: raise FileNotFoundError(path)
            cur=self.inodes[cur.children[p].uuid]
        return cur
    # 파일 uuid -> 청크 -> 서버 목록
    def resolve_blocks(self, uuid):
        nd=self.inodes[uuid]
        return [(c, self.loc.get(c, [])) for c in nd.chunks]

# 데모

ns=NameServer()
home=ns.mk_dir("home", ns.root); alice=ns.mk_dir("alice", home)
f=ns.mk_file(alice,"log.txt",["c1","c2"])
ns.set_chunk_locs("c1",["s1","s2"]); ns.set_chunk_locs("c2",["s2","s3"])
inode = ns.lookup("/home/alice/log.txt"); print(inode.uuid, ns.resolve_blocks(inode.uuid))
```

*설명*: **UUID** 는 이동/복제가 일어나도 **불변**. `resolve_blocks()`는 위치 투명성을 위한 **한 번 더의 간접화**.

---

### 마운트/조립 및 이동·복제 투명성

- **Mount table**: `/data` → DFS-A, `/home` → DFS-B …
- **바인드/오버레이**: 특정 디렉터리에 다른 뷰를 덧씌움(읽기-복제, 쓰기-스루).
- **이동**: 서버에서 디렉터리를 옮겨도 **UUID 고정** → 핸들(파일 식별자) 불변.
- **복제**: 여러 위치를 반환하고, 클라이언트는 **정책(가까움/부하/헬스)** 으로 선택.

> **복제 선택**의 간단 모델
> $$\text{score} = w_1\cdot \text{RTT}^{-1} + w_2\cdot \text{health} + w_3\cdot \text{load}^{-1}$$

---

### 캐싱·무효화(콜백/리스)와 투명성

- **콜백 무효화**(AFS/NFSv4 delegation): 서버가 변경 시 “해당 핸들 캐시 무효화” 알림.
- **리스(Lease)**: 일정 시간 **읽기/쓰기 위임**을 줘서 서버 round-trip을 줄임.
- **Negative cache**: “없음” 결과도 TTL로 캐시(디렉터리 폭주 완화).

```python
# cache_lease.py — 파일 메타/위치 캐시 + TTL

import time
class TTLCache:
    def __init__(self, ttl=1.0): self.ttl=ttl; self.c={}
    def get(self, k, fetch):
        v,exp=self.c.get(k,(None,0))
        if time.time()<exp: return v
        v=fetch(); self.c[k]=(v, time.time()+self.ttl); return v
```

---

## Remote File Access

### 서버 상태(Stateful vs Stateless)

- **Stateless 서버 (NFSv3)**
  - 서버는 오픈 상태를 보존하지 않음 → 클라이언트가 **파일 핸들**과 **오프셋**을 매번 보냄.
  - *장점*: 크래시 복구 용이. *요구*: **idempotent** 연산 설계(재전송·중복에 안전).

- **Stateful 서버 (NFSv4/SMB/AFS)**
  - 오픈/락/리스/위임 등 **세션 상태**를 관리.
  - *장점*: 캐시·락·위임으로 성능↑. *단점*: 복구 프로토콜 필요.

---

### 일관성 모델

- **Close-to-Open Consistency**(NFS 스타일):
  - 쓰기한 클라이언트가 `close()` 하면 서버에 **커밋**, 다른 클라이언트의 `open()` 은 최신을 보게 됨.
- **콜백 일관성**(AFS 스타일):
  - 서버가 “이 핸들 캐시 버려” 콜백 → 강한 읽기 최신성 보장.
- **엄격 POSIX**: 모든 쓰기가 즉시 관측 가능(네트워크 분산에서 비용↑).

---

### 클라이언트 캐시와 쓰기 정책

- **Write-through**: 각 쓰기 즉시 서버 반영(안전↑, 지연↑).
- **Write-back(Delay)**: 페이지 캐시에 모았다가 **flush/fsync** 시 전송(성능↑, 장애시 주의).
- **Attribute cache**: `stat()` 결과 TTL 캐시. 짧은 TTL과 서버 핑/변경 세대(generation)로 신선도 유지.

---

### (학습용) 미니 원격 파일 접근 — RPC 뼈대

> 아래 예시는 *개념 전달용*으로, **idempotent write**(버전/오프셋 기반)와 **close-to-open** 느낌을 흉내낸다.

```python
# rfa_server.py — 극단 단순화: 파일 핸들 기반 read/write/commit

import asyncio, json, hashlib, os

FILES = {}      # handle -> bytearray
VERS  = {}      # handle -> version

def handle_id(path):
    return hashlib.sha1(path.encode()).hexdigest()

async def handle_rpc(reader, writer):
    try:
        req=json.loads((await reader.readuntil(b"\n")).decode())
        op=req["op"]
        if op=="open":
            h=handle_id(req["path"]); FILES.setdefault(h, bytearray()); VERS.setdefault(h,0)
            resp={"ok":1,"handle":h,"ver":VERS[h]}
        elif op=="read":
            h,off,n=req["handle"],req["off"],req["n"]
            buf=FILES.get(h,bytearray()); data=bytes(buf[off:off+n])
            resp={"ok":1,"data":data.decode(errors="ignore"),"ver":VERS.get(h,0)}
        elif op=="write":
            # idempotent: (ver, off, data) 조합이 동일하면 재적용해도 결과 동일
            h,off,data=req["handle"],req["off"],req["data"].encode()
            buf=FILES.setdefault(h,bytearray())
            if len(buf)<off+len(data): buf.extend(b"\x00"*(off+len(data)-len(buf)))
            buf[off:off+len(data)]=data
            resp={"ok":1,"len":len(data)}
        elif op=="commit":
            h=req["handle"]; VERS[h]=VERS.get(h,0)+1
            resp={"ok":1,"ver":VERS[h]}
        else:
            resp={"ok":0,"err":"badop"}
    except Exception as e:
        resp={"ok":0,"err":str(e)}
    writer.write((json.dumps(resp)+"\n").encode()); await writer.drain(); writer.close()

async def main():
    s=await asyncio.start_server(handle_rpc,"0.0.0.0",9090)
    async with s: await s.serve_forever()
asyncio.run(main())
```

```python
# rfa_client.py — close-to-open 근사: commit 후 open이 최신 ver을 받음

import asyncio, json

async def rpc(x):
    r,w=await asyncio.open_connection("127.0.0.1",9090)
    w.write((json.dumps(x)+"\n").encode()); await w.drain()
    resp=json.loads((await r.readuntil(b"\n")).decode()); w.close(); await w.wait_closed()
    if not resp.get("ok"): raise RuntimeError(resp.get("err"))
    return resp

async def demo():
    o=await rpc({"op":"open","path":"/data/notes.txt"}); h,ver=o["handle"],o["ver"]
    await rpc({"op":"write","handle":h,"off":0,"data":"hello"})
    c=await rpc({"op":"commit","handle":h})     # close() 시점에 commit
    # 다른 클라이언트의 open이 최신 ver을 보게 됨
    o2=await rpc({"op":"open","path":"/data/notes.txt"})
    r=await rpc({"op":"read","handle":o2["handle"],"off":0,"n":5})
    print("ver:",c["ver"],"read:",r["data"])
asyncio.run(demo())
```

*핵심*: `write()` 는 **멱등(idempotent)** 하게, `commit()` 은 **버전 증가**로 “close-to-open”의 최신성 근사.

---

### 락/위임(Delegation)과 동시성

- **Advisory/Posix Locks**: 레코드 잠금, 데드락 방지/타임아웃 설계.
- **Delegation/Oplock/Lease**: 읽기/쓰기 권한을 **클라이언트에게 위임**해 로컬 캐시 최대화, *회수* 요청 시 반드시 응답해야 함.

```python
# lease_server_skel.py — (개념) 쓰기 리스를 한 번에 하나만

class LeaseTable:
    def __init__(self): self.wlease={}  # handle -> client_id
    def try_write_lease(self, h, who):
        if h not in self.wlease or self.wlease[h]==who:
            self.wlease[h]=who; return True
        return False
    def revoke(self, h): self.wlease.pop(h, None)
```

---

### 에러/복구

- **Stale handle**: 파일이 삭제/재생성되면 핸들 불일치(세대 번호 *generation* 로 방지).
- **네트워크 단절**: 재시도/백오프, idempotent 재적용, 세션 재협상.
- **서버 크래시**: stateless는 자동 복구 용이, stateful은 **recovery protocol** 필요(락 복원/리스 재협상).

---

## Final Thoughts on Distributed File Systems

### 아키텍처 선택 가이드

| 요구 | 권장 접근 |
|---|---|
| 대용량 순차 처리(로그·동영상) | **메타/데이터 분리 + 큰 청크(64–256MiB) + 파이프라인 쓰기** |
| 소파일 폭주(메타 집중) | **메타 스케일아웃(MDS 샤딩) + 파일 패킹 + 캐시/리스** |
| 강한 일관성(POSIX 근접) | **Stateful(NFSv4/SMB/AFS)** + 콜백/위임/락 |
| 멀티리전/지연 민감 | **읽기 근접 복제**, 쓰기는 **리전 로컬 로그 + 비동기 합일** |
| 비용 절감 | **Erasure Coding(EC)** + 핫 데이터만 복제 |

> **복제 vs EC 저장 효율**
> 복제(3중): $$\frac{\text{used}}{\text{data}} = 3.0$$
> EC(k+m): $$\frac{k+m}{k}$$ (예: 10+4 → 1.4배). 작은 쓰기/복구 지연이 대가.

---

### 핫스팟 제거/균형

- **디렉터리 분할/해싱 네이밍**: `a/b/cd/…` 또는 날짜/해시로 파일 분산.
- **Consistent Hashing + VNode**: 청크 서버 간 균등화, 리밸런싱 비용 최소화.
- **Prefetch/Read-ahead**: 순차 워크로드의 RTT를 배치화.

---

### 관측성과 SLO

- **지표**: open/lookup p95, read/write p99, 서버 큐 길이, 콜백 지연, 레플리카 랙.
- **탐지**: Stale read 비율, 캐시 hit/miss, delegation 회수 빈도.
- **SLO 예**: “open p95 < 15ms, read 1MiB p99 < 40ms (리전 내)”.

---

### 보안/감사

- **경로 암호화(mTLS)**, **권한(ACL/RBAC)**, **감사 로그**(who/what/when/where).
- **키 회전/폐기** 자동화, 데이터-평면 암호화(서버·디스크·객체 레벨).

---

### 장애/데이터 보존

- **MTTDL(대략)**: 독립 실패율 \(\lambda\), 복제수 \(r\), 복구율 \(\mu\) 가정 시
  $$ \text{MTTDL} \propto \frac{1}{N \cdot \lambda^{r} \cdot \mu^{\,r-1}} $$
  \(N\): 디스크 수. 복제수↑, 복구 빠를수록 ↑.
- **복구 전략**: 빠른 **re-replication**, **우선순위 스케줄링**(핫/저복제 우선).

---

### 체크리스트(운영)

1) **네임스페이스**: 전역/조립형 결정, negative cache/TTL/콜백 정책.
2) **일관성**: close-to-open, delegation/lease, 락 실패 전략.
3) **데이터 플레인**: 큰 청크, 파이프라인, 병렬 큐/멀티 스트림.
4) **복제/EC**: 계층별(핫/콜드) 정책, 리밸런싱 모니터링.
5) **보안**: mTLS, 권한/감사, 키 관리.
6) **관측성**: p95/p99, 오류/재시도율, 콜백 회수 지연, 스냅샷/GC 시간.
7) **DR**: 스냅샷/버전, 오브젝트 잠금, 지리적 복제/리전 격리.

---

### 종합 예제 — “이름→위치→원격읽기” 풀 경로

```python
# dfs_client.py — NameServer + Remote Read 클라이언트 결합(학습용)

import asyncio, json, random

class DFSClient:
    def __init__(self, ns, fetch_blk):
        self.ns=ns       # 위의 NameServer 인스턴스 사용
        self.fetch_blk=fetch_blk  # (server, chunk) -> bytes
    async def read_all(self, path):
        inode=self.ns.lookup(path)
        if inode.is_dir: raise IsADirectoryError(path)
        out=bytearray()
        for chunk, servers in self.ns.resolve_blocks(inode.uuid):
            random.shuffle(servers)
            last_err=None
            for s in servers:
                try:
                    data=await self.fetch_blk(s, chunk)  # 원격 블록 읽기 (RPC 가정)
                    out.extend(data); break
                except Exception as e:
                    last_err=e
            else:
                raise last_err or IOError("no replica ok")
        return bytes(out)

# 가짜 블록 서버(메모리)

BLKDATA={"s1":{"c1":b"HELLO "},"s2":{"c1":b"HELLO ","c2":b"WORLD!"},"s3":{"c2":b"WORLD!"}}
async def fetch_blk(server, chunk):
    await asyncio.sleep(0.005)  # 네트워크 지연 흉내
    return BLKDATA[server][chunk]

# 조립

from ns_core import ns  # 앞서 만든 NameServer 데모를 재사용한다고 가정
client=DFSClient(ns, fetch_blk)
print(asyncio.run(client.read_all("/home/alice/log.txt")).decode())
```

*포인트*: **네임(경로)** → **UUID** → **블록/복제 위치** → **원격 블록 읽기**. 장애 시 **다음 레플리카**로 폴백.

---

## 요약

- **19.7 Naming & Transparency**: UUID 기반 간접화로 **이동/복제/위치** 투명성. 네임스페이스를 전역/조립형 중 선택하고, TTL/콜백/리스로 신선도·성능 균형.
- **19.8 Remote File Access**: stateless(idempotent) vs stateful(락/위임) 트레이드오프, close-to-open·콜백·리스로 일관성 제공. 예제 RPC로 **멱등 쓰기 + commit 버전** 제시.
- **19.9 Final Thoughts**: 워크로드별 아키텍처(큰 청크·샤딩·EC), 관측성/SLO/보안/DR 체크리스트. *핫스팟 제거, 복제 선택, 오류/복구* 가 실전의 승부처.
