---
layout: post
title: 확률과 통계 - 이질적 효과와 개인화
date: 2025-09-20 19:25:23 +0900
category: 확률과 통계
---
# 15. 이질적 효과와 개인화 ― CATE/메타러너 기본기

> **목표**
> - **이질적 처리효과(HTE)**, **개별/조건부 평균처리효과(ITE/CATE)** 정의부터 **식별 가정**(무작위화·겹침·SUTVA)을 정확히 정리한다.  
> - **메타 러너(meta-learner)** 계열(**S/T/X/R/DR-learner**)과 **인과 트리·포레스트**(CT/CF/GRF), **업리프트 모델링**의 핵심 수식과 절차를 예제로 익힌다.  
> - **평가**: **PEHE**, **정책가치(Policy Value)**, **AIPW/DR** 오프폴리시 추정, **Qini/AUUC**(업리프트 곡선), **CATE 캘리브레이션**을 배운다.  
> - **정책화**: CATE를 임계값/예산제약/페널티로 **의사결정**(누구에게 보낼 것인가?)에 연결한다.  
> - **함정**: 사후 공변량·누수, 과적합, 완전분리/희귀사건, 간섭(Interference)·클러스터 랜덤화, 공정성·안정성 리스크를 체크한다.

---

## 0. Hook — “모든 사용자에게 푸시를 보낼 필요는 없다”
- 같은 푸시라도 **A 유저**는 **전환 +1.2pp** 상승, **B 유저**는 **무효**, **C 유저**는 **이탈↑(음의 효과)** 일 수 있다.  
- **개인화 정책**은 “**누구에게 보낼지**”를 정한다. 이때 필요한 것이 **CATE**:  
  $$\tau(x)\equiv E[Y(1)-Y(0)\mid X=x].$$  
  여기서  
  - \(Y(1)\), \(Y(0)\): 같은 사용자가 **처리**(푸시) 받았을 때/안 받았을 때의 **잠재결과**  
  - \(X\): 사전(Pre) 특징(디바이스, 과거행동, 지역, 신규/복귀 등)

**무작위화된 A/B**라면 CATE 학습의 **식별 가정**이 비교적 쉽게 성립한다. **관측연구**라면 추가 가정과 설계가 필요하다(§2).

---

## 1. 표기와 정의

- **ATE**: 평균처리효과  
  $$\text{ATE} = E[Y(1)-Y(0)].$$
- **ITE**(개별):  
  $$\tau_i = Y_i(1)-Y_i(0) \quad(\text{관측 불가}).$$
- **CATE**(조건부 평균):  
  $$\tau(x) = E[Y(1)-Y(0)\mid X=x].$$
- **Uplift** = CATE (마케팅 문맥에서 흔히 쓰는 용어)

> **목표**: **\(\tau(x)\)** 를 예측하는 함수 \(\hat\tau(x)\)를 학습하고, 이를 기반으로 **정책**  
> \( \pi(x) \in \{0,1\} \) (보내기/안 보내기) 를 결정한다.

---

## 2. 식별 가정과 설계

### 2.1 무작위화(무혼란성, Unconfoundedness)
- **A/B 실험**:  
  $$ (Y(1),Y(0)) \perp T \mid X \quad \text{(또는 단순 무조건 독립)}.$$  
  특히 **완전 무작위화**면 \( X \) 조건조차 불필요.
- **관측연구**:  
  $$ (Y(1),Y(0)) \perp T \mid X \quad \text{(선택이 X로 설명된다는 가정)}.$$

### 2.2 겹침(Overlap / Positivity)
- 모든 \( x \)에서  
  $$0 < e(x)=P(T=1\mid X=x) < 1.$$  
  특정 세그먼트가 **항상 처리** 또는 **항상 대조**면 해당 x에서 CATE 식별 불가.

### 2.3 SUTVA (간섭 없음, Well-defined Treatment)
- 한 사용자의 결과가 **다른 사용자의 처리에 영향받지 않음**.  
- **간섭/스필오버**(네트워크·피드 노출)는 **클러스터 랜덤화**로 다루거나 IV/노출모형 등 고급 주제(§13).

### 2.4 랜덤화 단위와 분석 단위 일치
- 사용자 단위로 배정했으면, 분석/부트스트랩/퍼뮤테이션도 **사용자 단위**로. (11편, 12편 원칙 재강조)

---

## 3. 메타 러너(meta-learner) 지도

| 방법 | 아이디어 | 장점 | 유의점 |
|---|---|---|---|
| **S-learner** | 하나의 예측기 \(m(x,t)\)로 \(E[Y\mid X=x,T=t]\) 추정, \(\hat\tau(x)=m(x,1)-m(x,0)\) | 간단·견고 | T의 효과가 약하면 모델이 T를 무시할 수 있음 |
| **T-learner** | 처리/대조 두 개의 예측기 \(m_1(x),m_0(x)\) | 구현 쉬움, 강한 비선형 가능 | 표본 분할로 분산↑, 불균형시 취약 |
| **X-learner** | T의 결측 결과를 대체해 pseudo ITE 만들고 학습 | 불균형 견고, 정밀도↑ | 단계가 많아 조정 필요 |
| **R-learner** | 잔차화 후 최소화: \(Y-Q(X) - (T-e(X))\tau(X)\) | 이론적 근거 단단(Orthogonal) | 누이즈 추정(Q,e) 품질 의존 |
| **DR-learner** | 더블로버스트 성질의 메타학습 | 누이즈 중 하나가 잘못돼도 일관성 | 구현 복잡, 교차적합 필수 |

여기서  
- \( Q(x)=E[Y\mid X=x] \) (아웃컴 회귀),  
- \( e(x)=P(T=1\mid X=x) \) (성향점수, Propensity)  
를 **누이즈(nuisance)** 라고 부른다. **교차적합(cross-fitting)** 으로 오버피팅·편향을 줄인다.

---

## 4. 각 메타 러너의 절차 (수식 & 예시)

### 4.1 S-learner
- 하나의 지도학습기(트리/GBM/NN 등)로 \( m(x,t)\approx E[Y\mid X=x,T=t] \) 학습.  
- 예측 시:  
  $$\hat\tau(x)=m(x,1)-m(x,0).$$

**장점**: 단순, 하이퍼튜닝 쉬움.  
**주의**: \(T\)가 희귀하거나 효과가 약하면 모델이 \(T\)를 무시(=평평한 \(\hat\tau\)).

---

### 4.2 T-learner (Two-model)
1) 처리군 데이터로 \(m_1(x)\approx E[Y\mid X=x,T=1]\) 학습  
2) 대조군 데이터로 \(m_0(x)\approx E[Y\mid X=x,T=0]\) 학습  
3) \(\hat\tau(x)=m_1(x)-m_0(x)\)

**장점**: 각 군의 특성을 잘 잡음.  
**주의**: 샘플이 반으로 갈라져 분산↑, 불균형(예: \(n_1\ll n_0\))에서 취약 → **X-learner** 고려.

---

### 4.3 X-learner (불균형 친화)
아이디어: 각 개인의 **결측 잠재결과**를 대체해 **의사 ITE**를 만들고 학습.

1) \(m_1,m_0\)를 1단계로 학습  
2) 처리군 \(i\in \mathcal{I}_1\):  
   $$D_i^{(1)}=Y_i - m_0(X_i)\quad(\text{대조의 예측값을 빼 의사 ITE})$$
   대조군 \(j\in \mathcal{I}_0\):  
   $$D_j^{(0)}=m_1(X_j) - Y_j$$
3) 두 개의 **ITE 모델** 학습:  
   \(g_1(x)\approx E[D^{(1)}\mid X=x]\), \(g_0(x)\approx E[D^{(0)}\mid X=x]\)  
4) **가중 결합**:  
   $$\hat\tau(x)=w(x)\,g_0(x) + (1-w(x))\,g_1(x),\quad w(x)\approx e(x)\ \text{(혹은 }n_1/(n_0+n_1)\text{)}$$

**장점**: 불균형에 강함, 분산↓.  
**주의**: 단계마다 오차 누적 → **교차적합**, **정규화** 권장.

---

### 4.4 R-learner (Orthogonal / Residual-on-Residual)
Kennedy & Nie(2021) 계열.  
1) 누이즈 추정: \( \hat Q(x), \hat e(x) \). **교차적합**으로 오버피팅 방지.  
2) 잔차 계산:  
   $$\tilde Y_i=Y_i-\hat Q(X_i),\qquad \tilde T_i=T_i-\hat e(X_i).$$
3) **\(\tau\)** 추정 문제를 다음 최적화로 정의:  
   $$
   \hat\tau = \arg\min_\tau \frac{1}{n}\sum_i \Big(\tilde Y_i-\tilde T_i \tau(X_i)\Big)^2 + \lambda\mathcal{R}(\tau).
   $$
   (여기서 \(\tau\)는 함수공간; 실제로는 트리/커널/NN 등으로 파라미터화)

**장점**: 누이즈 추정 에러에 **정준(orthogonal)**, 편향에 강함.  
**주의**: 구현 비복잡, **교차적합 필수**.

---

### 4.5 DR-learner (Doubly Robust)
DML(Double Machine Learning) 사상.  
- 기본형(개략):  
  $$
  \hat\tau(x)=\Big(\hat\mu_1(x)-\hat\mu_0(x)\Big) + \frac{T-\hat e(X)}{\hat e(X)(1-\hat e(X))}\Big(Y-\hat\mu_T(X)\Big)\cdot \text{learner}(x),
  $$
  또는 점별 DR 추정량을 **2단계 회귀**로 학습.  
- 의미: **\(\hat\mu\)** (아웃컴 회귀)와 **\(\hat e\)** (성향점수) 중 하나라도 타당하면 **일치성** 확보.

**장점**: **더블로버스트**.  
**주의**: 파이프라인 복잡, 구현시 **샘플 분할**·**교차적합**이 핵심.

---

## 5. 인과 트리/포레스트 & 업리프트 트리

### 5.1 인과 트리(CT, Causal Tree)
- CART의 분할 기준을 **효과 차**에 맞게 수정. 각 노드에서  
  $$\widehat{\tau}_{\text{node}} = \bar Y_{1,\text{node}} - \bar Y_{0,\text{node}}$$
- 분할은 **효과 이질성**을 크게 만드는 후보를 선택(예: t-통계량 최대).  
- **Honest splitting**: **학습/평가** 데이터를 분리하여 **편향**을 낮춤.

### 5.2 인과 포레스트 / GRF (Generalized Random Forest)
- 다수의 **honest tree**를 앙상블, 국소가중 회귀로 \(\hat\tau(x)\) 추정.  
- **점추정 + 불확실성(표준오차 추정)** 제공.  
- 비선형·상호작용 포착에 강함.

### 5.3 업리프트 트리/랜덤포레스트
- 분할 기준을 **처리군 예측확률 – 대조군 예측확률**(업리프트)을 키우는 방향으로.  
- 구현은 S/T-learner와 유사하나, **분할 목적**이 업리프트 최대화에 초점.

**주의(공통)**: 분할이 많아질수록 **과적합** 위험. **Honest** 설계·**최소 노드 크기**·**복잡도 벌점** 필요.

---

## 6. 평가: \(\hat\tau(x)\)는 어떻게 “옳다”고 할 수 있나?

### 6.1 합성지표(시뮬레이션/반복실험 가능)
- **PEHE**(Precision in Estimation of Heterogeneous Effect):  
  $$
  \epsilon_{\text{PEHE}} = \sqrt{\frac{1}{n}\sum_i \big(\hat\tau(X_i)-\tau(X_i)\big)^2}
  $$
  (현실 데이터에선 \( \tau \) 미관측 → 주로 시뮬레이션 또는 준실험에서 사용)

### 6.2 정책가치(Policy Value) — 오프폴리시 추정
- 정책 \(\pi(x)\in\{0,1\}\)의 **기대 결과**  
  $$V(\pi)=E[Y(\pi(X))].$$
- A/B 로그로 **IPS/AIPW/DR** 추정:
  - **IPS**  
    $$\hat V_{\text{IPS}}=\frac{1}{n}\sum_i \frac{\mathbf{1}\{T_i=\pi(X_i)\}}{P(T_i\mid X_i)}\,Y_i.$$
  - **AIPW/DR**(권장)  
    $$
    \hat V_{\text{DR}}=\frac{1}{n}\sum_i\Big[\hat\mu_{\pi(X_i)}(X_i) + \frac{\mathbf{1}\{T_i=\pi(X_i)\}-\hat e_{\pi}(X_i)}{\hat e_{\pi}(X_i)(1-\hat e_{\pi}(X_i))}\big(Y_i-\hat\mu_{T_i}(X_i)\big)\Big],
    $$
    여기서 \(\hat e_\pi(X)=P(T=\pi(X)\mid X)\).  
- **베이스라인 정책**(전원 처리/전원 미처리/랜덤) 대비 **가치 차**와 **CI** 보고.

### 6.3 업리프트 곡선, Qini, AUUC
- 사용자들을 \(\hat\tau(x)\) **내림차순**으로 정렬, 상위 q%에 처리 적용 가정 하 **추가 성과** 누적 곡선을 그림.  
- **Qini 계수**: 업리프트 곡선과 무작위 정책의 차의 면적.  
- **AUUC**: 정규화된 면적. 모델간 **상대 비교**에 유용.

### 6.4 CATE **캘리브레이션**
- \(\hat\tau\) 분포를 **K-분위**로 나눈 뒤, 각 분위에서의 **실제 ATE**(A/B 로그에서 추정)를 비교 → **단조성**·**보정 필요성** 판단.  
- 필요 시 **등분산 가중 회귀**나 **등가 변환**으로 보정.

---

## 7. 정책화: \(\hat\tau(x)\) → **누구에게 보낼까?**

### 7.1 임계값 정책
- 비용 \(c\)와 처리로 얻는 가치 \(v\)가 있을 때,  
  $$\text{순이익}(x) \approx v\cdot \tau(x) - c,$$
  $$\Rightarrow \text{보내기 if } \tau(x) \ge c/v.$$

### 7.2 예산/캐패시티 제약
- 상위 **K명**에게만 보낼 수 있다면: \(\hat\tau(x)\) 내림차순 **Top-K** 선택.  
- 비용 이질적이면 **Knapsack**: \( \max\sum_i v_i \tau(x_i)z_i - c_i z_i,\ \sum c_i z_i \le B,\ z_i\in\{0,1\}\).

### 7.3 리스크-페널티
- 이탈/불만 등 **부정 효과** 위험 가중시  
  $$\max \sum_i \big(v\tau_i - \lambda\cdot \text{Risk}_i\big)z_i,$$  
  모수 \(\lambda\) 로 안전-효율 트레이드오프.

---

## 8. 불확실성 정량화(신뢰구간)

- **포레스트 기반**: 가중 국소회귀의 **점근 분산**으로 **표준오차** 제공(예: GRF).  
- **부트스트랩/재표본**: 사용자 단위 **클러스터 부트스트랩**으로 \(\hat\tau\) 분위별 ATE CI.  
- **적응형 빈·컨포멀**: CATE 구간 추정(고급).  
- **정책가치 CI**: **DR 추정량**에 대한 샌드위치/부트스트랩.

---

## 9. 데이터와 특징 설계

- **사전(Pre) 특징만**: 처리 이후 생성되는 로그(노출/클릭/세션) 사용 금지(누수).  
- **스칼라+카테고리+시계열 요약**: 최근 28일 평균/표준편차/추세, 주기성 인덱스 등.  
- **스파스/하이카디널리티**: 해싱/임베딩 사용 가능(단, 해석-안정성 트레이드오프).  
- **스케일링/센터링**: 메타러너(특히 R-learner) 수치안정에 도움.

---

## 10. 작은 표본/희귀사건 대응

- **정규화**(Ridge/L1), **베이지안 축소**(계층모형)로 **분산↓**.  
- **희귀 전환**: 로지스틱의 **완전분리** 위험 → **Firth**/L2, 또는 **포아송+강건 SE**로 RR 근사.  
- **특징 축소**(PCA/PLS) 또는 **도메인 축약**(핵심 10~50개 특징) 후 메타러너.

---

## 11. 안정성·해석가능성

- **Global surrogate**: \(\hat\tau(x)\)에 대한 간단한 트리/선형모형으로 **설명 모델** 적합.  
- **상호작용 탐색**: SHAP for uplift, ALE/ICE(주의: 인과적 해석은 제한).  
- **세그먼트 지표**: 각 분위에서 **정책가치**·**실제 uplift**·**구성비**를 리포트.

---

## 12. 공정성·안전성

- **공정성**: 민감 속성(성별/연령 등)을 직접 사용하지 않더라도 프록시가 **차별** 유발 가능 → **세그먼트별 uplift** 비교·제약.  
- **가드레일 KPI**: 이탈률/불만건수/CS 티켓 등의 **상한 제약**과 함께 정책 평가.  
- **SRM/무작위화 점검**: 표본비율 불일치(SRM) 시 HTE 학습 **중단**.

---

## 13. 간섭/클러스터/순응·IV (스케치)

- **간섭**: 친구에게 푸시가 가면 내 전환도 변함 → **클러스터 랜덤화**(예: 그룹/네트워크)로 처리. CATE는 **클러스터 특징** 포함.  
- **불완전 순응(컴플라이언스)**: 배정 \(Z\)≠ 실제 처리 \(T\). **IV**로 LATE 추정:  
  $$\text{LATE}=\frac{E[Y\mid Z=1]-E[Y\mid Z=0]}{E[T\mid Z=1]-E[T\mid Z=0]}.$$
  CATE-유사 확장은 고급 주제(로컬 CATE, IV-learner).

---

## 14. 케이스 스터디 — “푸시 타이밍 개인화”

### 14.1 문제
- 처리 \(T\): “저녁 푸시 발송(1) vs 미발송(0)”.  
- 결과 \(Y\): 다음 24시간 전환(0/1).  
- \(X\): 과거 28일 세션 패턴, 야간 활동성, 기기, 국가, 신규/복귀.

### 14.2 파이프라인
1) **A/B 랜덤화**(사용자 단위), **사전 특징** 수집  
2) **R-learner**(교차적합)로 \(\hat\tau(x)\) 학습  
3) **정책**: \( \hat\tau(x)\ge \tau_0\Rightarrow \) 발송  
4) **오프폴리시 평가**: DR로 정책가치 추정, 베이스라인 대비 **∆정책가치**와 CI  
5) **안전성**: 이탈/불만 가드레일 준수 확인  
6) **롤아웃**: 10% 트래픽로 A/B/C(정책/전원발송/무발송) **검증 실험**

### 14.3 리포트 문장(예시)
> “R-learner 기반 개인화 정책은 **상위 35% 유저만 발송**하여 **전환 +0.42pp**(DR 95% CI +0.18~+0.66pp),  
> 발송량 **−65%**로 비용 절감. 야간활동 높은 세그먼트에서 uplift 최대.”

---

## 15. Python 개념 코드 — S/T/X/R-learner, DR 평가 (sklearn 기반 스케치)

```python
# 개념용 코드(실행 예시): 실제 업무에선 econml/causalml, grf(py) 등 검증 라이브러리 권장
import numpy as np, pandas as pd
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss

# df: columns = ['Y','T'] + feature cols X..., Y in {0,1} (전환), T in {0,1}
X = df.drop(columns=['Y','T']).values
y = df['Y'].values
t = df['T'].values

# --------- S-learner (prob. model) ----------
# 하나의 분류기 m(x,t)
Xt = np.column_stack([X, t.reshape(-1,1)])
clf = RandomForestClassifier(n_estimators=200, min_samples_leaf=50)
clf.fit(Xt, y)
# tau_hat(x) = m(x,1) - m(x,0)
p1 = clf.predict_proba(np.column_stack([X, np.ones((len(X),1))]))[:,1]
p0 = clf.predict_proba(np.column_stack([X, np.zeros((len(X),1))]))[:,1]
tau_s = p1 - p0

# --------- T-learner ----------
clf1 = RandomForestClassifier(n_estimators=200, min_samples_leaf=50).fit(X[t==1], y[t==1])
clf0 = RandomForestClassifier(n_estimators=200, min_samples_leaf=50).fit(X[t==0], y[t==0])
tau_t = clf1.predict_proba(X)[:,1] - clf0.predict_proba(X)[:,1]

# --------- X-learner ----------
# 1단계
m1 = RandomForestClassifier(n_estimators=200, min_samples_leaf=50).fit(X[t==1], y[t==1])
m0 = RandomForestClassifier(n_estimators=200, min_samples_leaf=50).fit(X[t==0], y[t==0])
d1 = y[t==1] - m0.predict_proba(X[t==1])[:,1]
d0 = m1.predict_proba(X[t==0])[:,1] - y[t==0]
# 2단계
g1 = RandomForestRegressor(n_estimators=200, min_samples_leaf=50).fit(X[t==1], d1)
g0 = RandomForestRegressor(n_estimators=200, min_samples_leaf=50).fit(X[t==0], d0)
# 가중 결합(간단히 전체 비율)
w = t.mean()
tau_x = w * g0.predict(X) + (1-w) * g1.predict(X)

# --------- R-learner (교차적합 간단 스케치) ----------
K = 2
kf = KFold(n_splits=K, shuffle=True, random_state=0)
tau_r = np.zeros(len(X))
for tr, te in kf.split(X):
    # nuisance on train
    q = RandomForestClassifier(n_estimators=200, min_samples_leaf=50).fit(X[tr], y[tr])
    e = LogisticRegression(max_iter=1000).fit(X[tr], t[tr])
    qhat = q.predict_proba(X[te])[:,1]
    ehat = e.predict_proba(X[te])[:,1]
    ytil = y[te] - qhat
    ttil = t[te] - ehat
    # second-stage: residual-on-residual
    # 간단히 선형회귀로 추정(실무는 L2/GBM/NN 사용)
    from sklearn.linear_model import Ridge
    rr = Ridge(alpha=1.0).fit(X[te]*ttil.reshape(-1,1), ytil)  # trick: 가중화
    tau_r[te] = rr.predict(X[te])

# --------- 정책 평가: DR(AIPW) ----------
# 1. nuisance 전체 학습
mu1 = RandomForestClassifier(n_estimators=200, min_samples_leaf=50).fit(X[t==1], y[t==1])
mu0 = RandomForestClassifier(n_estimators=200, min_samples_leaf=50).fit(X[t==0], y[t==0])
m1 = mu1.predict_proba(X)[:,1]; m0 = mu0.predict_proba(X)[:,1]
e = LogisticRegression(max_iter=1000).fit(X, t)
eh = e.predict_proba(X)[:,1]

def policy_value_dr(tau_hat, top_frac=0.3):
    # 정책: tau_hat 상위 top_frac 에게 처리
    k = int(len(tau_hat)*top_frac)
    thr = np.partition(tau_hat, -k)[-k]
    pi = (tau_hat >= thr).astype(int)
    # DR: \hat mu + AIPW 잔차
    mu_pi = m1*pi + m0*(1-pi)
    numer = ((t==pi).astype(int) - (pi*eh + (1-pi)*(1-eh)))
    denom = (pi*eh + (1-pi)*(1-eh)) * (1 - (pi*eh + (1-pi)*(1-eh)))
    # 안정화를 위한 clip
    w = np.clip(denom, 1e-6, None)
    dr = mu_pi + numer/w * (y - (m1*t + m0*(1-t)))
    return dr.mean()

val_s = policy_value_dr(tau_s, top_frac=0.3)
val_t = policy_value_dr(tau_t, top_frac=0.3)
val_x = policy_value_dr(tau_x, top_frac=0.3)
val_r = policy_value_dr(tau_r, top_frac=0.3)
print("Policy Value (DR): S/T/X/R =", val_s, val_t, val_x, val_r)
```

> **주의**  
> - 위 코드는 **개념 스케치**다. 실제로는 **교차적합**·**샘플 분할**을 더 정교히 하고, AIPW/DR의 **분모 안정화**와 **신뢰구간**(부트스트랩/샌드위치)을 제공해야 한다.  
> - 라이브러리 예: **EconML**(DRLearner, XLearner, CausalForestDML), **CausalML**(UpliftTree/Forest, meta-learners), **grf**(R, Python 포팅).

---

## 16. 보고 템플릿

> **목표**: 저녁 푸시 개인화(보낼 사용자 선별)로 **정책가치**를 높인다.  
> **데이터**: 사용자 단위 A/B(랜덤화), 사전 28일 특징만 사용, N=1,200,000.  
> **모델**: R-learner(교차적합, Ridge+RF 누이즈), X-learner(GBM).  
> **평가**: DR(AIPW) 오프폴리시로 **정책가치** 추정, 업리프트 곡선(AUUC), 분위별 실제 uplift 캘리브레이션.  
> **결과**: Top-35% 정책이 **∆정책가치 +0.38pp**(95% CI +0.20~+0.56pp), AUUC=0.021(무작위 대비 +48%).  
> **안전성**: 이탈률↑ 세그먼트에 페널티 적용 후 **여전히 +0.31pp**.  
> **해석**: **야간활동·최근구매** 특징이 강한 상호작용. 신규유저 uplift 불안정(표본↓) → 2주 추가 데이터 수집 권고.  
> **제한**: SUTVA 위반 가능(친구효과), 신시즌 드리프트 리스크 → 2주 주기로 리핏 학습.

---

## 17. 체크리스트 (실무)

- [ ] **무작위화/겹침/SUTVA** 가정 점검, **SRM** 검사 통과  
- [ ] **사전 특징만** 사용(누수 금지), 랜덤화 단위=분석 단위 일치  
- [ ] **교차적합**(누이즈/메타 모두), 정규화·하이퍼튜닝  
- [ ] **과적합 감시**: Out-of-fold 성능, 캘리브레이션, 업리프트 곡선  
- [ ] **정책 평가**: DR 오프폴리시 **정책가치 + CI**, 베이스라인 대비  
- [ ] **안전성/공정성**: 가드레일 KPI, 민감 세그먼트 점검, 페널티/제약  
- [ ] **롤링 재학습**: 시즌성·드리프트 대응, 온라인 모니터링  
- [ ] **문서화**: 가설, 사전등록, 모델/평가 설정, 리스크, 롤아웃 플랜

---

## 18. 자주 묻는 질문(FAQ)

**Q1. A/B가 아니라 관측데이터인데 CATE 가능?**  
A. 가능하지만 **무혼란성**이 핵심. 성향점수 매칭/가중/DR/DML 등으로 보정. **측정되지 않은 혼란** 리스크는 남음(민감도 분석 병기).

**Q2. 왜 DR/교차적합을 강조?**  
A. 누이즈 추정의 오버피팅이 **편향**을 만든다. **Orthogonal/DR** 구조와 **교차적합**은 그 편향에 **둔감**하다.

**Q3. 업리프트 곡선만 좋으면 충분?**  
A. 곡선은 상대 비교엔 좋지만 **정책가치**(실제 이익)를 수치화해 CI까지 보고하는 것이 채택 의사결정에 필수.

**Q4. 음의 uplift 사용자에게는?**  
A. **제외**하거나, **다른 처리**(예: 부드러운 메시지/다른 채널)를 실험. “누구에게 무엇을?”의 다중처리 확장.

**Q5. 해석 가능한 모델이 필요한데?**  
A. 먼저 **정확도**로 후보 선별 → **간단 트리**로 서브그룹 규칙 추출(글로벌 서러게이트). 단, 정책적 리스크와 함께 병기.

---

## 19. 연습문제(정답 스케치)

1) **S/T/X 중 불균형(처리 10%)에서 유리한 러너는?**  
   → **X-learner**(대조가 크면 \(D^{(0)}\) 추정이 안정).

2) **정책가치 오프폴리시 추정에서 DR를 쓰는 이유?**  
   → **더블로버스트**: \(\hat e\) 또는 \(\hat\mu\) 중 하나가 잘못되어도 일치성.

3) **겹침 위반의 신호와 처방?**  
   → \(e(x)\)가 0/1 가까운 영역. 처방: **해당 영역 정책 고정**(보수) 또는 **그 영역에서는 CATE 보고 피함**.

4) **캘리브레이션 체크는 어떻게?**  
   → \(\hat\tau\) 분위별 실제 ATE를 추정해 **단조성**·**스케일** 확인. 필요 시 재보정.

5) **예산 제약 정책을 DR로 평가하려면?**  
   → Top-K 정책을 정의, DR 식으로 \(V(\pi_{K})\) 추정하고 **K**에 따른 가치 곡선/CI를 그린다.

---

## 20. TL;DR
- **CATE**는 “누구에게 효과가 큰가”를 수치화한다.  
- **S/T/X/R/DR-learner** + **교차적합**으로 견고한 \(\hat\tau(x)\)를 학습하라.  
- **인과 트리/포레스트**와 **업리프트 모델**은 비선형·상호작용 포착에 강력.  
- **평가는 정책가치와 DR**, **업리프트 곡선(AUUC)**, **캘리브레이션**을 함께.  
- **정책화**는 임계값/예산/리스크 제약을 포함해 실제 **의사결정**에 연결.  
- **항상**: 가정·누수·겹침·간섭·공정성·안전성 체크리스트를 지키자.