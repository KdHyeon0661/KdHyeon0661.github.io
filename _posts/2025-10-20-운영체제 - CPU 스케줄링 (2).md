---
layout: post
title: 운영체제 - CPU 스케줄링 (2)
date: 2025-10-20 16:25:23 +0900
category: 운영체제
---
# Multi-Processor Scheduling • Real-Time CPU Scheduling • OS Examples • Algorithm Evaluation

이 문서는 다음을 **예제 코드**와 **측정 방법**까지 포함해 깊게 다룬다.

- **5.5 다중 프로세서 스케줄링**: SMP/NUMA, per-CPU runqueue, load balancing, affinity, work-stealing, gang scheduling 개념과 실습
- **5.6 실시간 스케줄링**: RM/EDF 이론과 구현 스케치, 우선순위 역전/상속, 주기 태스크 시뮬레이터
- **5.7 OS 예시**: Linux(CFS, RT), Windows(IOCP + 스케줄러 개요), macOS/iOS(GCD + QoS), Android(Binder thread pool)
- **5.8 알고리즘 평가**: 메트릭 정의, 추적/추정, 시뮬레이션·벤치마크 프레임워크, 파라미터 스윕

---

## Multi-Processor Scheduling

### 모델과 과제

- **SMP**(대칭 다중 처리): 모든 코어가 동등, **per-CPU runqueue**가 일반적
- **NUMA**: 노드별 메모리 지연이 다름 → **locality** 중시
- **문제**:
  1) **부하 균형(load balancing)** — 한 코어가 과부하/유휴면 성능 손실
  2) **캐시 로컬리티** — 자주 migrate하면 캐시 미스↑
  3) **동시 멀티스레딩(SMT/HT)** — 같은 물리 코어에 두 쓰레드가 자원 경쟁
  4) **우선순위/공정성** — global fairness vs local efficiency

### 글로벌 vs 로컬 큐

- **Global runqueue**: 공정성/단순성↑, lock 경합↑
- **Per-CPU runqueue**: 확장성↑, load balancer가 주기적으로 재분배

### Work-Stealing 개요

- 각 워커가 **자기 덱(Deque)** 의 front에서 pop, **빈 워커는 다른 워커의 뒤에서 steal**
- 장점: **균형**과 **캐시-로컬리티**의 타협

#### 파이썬: 간단 Work-Stealing 스케치(학습용)

```python
# ws_demo.py — 4 워커, 랜덤 작업 시간, 단순 steal

import random, time, threading
from collections import deque

NWORK=4
deques=[deque() for _ in range(NWORK)]
lock=threading.Lock()
done=False

def worker(i):
    random.seed(100+i)
    while not done:
        task=None
        with lock:
            if deques[i]:
                task=deques[i].popleft()
            else:
                # steal
                for j in range(NWORK):
                    if j!=i and deques[j]:
                        task=deques[j].pop()
                        break
        if task is None:
            time.sleep(0.001); continue
        # 실행(가중치: 로컬이면 더 빠르다고 가정)
        t,origin=task
        if origin==i: time.sleep(t*0.8)
        else: time.sleep(t*1.0)

def main():
    global done
    # 작업 생성: 원래 i 워커가 선호하는 작업
    for i in range(NWORK):
        for _ in range(20):
            deques[i].append((random.uniform(0.001,0.003), i))
    ts=[threading.Thread(target=worker,args=(i,),daemon=True) for i in range(NWORK)]
    for t in ts: t.start()
    # 대기: 작업이 비면 종료
    while any(deques):
        time.sleep(0.02)
    done=True
    for t in ts: t.join(timeout=0.01)

if __name__=="__main__":
    main()
```
**관찰 포인트**: 로컬 pop(앞) vs 원격 steal(뒤)로 **캐시 로컬리티** 유지 시도.

---

### Per-CPU Runqueue + 주기적 Load Balancing 시뮬레이터

아래 시뮬레이터는 **per-CPU runqueue**, **이주 비용**, **부하평준화 주기**를 가진다.

```python
# mpsched_sim.py — 다중코어 스케줄러 미니 실험실

from dataclasses import dataclass
import random, statistics

@dataclass
class Task:
    arrival: int
    burst: int
    id: str

def simulate(ncpu=4, tasks=None, balance_period=5, migrate_cost=0.2):
    if tasks is None:
        random.seed(7)
        t=0; tasks=[]
        for i in range(60):
            t+=random.randint(0,2)
            tasks.append(Task(t, random.randint(1,8), f"T{i}"))
    # runqueues
    rq=[[] for _ in range(ncpu)]
    t=0; i=0
    remaining={}
    start, finish = {}, {}
    # 메트릭
    busy=[0]*ncpu; switches=[0]*ncpu; migrations=0
    # 간단한 가정: 각 tick마다 cpu는 맨 앞 task를 1만큼 실행
    while i<len(tasks) or any(rq):
        # 도착
        while i<len(tasks) and tasks[i].arrival<=t:
            # 가장 짧은 rq에 push
            j=min(range(ncpu), key=lambda k: len(rq[k]))
            rq[j].append(tasks[i].id)
            remaining[tasks[i].id]=tasks[i].burst
            start.setdefault(tasks[i].id, t)
            i+=1
        # 스케줄링
        for c in range(ncpu):
            if rq[c]:
                busy[c]+=1
                cur=rq[c][0]
                remaining[cur]-=1
                if remaining[cur]==0:
                    finish[cur]=t+1
                    rq[c].pop(0)
            # else idle
        # 주기적 부하평준화: 가장 긴 rq에서 가장 짧은 rq로 머리 아닌 꼬리 일부 이동
        if balance_period and t>0 and t%balance_period==0:
            hi=max(range(ncpu), key=lambda k: len(rq[k]))
            lo=min(range(ncpu), key=lambda k: len(rq[k]))
            if len(rq[hi]) - len(rq[lo]) > 1:
                # 머리(실행중) 제외한 뒤에서 하나 이동
                if len(rq[hi])>=2:
                    moved=rq[hi].pop()   # 꼬리에서 하나
                    rq[lo].append(moved)
                    # migrate penalty: 다음 tick에만 적용(단순히 실행 지연으로 모델링)
                    for c in range(ncpu):
                        if rq[c] and rq[c][0]==moved:
                            pass  # migrated job이 즉시 선두로 오지 않는다고 가정
                    migrations+=1
        t+=1
        if i>=len(tasks) and not any(rq): break
    waits=[finish[k]-start[k]-remaining.setdefault(k,0) for k in finish]
    turns=[finish[k]-int(k[1:]) for k in finish]  # 단순 지표
    return {
        "util_per_cpu": [b/max(1,t) for b in busy],
        "avg_wait": statistics.mean([(finish[k]-start[k]) - (finish[k]-start[k]-waits[idx]) for idx,k in enumerate(finish)]),
        "migrations": migrations
    }

if __name__=="__main__":
    print(simulate())
```

**실험 제안**
- `balance_period={0,5,20}`, `migrate_cost={0,0.2,0.5}` 를 스윕하여 **이주 빈도/대기시간/유휴율**의 관계를 관찰하라.
- NUMA 환경을 모사하려면 “원격 CPU로의 이주”에 **추가 벌점**을 부여하라.

---

### Affinity, NUMA, SMT

- **Affinity**: 스레드를 특정 CPU에 바인딩 → 캐시/NUMA 로컬리티↑
- **NUMA**: first-touch 정책, 메모리 바인딩으로 원격 접근 최소화
- **SMT/HT**: 논리 코어가 동일한 물리 코어의 실행 포트를 공유 → **핫 스레드**는 서로 다른 물리 코어로 배치

#### C(리눅스): 스레드 바인딩 예제

```c
// pin.c — 현재 스레드를 CPU0에 바인딩
#define _GNU_SOURCE
#include <pthread.h>
#include <sched.h>
#include <stdio.h>

void pin_cpu0(){
  cpu_set_t set; CPU_ZERO(&set); CPU_SET(0, &set);
  pthread_setaffinity_np(pthread_self(), sizeof(set), &set);
}
int main(){
  pin_cpu0();
  volatile long long x=0; for(long long i=0;i<1e9;i++) x+=i;
  printf("done on CPU0\n");
}
```

---

### Gang Scheduling (개념+스케치)

- **동기 병렬 태스크**(예: MPI)에서 구성 스레드를 **같은 시각에 함께** 스케줄
- 장점: 락/배리어 지연 감소, 단점: 외부 작업과의 공정성 이슈

```python
# gang_sketch.py — 두 개의 gang(A,B)을 같은 tick에 올림

gangA = ["A1","A2","A3","A4"]; gangB=["B1","B2","B3","B4"]
# tick마다 A 전체, 다음 tick은 B 전체를 예약하는 라운드로빈 → 배리어 지연 최소화

```

---

## Real-Time CPU Scheduling

### 하드 vs 소프트 실시간

- **하드 RT**: 데드라인 위반 불가(항공/의료/제어)
- **소프트 RT**: 평균/확률적 보장(멀티미디어)

### Fixed-Priority: Rate-Monotonic (RM)

- 주기 $$T_i$$ 가 짧을수록 우선순위가 높음
- **스케줄 가능성 충분조건**: $$ U = \sum_{i=1}^n \frac{C_i}{T_i} \le n\left(\sqrt[n]{2}-1\right) $$
  $$n\to\infty$$ 에서 상한은 $$\ln 2 \approx 0.693$$

### Dynamic-Priority: Earliest-Deadline-First (EDF)

- **가장 임박한 데드라인** 우선
- 단일 프로세서에서 **필요충분조건**: $$ U \le 1 $$ (이상적인 조건 하)

### (RM)

작업 $$i$$ 의 최악응답시간 $$R_i$$ 는 다음 반복식의 고정점으로 근사:
$$
R_i^{(k+1)} = C_i + \sum_{j \in hp(i)} \left\lceil \frac{R_i^{(k)}}{T_j} \right\rceil C_j,
$$
여기서 \(hp(i)\) 는 태스크 \(i\) 보다 **우선순위가 높은 집합**.
만약 $$R_i \le D_i$$ (상대 기한) 이면 스케줄 가능.

#### 파이썬: RM RTA 검사기

```python
# rta_rm.py — 고정 우선순위 응답시간 분석

import math

def rta_rm(tasks):
    """
    tasks: list of dict {C, T, D} with RM priority (T가 짧을수록 우선)
    return: per-task response time and feasibility
    """
    tasks=sorted(tasks, key=lambda x: x["T"])  # RM
    res=[]
    feasible=True
    for i,ti in enumerate(tasks):
        C,T,D=ti["C"], ti["T"], ti.get("D",ti["T"])
        R=C
        while True:
            interf=0
            for j,tj in enumerate(tasks[:i]):  # higher priority
                interf += math.ceil(R/tj["T"]) * tj["C"]
            Rn = C + interf
            if Rn==R or Rn>10**9: break
            R=Rn
        res.append({"R":R,"D":D,"ok":R<=D})
        feasible &= (R<=D)
    return res, feasible

if __name__=="__main__":
    T=[ {"C":1,"T":4,"D":4}, {"C":1,"T":5,"D":5}, {"C":2,"T":20,"D":20} ]
    print(rta_rm(T))
```

### EDF Demand-Bound Function(DBF)

DBF는 구간 길이 $$t$$에서 필요한 총 실행 시간:
$$
\operatorname{dbf}(t) = \sum_i \max\left(0, \left\lfloor \frac{t - D_i}{T_i} \right\rfloor + 1\right) C_i.
$$
모든 $$t>0$$ 에 대해 $$\operatorname{dbf}(t) \le t$$ 이면 스케줄 가능(단일 CPU, 독립 태스크).

```python
# edf_dbf.py — EDF DBF 스케줄 가능성 검사(유한 t 탐색)

import math

def edf_dbf(tasks, tmax=200):
    def dbf(t):
        s=0
        for C,T,D in tasks:
            k = math.floor((t-D)/T)+1
            s+= max(0,k)*C
        return s
    for t in range(1,tmax+1):
        if dbf(t) > t:
            return False, t
    return True, None

if __name__=="__main__":
    # (C,T,D)
    tasks=[(1,4,4),(1,5,5),(2,7,7)]
    print(edf_dbf(tasks))
```

### 우선순위 역전과 상속(실시간)

- **Priority Inheritance**: 낮은 우선순위가 락 보유 시 임시로 높은 우선순위 상속
- **Priority Ceiling/Protect**: 해당 락의 ceiling priority 부여, 교착 회피에 유리

#### POSIX PI 뮤텍스

```c
// pi_mutex.c — PTHREAD_PRIO_INHERIT 적용
#include <pthread.h>
#include <stdio.h>

int main(){
  pthread_mutexattr_t a; pthread_mutexattr_init(&a);
  pthread_mutexattr_setprotocol(&a, PTHREAD_PRIO_INHERIT);
  pthread_mutex_t m; pthread_mutex_init(&m,&a);
  // ... H/M/L 스레드 생성 및 실험 (실시간 스케줄러 필요)
  return 0;
}
```

### POSIX 실시간 정책 적용(주의: root/권한 필요)

```c
// set_rt_rr.c — 현재 스레드를 SCHED_RR로 설정
#define _GNU_SOURCE
#include <sched.h>
#include <stdio.h>

int main(){
  struct sched_param sp={.sched_priority=50};
  if(sched_setscheduler(0, SCHED_RR, &sp)<0){ perror("sched_setscheduler"); return 1; }
  // 주기 작업 시뮬레이션
  for(volatile long long i=0;i<1e8;i++);
  puts("done RT");
}
```

---

## Operating-System Examples

### Linux

- **CFS(일반)**: **red-black tree**로 가장 작은 **vruntime**를 가진 태스크 선택
  - `sched_latency_ns`, `min_granularity_ns`, `nice`→ weight
  - **cgroups** 로 그룹 공정성/비례 점유
- **RT 스케줄러**: `SCHED_FIFO`, `SCHED_RR` (고정 우선순위 1–99)
- **CPU Load Balancing**: per-CPU runqueue 간 주기적 리밸런싱, **NUMA balancing** 옵션
- **성능 관측**: `sched:sched_switch`, `perf stat`, `trace-cmd`, `bpftrace`

#### CFS-like 이해용 미니 모델 (파이썬)

```python
# cfs_like.py — vruntime 기반 타임슬라이스 할당(학습용 단순화)

import heapq
def cfs(jobs, slice_ns=2):
    # jobs: [(arrival, runtime, nice)]
    t=0; rq=[]; i=0
    weight={-5:1.5,0:1.0,5:0.7}
    res=[]
    while i<len(jobs) or rq:
        while i<len(jobs) and jobs[i][0]<=t:
            a,r,n=jobs[i]; heapq.heappush(rq,[0.0, weight.get(n,1.0), r, a, n, f"J{i}"]); i+=1
        if not rq: t=jobs[i][0]; continue
        vr,w,rem,a,n,name = heapq.heappop(rq)
        run=min(slice_ns, rem); rem-=run; t+=run; vr += run/w
        if rem>0: heapq.heappush(rq,[vr,w,rem,a,n,name])
        else: res.append((name,t-a))
    return res
```

### Windows

- **우선순위 클래스(프로세스)** + **상대 우선순위(스레드)**
- **Dynamic boosting**(인터랙티브 응답성 개선)
- **IOCP + 쓰레드풀** 로 고동시성 I/O 처리(스레드 수를 소켓 수보다 훨씬 작게 유지)
- **SRWLOCK/Condition Variable** 로 경량 동기화

### macOS / iOS

- **pthreads + GCD(Grand Central Dispatch)**
- **QoS 클래스**(User-Interactive, User-Initiated, Utility, Background)
- 시스템이 작업량/열/전력에 따라 워커 스레드 수를 자동 조절

### Android

- **Binder** 기반 RPC — system_server와 각 서비스의 **thread pool**
- 앱 레벨: `Executors`, `HandlerThread`, **Kotlin coroutines**(Dispatchers.Default/IO/Main)
- 포그라운드 vs 백그라운드 제약(배터리/Doze/App Standby)과 스케줄링 협조

---

## Algorithm Evaluation (알고리즘 평가)

### 메트릭 정의

- **평균/백분위 지연**: $$\bar{W},\ p95,\ p99$$
- **반환시간**: $$\text{turn}_i = \text{finish}_i - \text{arrival}_i$$
- **처리량**: 단위시간당 완료 작업 수
- **오버헤드 비율(컨텍스트 스위치)**: $$\frac{c}{q+c}$$ (단순 모델)
- **Migration/NUMA 원격비율**, **캐시미스/L2/L3**, **전력/열**

### 실험 설계 원칙

1) **통제**: 같은 작업 세트·같은 시드로 알고리즘만 바꾼다
2) **Warm-up/Steady-state** 분리
3) **반복·통계**: 최소 30회 이상, 신뢰구간(bootstrap)
4) **관측 오버헤드** 최소화(샘플링)
5) **교란 요인**: 터보부스트/온도/배경 잡업 제거

### 파라미터 스윕 & 대시보드

아래 스크립트는 5.3의 시뮬레이터를 **멀티코어/멀티정책**으로 확장해 파라미터 스윕한다.

```python
# eval_sweep.py — 정책/파라미터 스윕 예시(의존: sched_lab.py / mpsched_sim.py)

from sched_lab import gen_jobs, fcfs, sjf_nonpreemptive, srtf, rr, priority_preemptive, mlfq, cfs_like
from mpsched_sim import simulate
import statistics

def sweep_rr_quanta(quanta=[1,2,4,8]):
    J=gen_jobs()
    out=[]
    for q in quanta:
        js, m = rr(J, q)
        out.append((q, m["avg_wait"], m["avg_turnaround"]))
    return out

def sweep_balance(periods=[0,5,10,20]):
    res=[]
    for p in periods:
        r=simulate(balance_period=p)
        res.append((p, sum(r["util_per_cpu"])/len(r["util_per_cpu"]), r["migrations"]))
    return res

if __name__=="__main__":
    print("RR sweep:", sweep_rr_quanta())
    print("Balance sweep:", sweep_balance())
```

### 실제 시스템 계측

- **Linux**:
  - 스케줄 이벤트: `sudo perf sched record`, `perf sched timehist`
  - 컨텍스트 스위치/CPU: `perf stat -e sched:sched_switch,context-switches,task-clock -a -- sleep 5`
  - NUMA: `numastat`, `perf c2c`
- **Windows**:
  - Windows Performance Recorder/Analyzer(WPR/WPA), ETW(스케줄러/CPU 샘플)
- **macOS**:
  - Instruments(Time Profiler), `powermetrics`

---

## 종합 실습 시나리오

### 시나리오 A — “웹 서버 + 백엔드 작업” 튜닝

1) **네트워크**: epoll 이벤트 루프(1~2 스레드), **작업 큐**로 CPU-바운드 파싱/압축을 스레드풀에 위임
2) **스케줄링 정책**:
   - 리눅스 기본(CFS)에서 **nice** 조절로 “백엔드 압축” 가중치↓
   - RR(실시간)은 피하고, 필요한 구간만 **SCHED_BATCH** 또는 cgroup 비용 제한
3) **Affinity**: NIC interrupt CPU와 워커가 같은 NUMA 노드에서 동작하도록 바인딩
4) **평가**: p95/99 응답시간 vs 처리량, 컨텍스트 스위치 수, migrations, L3 miss

### 시나리오 B — “주기 제어 태스크(하드 RT)”

1) 태스크 집합 \((C_i,T_i,D_i)\) 정의 → **RM RTA**로 사전 검증
2) **SCHED_FIFO** + **PI 뮤텍스** 도입, 비핵심 경로는 비RT 스레드로 분리
3) **타이머**: `clock_nanosleep(CLOCK_MONOTONIC, TIMER_ABSTIME, …)` 로 **절대시각** 주기 트리거
4) **평가**: deadline miss counting, jitter(표준편차), preemption count

```c
// abs_timer.c — 절대시간 주기 타이머
#define _GNU_SOURCE
#include <time.h>
#include <stdio.h>

int main(){
  struct timespec ts; clock_gettime(CLOCK_MONOTONIC, &ts);
  const long period_ns = 1000000; // 1ms
  for(int k=0;k<1000;k++){
    ts.tv_nsec += period_ns;
    while(ts.tv_nsec >= 1000000000L){ ts.tv_nsec -= 1000000000L; ts.tv_sec++; }
    // 작업
    // ...
    // 다음 주기까지 절대 슬립
    clock_nanosleep(CLOCK_MONOTONIC, TIMER_ABSTIME, &ts, NULL);
  }
}
```

---

## 핵심 요약

- **5.5**: 다중코어에서는 **per-CPU 큐 + 주기적 리밸런싱**이 표준. Affinity/NUMA/SMT를 의식해 **이주 빈도 vs 캐시 로컬리티**를 균형 잡아라. Work-stealing은 태스크형 워크로드에 강력.
- **5.6**: **RM/EDF** 이론을 바탕으로 **RTA/DBF**로 사전 검증, 실전에서는 **PI/PC**로 우선순위 역전을 차단하고 절대시간 타이머로 지터를 관리한다.
- **5.7**: Linux(CFS/RT), Windows(동적 부스트/IOCP), macOS(GCD/QoS), Android(Binder)의 **스케줄링 생태**를 이해하면 성능/응답성/전력 모두에서 이점을 얻는다.
- **5.8**: 평가는 **지표 정의 → 설계 → 시뮬레이션/실측 → 파라미터 스윕**의 반복. p95/p99 지연, migrations, 캐시 미스, NUMA 원격 비율을 함께 본다.

---

## 추가 과제

1) `mpsched_sim.py`에서 **NUMA 노드 2개**를 도입하고, **원격 이주에 가산 비용**을 넣어 `balance_period`×`migrate_cost` 히트맵을 그려라.
2) `rta_rm.py`에 **자원 공유 블로킹 용어** \(B_i\) 를 추가해 $$R_i^{(k+1)}=C_i+B_i+\sum\ldots$$ 를 구현하고 PI 유무에 따른 결과를 비교하라.
3) `cfs_like.py`에서 `nice`→weight 맵을 변경하며 **비례 점유** 실험을 수행, vruntime 트레이스(시간 vs vruntime)를 플롯하라.
4) 리눅스에서 **`perf sched timehist`** 와 **bpftrace** 를 사용해 실제 서버의 **preemption/migration hot spot**을 식별하고, affinity/irq pinning으로 개선해보라.
5) Windows에서 동일 워크로드를 **IOCP + ThreadPool** 로 구현하고 스레드 수를 조정하며 **컨텍스트 스위치/CPU 사용률**을 비교하라.
