---
layout: post
title: 디지털신호처리 - 유한 정밀도 수치 영향의 개관, 수의 표현
date: 2025-11-16 14:25:23 +0900
category: 디지털신호처리
---
# 유한 정밀도 수치 영향의 개관, 수의 표현 — 실수 이론 vs 컴퓨터 현실

> 이 글은 앞에서 다뤘던 **계수 양자화**, **필터 구조별 정량화 오차** 같은 내용을 더 넓은 관점에서 감싸는 “상위 개념”이다.
> 즉, **컴퓨터가 수를 어떻게 표현하고**, 그로 인해 **어떤 수치적 현상과 오류가 생기는지**를 체계적으로 정리하는 것이 목적이다.

구성:

- 유한 정밀도(有限精密度, finite precision)의 개관
- 수의 표현: 정수, 고정소수점, 부동소수점(IEEE 754 중심)
- 반올림 오차, 머신 엡실론, 오버플로/언더플로
- 소멸(cancellation), 누적 오차, 불안정 알고리즘
- 간단한 코드 예제(Python, C, Octave 스타일)와 실제 상황

---

## 유한 정밀도 수치의 큰 그림

### vs 컴퓨터의 수(유한 집합)

수학에서 다루는 실수 집합 $$\mathbb{R}$$ 은:

- 연속이고
- 셀 수 없을 정도로 많으며(비가산 무한)
- 유리수와 무리수를 모두 포함한다.

하지만 컴퓨터는 **유한한 비트 수**로 수를 표현한다.

- 32비트 정수 → 가능한 값 개수는 정확히 $$2^{32}$$ 개.
- 64비트 부동소수점(double) → 비트 조합 수는 $$2^{64}$$ 개이지만, 그 안에는 NaN 등 특수 값이 포함되고, 실수가 균등하게 채워져 있지도 않다.

즉, 현실에서는

$$
\mathbb{R} \supset F
$$

- $$F$$: **컴퓨터가 표현할 수 있는 수들의 집합** (finite set).
- 실제 계산은 모두 $$F$$ 안에서만 일어난다.

여기서 발생하는 핵심 아이디어:

1. 입력 실수 $$x$$ 가 주어져도, 컴퓨터는 **가장 가까운 표현가능한 수** $$\mathrm{fl}(x) \in F$$ 로 반올림해서 저장한다.
2. 연산 결과도 마찬가지로 다시 반올림된다:
   $$ \mathrm{fl}(x \circ y) = \mathrm{fl}(\mathrm{fl}(x) \circ \mathrm{fl}(y)) $$
   (여기서 $$\circ \in \{+, -, \times, \div\}$$)

이렇듯 “진짜 수”와 “기계 수” 사이에 항상 작은 간극이 존재하고, 이것이 **유한 정밀도 수치 영향**의 모든 현상의 근원이다.

---

### 수치 오차의 기본 분해

실제 값 $$x$$ 와 기계가 표현한 값 $$\hat{x} = \mathrm{fl}(x)$$ 사이의 오차는 보통 다음 두 가지로 본다.

- 절대 오차:

  $$
  |x - \hat{x}|
  $$

- 상대 오차(비율):

  $$
  \left|\frac{x - \hat{x}}{x}\right| \quad (x \ne 0)
  $$

부동소수점 이론에서는 흔히 다음과 같은 간단한 모델을 쓴다.

$$
\mathrm{fl}(x) = x (1 + \delta)
$$

어떤 상수 $$u$$ (unit roundoff 또는 머신 엡실론 수준) 에 대해

$$
|\delta| \le u
$$

라고 놓는 것이다. 즉, **모든 반올림은 상대 오차가 최대 $$u$$ 인 거의 정확한 값**이라고 생각한다. 이 모델이 매우 단순하지만, 알고리즘의 수치 안정성을 분석할 때 강력하게 쓰인다.

이제 **유한 정밀도가 어디서 오는지(수의 표현)** 부터 자세히 본다.

---

## 수의 표현: 진법과 자리값

### 자리값 표기법(위치 표기법)의 일반형

우리가 쓰는 10진수는 **자리값(position value)** 을 사용하는 표기법이다.

10진에서 어떤 수 $$x$$ 를 다음과 같이 쓸 수 있다.

$$
x = \pm \left( d_k d_{k-1} \dots d_1 d_0 . d_{-1} d_{-2} \dots \right)_{10}
$$

이는 수식으로:

$$
x = \pm \sum_{i=-\infty}^{k} d_i 10^i
$$

- 각 자릿수 $$d_i \in \{0,1,2,\dots,9\}$$
- 소수점 왼쪽: 양의 지수
- 소수점 오른쪽: 음의 지수

이 개념을 **일반 진법 $$\beta$$** 로 확장하면:

$$
x = \pm \sum_{i=-\infty}^{k} d_i \beta^i
$$

- $$\beta$$: 진법(base, radix), 보통 2, 8, 10, 16.
- 이진법에서 $$\beta = 2$$, 각 자릿수 $$d_i \in \{0,1\}$$.

디지털 시스템에서는 대부분 **2진법(이진수)** 를 사용한다.
정수, 고정소수점, 부동소수점 모두 결국 이런 **이진 자리값 표기**를 기반으로 설계된다.

---

## 정수 표현

### 부호 없는 정수(Unsigned)

비트 수가 $$n$$ 인 부호 없는 정수는 다음처럼 표현한다.

$$
x = \sum_{i=0}^{n-1} b_i 2^i,
\quad b_i \in \{0,1\}
$$

- 최솟값: 0 (모든 비트 0)
- 최댓값: $$2^n - 1$$ (모든 비트 1)

예: 8비트 부호 없는 정수

| 비트 패턴 | 값 |
|----------|----|
| 0000 0000 | 0 |
| 0000 0001 | 1 |
| ... | ... |
| 1111 1111 | 255 (= \(2^8 - 1\)) |

### 부호 있는 정수 – 2의 보수(Two’s complement)

현대 CPU 에서 부호 있는 정수는 거의 항상 **2의 보수 표현**을 사용한다.

- 가장 왼쪽 비트(MSB)가 부호 비트를 겸한다.
- 비트 패턴을 $$b_{n-1} b_{n-2} \dots b_1 b_0$$ 라고 하면, 값은

  $$
  x = -b_{n-1} 2^{n-1} + \sum_{i=0}^{n-2} b_i 2^i
  $$

예: 8비트 signed

| 비트 패턴 | 값 |
|----------|----|
| 0000 0000 | 0 |
| 0000 0001 | 1 |
| ... | ... |
| 0111 1111 | 127 (= \(2^7 - 1\)) |
| 1000 0000 | -128 |
| 1000 0001 | -127 |
| ... | ... |
| 1111 1111 | -1 |

범위는:

- 최소: $$-2^{n-1}$$
- 최대: $$2^{n-1} - 1$$

이 표현의 장점:

- 덧셈/뺄셈 회로가 부호 없는 경우와 거의 동일하다.
- 0이 한 가지 표현만 가진다(부호-크기 표현에는 +0/-0 이 따로 있을 수 있다).

### 정수 오버플로 예제 (C 스타일)

```c
#include <stdio.h>
#include <stdint.h>

int main(void) {
    int8_t a = 120;
    int8_t b = 10;
    int8_t c = a + b;

    printf("a = %d, b = %d, a+b = %d\n", a, b, c);
    return 0;
}
```

이 코드는 수학적으로는 \(120 + 10 = 130\) 이지만,
8비트 signed 범위는 [-128, 127] 이므로 **오버플로**가 발생한다.

많은 C 컴파일러/플랫폼에서 이 경우 결과는 **정의되지 않거나**,
2의 보수 연산 결과를 그대로 해석했을 때  -126 등으로 wrap-around 된다.

정수 영역에서는:

- 산술 오버플로 → **wrap-around 또는 예외** (언어/플랫폼에 따라 다름)
- 이 자체가 유한 정밀도 수치 영향의 한 예다.

---

## 표현

### 개념

고정소수점은 **정수 표현을 기반으로 소수점 위치를 “고정”해 두고 쓰는 방식**이다.

예를 들어 **Qm.n 형식**을 자주 쓴다.

- 전체 비트 수: $$m + n$$ (부호 비트 1개 포함이라고 보는 관례도 있음)
- 정수부 비트: $$m$$
- 소수부 비트: $$n$$

값은:

$$
x = \text{정수값} \times 2^{-n}
$$

예: Q1.7 (8비트, 1비트 정수부, 7비트 소수부)

- 비트 패턴 0000 0000 → 정수값 0 → 실수 0
- 비트 패턴 0000 0001 → 정수값 1 → 실수 \(1 \times 2^{-7} = 1/128 \approx 0.0078125\)
- 비트 패턴 0111 1111 → 정수값 127 → 실수 \(127/128 \approx 0.9921875\)
- 부호 있는 2의 보수 Q1.7 이면, 1000 0000 은 -1.0, 1111 1111 은 -1/128.

**해상도(resolution)**:

$$
\Delta = 2^{-n}
$$

즉, 표현 가능한 값 사이의 간격이 일정하다.

### DSP에서의 고정소수점

DSP 칩(특히 옛날/저가형)에서는 부동소수점 대신 고정소수점을 사용한다.

- 장점:
  - 연산 회로가 단순하고 빠르다.
  - 곱셈 후 shift 로 스케일 조절이 쉽다.
- 단점:
  - 표현 범위가 좁다.
  - 스케일링과 saturation(포화) 관리가 중요하다.

예: 16비트 Q1.15 형식

- 해상도: $$2^{-15} \approx 3.05 \times 10^{-5}$$
- 대략 -1.0 ~ +0.99997 범위를 표현.

고정소수점 필터 구현에서:

- 계수, 내부 상태를 Q 형식으로 두고
- 곱셈/덧셈 후 적절히 shift & saturate 를 수행해야
- 오버플로와 성능을 동시에 관리할 수 있다.

---

## 표현 — 일반 모형

### 일반 부동소수점 모형

부동소수점 수는 일반적으로 다음의 형태를 가진다.

$$
x = \pm m \times \beta^e
$$

- $$\beta$$: 진법(base), 보통 2 또는 10.
- $$m$$: 유효숫자(significand, mantissa), 보통 1과 $$\beta$$ 사이의 정규화된 수.
- $$e$$: 지수(exponent), 정수.

이를 더 엄밀하게 표현하면:

$$
x = (-1)^s \times \left(\sum_{i=0}^{p-1} d_i \beta^{-i}\right) \times \beta^e
$$

- $$s$$: 부호 비트
- $$p$$: 유효숫자 자릿수(정밀도)
- $$d_i$$: 각 자리수, 이진 부동소수점이면 $$d_i \in \{0,1\}$$

### 정규화

정규화된 부동소수점에서는

$$
d_0 \ne 0
$$

을 강제한다.

이진 부동소수점에서 $$\beta=2$$ 이고, 정규화하면 significand 는 다음 범위에 있다.

$$
1 \leq m < 2
$$

즉, 가장 앞자리 비트는 항상 1이므로,
그 비트를 **저장하지 않고 숨겨진 비트(hidden bit)** 로 취급할 수 있다.
이를 통해 저장 효율을 높인다.

---

## IEEE 754 이진 부동소수점 형식(개관)

IEEE 754 표준은 현대 대부분의 CPU/GPU에서 사용하는 부동소수점 형식을 정의한다.
여기서는 **binary32(단정도)** 와 **binary64(배정도)** 를 중심으로 본다.

### binary32 (single precision, float)

32비트 단정도 부동소수점의 비트 구성:

| 필드 | 비트 수 | 의미 |
|------|--------|------|
| sign | 1 | 부호 |
| exponent | 8 | 지수 (bias = 127) |
| fraction | 23 | 가수의 하위 비트들 |

값은 다음처럼 해석한다.

1. 정규화된 수 (0 < exponent < 255):

   - sign 비트: $$s \in \{0,1\}$$
   - 지수 필드: $$E = $$ exponent 값(0~255)
   - 실질 지수: $$e = E - 127$$
   - 가수:

     $$
     m = 1 + \sum_{i=1}^{23} f_i 2^{-i}
     $$

   - 값:

     $$
     x = (-1)^s \times m \times 2^{e}
     $$

2. 지수 필드가 0 또는 255일 때는 특수 해석(0, 서브노멀, ∞, NaN).

정규화된 양수 최소값:

- 가수 $$m = 1.0$$
- 지수 필드 최소: 1 → 실질 지수 $$e = 1 - 127 = -126$$

$$
x_{\min}^{+} \approx 1.0 \times 2^{-126}
$$

정규화된 양수 최대값:

- 가수 $$m \approx 2 - 2^{-23}$$ (모든 fraction 비트 1)
- 지수 필드 최대: 254 → 실질 지수 $$e = 127$$

$$
x_{\max} \approx (2 - 2^{-23}) \times 2^{127}
$$

### binary64 (double precision, double)

64비트 배정도 부동소수점의 비트 구성:

| 필드 | 비트 수 | 의미 |
|------|--------|------|
| sign | 1 | 부호 |
| exponent | 11 | 지수 (bias = 1023) |
| fraction | 52 | 가수 하위 비트 |

정규화된 수의 해석:

- 실질 지수: $$e = E - 1023$$
- 가수:

  $$
  m = 1 + \sum_{i=1}^{52} f_i 2^{-i}
  $$

- 값:

  $$
  x = (-1)^s \times m \times 2^e
  $$

이 형식은 약 15~17자리 십진수 유효 숫자를 제공한다.

---

### 특수 값: 0, 서브노멀, ∞, NaN

IEEE 754 에서 지수/가수 조합에 따라 특수 값들이 정의된다.

| exponent | fraction | 의미 |
|----------|----------|------|
| 0 | 0 | ±0 |
| 0 | ≠0 | 서브노멀(subnormal) |
| 1~max-1 | * | 정규화된 수 |
| all 1 | 0 | ±∞ |
| all 1 | ≠0 | NaN |

- **±0**: 부호가 있는 0, +0과 -0은 부동소수점에서는 구분되지만, 보통 비교에서는 동일 취급.
- **서브노멀**: 정규화된 수보다 더 작은 크기의 수.
  가수의 숨겨진 비트가 0 이라고 가정하여

  $$
  x = (-1)^s \times (0 + \sum_{i=1}^{p-1} f_i 2^{-i}) \times 2^{e_{\min}}
  $$

  이런 값들은 **점진적 언더플로(gradual underflow)** 를 제공하여,
  0 근처에서 갑자기 0으로 떨어지지 않고 점진적으로 사라지게 한다.
- **±∞**: 오버플로 결과나 1/0 (부호와 함께) 연산에서 등장.
- **NaN**: 정의되지 않은 결과(0/0, ∞-∞ 등) 또는 잘못된 연산을 나타내는 특수 값.

---

## 머신 엡실론(machine epsilon)과 단위 반올림(unit roundoff)

### 정의

머신 엡실론은 부동소수점 형식에서

> 1과 1보다 큰 **가장 가까운 부동소수점 수** 사이의 차이

를 가리키는 실수 값이다.

이진 부동소수점, 정규화된 가수 범위가 [1, 2) 에서 유효숫자 비트 수를 $$p$$ 라 하면,

- 1 바로 위의 수는

  $$
  1 + 2^{-p}
  $$

- 따라서

  $$
  \varepsilon_{\text{machine}} = 2^{-p}
  $$

예:

- binary32 (float): 가수 비트 24(숨겨진 비트 포함) → $$\varepsilon_{\text{machine}} = 2^{-23} \approx 1.19 \times 10^{-7}$$
- binary64 (double): 가수 비트 53 → $$\varepsilon_{\text{machine}} = 2^{-52} \approx 2.22 \times 10^{-16}$$

수치해석에서는 보통

- unit roundoff $$u = \varepsilon_{\text{machine}} / 2$$ 로 정의하기도 한다.

### Python으로 머신 엡실론 확인

```python
import numpy as np

eps_double = np.finfo(float).eps      # double precision epsilon
eps_float  = np.finfo(np.float32).eps # single precision epsilon

print("double epsilon:", eps_double)
print("float  epsilon:", eps_float)

# + eps/2 와 1 + eps 비교

one = 1.0
print("1 + eps/2 == 1 ?", (one + eps_double/2) == one)
print("1 + eps   == 1 ?", (one + eps_double)   == one)
```

실행하면 대략 다음과 같은 결과를 보게 된다(환경에 따라 약간 다를 수 있지만 개념은 동일).

- `double epsilon` ≈ 2.220446049250313e-16
- `float epsilon` ≈ 1.1920929e-07
- `1 + eps/2 == 1` → `True`
- `1 + eps == 1` → `False`

이는 곧:

- **`eps/2` 만큼 더하면 반올림 후 다시 1이 되어 표현상 구분이 안 된다.**
- **`eps` 만큼 더하면 1보다 큰 다음 수로 반올림된다.**

즉, 머신 엡실론은 **1 근처에서의 최소 구분 간격**이자,
상대오차의 대표적인 단위 척도다.

---

## 모드와 fl 연산 모델

### 반올림 모드

IEEE 754에서는 여러 반올림 모드를 정의한다.

- **가장 가까운 값, 짝수 쪽으로 (round to nearest, ties to even)**
  → 가장 많이 쓰이는 기본 모드.
- 0 쪽으로 (toward zero)
- +∞ 쪽으로 (toward +∞)
- -∞ 쪽으로 (toward -∞)

일반적으로 수치해석 이론에서는 **round to nearest** 를 가정한다.
이때, 반올림 연산을 다음처럼 모델링한다.

$$
\mathrm{fl}(x) = x(1 + \delta), \quad |\delta| \le u
$$

여기서 $$u$$ 는 unit roundoff.

### 기본 연산의 오차 모델

두 실수 $$x, y$$ 에 대해 부동소수점 연산을 할 때:

- 덧셈:

  $$
  \mathrm{fl}(x + y) = (x + y)(1 + \delta_1), \quad |\delta_1| \le u
  $$

- 곱셈:

  $$
  \mathrm{fl}(x \times y) = (x \times y)(1 + \delta_2), \quad |\delta_2| \le u
  $$

이 단순한 모델을 반복 적용하면, 여러 번의 연산에서 오차가 **어떻게 누적되는지**를 이론적으로 분석할 수 있다.

예: $$n$$ 번 덧셈 후 상대 오차가 대략 $$\mathcal{O}(nu)$$ 정도로 커질 수 있다.

---

## 오버플로(Overflow)와 언더플로(Underflow)

### 오버플로

부동소수점 형식에서 표현 가능한 최대값을 넘는 결과가 나올 때,
연산 결과는 **∞(무한대)** 가 되거나 예외가 발생한다(구체적인 동작은 언어/환경 설정에 따라 다름).

예: double 에서 매우 큰 숫자 곱하기

```python
x = 1e308
y = 10.0
z = x * y
print(z)
```

대부분의 환경에서 `z` 는 `inf` 로 출력된다.

오버플로는 수치적으로 다음과 같은 문제를 만든다.

- 알고리즘이 실제로는 유한한 결과를 가져야 하는데, 중간 계산에서 오버플로가 나면 전체 계산이 `inf` / `NaN` 으로 오염된다.
- 특히 지수적 증가/감소, 곱셈이 반복되는 알고리즘(예: 확률 곱, 파워 메서드)에서는 스케일링을 통해 오버플로를 피해야 한다.

### 언더플로와 서브노멀

반대로, 매우 작은 수를 연산하다 보면 **표현 가능한 최소값보다 더 작아져서 0으로 떨어지는 현상**이 발생한다. 이를 **언더플로(underflow)** 라고 한다.

하지만 IEEE 754는 서브노멀(subnormal) 영역을 통해 **점진적 언더플로(gradual underflow)** 를 제공한다.

- 정규화된 최소 양수보다 조금 더 작은 값들은 서브노멀로 표현
- 서브노멀보다 더 작은 값은 0 으로 반올림

언더플로 자체는 흔히 “치명적”이지 않다.
하지만 서브노멀 영역에서는 상대오차가 커질 수 있고, 성능 상 penalty(연산 속도 저하)가 생기는 하드웨어도 있다.

---

## 소멸(cancellation)과 유효숫자 손실

### 거의 같은 두 수의 뺄셈

부동소수점에서 **수치적으로 “가장 위험한 연산” 중 하나가, 거의 같은 두 수의 뺄셈**이다.

예:

$$
x = 1.23456789012345, \quad y = 1.23456788999999
$$

수학적으로:

$$
x - y = 0.00000000012346
$$

하지만, 부동소수점 표현에서 \(x\) 와 \(y\) 가 위 수보다 약간씩 반올림된 형태로 저장되어 있다면,
뺄셈 후 결과는 몇 자리 안 남고 오차가 상대적으로 매우 크게 될 수 있다.

이를 **catastrophic cancellation(치명적인 소멸)** 이라고 부른다.

### 코사인 예제: \(1 - \cos(x)\)

작은 $$x$$ 에 대해

$$
1 - \cos x
$$

을 직접 계산하면 소멸 문제가 생긴다.

테일러 전개:

$$
\cos x = 1 - \frac{x^2}{2} + \frac{x^4}{24} - \cdots
$$

따라서 작은 $$x$$ 에서

$$
1 - \cos x \approx \frac{x^2}{2}
$$

하지만 부동소수점에서는

- \(\cos x\) 가 1에 매우 가깝게 반올림됨
- 1에서 거의 1인 값을 빼면서 유효숫자 대부분이 날아간다.

Python 예제:

```python
import numpy as np

def naive(x):
    return 1 - np.cos(x)

def stable(x):
    # 1 - cos(x) ~ 2 * sin^2(x/2)
    return 2 * np.sin(x / 2)**2

xs = [1e-1, 1e-3, 1e-6, 1e-8]
for x in xs:
    n = naive(x)
    s = stable(x)
    print(f"x = {x: .1e}, naive = {n:.20e}, stable = {s:.20e}")
```

결과를 보면:

- 큰 x(1e-1) 에서는 둘이 비슷
- 매우 작은 x(1e-8) 에서는 naive 결과가 0 이 되어버리거나, stable 결과와 크게 다른 것을 볼 수 있다.

이 예제는 **수학적으로 동등한 식이라도, 수치적으로는 다르게 행동**할 수 있음을 보여준다.

---

## 누적 오차와 합산 알고리즘: Kahan 합산

### 많은 수를 더할 때의 문제

부동소수점에서 많은 수를 순차적으로 더하면, 작은 값들이 큰 값에 더해지는 과정에서 **점점 영향력이 사라지는 문제**가 있다.

예:

- 초기에 1.0 을 저장
- 그 다음에 매우 작은 수 1e-16 을 100만 번 더하면?

이론적으로는 \(1 + 10^{-16} \times 10^{6} = 1 + 10^{-10}\) 이 되지만,
부동소수점에서는 많은 덧셈이 **반올림되어 사라질 수 있다.**

### Kahan 보정 합산 알고리즘

Kahan summation 은 누적 반올림 오차를 보정하기 위한 간단한 알고리즘이다.

의사 코드:

- `sum`: 현재 합
- `c`: 보정값(오차 추적용)

```python
def kahan_sum(values):
    s = 0.0
    c = 0.0
    for x in values:
        y = x - c
        t = s + y
        c = (t - s) - y
        s = t
    return s
```

설명:

- `y = x - c` : 이전 단계에서 잃어버린 작은 오차 `c` 를 보정해 반영한다.
- `t = s + y` : 실제로 더한다.
- `c = (t - s) - y` : 덧셈 과정에서 새로 발생한 오차를 다시 `c` 에 저장.

이렇게 하면 매우 작은 값들도 누적해서 더 정확한 합을 얻을 수 있다.

예제:

```python
import numpy as np

def naive_sum(values):
    s = 0.0
    for x in values:
        s += x
    return s

values = [1.0] + [1e-16] * 1000000

print("naive:", naive_sum(values))
print("kahan:", kahan_sum(values))
```

- `naive` 는 1.0 에서 거의 변하지 않을 수 있다.
- `kahan` 은 1.0000000001 근처 값을 줄 수 있어, 수학적 기대와 더 가깝다.

---

## 조건수(Condition Number)와 알고리즘 안정성(개관)

유한 정밀도 영향은 크게 두 부분으로 나눌 수 있다.

1. **문제 자체의 민감도** (condition)
   입력에 작은 perturbation 를 줬을 때, 해가 얼마나 크게 변하는가?
2. **알고리즘의 수치 안정성** (stability)
   기계 연산이 도입하는 반올림 오차가 해에 얼마나 증폭되는가?

### 조건수의 정의(단변수 함수)

단변수 함수 $$f(x)$$ 에 대해 condition number 를 다음과 같이 정의할 수 있다.

$$
\kappa_f(x) = \left| \frac{x f'(x)}{f(x)} \right|
$$

- 입력에 상대 오차 $$\frac{\Delta x}{x}$$ 을 넣었을 때,
- 출력 상대 오차가 대략:

  $$
  \frac{\Delta f}{f} \approx \kappa_f(x) \frac{\Delta x}{x}
  $$

큰 조건수 → 문제 자체가 민감하다(ill-conditioned).

### 선형 시스템의 조건수(언급만)

선형 시스템 $$Ax = b$$ 에서 조건수:

$$
\kappa(A) = \|A\| \cdot \|A^{-1}\|
$$

- $$\kappa(A)$$ 가 크면, $$b$$ 에 작은 오차가 있어도 $$x$$ 에 큰 오차가 날 수 있다.
- 수치해석에서 중요한 기준.

여기서는 “유한 정밀도의 영향은 문제의 조건과 알고리즘의 안정성 두 축에서 봐야 한다” 정도로 개관만 하고 넘어간다.

---

## 유한 정밀도와 DSP: 계수 양자화, 상태 양자화, limit cycle 개관

앞에서 필터 계수 양자화, IIR 구조, FIR 구조 등을 이미 다뤘으므로,
**유한 정밀도 관점에서 DSP 필터를 다시 정리**해 보자.

### 계수 양자화

필터 계수 \(b_k, a_k\) 는 이론적으로 실수지만, 실제 구현에서는

- 고정소수점 Q 형식 또는
- 부동소수점 형식

으로 표현해야 한다. 따라서

$$
\hat{b}_k = \mathrm{fl}(b_k), \quad \hat{a}_k = \mathrm{fl}(a_k)
$$

이며, 이들이 실제 구현에서 사용하는 계수다.

전송함수는 이론적으로는

$$
H(z) = \frac{\sum b_k z^{-k}}{1 + \sum a_k z^{-k}}
$$

이지만, 실제 구현은

$$
\hat{H}(z) = \frac{\sum \hat{b}_k z^{-k}}{1 + \sum \hat{a}_k z^{-k}}
$$

이 된다. 극점/영점이 약간 이동하며, 필터 응답이 변형된다.

특히:

- 고차 IIR 을 직접 구현하면 극점이 크게 이동할 수 있다.
- 2차 섹션(SOS) + cascade 구조로 표현하면 각 섹션의 민감도가 줄어든다.
- 격자 구조는 반사계수 범위를 제한해 안정성을 확보하기 쉽다.

### 상태 양자화와 limit cycle

IIR 필터의 내부 상태(지연선 값) 역시 고정소수점일 때,
매 연산마다 반올림이 발생한다.

- 입력이 0 이어도, 내부 상태가 완전히 0으로 수렴하지 않고
- 작은 주기로 진동하는 현상이 나타날 수 있다.

이를 **limit cycle** 이라고 부른다.

예: 단순 IIR:

$$
y[n] = 0.5 y[n-1]
$$

이론적으로는 0으로 수렴하지만, 고정소수점에서는 round-off 때문에
\(y[n]\) 이 일정 범위 내에서 오르내리며 멈추지 않을 수 있다.

### practical 대응

- 고정소수점에서는 포화(saturation) 연산, 데드존(deadzone) 도입 등으로 limit cycle 을 줄일 수 있다.
- 필터 구조(Direct Form I, II, SOS, Lattice 등)를 선택할 때 유한 정밀도 관점에서 비교해야 한다.
- 필터 설계 시 **계수 양자화 시뮬레이션**을 통해 주파수 응답 변형, 노이즈 레벨, SNR 등을 실제로 확인하는 것이 중요하다.

---

## 다양한 언어에서 부동소수점 함정 예제

### + 0.2 != 0.3 (Python 예)

```python
a = 0.1
b = 0.2
c = a + b

print("a + b =", c)
print("a + b == 0.3 ?", c == 0.3)
print("repr(c):", repr(c))
```

일반적으로:

- `a + b` 는 `0.30000000000000004` 등으로 출력될 수 있다.
- `a + b == 0.3` 는 `False`.

이유:

- 0.1, 0.2, 0.3 은 2진 부동소수점에서 **유한한 비트 길이로 정확히 표현되지 않는다.**
- 각각이 가장 가까운 표현가능한 수로 저장되고,
- 연산 결과도 반올림되면서 기대와 약간의 차이가 생긴다.

실무에서는

- `==` 비교 대신 **오차 허용 범위 내 비교**를 사용한다.

```python
import math

def almost_equal(x, y, tol=1e-12):
    return abs(x - y) <= tol * max(1.0, abs(x), abs(y))

print(almost_equal(c, 0.3))
```

### C/C++에서 double 비교 주의

```c
#include <stdio.h>
#include <math.h>

int main(void) {
    double a = 0.1;
    double b = 0.2;
    double c = a + b;

    printf("c = %.17f\n", c);
    printf("c == 0.3 ? %s\n", (c == 0.3) ? "true" : "false");

    double diff = fabs(c - 0.3);
    printf("diff = %.17e\n", diff);

    return 0;
}
```

출력에서는 0.3 과 약간 다른 숫자를 보여줄 것이다.
이는 언어 문제가 아니라 **부동소수점 표현의 본질적 특성**이다.

---

## 실무 팁: 유한 정밀도를 의식한 프로그래밍

### 알고리즘 설계 수준

1. **소멸(cancellation)을 피하는 식 변형**
   - 예: \(1 - \cos x\) → \(2 \sin^2(x/2)\)
   - 예: quadratic formula 에서 원래 공식 대신 수치 안정한 형태 사용
2. **scaling 활용**
   - 큰 수/작은 수를 곱하거나 나눌 때 적절히 스케일링해서 오버플로/언더플로 방지
3. **조건수 분석**
   - 선형 시스템, 다항식 근 찾기 등에서 문제 자체가 ill-conditioned 인지 확인
4. **정렬된 합산, Kahan 합산**
   - 큰 값과 작은 값을 따로 더하거나, Kahan summation 으로 합산 정확도 향상

### 구현 수준

1. **타입 선택**
   - float vs double, 혹은 고정소수점(Q 형식) 등 목적에 맞는 표현 선택
2. **테스트 케이스 설계**
   - 극단적인 입력(매우 크거나 작은 값, 서로 매우 가까운 값)으로 알고리즘을 테스트
3. **에러 로깅**
   - NaN, inf 가 발생하면 즉시 탐지할 수 있도록 assertion 또는 로그를 넣는다.
4. **플랫폼 차이 고려**
   - 일부 하드웨어는 `long double` 지원, 일부는 단정도만 빠르게 처리하는 등 특성이 다르다.

---

## 요약 정리

이 글에서는 **“유한 정밀도 수치 영향의 개관, 수의 표현”** 이라는 제목으로:

1. **실수 vs 유한 집합**
   - 컴퓨터가 표현할 수 있는 수는 유한 집합 $$F$$
   - 실제 계산은 항상 $$\mathrm{fl}(\cdot)$$ 로 반올림된 값에서 이루어짐.
2. **수의 표현**
   - 자리값 표기법(진법, base $$\beta$$)
   - 정수 표현: 부호 없는 정수, 2의 보수
   - 고정소수점(Q 형식): 일정한 스케일을 가진 정수로 소수 표현
   - 부동소수점: sign, exponent, significand 로 표현되는 형태
   - IEEE 754 binary32, binary64: 비트 구조, 정규화, 서브노멀, ∞, NaN
3. **유한 정밀도의 핵심 수치 개념**
   - 머신 엡실론, unit roundoff
   - 반올림 모드와 fl 연산 모델
   - 오버플로/언더플로, 서브노멀
4. **대표적인 유한 정밀도 영향**
   - 소멸(cancellation)과 유효숫자 손실
   - 누적 오차, 합산 알고리즘(Kahan)
   - 조건수와 알고리즘 안정성
5. **DSP 및 필터 설계와의 연결**
   - 계수 양자화, 상태 양자화, limit cycle
   - 필터 구조 선택(SOS, Lattice 등)과 유한 정밀도 고려
6. **실무 팁**
   - 안정한 식 변형, scaling, 안전한 비교, 타입 선택, 극단 입력 테스트 등

앞에서 공부한 **계수 양자화, 필터 구조별 특성**은 사실 이 “유한 정밀도 수치”라는 더 큰 틀 안에 들어가는 특수 사례다.
이 글에서 정리한 개념들을 머릿속에 두고 보면, DSP 뿐 아니라 **수치해석, 시뮬레이션, 머신러닝, 금융 계산** 등 다양한 분야에서
“왜 이렇게 이상한 값이 나오는지”를 이해하고, 이를 피하거나 제어하는 감각을 얻을 수 있다.
