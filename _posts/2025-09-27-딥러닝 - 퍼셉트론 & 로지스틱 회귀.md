---
layout: post
title: 딥러닝 - 퍼셉트론 & 로지스틱 회귀
date: 2025-09-27 14:25:23 +0900
category: 딥러닝
---
# 1.3 퍼셉트론 & 로지스틱 회귀
**단층 퍼셉트론(한계와 XOR 문제) · 다층퍼셉트론(MLP) 확장 · 로지스틱 회귀(시그모이드, BCE 손실, 결정경계)**

## A. 퍼셉트론(Perceptron)

### A-1. 모델과 결정경계

- **모델**: 입력 $$\mathbf{x}\in\mathbb{R}^d$$, 가중치 $$\mathbf{w}\in\mathbb{R}^d$$, 바이어스 $$b\in\mathbb{R}$$ 일 때
  $$
  f(\mathbf{x}) = \mathrm{sign}\!\big(\mathbf{w}^\top \mathbf{x} + b\big)\in\{-1, +1\}.
  $$
- **결정경계**:  
  $$
  \mathbf{w}^\top \mathbf{x} + b = 0 \quad\text{(초평면)}
  $$
  이 선(면)을 기준으로 **양의 클래스(+1)** 와 **음의 클래스(-1)** 를 분리한다.

### A-2. 퍼셉트론 학습 규칙(온라인 업데이트)

- 라벨을 $$y\in\{-1,+1\}$$ 로 가정. 한 샘플 $$(\mathbf{x}, y)$$ 가 **오분류**되면:
  $$
  \mathbf{w} \leftarrow \mathbf{w} + \eta\, y\, \mathbf{x},\qquad
  b \leftarrow b + \eta\, y,
  $$
  여기서 $$\eta>0$$ 는 학습률.
- **직관**: **오분류된 방향으로** 초평면을 조금 밀어 클래스 분리 개선.
- **수렴 정리(Perceptron Convergence Theorem)**: 데이터가 **엄밀히 선형 분리 가능**하면 유한 번의 업데이트로 오분류가 사라진다(수렴).  
  선형 분리가 안 되면 수렴 보장 없음(무한 반복 혹은 진동).

### A-3. 퍼셉트론의 한계 — XOR 문제

- **XOR 진실표** (2차원 입력):
  - (0,0)→0, (1,1)→0, (0,1)→1, (1,0)→1 (또는 ±1 표기)
- 어느 직선도 네 점을 **오류 없이 한 번에** 분리할 수 없다 ⇒ **단층 퍼셉트론 불가**.
- 해결: **비선형성**을 추가(은닉층 + 활성함수) → **MLP(다층 퍼셉트론)** 가 필요.

---

## B. 퍼셉트론 — 합성 데이터 실험 (PyTorch, 수동 업데이트)

아래 코드는 **선형 분리 가능**한 2D 데이터를 만들고, **퍼셉트론 규칙**으로 학습이 수렴하는지 확인한다.  
(의도적으로 autograd/optimizer 없이 **수동 업데이트**로 구현한다.)

```python
# 퍼셉트론: 선형 분리 데이터에서 수렴 실험
import torch, math, random
torch.manual_seed(0)

def make_linear_separable(n_per_class=100):
    # 두 개의 2D 가우시안에서 클래스 생성
    mean_pos = torch.tensor([2.0, 2.0])
    mean_neg = torch.tensor([-2.0, -2.0])
    cov = torch.eye(2) * 0.6
    X_pos = torch.distributions.MultivariateNormal(mean_pos, cov).sample((n_per_class,))
    X_neg = torch.distributions.MultivariateNormal(mean_neg, cov).sample((n_per_class,))
    X = torch.cat([X_pos, X_neg], dim=0)
    y = torch.cat([torch.ones(n_per_class), -torch.ones(n_per_class)])  # +1, -1
    # 셔플
    idx = torch.randperm(len(X))
    return X[idx], y[idx]

X, y = make_linear_separable(200)

# 가중치/바이어스 초기화
w = torch.zeros(2)  # d=2
b = torch.zeros(1)
eta = 0.1

def predict(x):
    return torch.sign(x @ w + b)  # -1, +1

def accuracy(X, y):
    with torch.no_grad():
        pred = torch.sign(X @ w + b)
        return (pred == y).float().mean().item()

max_epochs = 50
for epoch in range(1, max_epochs+1):
    errors = 0
    for i in range(len(X)):
        xi, yi = X[i], y[i]
        if (xi @ w + b) * yi <= 0:  # 오분류 또는 경계 위
            w += eta * yi * xi
            b += eta * yi
            errors += 1
    acc = accuracy(X, y)
    print(f"epoch {epoch:02d} | mistakes={errors:3d} | train-acc={acc:.3f}")
    if errors == 0:
        print("Perceptron converged.")
        break

print("Final w,b:", w, b)
```

> 관찰 포인트  
> - `mistakes`가 점차 줄고, 선형 분리 데이터에서는 **0**이 되어 수렴하는 것을 기대.  
> - 학습률 `eta`가 너무 크면 진동, 너무 작으면 느린 수렴.

---

## C. XOR — 퍼셉트론 실패 vs MLP 성공

### C-1. XOR 데이터에서 퍼셉트론/로지스틱 실패 확인

```python
# XOR 데이터 생성: (0,0)->0, (1,1)->0, (0,1)->1, (1,0)->1
import torch
torch.manual_seed(1)

X_xor = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])
y_xor_pm = torch.tensor([-1., +1., +1., -1.])  # 퍼셉트론 표기(±1)
y_xor01 = torch.tensor([0., 1., 1., 0.])       # 로지스틱 표기(0/1)

# 퍼셉트론 수동 학습 (작은 에폭 반복)
w = torch.zeros(2); b = torch.zeros(1)
eta = 0.2
for epoch in range(30):
    mistakes = 0
    for i in range(len(X_xor)):
        xi, yi = X_xor[i], y_xor_pm[i]
        if (xi @ w + b) * yi <= 0:
            w += eta * yi * xi
            b += eta * yi
            mistakes += 1
    if mistakes == 0:
        break

with torch.no_grad():
    pred = torch.sign(X_xor @ w + b)
    acc = (pred == y_xor_pm).float().mean().item()

print("Perceptron on XOR — acc:", acc, "w,b=", w.tolist(), b.item())
# 대체로 완전 분리는 실패(우연히 맞는 경우도 있지만 일반적으론 불가).
```

단층 선형 경계는 XOR을 올바르게 **한 번에** 분리할 수 없다.

### C-2. MLP(은닉층 + 비선형)로 XOR 해결

```python
import torch.nn as nn
import torch.nn.functional as F

class MLP_XOR(nn.Module):
    def __init__(self, hidden=4):
        super().__init__()
        self.fc1 = nn.Linear(2, hidden)
        self.fc2 = nn.Linear(hidden, 1)  # binary
    def forward(self, x):
        h = torch.tanh(self.fc1(x))      # 비선형(예: tanh/ReLU/GELU)
        z = self.fc2(h).squeeze(1)       # 로짓
        return z

torch.manual_seed(1)
model = MLP_XOR(hidden=4)
opt = torch.optim.AdamW(model.parameters(), lr=0.05, weight_decay=1e-4)
bce = nn.BCEWithLogitsLoss()

for epoch in range(2000):
    z = model(X_xor)
    loss = bce(z, y_xor01)
    opt.zero_grad(); loss.backward(); opt.step()

with torch.no_grad():
    prob = torch.sigmoid(model(X_xor))
    pred = (prob > 0.5).float()
    acc = (pred == y_xor01).float().mean().item()
print("MLP on XOR — acc:", acc)
```

> 핵심: **은닉층 + 비선형 활성**으로 **결정영역을 조합**해 XOR 같은 **비선형 분리**를 학습한다.

---

## D. 로지스틱 회귀(Logistic Regression)

### D-1. 모델과 시그모이드

- **로짓(logit)** $$z = \mathbf{w}^\top \mathbf{x} + b$$  
- **시그모이드(로지스틱 함수)**  
  $$
  \sigma(z) = \frac{1}{1+e^{-z}} \in (0,1)
  $$
- **해석**: $$p(y=1\mid \mathbf{x}) \approx \sigma(z)$$ 로 **확률**을 직접 모델링.

### D-2. 우도 최대화(MLE)와 BCE 손실

- **이진 라벨** $$y\in\{0,1\}$$, 예측확률 $$\hat p = \sigma(z)$$ 일 때, **로그우도(한 샘플)**:
  $$
  \log \mathcal{L} = y\log\hat p + (1-y)\log(1-\hat p).
  $$
- **음의 로그우도(=손실)** 를 평균하면 **Binary Cross-Entropy**:
  $$
  \ell(\hat p, y) = -\Big(y\log\hat p + (1-y)\log(1-\hat p)\Big).
  $$
- **BCEWithLogits의 안정형** (수치 안정성 개선):
  $$
  \ell(z,y) = \max(z,0) - z\,y + \log\!\big(1+e^{-|z|}\big).
  $$

### D-3. 그래디언트(경사)

- $$\hat p=\sigma(z),\ z=\mathbf{w}^\top\mathbf{x}+b$$ 에 대해
  $$
  \frac{\partial \ell}{\partial \mathbf{w}} = (\hat p - y)\,\mathbf{x},
  \qquad
  \frac{\partial \ell}{\partial b} = \hat p - y.
  $$
- **해석**: 예측확률과 정답의 차이(오차)가 입력 방향으로 **미는 힘**이 된다.

### D-4. 결정경계

- $$\hat p=0.5 \iff z=0 \iff \mathbf{w}^\top \mathbf{x} + b=0$$  
- 퍼셉트론과 마찬가지로 **선형 초평면**이지만, 퍼셉트론의 0/1 경계와 달리 **확률적**인 스무딩을 제공.

### D-5. 정규화(가중치 감쇠)와 클래스 불균형

- **L2 정규화**(weight decay): 
  $$
  \lambda\|\mathbf{w}\|_2^2
  $$
  를 손실에 추가하여 과적합을 억제(AdamW의 `weight_decay` 권장).
- **클래스 불균형**:  
  - 손실 가중치(양/음 클래스 가중)  
  - 임곗값 조정(0.5가 최적이 아닐 수 있음)  
  - PR-AUC, F1 등 적절 지표 병행.

---

## E. 로지스틱 회귀 — 합성 데이터 실험 (PyTorch)

**선형 분리 가능/불가능** 데이터를 각각 만들어 **로지스틱**과 **퍼셉트론**을 비교한다.

```python
import torch, math
import torch.nn as nn
torch.manual_seed(42)

def make_blobs(n=400, sep=2.0, std=1.0):
    # sep↑ -> 선형 분리 쉬움
    mean_pos = torch.tensor([sep, sep])
    mean_neg = torch.tensor([-sep, -sep])
    cov = torch.eye(2) * std
    X_pos = torch.distributions.MultivariateNormal(mean_pos, cov).sample((n//2,))
    X_neg = torch.distributions.MultivariateNormal(mean_neg, cov).sample((n//2,))
    X = torch.cat([X_pos, X_neg], dim=0)
    y = torch.cat([torch.ones(n//2), torch.zeros(n//2)])
    idx = torch.randperm(n)
    return X[idx], y[idx]

X, y = make_blobs(n=600, sep=2.2, std=0.9)
n = len(X)
n_tr = int(n*0.8)
X_tr, y_tr = X[:n_tr], y[:n_tr]
X_te, y_te = X[n_tr:], y[n_tr:]

class LogReg(nn.Module):
    def __init__(self, d):
        super().__init__()
        self.lin = nn.Linear(d, 1)  # 로짓 z
    def forward(self, x):
        return self.lin(x).squeeze(1)  # (B,)

model = LogReg(d=2)
opt = torch.optim.AdamW(model.parameters(), lr=5e-2, weight_decay=1e-3)
bce = nn.BCEWithLogitsLoss()

def acc_from_logits(z, y, thr=0.5):
    with torch.no_grad():
        p = torch.sigmoid(z)
        pred = (p > thr).float()
        return (pred == y).float().mean().item()

for epoch in range(200):
    z = model(X_tr)
    loss = bce(z, y_tr)
    opt.zero_grad(); loss.backward(); opt.step()
    if (epoch+1) % 40 == 0:
        tr_acc = acc_from_logits(z, y_tr)
        te_acc = acc_from_logits(model(X_te), y_te)
        print(f"[{epoch+1:03d}] loss={loss.item():.4f} | tr={tr_acc:.3f} te={te_acc:.3f}")

with torch.no_grad():
    W = model.lin.weight.data.clone().view(-1)
    b = model.lin.bias.data.item()
print("w,b:", W.tolist(), b)
# 결정경계는 W[0]*x1 + W[1]*x2 + b = 0
```

> 관찰 포인트  
> - 선형 분리 가능성이 높을수록 학습이 빠르고 정확도 높음.  
> - 로지스틱은 확률 출력이 있어 **임곗값/교정(calibration)** 응용에 유리.

---

## F. 퍼셉트론 vs 로지스틱 — 개념 비교

| 항목 | 퍼셉트론 | 로지스틱 회귀 |
|---|---|---|
| 출력 | 이산(±1), 부호 | 연속 확률 $$\sigma(z)\in(0,1)$$ |
| 손실 | 오분류시만 업데이트(마진 미고려) | **BCE(연속)**, 모두에 대해 미분 가능한 손실 |
| 해석 | 결정경계만 | **확률/로그우도** 해석 가능 |
| 최적화 | 비볼록(업데이트 규칙 기반) | **볼록(이진)** — 전역 최적 보장(배치 설정) |
| 장점 | 구현 간단, 수렴 정리(선형 분리 가능 시) | 확률 출력, 임곗값 조절, MLE/정규화, 실무 친화 |
| 한계 | 비선형 분리 불가(XOR) | 선형 경계 한계(MLP로 확장 필요) |

- **요약**: 실무에서는 대개 **로지스틱 회귀**를 기본 분류 베이스라인으로 사용.  
  비선형성이 필요하면 **MLP(은닉층+비선형)** 또는 CNN/Transformer 등으로 확장.

---

## G. 다층퍼셉트론(MLP)로의 확장

### G-1. 구조와 표현력

- **한 은닉층 MLP**:
  $$
  f(\mathbf{x}) = \mathbf{W}_2^\top \,\phi(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) + b_2,
  $$
  여기서 $$\phi$$ 는 ReLU/tanh/GELU 등 비선형 활성.
- **보편근사정리**: 충분한 은닉 유닛이 있으면 연속함수 근사 가능(실제 학습/일반화는 또 다른 문제).

### G-2. 코드 — 로지스틱(선형) vs MLP 비교

```python
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, d_in=2, h=32, d_out=1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_in, h),
            nn.ReLU(),
            nn.Linear(h, d_out)
        )
    def forward(self, x):
        return self.net(x).squeeze(1)

# 복잡한(비선형) 데이터로 실험하려면 두 개의 원형/스파이럴 등으로 교차시켜도 됨.
X2, y2 = make_blobs(n=800, sep=0.8, std=1.5)  # 선형 분리 어렵게

n2 = len(X2); n2_tr = int(n2*0.8)
X2_tr, y2_tr = X2[:n2_tr], y2[:n2_tr]
X2_te, y2_te = X2[n2_tr:], y2[n2_tr:]

def train_and_eval(model, Xtr, ytr, Xte, yte, epochs=200, lr=1e-2):
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)
    bce = nn.BCEWithLogitsLoss()
    for ep in range(epochs):
        z = model(Xtr)
        loss = bce(z, ytr)
        opt.zero_grad(); loss.backward(); opt.step()
    with torch.no_grad():
        tr = (torch.sigmoid(model(Xtr))>0.5).float().eq(ytr).float().mean().item()
        te = (torch.sigmoid(model(Xte))>0.5).float().eq(yte).float().mean().item()
    return tr, te

# 로지스틱(선형)
lin = LogReg(d=2)
tr1, te1 = train_and_eval(lin, X2_tr, y2_tr, X2_te, y2_te, epochs=400, lr=5e-2)
# MLP(비선형)
mlp = MLP(d_in=2, h=64, d_out=1)
tr2, te2 = train_and_eval(mlp, X2_tr, y2_tr, X2_te, y2_te, epochs=400, lr=1e-2)

print(f"Linear(LogReg)   -> tr={tr1:.3f} te={te1:.3f}")
print(f"Nonlinear(MLP)   -> tr={tr2:.3f} te={te2:.3f}")
```

> 관찰 포인트  
> - 선형 경계로는 한계가 있는 데이터에서 **MLP가 더 나은 결정영역**을 학습.

---

## H. 결정경계 시각적/수치적 이해

- **결정경계**는 $$\hat p(\mathbf{x})=0.5$$ 즉 $$\mathbf{w}^\top\mathbf{x}+b=0$$(선형),  
  MLP에서는 $$\sigma(f(\mathbf{x}))=0.5$$ 를 이루는 **곡선/비선형 경계**가 된다.
- **수치 확인**: 격자 포인트에 대해 $$p>0.5$$ 여부를 카운트하거나 **혼동행렬**로 오류 유형 확인.

```python
# 2D 격자에서 클래스 비율을 대략 확인(간이 수치 평가)
def grid_ratio(model, lim=3.0, step=0.5):
    xs = torch.arange(-lim, lim+1e-6, step)
    ys = torch.arange(-lim, lim+1e-6, step)
    cnt, pos = 0, 0
    with torch.no_grad():
        for a in xs:
            for b in ys:
                z = model(torch.tensor([[a,b]], dtype=torch.float))
                p = torch.sigmoid(z).item()
                cnt += 1; pos += (p>0.5)
    return pos/cnt

print("Linear grid pos-ratio:", grid_ratio(lin, lim=4.0, step=0.5))
print("MLP    grid pos-ratio:", grid_ratio(mlp, lim=4.0, step=0.5))
```

---

## I. (선택) 다중 클래스 개요 — 소프트맥스 로지스틱

- **Multinomial Logistic(Softmax Regression)**: 레이블 $$y\in\{1,\dots,K\}$$
  $$
  \mathbf{z} = \mathbf{W}^\top \mathbf{x} + \mathbf{b}\in\mathbb{R}^K,\quad
  p(y=k\mid\mathbf{x}) = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}.
  $$
- **손실**: 멀티클래스 CE  
  $$
  \ell(\mathbf{z}, y) = -\log \frac{e^{z_y}}{\sum_{j}e^{z_j}}.
  $$
- **결정경계**: 클래스별 **다중 초평면의 교차**. 비선형성이 없으면 여전히 **선형 분리 한계**.

---

## J. 실전 체크리스트

1. **특성 스케일링**: 선형 모델은 입력 스케일에 민감 → 표준화 권장.  
2. **정규화/가중치 감쇠**: 과적합 억제(특히 고차원 희소 피처).  
3. **클래스 불균형**: 손실 가중/샘플링/임곗값 최적화(F1/PR-AUC 기준).  
4. **확률 보정**: 필요시 Platt scaling/temperature scaling.  
5. **베이스라인**: 로지스틱 회귀를 **첫 베이스라인**으로 두고, 필요하면 **MLP**로 확장.

---

## K. 수식 큐시트(요점 정리)

- **퍼셉트론 업데이트**  
  $$
  \mathbf{w}\leftarrow\mathbf{w}+\eta\, y\,\mathbf{x},\quad b\leftarrow b+\eta\, y \quad(\text{오분류 시})
  $$
- **시그모이드**  
  $$
  \sigma(z)=\frac{1}{1+e^{-z}},\quad \sigma'(z)=\sigma(z)\big(1-\sigma(z)\big)
  $$
- **BCE(이진)**  
  $$
  \ell(\hat p,y)=-\big[y\log\hat p+(1-y)\log(1-\hat p)\big]
  $$
- **BCEWithLogits(안정)**  
  $$
  \ell(z,y)=\max(z,0)-zy+\log\!(1+e^{-|z|})
  $$
- **그래디언트**  
  $$
  \nabla_{\mathbf{w}}\ell=(\hat p-y)\,\mathbf{x},\quad \frac{\partial \ell}{\partial b}=\hat p-y
  $$
- **결정경계(이진)**  
  $$
  \mathbf{w}^\top\mathbf{x}+b=0
  $$

---

## L. 마무리

- **퍼셉트론**은 선형 분리에 한정되지만 단순/직관적이고,  
- **로지스틱 회귀**는 확률적 해석 + 미분가능 손실로 실무 베이스라인에 최적,  
- 비선형 데이터는 **MLP(은닉층+비선형)** 가 해결.