---
layout: post
title: 컴퓨터시스템 - 프로그램 성능의 표현
date: 2025-08-01 15:20:23 +0900
category: 컴퓨터시스템
---
# 프로그램 성능의 표현(Measurement & Metrics)

## 범위와 표기(Notation)

- **환경**: 리눅스/유닉스 계열을 기본 가정(Windows/MSVC도 개념 동일).
- **시간 단위**: s(초)/ms/µs/ns.
- **용량/대역**: B/KB/MB/GB, B/s(초당 바이트).
- **확률/통계**: 평균(mean), 중앙값(median), 표준편차(SD), 신뢰구간(CI), 백분위수(p95/p99/p999).
- **카운터**: cycles, instructions, cache-misses, branch-misses, LLC-load-misses 등.

---

## 핵심 개념 — Latency vs Throughput

- **지연(Latency)**: 하나의 작업이 완료되기까지 걸린 시간. 예) 단일 쿼리의 응답 시간.
- **처리량(Throughput)**: 단위 시간당 처리된 작업 수 또는 바이트 수. 예) RPS, GB/s.

둘은 종종 **트레이드오프** 관계다. 예를 들어 배치(batch) 크기를 키우면 처리량은 늘지만 개별 요청의 지연은 증가한다.

---

## 기본 성능 지표와 단위(필수 세트)

| 범주 | 지표 | 의미/단위 | 비고 |
|---|---|---|---|
| 시간 | **Wall-clock time** | 전체 경과 시간(s) | 사용자 체감과 직결 |
| CPU | **User/System/Total CPU time** | CPU에서 소비한 시간(s) | I/O 대기 제외 |
| 마이크로 | **cycles, instructions** | 하드웨어 카운터 | `perf` 등으로 측정 |
| 파이프 | **CPI/IPC** | $$\mathrm{CPI}=\frac{cycles}{instructions}$$, $$\mathrm{IPC}=\frac{1}{\mathrm{CPI}}$$ | 낮은 CPI/높은 IPC 권장 |
| 연산 | **FLOPS/OPS** | 초당 연산량 | GFLOPS/TFLOPS |
| 메모리 | **Bandwidth** | B/s(GB/s) | STREAM 등 |
| 캐시/분기 | **miss rate** | miss/references | L1/L2/LLC, branch |
| 에너지 | **Joules/Watts** | 총 에너지/전력 | RAPL/PMU |
| 신뢰성 | **p95/p99/p999** | tail latency | SLO에서 중요 |

---

## 근본 공식들(핵심 식 정리)

### CPU 실행시간

$$
\mathrm{CPU\ time}=\mathrm{Instruction\ Count}\times \mathrm{CPI}\times \mathrm{Clock\ Cycle\ Time}
$$
또는 클럭 주파수 \(f\)를 쓰면
$$
\mathrm{CPU\ time}=\frac{\mathrm{Instruction\ Count}\times \mathrm{CPI}}{f}.
$$

### 속도향상(Speedup)

$$
\mathrm{Speedup}=\frac{\mathrm{Old\ Time}}{\mathrm{New\ Time}}.
$$

### Amdahl의 법칙(부분 최적화의 전체 영향)

부분 비율 \(p\), 해당 부분 가속비 \(s\):
$$
\mathrm{Speedup}_{overall}=\frac{1}{(1-p)+\frac{p}{s}}.
$$

### Roofline(메모리 vs 연산 한계)

$$
\mathrm{Perf}=\min\big(\mathrm{Peak\ FLOPS},\ \mathrm{AI}\times \mathrm{Peak\ BW}\big),\quad
\mathrm{AI}=\frac{\mathrm{FLOPs}}{\mathrm{bytes\ moved}}.
$$

### Little의 법칙(간단한 대기행렬)

$$
L=\lambda \times W
$$
- \(L\): 시스템 내 평균 작업 수, \(\lambda\): 처리량(도착률), \(W\): 평균 지연.

---

## 측정 방법론 — 신뢰성 있는 벤치마크 절차

1. **목표 정의**: 응답시간 단축? 처리량 증대? 에너지 절감?
2. **환경 고정**: CPU 주파수 고정(성능 governor), 코어 고정(affinity), 하이퍼스레딩 고려, 터보/온도 영향 최소화.
3. **워밍업**: 캐시/분기 예측/JIT 안정화 후 측정.
4. **샘플 수**: 짧은 작업은 반복 횟수를 늘려 ns-level 노이즈 감소.
5. **통계 보고**: 최소 **median + p95**(또는 95% CI). 평균만 보고하지 않는다.
6. **문서화**: CPU/메모리/OS/컴파일러/플래그/입력/스레드/고정 절차를 기록.
7. **분리 측정**: I/O와 연산(Compute)을 분리해 측정.
8. **회귀 방지**: 동일 환경에서 전/후를 교차(A/B/A/B) 실행하여 드리프트 감시.

---

## 하드웨어 카운터와 도구(빠른 실전)

- **시간**: `clock_gettime(CLOCK_MONOTONIC, ...)`
- **카운터**: Linux `perf stat/record`, Intel VTune, PAPI, pmu-tools.
- **명령 예**
```bash
perf stat -e cycles,instructions,cache-references,cache-misses,branches,branch-misses \
  ./app

perf record -g ./app && perf report  # 샘플링 프로파일
```
- **메모리 대역**: STREAM benchmark 방식(삼중 루프), VTune/Advisor 메모리 분해.
- **에너지**: RAPL(인텔), `perf stat -e power/energy-cores/` 등.

---

## 마이크로벤치 설계 — 흔한 함정과 회피책

- **DCE(죽은 코드 제거) 함정**: 결과를 사용하지 않으면 컴파일러가 제거.
  - 대책: `volatile` 소비, 누적 합을 출력/검증.
- **비현실적 루프**: 단일 커다란 루프는 과도하게 최적화될 수 있음.
  - 대책: 적절한 함수 경계/메모리 접근 패턴 유지.
- **I/O 섞기 금지**: 디스크/네트워크는 별도 측정.
- **온도/전원/스케줄러**: 백그라운드 부하 차단, 고정 코어 핀, 성능 governor.
- **JIT/GC**: 워밍업·GC 모드 명시.
- **RDTSC 오용**: 주파수 변동/코어 마이그레이션/메모리 재정렬 이슈.
  - 대책: `rdtscp`/`lfence` 사용, 고정 코어, Invariant TSC 확인.

---

## C 타이밍 기본기 — `clock_gettime` 해치(Skeleton)

```c
// timing_basic.c
#include <time.h>
#include <stdio.h>

static double now_s(void) {
    struct timespec t;
    clock_gettime(CLOCK_MONOTONIC, &t);
    return t.tv_sec + t.tv_nsec * 1e-9;
}

static void work(size_t n, volatile double *sink) {
    double x = 0;
    for (size_t i = 0; i < n; ++i) x += 1.0 / (i + 1.0);
    *sink = x;
}

int main(void) {
    volatile double sink = 0;
    const int warmup = 3, runs = 11;
    for (int w = 0; w < warmup; ++w) work(10u<<20, &sink);

    double best = 1e9, sum = 0;
    for (int r = 0; r < runs; ++r) {
        double t0 = now_s();
        work(10u<<20, &sink);
        double t1 = now_s();
        double dt = t1 - t0;
        best = dt < best ? dt : best;
        sum += dt;
        printf("run %2d: %.6f s\n", r, dt);
    }
    printf("mean=%.6f s, best=%.6f s, sink=%g\n", sum/runs, best, sink);
    return 0;
}
```

포인트: `volatile` 소비(`sink`)로 DCE 방지, 워밍업 포함, 평균과 best를 함께 관찰.

---

## 코어 고정/NUMA/페이지 영향(실전 팁)

- **코어 고정**: 스레드/프로세스가 다른 코어로 이동하면 TSC/캐시 상태가 섞여 잡음↑.
- **NUMA**: first-touch로 데이터가 접근 스레드의 노드에 할당되도록 초기화.
- **HugePages**: TLB pressure 완화에 도움(워크로드 의존).
- **MLock**: 벤치 전 페이지 인을 고정해 페이지폴트 잡음↓.

```c
// pin_to_cpu.c (리눅스)
#define _GNU_SOURCE
#include <pthread.h>
#include <sched.h>
#include <stdio.h>

void pin_to_cpu(int cpu) {
    cpu_set_t set;
    CPU_ZERO(&set);
    CPU_SET(cpu, &set);
    if (pthread_setaffinity_np(pthread_self(), sizeof(set), &set))
        perror("pthread_setaffinity_np");
}
```

---

## RDTSC(사이클 카운터) 안전 사용(선택)

```c
// rdtsc_safe.c (x86_64)
#include <x86intrin.h>

static inline unsigned long long rdtsc_begin(void) {
    _mm_lfence();
    return __rdtsc();
}
static inline unsigned long long rdtsc_end(void) {
    unsigned int aux;
    unsigned long long t = __rdtscp(&aux);
    _mm_lfence();
    return t;
}
```
- **주의**: Invariant TSC가 아닌 시스템, 주파수 스케일링, 코어 migration 시 해석이 어려워진다. 가능하면 `clock_gettime`과 교차 검증.

---

## 하드웨어 카운터 연동(perf_event_open 간단 스니펫)

```c
// perf_count.c : cycles & instructions 동시 측정(리눅스)
#define _GNU_SOURCE
#include <linux/perf_event.h>
#include <sys/syscall.h>
#include <sys/ioctl.h>
#include <unistd.h>
#include <string.h>
#include <stdio.h>

static int pe_open(struct perf_event_attr *attr, pid_t pid, int cpu, int group_fd, unsigned long flags){
    return syscall(__NR_perf_event_open, attr, pid, cpu, group_fd, flags);
}

int main(void){
    struct perf_event_attr pe;
    memset(&pe, 0, sizeof(pe));
    pe.type = PERF_TYPE_HARDWARE;
    pe.size = sizeof(pe);
    pe.config = PERF_COUNT_HW_CPU_CYCLES;
    pe.disabled = 1;
    pe.inherit = 0;
    pe.exclude_kernel = 0;
    pe.exclude_hv = 1;

    int fd_cycles = pe_open(&pe, 0, -1, -1, 0);

    pe.config = PERF_COUNT_HW_INSTRUCTIONS;
    int fd_insts = pe_open(&pe, 0, -1, fd_cycles, 0); // 그룹

    ioctl(fd_cycles, PERF_EVENT_IOC_RESET, 0);
    ioctl(fd_cycles, PERF_EVENT_IOC_ENABLE, 0);

    // ---- 측정할 작업 ----
    volatile double s = 0;
    for (long i=0;i<10000000;i++) s += 1.0/(i+1.0);
    // ---------------------

    ioctl(fd_cycles, PERF_EVENT_IOC_DISABLE, 0);

    long long cycles=0, insts=0;
    read(fd_cycles, &cycles, sizeof(cycles));
    read(fd_insts, &insts, sizeof(insts));

    printf("cycles=%lld, insts=%lld, CPI=%.3f, IPC=%.3f\n",
           cycles, insts, (double)cycles/(double)insts, (double)insts/(double)cycles);
    close(fd_cycles); close(fd_insts);
    return 0;
}
```

---

## 메모리 대역 벤치(간단 STREAM-류)

```c
// stream_like.c : READ+WRITE 대역 측정
#include <stdlib.h>
#include <stdio.h>
#include <time.h>

static double now_s(){ struct timespec t; clock_gettime(CLOCK_MONOTONIC,&t);
  return t.tv_sec + t.tv_nsec*1e-9; }

int main(){
    const size_t N = 1ULL<<26; // 64M elements ≈ 256 MiB (double)
    double *a = aligned_alloc(64, sizeof(double)*N);
    double *b = aligned_alloc(64, sizeof(double)*N);
    for(size_t i=0;i<N;i++) a[i]=1.0, b[i]=2.0;

    // warmup
    for(size_t i=0;i<N;i++) b[i] += 3.0*a[i];

    double t0 = now_s();
    for(size_t i=0;i<N;i++) b[i] += 3.0*a[i];
    double t1 = now_s();

    double bytes = (sizeof(double)*N/*read a*/ + sizeof(double)*N/*read b*/ + sizeof(double)*N/*write b*/);
    double bw = bytes/(t1-t0)/1e9; // GB/s (decimal)
    printf("time=%.3f s, BW=%.1f GB/s\n", t1-t0, bw);

    free(a); free(b);
    return 0;
}
```
해석: **메모리 바운드**라면 CPI 상승·LLC miss 상승과 함께 BW가 시스템 피크에 근접한다.

---

## 프로파일 해석 — 카운터에서 원인으로

- **CPI 높음(IPC 낮음)**: 파이프라인 스톨. 캐시 미스(LLC-load-misses), 분기 미스(branch-misses), 포트 충돌, 메모리 의존 대기 점검.
- **cache-misses↑**: 데이터 레이아웃/접근 순서 변경(타일링, SoA), 프리패치 도입 고려.
- **branch-misses↑**: 분기 예측 실패. 브랜치리스(마스킹)/조건 재배치/데이터 정렬.
- **instructions↑**: 알고리즘 개선 여지(O(n²)→O(n log n)) 또는 인라인/언롤 과다.
- **BW는 높은데 FLOPS 낮음**: 명백한 메모리 바운드(Roofline 확인).
- **CPU 사용률 낮은데 Latency 높음**: I/O, 락 경합, 스레드 동기화 병목일 가능성.

---

## Roofline으로 빠르게 진단(예: SAXPY)

SAXPY \(y\leftarrow a x + y\) 의 AI:

- 한 반복 FLOPs ≈ 2(곱+덧셈), 이동 바이트 ≈ 12B(float 읽기 x 2 + 쓰기 1)
- $$\mathrm{AI}\approx \frac{2}{12} \approx 0.167\ \mathrm{FLOPs/B}.$$
피크 BW=100 GB/s라면 **메모리 상한 성능**은
$$
\mathrm{Perf}_{mem} = 0.167 \times 100 = 16.7\ \mathrm{GFLOPS}.
$$
피크 연산이 2000 GFLOPS라도, 이 커널은 **메모리 바운드**다.

---

## 통계적 표현과 비교법

- **중앙값(median)**: outlier에 강함(비대칭 분포에 적합).
- **기하평균(geomean)**: 속도비의 평균에 적합(여러 벤치 종합).
- **신뢰구간(CI)**: 샘플 분포에서 95% CI를 제시(부트스트랩 추천).
- **p95/p99/p999**: tail latency를 SLO와 직접 연결.

### 간단 백분위수 계산 C 코드(정렬 기반)

```c
// percentile.c
#include <stdlib.h>
#include <stdio.h>

static int cmpd(const void* a, const void* b){
    double x=*(const double*)a, y=*(const double*)b;
    return (x<y)?-1:(x>y);
}
double percentile(double* a, int n, double p){ // 0<p<=100
    qsort(a,n,sizeof(double),cmpd);
    double r = (p/100.0)*(n-1);
    int lo = (int)r, hi = lo+1;
    if (hi>=n) return a[lo];
    double frac = r - lo;
    return a[lo]*(1.0-frac) + a[hi]*frac;
}
```

---

## 보고서/README에 반드시 포함할 것(재현성 체크리스트)

- 하드웨어: CPU(모델/코어/캐시), 메모리(크기/속도), 스토리지.
- OS/커널/스케줄러 설정, 전원 governor, 하이퍼스레딩 여부.
- 컴파일러/링커 버전, 플래그(`-O3 -march=native -flto` 등), 라이브러리 버전.
- 입력 데이터셋/크기/특성, 스레드/프로세스 수, 바인딩 방법.
- 측정 방식: 워밍업 횟수, 반복 횟수, 카운터 세트, 스크립트/커밋 해시.
- 통계: 평균/중앙값/표준편차/신뢰구간 + raw 데이터/히스토그램(권장).

---

## Tail Latency와 SLO(서비스 지표)

- 서비스/DB/네트워크에서는 **p99**가 사용자 체감에 결정적. 평균이 좋아도 tail이 나쁘면 불만족.
- **장시간 GC/stop-the-world, 포그라운드/백그라운드 간섭, 부하 스파이크**가 tail을 악화.
- 히스토그램은 **로그 스케일**로 그리고, 버킷 폭은 **HDR Histogram** 같은 기법을 활용하면 좋다.

---

## 에너지/전력 지표(EDP/Energy/Power)

- **에너지(J)**: 작업 완료까지 소모된 총 에너지.
- **전력(W)**: 단위 시간당 에너지.
- **EDP**(Energy-Delay Product): $$\mathrm{EDP}=E\times T$$ (낮을수록 좋음) — 성능과 에너지의 균형 지표.

간단 RAPL(권장: `perf stat -e power/energy-cores/ ...`)로 수집 후 EDP를 계산해 보고서에 포함.

---

## 케이스 스터디(요약) — 메모리 참조 최적화 전/후

### 코드

```c
// before: 불필요 재로드
int f(int *p){
    int s = *p;
    *p = s + 1;
    return *p; // 재로드
}
// after: 레지스터 재사용
int f_opt(int *p){
    int s = *p;
    *p = ++s;
    return s; // 불필요 load 제거
}
```

### 해석

- `perf stat`에서 **loads** 감소, **instructions**·**cycles** 모두 감소, **CPI**가 소폭 개선.
- 마이크로하게는 수% 이득이라도, 핫 루프에 다수 존재하면 **누적 이득**이 큼.

---

## 대기행렬 관점에서의 부하-지연 곡선

단순 M/M/1 가정에서 도착률 \(\lambda\), 서비스율 \(\mu\):
$$
W=\frac{1}{\mu-\lambda},\quad \lambda\to\mu \Rightarrow W\to\infty.
$$
**시사점**: 처리량에 가까워질수록(고부하) **지연 폭증**. 스로틀링/큐 제한/배치-지연 절충을 설계에 반영.

---

## 컨테이너/가상화/노이즈

- **cgroups** 제한(CPU/mem/io), noisy neighbor, 스케줄러 간섭으로 변동성↑.
- 컨테이너 측정 시 **호스트 수준 카운터**와 병행, 코어 핀/리소스 예약으로 노이즈 감소.
- 클라우드 인스턴스는 **코어/메모리 대역/스토리지 스펙** 변동에 민감 → 동일 타입/영역 고정.

---

## 흔한 오해/실수 정리(요약)

- 평균만 보고 결론 내림 → **tail**을 놓침.
- 단일 수치만 보고 원인 추정 → 카운터/프로파일로 **원인-증거** 매핑 필요.
- 마이크로벤치가 빠르니 전체도 빠르다? → 호출 빈도/데이터 크기/메모리 패턴이 다르면 **일반화 불가**.
- `rdtsc`만 믿음 → 주파수/마이그레이션 이슈.
- LTO/PGO 없이 소스 수정보다 컴파일러 탓 → **빌드 플래그/프로파일**부터.

---

## 실전 리포트 템플릿(복붙용)

1. **목표**: 예) 이미지 파이프라인 p95 20% 단축.
2. **환경**: CPU/메모리/OS/컴파일러/플래그/고정 절차.
3. **워크로드**: 입력 데이터셋/크기/스레드/배치.
4. **측정법**: 워밍업/반복/카운터 목록.
5. **결과(전/후)**:
   - Wall time: \(T_{old}\to T_{new}\) (**Δ%**)
   - CPU: user/sys, cycles/instructions, **CPI/IPC**
   - 메모리/분기: L1/L2/LLC miss, branch-miss
   - BW/FLOPS(해당 시), 에너지/EDP(선택)
   - p95/p99 tail
6. **해석(원인-증거)**: 예) 인라인+LICM으로 inst↓ 12%, LLC miss↓ 18%, CPI 0.92→0.78.
7. **교훈/다음 단계**: 병렬화/타일링/데이터 레이아웃.

---

## 부록 — 여러 번 실행하고 중앙값/표준편차/p95 출력(C)

```c
// bench_stats.c
#include <time.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>

static double now_s(){ struct timespec t; clock_gettime(CLOCK_MONOTONIC,&t);
  return t.tv_sec + t.tv_nsec*1e-9; }

static int cmpd(const void* a,const void* b){
  double x=*(const double*)a, y=*(const double*)b;
  return (x<y)?-1:(x>y);
}

static void workload(int n, volatile double* sink){
  double x=0;
  for(int i=0;i<n;i++) x += 1.0/(i+1.0);
  *sink = x;
}

static double percentile(double* a,int n,double p){
  qsort(a,n,sizeof(double),cmpd);
  double r=(p/100.0)*(n-1);
  int lo=(int)r, hi=lo+1;
  if(hi>=n) return a[lo];
  double frac=r-lo;
  return a[lo]*(1.0-frac)+a[hi]*frac;
}

int main(int argc,char**argv){
  const int runs = 21, warm = 3, N=15*1000*1000;
  double *ts = malloc(sizeof(double)*runs);
  volatile double sink=0;

  for(int w=0;w<warm;w++) workload(N,&sink);
  for(int r=0;r<runs;r++){
    double t0=now_s();
    workload(N,&sink);
    double t1=now_s();
    ts[r]=t1-t0;
  }

  double sum=0, min=ts[0], max=ts[0];
  for(int i=0;i<runs;i++){ sum+=ts[i]; if(ts[i]<min)min=ts[i]; if(ts[i]>max)max=ts[i]; }
  double mean=sum/runs;

  double p50=percentile(ts,runs,50.0);
  double p95=percentile(ts,runs,95.0);

  double var=0;
  for(int i=0;i<runs;i++) var+=(ts[i]-mean)*(ts[i]-mean);
  double sd=sqrt(var/(runs-1));
  printf("runs=%d mean=%.6f median=%.6f p95=%.6f sd=%.6f min=%.6f max=%.6f sink=%g\n",
        runs,mean,p50,p95,sd,min,max,(double)sink);
  free(ts);
  return 0;
}
```

---

## 최종 체크리스트(요약)

- [ ] **목표/지표**를 우선 정의(응답시간? 처리량? 에너지?).
- [ ] **환경 고정**: 주파수/코어/NUMA/컨테이너/노이즈 관리.
- [ ] **워밍업 후** 여러 번 실행, **median + p95** 보고.
- [ ] **카운터 기반 해석**: CPI/IPC, miss, branch, BW로 원인 파악.
- [ ] **Roofline**으로 메모리 vs 연산 한계 진단.
- [ ] **보고서 템플릿**으로 재현성 확보(환경/입력/스크립트/플래그 포함).
- [ ] **전/후 비교는 A/B/A/B** 교차 실험으로 드리프트 최소화.
- [ ] **전체 시스템 시각**: 마이크로벤치 ↔ 실제 워크로드 상관 검증.

---

### 마무리

성능을 “숫자”로 말하려면 **정의→측정→표현→해석**의 사슬이 필요하다.
이 글의 절차와 템플릿을 그대로 적용하면, 누구라도 **재현 가능하고 설득력 있는 성능 보고**를 만들 수 있다. 그 위에 알고리즘 개선·데이터 레이아웃·컴파일러/아키텍처 친화 최적화를 쌓아라 — 숫자는 솔직하다.
