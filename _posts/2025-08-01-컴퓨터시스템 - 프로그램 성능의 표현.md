---
layout: post
title: 컴퓨터시스템 - 프로그램 성능의 표현
date: 2025-08-01 15:20:23 +0900
category: 컴퓨터시스템
---
# 프로그램 성능의 표현(Measurement & Metrics) — 자세한 정리

프로그램 성능을 **정확하고 재현 가능하게 표현**하는 것은 성능 분석·튜닝의 출발점이다. 이 문서는 성능을 *무엇으로*, *어떻게* 측정하고 *어떤 지표*로 표현할지, 그리고 그 지표들이 **무엇을 의미하는지/어떻게 해석할지**를 실무 관점에서 자세하게 정리한다. 수학식은 MathJax로, 코드 예시는 C로 적었다.

---

## 1. 핵심 개념 — latency vs throughput

- **지연(Latency)**: 단일 작업(또는 트랜잭션)이 완료되는 데 걸리는 시간. 예: 한 쿼리 응답 시간.  
- **처리량(Throughput)**: 단위 시간당 처리된 작업의 수(또는 바이트). 예: 초당 요청 수, GB/s.

두 개념은 서로 트레이드오프가 될 수 있다. 예를 들어, 배치 처리로 처리량을 높이면 개별 작업의 지연이 늘어날 수 있다.

---

## 2. 기본 성능 지표와 단위

- **실행 시간 (Wall-clock time)**: 벽시계 시간(초, ms). 사용자 관점의 전체 시간.  
- **CPU 시간**: 프로세스가 실제로 CPU에서 실행된 시간(유저/시스템).  
- **클럭 사이클 (cycles)**: CPU 클럭 틱 수. 하드웨어 성능지표로 유용.  
- **명령어 수 (instructions)**: 실행된 명령의 수 (hardware or simulated).  
- **CPI (Cycles Per Instruction)**:
  \[
  \text{CPI} = \frac{\text{cycles}}{\text{instructions}}
  \]
- **IPC (Instructions Per Cycle)**:
  \[
  \text{IPC} = \frac{\text{instructions}}{\text{cycles}} = \frac{1}{\text{CPI}}
  \]
- **FLOPS / OPS**: 부동소수점 연산 또는 일반 연산의 초당 수행량 (GFLOPS 등).  
- **메모리 대역폭**: 바이트/초 (GB/s).  
- **캐시 미스율 / 분기 미스율**: 특정 이벤트의 비율(예: cache-misses / cache-references).  
- **에너지/전력**: Joules, Watts — 성능/전력 트레이드오프에서 중요.

---

## 3. 근본 공식들

### 3.1 CPU 실행시간
프로그램의 CPU 시간은 다음과 같이 표현된다:
\[
\text{CPU time} = \text{Instruction Count} \times \text{CPI} \times \text{Clock Cycle Time}
\]
또는 클럭 주파수 \(f\) 사용 시:
\[
\text{CPU time} = \frac{\text{Instruction Count} \times \text{CPI}}{f}
\]

### 3.2 속도향상 (Speedup)
어떤 최적화의 속도 향상은 다음으로 정의:
\[
\text{Speedup} = \frac{\text{Old Time}}{\text{New Time}}
\]

### 3.3 Amdahl의 법칙 (전체적 영향)
병렬화 또는 특정 최적화가 적용되는 비율 \(p\)와 그 부분의 가속비 \(s\)가 있을 때 전체 속도향상:
\[
\text{Speedup}_\text{overall} = \frac{1}{(1-p) + \frac{p}{s}}
\]

### 3.4 Roofline 모델 (메모리 vs 연산 한계)
성능 상한은 연산 집약도(Arithmetic Intensity)와 메모리 대역폭/피크 FLoPS에 의해 결정:
\[
\text{Perf} = \min\big(\text{Peak FLOPS},\; \text{AI} \times \text{Peak BW}\big)
\]
여기서 \(\text{AI} = \dfrac{\text{FLOPs}}{\text{bytes\_moved}}\).

### 3.5 대기행렬 이론(간단)
서버링/처리량 모델링에 Little의 법칙:
\[
L = \lambda \times W
\]
- \(L\): 시스템 내 평균 작업 수  
- \(\lambda\): 도착률(throughput)  
- \(W\): 평균 대기시간(latency)

---

## 4. 측정 방법론 — 신뢰성 있는 벤치마크를 위한 절차

1. **목표 정의**: 무엇을 개선/비교할지(예: 응답 시간 vs 처리량) 명확히.  
2. **재현성 환경 고정**: CPU 빈주파수 고정, 온도/전원 안정, hyperthreading/affinity 제어 등 가능한 한 환경을 고정.  
3. **샘플 수 확보**: 여러 번 실행하여 분포(평균, 중앙값, 표준편차)를 구함. 짧은 작업은 반복 루프로 충분히 늘린다.  
4. **워밍업(warm-up)**: 캐시/분기 예측/동적 런타임(JIT)이 안정화되도록 몇 번 실행한 뒤 측정.  
5. **통계 보고**: 최소 중앙값(median)과 95% 신뢰구간 또는 표준편차를 함께 보고.  
6. **결과 문서화**: 하드웨어(모델, 클럭), OS, 컴파일러(버전, 플래그), 입력, 스레드 수 등 환경 정보 포함.

---

## 5. 실무에서 자주 쓰는 측정 기법과 도구

- **시간 측정 (코드 내부)**  
  - `clock_gettime(CLOCK_MONOTONIC, ...)` — 정밀한 실수 벽시계 시간 측정.  
  - `rdtsc` (x86) — 사이클 카운터(주의: CPU 주파수 변동/코어 migration 문제).  

```c
// clock_gettime 예시
#include <time.h>
#include <stdio.h>

double now_sec() {
    struct timespec t;
    clock_gettime(CLOCK_MONOTONIC, &t);
    return t.tv_sec + t.tv_nsec * 1e-9;
}

int main() {
    double t0 = now_sec();
    // 측정할 작업
    for (volatile long i=0;i<100000000;i++);
    double t1 = now_sec();
    printf("Elapsed: %.6f s\n", t1 - t0);
    return 0;
}
```

- **하드웨어 성능 카운터**: `perf`(Linux), `pcm`(Intel PCM), PAPI 등으로 cycles, instructions, cache-misses, branch-misses를 측정.  
  예: (Linux)  
  ```bash
  perf stat -e cycles,instructions,cache-references,cache-misses,branch-misses ./myprog
  ```

- **프로파일러**:  
  - 샘플링: `perf record` + `perf report`, `Linux perf` → 낮은 오버헤드로 hotspot 파악  
  - 계측(Instrumentation): `gprof`, Intel VTune, `callgrind`(Valgrind) → 호출 그래프  
  - 플레임 그래프(Flame graph): 시각적으로 call-stack 기반 hot-path 확인

- **메모리/캐시 분석**: Valgrind Cachegrind, Intel VTune/Advisor(메모리 분석), stream benchmark(메모리 BW).

---

## 6. 마이크로벤치마크 설계 원칙 (피해야 할 함정들)

- **컴파일러 최적화 회피**: 측정 코드가 무의미한 결과로 제거되지 않도록(합계 검사나 `volatile` 사용).  
- **과도한 단일 반복**: 단일 루프 내에서 비현실적 최적화가 적용될 수 있음.  
- **I/O 포함 금지**: 디스크/네트워크 I/O는 별도로 측정하라.  
- **온도/전원/스케줄링 영향**: 시스템 부하(백그라운드), 스레드 스케줄러가 측정값을 훼손함.  
- **JIT/GC 영향**: 언어 런타임(JVM) 등은 워밍업/GC 패턴을 고려해야 함.

---

## 7. 프로파일 해석 — 카운터가 말해주는 것

- **높은 CPI / 낮은 IPC**: 파이프라인이 멈춘 상태. 원인 후보: 캐시 미스, 분기 미스, 스톨(데이터 의존).  
- **많은 캐시 미스(cache-misses)**: 메모리 대역폭/레이턴시 한계. 데이터 레이아웃 변경(블록 처리), prefetch, 알고리즘 개선 필요.  
- **많은 분기 미스(branch-misses)**: 분기 예측 실패 → 분기 제거/분기 재구성(조건 정렬), predication 고려.  
- **높은 instruction count**: 알고리즘적 비효율(예: O(n²) vs O(n log n)) 가능성.  
- **낮은 FLOPS, 높은 메모리 BW 사용**: 메모리 바운드 (roofline로 판단).  
- **CPU utilization low but latency high**: I/O나 동기화(lock) 때문일 가능성.

---

## 8. 모델 기반 표현 — Roofline 예시

Roofline 모델을 이용해 어느 영역(메모리 바운드 vs 컴퓨트 바운드)인지 판단:

- 계산:
  - 측정된 실행시간 또는 FLOPs와 bytes moved를 얻어 AI를 구함: \(\text{AI}=\dfrac{\text{FLOPs}}{\text{Bytes}}\).
  - 모델:
    \[
    \text{Attainable Perf} = \min(\text{PeakFLOPS},\; \text{AI} \times \text{PeakBW})
    \]
- 해석:
  - 만약 \(\text{AI} \times \text{PeakBW} < \text{PeakFLOPS}\) 이면 **메모리 바운드**.  
  - 아니면 **컴퓨트 바운드**.

---

## 9. 통계적 표현과 비교법

- **중앙값(median)**이 이상값(outlier)에 덜 민감해서 짧은 작업 반복 측정에 자주 사용됨.  
- **기하평균(geometric mean)**: 여러 벤치마크의 속도비(비율)를 평균낼 때 사용(특히 상대 성능 비교).  
- **신뢰구간**: 여러 실행의 표본으로 95% 신뢰구간 계산해 함께 보고.  
- **표준오차 (SE)**: 측정의 불확실성을 정량화.

---

## 10. 보고에 포함해야 할 항목(재현성 체크리스트)

- 하드웨어: CPU 모델, 코어 수, L1/L2/L3 캐시 크기, 메모리 용량/주파수.  
- OS/커널 버전, 스케줄러 설정.  
- 컴파일러/빌드: 컴파일러 버전, 최적화 플래그(예: `-O3 -march=native`), 링크 옵션.  
- 런타임: 입력 데이터셋(크기/특성), 스레드 수, 환경 변수, CPU governor(성능/ondemand).  
- 측정 방법: 워밍업 횟수, 반복 수, 사용한 카운터, 측정 스크립트(첨부).  
- 통계: 평균/중앙값/표준편차/신뢰구간, raw data 제공 권장.

---

## 11. 예시: 여러 번 실행하고 중앙값/표준편차 출력하는 C 측정 해치

```c
// simple_bench.c
#include <time.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>

double now_sec() {
    struct timespec t;
    clock_gettime(CLOCK_MONOTONIC, &t);
    return t.tv_sec + t.tv_nsec*1e-9;
}

void workload(void) {
    volatile double x = 0;
    for (long i = 0; i < 1000000; ++i) x += sin(i);
    (void)x;
}

int cmpd(const void *a, const void *b) {
    double da = *(double*)a, db = *(double*)b;
    if (da < db) return -1;
    if (da > db) return 1;
    return 0;
}

int main(int argc, char **argv) {
    int runs = 11;
    double *times = malloc(sizeof(double)*runs);
    for (int i=0;i<runs;++i) {
        // warmup some times first
        for (int w=0; w<3; ++w) workload();
        double t0 = now_sec();
        workload();
        double t1 = now_sec();
        times[i] = t1 - t0;
    }
    qsort(times, runs, sizeof(double), cmpd);
    double sum = 0;
    for (int i=0;i<runs;++i) sum += times[i];
    double mean = sum / runs;
    double median = times[runs/2];
    double var = 0;
    for (int i=0;i<runs;++i) var += (times[i]-mean)*(times[i]-mean);
    double sd = sqrt(var / (runs-1));
    printf("runs=%d mean=%.6f median=%.6f sd=%.6f\n", runs, mean, median, sd);
    free(times);
    return 0;
}
```

- 위 방식으로 `median`과 `sd`를 함께 보고하면 안정적인 표현이 된다.

---

## 12. 성능 표현의 한계와 주의사항 요약

- **숫자는 맥락(context)을 요구**: 단일 숫자(예: 10ms)는 입력/환경에 따라 전혀 다르게 해석될 수 있다.  
- **마이크로벤치마크는 일반화 위험**: 특정 루틴이 빠르다고 전체앱이 빨라지지는 않는다.  
- **하드웨어/OS/컴파일러 변화에 민감**: 재현성 확보가 중요.  
- **통계적 불확실성**: 단일 실행값만 보고 결론 내리지 말 것.  
- **언어/플랫폼 특성**: JIT/GC/메모리 모델 등 런타임 영향 고려.

---

### 마무리 팁 (실무 요약)
- 목표(무엇을 최적화할지)를 먼저 정하라.  
- **프로파일(Hotspot) 기반**으로 시간을 분배하라 — "80/20 법칙" 적용.  
- 측정은 **여러 번**, 환경 고정, 결과의 **중앙값+신뢰구간** 보고.  
- 하드웨어 카운터로 **원인(메모리/분기/CPU)** 을 파악하고, 먼저 **알고리즘 개선**을 고려하라.  
- 결과 문서화(빌드/환경/입력)로 재현성을 보장하라.
