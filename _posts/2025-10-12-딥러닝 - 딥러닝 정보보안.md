---
layout: post
title: 딥러닝 - 딥러닝 정보보안
date: 2025-10-12 17:25:23 +0900
category: 딥러닝
---
# 딥러닝 정보보안 총정리 — 위협모델 → 데이터/모델/파이프라인/서빙 보안 → PyTorch 실전 코드

> 목적
> - **딥러닝 전 주기(수집→학습→검증→배포→운영)**에서 발생하는 보안 위협을 **체계적으로 정리**하고,
> - **PyTorch 기준** 코드 예제로 **실전 방어 기법**(적대적 예제·중독/백도어·프라이버시 공격·DP-SGD·강건학습·서빙 보안 등)을 보여준다.
> - “억지로 길게”가 아니라, **놓치면 뼈아픈 포인트**를 빠짐없이 담는다.

---

## 큰 그림: “AI 보안” 위협 지형(Threat Landscape)

- **데이터 계층**
  - **데이터 유출/무단 접근**(PII, 기업 비밀), **데이터 중독(Poisoning)**, **라벨 오염**, **백도어 삽입(Trigger)**
- **모델 계층**
  - **모델 도난/추출(Model Stealing)**, **멤버십/속성/모델 반전 공격(Privacy)**, **적대적 회피(Evasion)**, **백도어 발현**
- **파이프라인/공급망**
  - **의존성/가중치 공급망 공격**(악성 `pickle`, 조작된 체크포인트), **CI/CD 러너 탈취**, **실험 추적 서버 노출**
- **서빙/운영**
  - **API 남용/과금 DoS**, **LLM/RAG 프롬프트 인젝션·리트리벌 중독**, **교차 테넌트 데이터 누출**, **모델 탈옥(Jailbreak)**

**Zero-Trust 원칙**: “데이터·모델·코드·요청 **모두 불신** → 신원·무결성·맥락으로 검증 → 최소권한 부여”.

---

## 데이터 보안: 수집·정제·버저닝 단계

### 민감정보(PII) 처리

- **수집 최소화**·**가명화/익명화**·**마스킹**
- 원본 저장소는 **암호화(At-Rest)** + **열람 워크플로(결재·감사)**
- 학습 전 **PII 스캐너**(정규식/룰·간단 NER)로 차단 리스트 유지

### 데이터 무결성/공급망

- 데이터셋/라벨의 **해시·서명** 추적, **신뢰 가능한 소스**만 사용
- 외부 가중치(체크포인트)는 **해시 검증 + 격리 환경에서 로딩**
- **Pickle 경계**: 임의 코드 실행 위험 → `torch.load(..., weights_only=True)` 사용 등

```python
# (PyTorch) 안전한 체크포인트 로딩 예시

import torch, os

def safe_load_model(model, path, map_location="cpu"):
    assert os.path.exists(path), "missing checkpoint"
    # PyTorch 2.0+: weights_only=True로 pickle 실행 경로를 피한다
    state = torch.load(path, map_location=map_location, weights_only=True)
    model.load_state_dict(state)
    return model
```

### 데이터 중독/백도어 대비

- **샘플 단위 이상치 탐지**(특이값·임베딩 밀도), **클래스별 통계 편차** 점검
- **트리거 후보**: 작은 패턴/스티커, 특정 픽셀 범위 → **무작위 크롭/컬러·블러**로 약화

---

## 모델 보안: 적대적 공격·프라이버시·중독/백도어

### 적대적 예제(Evasion) — FGSM/PGD & 강건학습

**위협**: 입력에 극소 잡음만 추가해 오탐/오분류 유발
**방어**: **적대적 훈련(Adversarial Training)**, **입력 정규화/랜덤화**, **검출기(gradient masking은 금지)**

#### FGSM(빠른 기울기 부호) 공격 예제

```python
# (PyTorch) FGSM 공격 & 평가 스니펫

import torch
import torch.nn.functional as F

def fgsm_attack(model, x, y, eps=8/255.0):
    model.eval()
    x_adv = x.clone().detach().requires_grad_(True)
    loss = F.cross_entropy(model(x_adv), y)
    loss.backward()
    # 입력 스케일이 [0,1] 가정
    x_adv = (x_adv + eps * x_adv.grad.sign()).clamp(0, 1).detach()
    return x_adv

@torch.no_grad()
def accuracy(model, loader, device):
    ok=0; total=0
    for x,y in loader:
        x,y=x.to(device),y.to(device)
        ok += (model(x).argmax(1)==y).sum().item()
        total += y.numel()
    return ok/total

def eval_fgsm(model, loader, device, eps=8/255.0):
    ok=0; total=0
    for x,y in loader:
        x,y=x.to(device),y.to(device)
        x_adv = fgsm_attack(model, x, y, eps)
        ok += (model(x_adv).argmax(1)==y).sum().item()
        total += y.numel()
    return ok/total
```

#### PGD(k-step) 공격 & 적대적 훈련 루프

```python
def pgd_attack(model, x, y, eps=8/255.0, alpha=2/255.0, steps=7):
    x0 = x.clone().detach()
    x_adv = x.clone().detach() + torch.empty_like(x).uniform_(-eps, eps)
    x_adv.clamp_(0,1)
    for _ in range(steps):
        x_adv.requires_grad_(True)
        loss = F.cross_entropy(model(x_adv), y)
        grad = torch.autograd.grad(loss, x_adv)[0]
        with torch.no_grad():
            x_adv = x_adv + alpha * grad.sign()
            # Project back to eps-ball around x0
            x_adv = torch.max(torch.min(x_adv, x0+eps), x0-eps)
            x_adv.clamp_(0,1)
    return x_adv.detach()

def train_adv(model, loader, optimizer, device, eps=8/255., alpha=2/255., steps=3):
    model.train()
    for x,y in loader:
        x,y=x.to(device),y.to(device)
        x_adv = pgd_attack(model, x, y, eps, alpha, steps)
        logits = model(x_adv)
        loss = F.cross_entropy(logits, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

> 팁: **표준+적대 로스 혼합**(clean/adv 0.5:0.5), **학습률/스케줄** 튜닝, **EMA**로 일반화 개선.

---

### 프라이버시 공격 — 멤버십/모델 반전/속성 추론

#### 멤버십 추론(MIA) — 손실 임계 기반 간단 공격

**아이디어**: 학습셋 샘플은 보통 **낮은 손실** → 손실 임계로 ‘멤버’ 추정

```python
@torch.no_grad()
def membership_scores(model, loader, device):
    scores=[]  # (loss, y_true, id)
    for i,(x,y) in enumerate(loader):
        x,y=x.to(device),y.to(device)
        probs = F.log_softmax(model(x), dim=1)
        loss = F.nll_loss(probs, y, reduction='none')
        for j in range(y.size(0)):
            scores.append((loss[j].item(), int(y[j].item()), i))
    return scores

# 임계값을 정하면 (예: 검증셋 분위수) 간단한 MIA 가능

```

**대응**: **과적합 억제**(정규화/데이터 증강), **DP-SGD**, **라벨 스무딩**, **예측 확률 절제(temperature/칼리브레이션)**.

#### (개념) 모델 반전/속성 추론

**대응**: **DP-SGD**, **공개 가능한 통계 최소화**, **민감 특성 비학습/분리**, **API 출력 제한**(Top-k/확률 비공개).

---

### 데이터 중독/백도어 공격과 방어

#### 백도어(Trigger) 삽입 예시(데이터 변조)

```python
# (예시) 작은 흰색 사각형을 우하단에 붙여 특정 라벨로 잘못 학습시키기

def add_trigger(imgs, y, target_label, box=4):
    imgs = imgs.clone()
    b,c,h,w = imgs.shape
    imgs[:,:,-box:,-box:] = 1.0  # white square trigger
    y[:] = target_label
    return imgs, y

# 학습 데이터 일부 p%를 트리거 삽입 → 오염

```

**탐지/완화**
- **Activation/Gradient 클러스터링**으로 **이상 그룹** 탐지
- **데이터 정제**(kNN/센터드롭), **강건 학습**(MixUp/CutMix/Adversarial training), **입력 전처리**(강한 증강, 랜덤 압축/블러)
- **파인튜닝 시 프리즈+헤드 재학습**으로 백도어 영향 국소화

---

## 차등프라이버시(DP-SGD) — PyTorch(표준 기능)로 구현

**원리**: 각 샘플의 기여도(그라디언트)를 **클리핑**하고, 합에 **가우시안 노이즈** 추가 → 개인 정보 유출 위험 상한화(ε).

아래는 **PyTorch 2.x**의 `torch.func`를 활용한 **per-sample gradient** 기반 **DP-SGD 스켈레톤**(외부 프레임워크 불필요).

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.func import grad, vmap

def loss_fn(params, model, x, y):
    # params를 model에 임시 주입하는 간단 함수가 필요할 수 있으나,
    # 여기서는 model이 leaf parameter를 갖고 있다고 가정하고 .parameters()로 접근
    out = model(x)
    return F.cross_entropy(out, y)

def per_sample_grads(model, x, y):
    # vmap(grad(loss)) 형태로 각 샘플에 대한 gradient 구하기
    def single_loss(xi, yi):
        out = model(xi.unsqueeze(0))
        return F.cross_entropy(out, yi.unsqueeze(0))
    g_fn = vmap(grad(single_loss))
    grads = g_fn(x, y)  # 파라미터 텐서별 gradient를 얻으려면 functorch로 param-level 핸들링 필요
    # 실무에서는 functorch.make_functional_with_buffers 등으로 파라미터 추출/주입
    return grads

def dp_sgd_step(model, optimizer, x, y, C=1.0, noise_mult=1.0):
    # 개념 시연용(단순화). 실제 per-parameter clip/aggregate 필요.
    optimizer.zero_grad()
    batch_size = x.size(0)
    # 간단 대안: per-sample loss로 backward, hooks로 grad_sample 모으는 기법(학습용)
    losses = F.cross_entropy(model(x), y, reduction='none')
    losses.mean().backward()  # 일단 기본 grad 계산(표준)
    # 실제 DP-SGD는 per-sample grad를 수집해 L2 clip 후 합산 + noise 추가가 필요.
    # 아래는 "배치 그래디언트"를 clip/noise하는 단순화(정밀 DP 보장은 아님).
    total_norm = torch.sqrt(sum((p.grad.detach().data**2).sum() for p in model.parameters()))
    clip_coef = (C / (total_norm + 1e-6)).clamp(max=1.0)
    for p in model.parameters():
        if p.grad is not None:
            p.grad.detach().mul_(clip_coef)
            p.grad.add_(torch.randn_like(p.grad) * C * noise_mult / max(1, batch_size))
    optimizer.step()
```

> **실무 팁**
> - **진짜 DP 보장**을 원하면 **샘플별 그라디언트**를 정확히 구해 **각 샘플 L2-clip** 후 합산 → **가우시안 노이즈 추가**가 필요.
> - PyTorch 2.x의 `torch.func`로 **functionalize** 해 파라미터별 per-sample grad를 얻는 패턴을 적용(코드가 길어 생략).
> - **ε, δ 회계(privacy accounting)** 필수(모멘트 회계/PRV accountant 참고).

---

## 파이프라인/공급망 보안

### 의존성/아티팩트

- **의존성 동결/해시 고정**(SHA256), **빌드 캐시 격리**, **개발/운영 키 분리**
- 체크포인트/데이터셋을 **검증된 경로**로만 인입, **사내 프록시 저장소** 운영

### 실험 추적/메타데이터 서버(예: MLflow/W&B 대체)

- **인증/MFA**, **프로젝트별 ACL**, **비공개 프로젝트 기본값**
- **모델 카드/데이터 카드**에 **보안 속성**(PII 여부, 허용된 사용처, 저작권)을 기록

### CI/CD

- 러너에 **OIDC 단기 토큰**, **시크릿 스캔**, **서명된 릴리스**(SBOM 포함)
- **학습→검증→배포** 아티팩트는 **불변 경로**에 저장, **서명체크** 후만 배포

---

## 서빙/운영 보안

### API 보안

- **인증/인가/요율 제한**(IP/사용자/테넌트), **페이로드 크기/빈도 제한**
- 추론 요청 로그는 **PII 최소화/익명화**, **모델 응답 캐시**의 **격리** 유지

### 모델 도난/추출 방어

- **출력 엔트로피/확률 절제**(Top-k/라운딩), **쿼리 패턴 이상탐지**(대량 경계 주변 질의)
- **수요 기반 가격/쿼터**, **워터마킹/핑거프린팅**(완전 방어는 아님, 억지력)

### LLM/RAG 특유 위협(요약)

- **프롬프트 인젝션/도구 호출 오남용**: **명령 계층(시스템>개발>사용자)** 고정, **외부 호출 화이트리스트**, **출력 산술·코드 실행 차단**
- **리트리벌 중독/독소 문서**: **문서 신뢰등급 메타데이터**로 필터, **출처 서명/도메인 화이트리스트**, **스팬 하이라이트**로 소스 귀속 강제
- **크로스 테넌트 유출**: 임베딩 저장소/벡터DB에 **테넌트 키**를 인덱스 키에 포함 + **Row-Level Security**

---

## 강건성/보안 평가 체크리스트

**적대적 강건**
- [ ] Clean Acc / FGSM Acc@ε / PGD Acc@ε
- [ ] 입력 변환(리사이즈/블러/압축) 후 안정성

**프라이버시**
- [ ] 멤버십 추론 AUC(↓), 라벨 스무딩/DP 적용 시 개선 확인
- [ ] 공개 메트릭/샘플 개수 제한

**중독/백도어**
- [ ] 트리거 탐지 스캔(특이 임베딩 클러스터/스펙트럼)
- [ ] 샘플 세척 후 성능 비교(클린/오염 차이)

**서빙**
- [ ] 요율 제한/쿼터/콜드다운
- [ ] 민감 기능(파일/URL 읽기, 코드 실행) **기본 OFF** + 화이트리스트
- [ ] 감사로그: `{ts, actor, model, input_hash, output_hash, decision}`

---

## 실전 시나리오 & PyTorch 예제

### 분류기(CIFAR-10 가정) — 적대적 훈련 + 평가

```python
import torch, torch.nn as nn, torch.optim as optim
import torch.nn.functional as F

class SmallCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.f = nn.Sequential(
            nn.Conv2d(3,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(64,128,3,padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1),
        )
        self.h = nn.Linear(128, num_classes)
    def forward(self,x):
        z = self.f(x).squeeze(-1).squeeze(-1)
        return self.h(z)

def train_mixed(model, loader, optimizer, device, eps=8/255., alpha=2/255., steps=2, lam=0.5):
    model.train()
    for x,y in loader:
        x,y = x.to(device), y.to(device)
        # clean
        logits_c = model(x)
        loss_c = F.cross_entropy(logits_c, y)
        # adversarial
        x_adv = pgd_attack(model, x, y, eps, alpha, steps)
        logits_a = model(x_adv)
        loss_a = F.cross_entropy(logits_a, y)
        loss = lam*loss_c + (1-lam)*loss_a
        optimizer.zero_grad(); loss.backward(); optimizer.step()
```

### 멤버십 추론 방어 — 라벨 스무딩 + 칼리브레이션

```python
def xent_label_smoothing(logits, y, smoothing=0.1):
    n = logits.size(1)
    with torch.no_grad():
        true_dist = torch.zeros_like(logits)
        true_dist.fill_(smoothing/(n-1))
        true_dist.scatter_(1, y.unsqueeze(1), 1 - smoothing)
    logp = F.log_softmax(logits, dim=1)
    return -(true_dist * logp).sum(dim=1).mean()

# 학습 시 loss를 xent_label_smoothing으로 교체

```

---

## 운영 표준(Policies) — 실전에 바로 쓰는 규정 초안

1) **데이터**
   - 원본 접근은 승인자 2인, 모든 복사본은 **암호화** 및 **만료**
   - 외부 데이터/체크포인트는 **해시/서명** 확인 후만 사용

2) **모델**
   - 배포 아티팩트는 **불변 경로 + 서명 검증**
   - 공개 API는 **쿼터/과금/탐지** 체계 필수

3) **파이프라인**
   - CI 러너는 단기 자격/임시 VPC, 아티팩트 **서명**
   - 실험 추적/모델카드에 **보안 속성** 필드 의무화

4) **서빙**
   - 고위험 기능(파일/웹 접근·임의 코드)은 **기본 비활성**, 프로젝트별 화이트리스트
   - **감사로그** 중앙화 + WORM 저장 + 경보(403 급증 등)

---

## 마무리 & 적용 가이드

- **위협모델→통제 설계→지속 평가**의 루프를 확립하라.
- “정확도 몇 %”보다 **강건성/프라이버시/운영 안전성** 메트릭을 **동일 등급**으로 관리하라.
- 본 문서의 PyTorch 스니펫(FGSM/PGD/적대학습/라벨스무딩/DP-SGD 스켈레톤)을 **프로젝트 템플릿**에 편입해,
  **학습/검증 단계부터 보안 평가지표**를 자동 산출·대시보드화하라.
