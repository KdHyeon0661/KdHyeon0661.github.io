---
layout: post
title: 기계학습 - 분류 평가 지표
date: 2025-08-20 15:25:23 +0900
category: 기계학습
---
# 분류 평가 지표: Accuracy, Precision, Recall, F1, ROC-AUC 완전 정리

분류(Classification) 모델은 **정확도 하나**로는 온전히 평가할 수 없습니다.  
특히 **클래스 불균형**(예: 사기 1%, 정상 99%) 상황에선 더 다양한 지표가 필요합니다.  
아래에서는 실무에서 가장 자주 쓰는 **5대 지표**를 개념 → 수식 → 해석 → 활용 순으로 정리합니다.

---

## 0) 기본: 혼동행렬(Confusion Matrix)

이진 분류에서 예측(가로)과 실제(세로)의 교차표:

|            | Pred + | Pred − |
|------------|--------|--------|
| **Actual +** | **TP**   | **FN**   |
| **Actual −** | **FP**   | **TN**   |

- **TP**(True Positive): 실제 +를 +로 맞춤  
- **FP**(False Positive): 실제 −를 +로 잘못 예측  
- **FN**(False Negative): 실제 +를 −로 놓침  
- **TN**(True Negative): 실제 −를 −로 맞춤

이 네 값으로 대부분의 분류 지표가 계산됩니다.

---

## 1) Accuracy (정확도)

- 정의: 전체 중 맞춘 비율 (임계값에 의존)
\[
\mathrm{Accuracy}=\frac{TP+TN}{TP+FP+FN+TN}
\]
- 장점: 직관적, 한 줄 요약에 좋음  
- 단점: **불균형 데이터에서 왜곡**(항상 '정상'만 예측해도 높게 나올 수 있음)  
- 응용: 데이터가 균형이고 FP/FN 비용이 비슷할 때

> 보완: **Balanced Accuracy** \(=\frac{\mathrm{TPR}+\mathrm{TNR}}{2}\) 도 있다 (클래스별 민감도 평균).

---

## 2) Precision (정밀도)

- 정의: **+라고 예측한 것 중** 실제로 +인 비율
\[
\mathrm{Precision}=\frac{TP}{TP+FP}
\]
- 해석: “잡아온 것 중에 진짜가 몇 %인가?”  
- 단점: FN(놓침)에 둔감  
- 응용: **거짓 경보(FP) 비용이 큰 문제**(스팸 필터에서 정상 메일을 스팸으로 분류하면 곤란)

---

## 3) Recall (재현율, 민감도, TPR)

- 정의: **실제 + 중** 얼마나 +로 맞췄는가
\[
\mathrm{Recall}=\frac{TP}{TP+FN}
\]
- 해석: “진짜를 얼마나 잘 놓치지 않는가?”  
- 단점: FP 증가와의 트레이드오프  
- 응용: **놓치면 치명적(FN 비용↑)** 인 문제(질병 스크리닝, 이상 징후 탐지)

---

## 4) F1 Score (조화평균)

- 정의: Precision과 Recall의 **조화평균**
\[
\mathrm{F1}=2\cdot\frac{\mathrm{Precision}\cdot\mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}
\]
- 성질: Precision/Recall 모두 높아야 F1도 높음 (둘 중 하나가 낮으면 크게 깎임)  
- 응용: **불균형 데이터**에서 한 숫자로 종합 판단할 때, 혹은 임계값 튜닝 목표로 사용  
- 확장: **F\(_\beta\)** (Recall에 더 가중치)  
\[
F_\beta=(1+\beta^2)\cdot\frac{PR}{\beta^2 P+R}
\]
(예: \(\beta=2\)면 Recall을 2배 더 중시)

---

## 5) ROC-AUC (임계값 불변 랭킹 지표)

### (1) ROC 곡선
- 가로축: **FPR** \(=\frac{FP}{FP+TN}\)  
- 세로축: **TPR(Recall)** \(=\frac{TP}{TP+FN}\)  
- 임계값을 1→0으로 바꾸며 얻는 (FPR, TPR) 점들의 궤적

### (2) AUC (Area Under the Curve)
- ROC 곡선 아래 면적 (0.5=랜덤, 1.0=완벽)
- **확률적 해석**: 임의의 **양성**이 임의의 **음성**보다 **더 높은 점수**를 받을 확률
- 장점: 임계값에 의존하지 않는 **순위 품질** 지표, 데이터 불균형에도 비교적 안정  
- 주의: 극단적 불균형/희귀 이벤트에선 **PR-AUC**가 더 설명력 있는 경우도 많음

---

## 6) 지표 간 트레이드오프와 임계값(Threshold)

- 분류기는 보통 **확률/점수**를 내고, 임계값으로 레이블을 정합니다.
- 임계값 ↑ → **Precision↑, Recall↓** 경향  
- 임계값 ↓ → **Recall↑, Precision↓** 경향
- 실무: **비즈니스 비용 함수**에 맞춰 임계값을 정하거나,  
  PR/ROC 곡선을 보고 **목표 지표 최대화 점**(예: F1 최대, Youden’s J=TPR−FPR 최대) 선택

---

## 7) 다중 분류(Multi-class)에서의 평균 방식

- **Macro**: 클래스별 지표를 **동일 가중 평균** → 클래스 불균형에 **민감**(소수 클래스 성능 반영↑)  
- **Weighted**: 클래스별 지표를 **샘플 수 가중 평균** → 전체 분포 반영  
- **Micro**: 모든 TP/FP/FN/TN을 **글로벌 집계** 후 계산 → **샘플 수준** 평균, 불균형에서 주로 다수 클래스 영향

> 보고서에는 보통 **(micro, macro, weighted) F1**을 함께 제시하면 좋습니다.

---

## 8) 작은 예제로 한 번에 이해

혼동행렬: TP=15, FP=10, FN=5, TN=70 (총 100)

- Accuracy \(=\frac{15+70}{100}=0.85\)  
- Precision \(=\frac{15}{15+10}=0.60\)  
- Recall \(=\frac{15}{15+5}=0.75\)  
- F1 \(=2\cdot\frac{0.60\cdot0.75}{0.60+0.75}=0.667\)  
- ROC의 한 점:  
  - TPR(=Recall)=0.75  
  - FPR \(=\frac{10}{10+70}=0.125\)

→ **정확도는 높지만**(0.85) 정밀도는 0.60으로 **거짓 경보**가 꽤 있다는 사실을 알 수 있습니다.

---

## 9) scikit-learn 실전 스니펫

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve, classification_report
)
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
import numpy as np

# 데이터 준비 (불균형 예시)
X, y = make_classification(n_samples=5000, n_features=20, weights=[0.9, 0.1],
                           random_state=42)
X_tr, X_te, y_tr, y_te = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# 모델 학습
clf = LogisticRegression(max_iter=1000)
clf.fit(X_tr, y_tr)

# 예측
proba = clf.predict_proba(X_te)[:, 1]     # 양성 확률
y_pred = (proba >= 0.5).astype(int)       # 임계값 0.5

# 지표
print("Accuracy :", accuracy_score(y_te, y_pred))
print("Precision:", precision_score(y_te, y_pred, zero_division=0))
print("Recall   :", recall_score(y_te, y_pred))
print("F1       :", f1_score(y_te, y_pred))
print("ROC-AUC  :", roc_auc_score(y_te, proba))  # 확률 입력 (임계값 독립)

# 임계값 튜닝 예: F1 최대가 되는 임계값 찾기
prec, rec, thr = precision_recall_curve(y_te, proba)
f1s = 2 * prec * rec / (prec + rec + 1e-12)
best_idx = np.argmax(f1s)
print("Best threshold for F1:", thr[best_idx-1], "F1:", f1s[best_idx])
print(classification_report(y_te, y_pred, digits=3))
```

> 팁  
> - **ROC-AUC은 확률/점수**를 넣어야 올바릅니다(이진 레이블 X).  
> - 멀티클래스 ROC-AUC: `roc_auc_score(y_true, proba, multi_class='ovr' or 'ovo')`

---

## 10) 언제 어떤 지표를 쓸까? (퀵 가이드)

| 상황 | 우선 지표 | 보조 지표/메모 |
|---|---|---|
| 데이터 균형, 비용 대칭 | **Accuracy** | F1로 보조 |
| 거짓 경보가 비싸다(FP 비용↑) | **Precision** | Precision@k, PR-AUC |
| 놓치면 위험(FN 비용↑) | **Recall** | Sensitivity, F\(_\beta\) (\(\beta>1\)) |
| 불균형 + 하나의 점수 필요 | **F1** | Macro/Weighted F1 함께 |
| 임계값 독립 랭킹 품질 | **ROC-AUC** | 극단적 불균형이면 PR-AUC도 |

---

### ✅ 정리
- **Accuracy**: 전체 정답률(불균형에 취약)  
- **Precision**: +라고 한 것의 정확성(거짓 경보 비용↑에서 중요)  
- **Recall**: +를 놓치지 않는 비율(놓치면 위험한 문제에서 중요)  
- **F1**: Precision·Recall 균형(불균형에 유용)  
- **ROC-AUC**: 임계값과 무관한 순위 품질(모델 **분리력** 평가)

필요한 문제 설정(비즈니스 비용, 데이터 불균형, 임계값 전략)에 맞춰 **지표를 조합**해서 쓰는 것이 베스트입니다.