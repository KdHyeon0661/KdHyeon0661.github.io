---
layout: post
title: 기계학습 - 분류 평가 지표
date: 2025-08-20 15:25:23 +0900
category: 기계학습
---
# 분류 평가 지표: Accuracy, Precision, Recall, F1, ROC-AUC

## 0. 기초: 혼동행렬(Confusion Matrix)와 표기

이진 분류에서 예측(가로) × 실제(세로):

|            | Pred + | Pred − |
|------------|--------|--------|
| **Actual +** | **TP**   | **FN**   |
| **Actual −** | **FP**   | **TN**   |

- **TP**(True Positive): 실제 +를 +로 맞춤
- **FP**(False Positive): 실제 −를 +로 오경보
- **FN**(False Negative): 실제 +를 −로 놓침
- **TN**(True Negative): 실제 −를 −로 맞춤

아래 대부분의 지표는 **TP/FP/FN/TN**의 함수로 정의됩니다.

---

## 1. Accuracy (정확도)

### 정의
$$
\mathrm{Accuracy}=\frac{TP+TN}{TP+FP+FN+TN}.
$$

### 해석·특징
- 전체 샘플 중 **정답 비율**. 직관적이고 요약에 좋음.
- **클래스 불균형**에서 오해 소지 큼(항상 다수 클래스로 예측해도 높을 수 있음).

### 변형
- **Balanced Accuracy**: 클래스별 민감도 평균
  $$
  \mathrm{Balanced\ Acc.}=\frac{\mathrm{TPR}+\mathrm{TNR}}{2},\quad
  \mathrm{TPR}=\frac{TP}{TP+FN},\ \mathrm{TNR}=\frac{TN}{TN+FP}.
  $$

---

## 2. Precision (정밀도)

### 정의
$$
\mathrm{Precision}=\frac{TP}{TP+FP}.
$$

### 해석·특징
- “**잡아온 것 중 진짜가 몇 %**인가?”
- **거짓 경보(FP)** 비용이 큰 환경(스팸 필터의 정상 메일 오탐, 사기탐지의 정상거래 차단 등)에서 중요.

---

## 3. Recall (재현율, 민감도, TPR)

### 정의
$$
\mathrm{Recall}=\frac{TP}{TP+FN}.
$$

### 해석·특징
- “**진짜를 얼마나 놓치지 않는가**?”
- **놓치면 위험(FN 비용↑)** 한 문제(질병 스크리닝, 이상 탐지)에서 중요.
- TPR은 ROC의 세로축으로도 쓰입니다.

---

## 4. F1 Score (조화평균) 및 \(F_\beta\)

### 정의
$$
\mathrm{F1}=2\cdot\frac{\mathrm{Precision}\cdot\mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}.
$$

- Precision·Recall 모두 높아야 상승(둘 중 하나가 낮으면 크게 깎임).
- **불균형 데이터**에서 한 숫자로 성능을 요약할 때 유용.

### 일반화 \(F_\beta\)
$$
F_\beta=(1+\beta^2)\cdot\frac{(\mathrm{Precision}\cdot \mathrm{Recall})}{\beta^2\cdot \mathrm{Precision}+\mathrm{Recall}},
$$
- \(\beta>1\): **Recall**을 더 중시(예: \(\beta=2\)면 Recall 가중이 2배).
- \(\beta<1\): **Precision**을 더 중시.

---

## 5. ROC-AUC (임계값 불변 랭킹 지표)

### ROC(Receiver Operating Characteristic) 곡선
- 가로축: **FPR** \(=\frac{FP}{FP+TN}\)
- 세로축: **TPR(Recall)** \(=\frac{TP}{TP+FN}\)
- **임계값**을 1→0으로 움직이며 (FPR,TPR) 궤적을 그림.

### AUC(Area Under Curve)
- ROC 곡선 아래 면적.
- **확률적 해석**: 임의의 **양성**이 임의의 **음성**보다 **더 높은 점수**를 받을 확률.
- **임계값 독립**이며, 불균형에도 비교적 안정.
- 단, **극단적 불균형**에서는 **PR-AUC**가 더 설명력 있는 경우가 많음(아래 §6.3).

---

## 6. 추가·보완 지표 (실무에서 자주 쓰는 확장)

### 6.1 Specificity(특이도), NPV, Youden’s J, G-mean
- **Specificity(=TNR)**: 음성을 음성으로 맞출 비율
  $$
  \mathrm{Specificity}=\frac{TN}{TN+FP}.
  $$
- **NPV**(Negative Predictive Value): −라고 예측한 것 중 실제 − 비율
  $$
  \mathrm{NPV}=\frac{TN}{TN+FN}.
  $$
- **Youden’s J**: ROC에서 \(\mathrm{TPR}-\mathrm{FPR}\) 최대점(임계값 선택 보조)
  $$
  J=\mathrm{TPR}-\mathrm{FPR}.
  $$
- **G-mean**: TPR과 TNR의 기하평균(양·음성 균형)
  $$
  \mathrm{G\text{-}mean}=\sqrt{\mathrm{TPR}\cdot \mathrm{TNR}}.
  $$

### 6.2 MCC(Matthews Correlation Coefficient), Cohen’s Kappa
- **MCC**: 불균형에 강건, −1~+1(1이 완벽, 0은 랜덤)
$$
\mathrm{MCC}=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}.
$$
- **Cohen’s Kappa**: 우연 일치 보정된 정확도 대체

### 6.3 PR Curve & PR-AUC(불균형 핵심)
- **PR Curve**: x축 Recall, y축 Precision.
- **AP(Avg Precision)** 또는 **PR-AUC** 사용.
- **희귀 이벤트(양성 적음)**에서는 ROC-AUC가 높아도 **운영 구간 Precision/Recall은 낮을 수** 있으므로, **PR-AUC**와 **Precision@k, Recall@FPR≤α** 등을 같이 봐야 합니다.

---

## 7. 임계값(Threshold)과 트레이드오프

- 분류기는 보통 **점수/확률**을 내고, 임계값으로 라벨을 정함.
- 임계값 ↑: **Precision↑, Recall↓** 경향
- 임계값 ↓: **Recall↑, Precision↓** 경향
- 선택 전략
  - **F1 최대** 임계값(\(F_\beta\) 가능)
  - **Youden’s J 최대** 임계값
  - **비용 최소화** 임계값(§10 참조)
  - **제약 최적화**: 예) FPR ≤ 1%에서 Recall 최대

---

## 8. 멀티클래스/멀티라벨에서의 평균 방식

- **Macro**: 클래스별 지표의 단순 평균 → **소수 클래스 반영↑**
- **Weighted**: 클래스별 지표를 **샘플 수 가중 평균** → 전체 분포 반영
- **Micro**: 모든 TP/FP/FN/TN을 **글로벌 집계** 후 계산 → **샘플 기반** 평균

> 보고서에는 보통 **(micro, macro, weighted) F1**을 함께 제시.
> 멀티라벨(한 샘플에 다수 라벨 가능)에서는 **subset accuracy**(모든 라벨 일치)와 **label-based micro/macro**를 병기.

---

## 9. 작은 수치 예시(직관 고정)

혼동행렬: \(TP=15, FP=10, FN=5, TN=70\) (총 100)

- Accuracy \(=\frac{15+70}{100}=0.85\)
- Precision \(=\frac{15}{15+10}=0.60\)
- Recall \(=\frac{15}{15+5}=0.75\)
- F1 \(=2\cdot\frac{0.60\cdot0.75}{0.60+0.75}\approx0.667\)
- FPR \(=\frac{10}{10+70}=0.125\)

→ 정확도는 높지만(**0.85**), 정밀도는 **0.60**으로 **거짓 경보가 적지 않음**을 확인.

---

## 10. 비용 기반 의사결정(Threshold by Cost)

현실에서는 FP·FN의 **비용(또는 효용)**이 다릅니다.
**기대 비용**을 최소화하는 임계값 \(\tau\)를 선택합니다.

- 클래스 사전확률(유병률) \(\pi=P(y=1)\)
- 비용 \(C_{FP}, C_{FN}\) (필요 시 \(C_{TP},C_{TN}\)도 포함)

**점수** \(s(x)\)가 **잘 보정된 확률**이라면, 베이지안 최적 임계값은
$$
\tau^\*=\frac{C_{FP}\cdot (1-\pi)}{C_{FN}\cdot \pi + C_{FP}\cdot (1-\pi)}.
$$

> 확률이 미보정이면(예: 트리/부스팅) **Calibration**(Platt/Isotonic) 후 적용이 바람직(§12).

---

## 11. scikit-learn 실전 스니펫

### 11.1 지표 계산·리포트·ROC/PR 곡선·임계값 최적화(F1)

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve,
    classification_report, confusion_matrix, average_precision_score
)
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
import numpy as np
import matplotlib.pyplot as plt

# 불균형 데이터
X, y = make_classification(
    n_samples=8000, n_features=20, weights=[0.95, 0.05],
    class_sep=1.0, random_state=42
)
X_tr, X_te, y_tr, y_te = train_test_split(
    X, y, stratify=y, test_size=0.3, random_state=42
)

# 모델
clf = LogisticRegression(max_iter=1000)
clf.fit(X_tr, y_tr)

# 점수/예측
proba = clf.predict_proba(X_te)[:, 1]
y_pred_05 = (proba >= 0.5).astype(int)

# 기본 지표
print("Accuracy :", accuracy_score(y_te, y_pred_05))
print("Precision:", precision_score(y_te, y_pred_05, zero_division=0))
print("Recall   :", recall_score(y_te, y_pred_05))
print("F1       :", f1_score(y_te, y_pred_05))
print("ROC-AUC  :", roc_auc_score(y_te, proba))
print("PR-AUC(AP):", average_precision_score(y_te, proba))

# F1 최대 임계값 탐색
prec, rec, thr = precision_recall_curve(y_te, proba)
f1s = 2 * prec * rec / (prec + rec + 1e-12)
best = np.nanargmax(f1s)
best_thr = thr[best-1]  # thr 길이는 prec/rec보다 1 작음
print("Best threshold (F1):", best_thr, " F1:", f1s[best])

# ROC/PR 플롯
fpr, tpr, _ = roc_curve(y_te, proba)
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(fpr, tpr, label=f"ROC AUC={roc_auc_score(y_te, proba):.3f}")
plt.plot([0,1],[0,1],'--',color='gray'); plt.xlabel("FPR"); plt.ylabel("TPR(Recall)")
plt.legend(); plt.title("ROC")

plt.subplot(1,2,2)
plt.plot(rec, prec, label=f"AP={average_precision_score(y_te, proba):.3f}")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.legend(); plt.title("PR")
plt.tight_layout(); plt.show()

# 분류 리포트(멀티클래스/가중 평균 포함)
print(classification_report(y_te, y_pred_05, digits=3))
print("Confusion\n", confusion_matrix(y_te, y_pred_05))
```

### 11.2 제약 최적화 예: “FPR ≤ 1%에서 Recall 최대”
```python
target_fpr = 0.01
fpr, tpr, thr = roc_curve(y_te, proba)
mask = fpr <= target_fpr
best = np.argmax(tpr[mask])
thr_star = thr[mask][best]
print(f"Threshold @FPR≤{target_fpr*100:.1f}%:", thr_star, "Recall:", tpr[mask][best])
```

### 11.3 Precision@k, Recall@FPR≤α (랭킹 기반 운영지표)
```python
def precision_at_k(y_true, scores, k):
    idx = np.argsort(scores)[::-1][:k]
    return y_true[idx].mean()

k = 200  # 상위 200건만 수동 검토하는 시나리오
print("Precision@k:", precision_at_k(y_te, proba, k))
```

---

## 12. 확률 보정(Calibration)과 Brier Score

- ROC-AUC는 **순위 품질** 지표라 **확률 보정**과 무관할 수 있음.
- **임계값 최적화/비용 최소화**를 하려면 **확률이 보정(calibrated)** 되어야 유효합니다.
- **Brier Score**(확률 예측 오차), **Calibration Curve**로 점검.

```python
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import brier_score_loss
import matplotlib.pyplot as plt

base = LogisticRegression(max_iter=1000).fit(X_tr, y_tr)
raw = base.predict_proba(X_te)[:,1]
print("Brier(raw):", brier_score_loss(y_te, raw))

# Platt scaling(sigmoid) 또는 isotonic
cal = CalibratedClassifierCV(base, method="isotonic", cv=5).fit(X_tr, y_tr)
cal_p = cal.predict_proba(X_te)[:,1]
print("Brier(cal):", brier_score_loss(y_te, cal_p))

prob_true, prob_pred = calibration_curve(y_te, cal_p, n_bins=10)
plt.plot(prob_pred, prob_true, marker='o'); plt.plot([0,1],[0,1],'--',color='gray')
plt.xlabel("Predicted prob."); plt.ylabel("True freq."); plt.title("Calibration")
plt.show()
```

---

## 13. 멀티클래스/멀티라벨 구현 팁

- **멀티클래스 ROC-AUC**:
  `roc_auc_score(y_true, proba, multi_class='ovr' or 'ovo', average='macro')`
  확률 `proba`는 각 클래스의 확률 열(softmax) 필요.
- **다중 평균 보고**: `classification_report(..., digits=3)`는 micro/macro/weighted 평균 F1 제공.
- **멀티라벨**: `average` 인자를 `samples|micro|macro|weighted`로 적절히 선택. **subset accuracy**는 엄격(모든 라벨 일치).

---

## 14. 어떤 지표를 쓸까? (의사결정 표)

| 상황 | 핵심 지표 | 보조/비고 |
|---|---|---|
| 데이터 균형, 비용 대칭 | Accuracy | F1 참고 |
| 거짓 경보(FP) 고비용 | Precision | Precision@k, PR-AUC |
| 놓치면(FN) 위험 | Recall | \(F_\beta(\beta>1)\), Sensitivity |
| 극단적 불균형 | PR-AUC(AP) | Precision@k, Recall@FPR≤α |
| 임계값 독립 랭킹 비교 | ROC-AUC | 운영 임계값 성능 함께 보고 |
| 양·음성 균형 모두 중요 | G-mean, Balanced Accuracy | Youden’s J로 임계값 보조 |
| 전반적 상관/견고 지표 | MCC | Kappa |

> **원포인트**: 운영 환경의 **비용 함수/용량 제약(검토 인력, 처리 슬롯)** 을 먼저 정의하고, 그에 맞는 지표·임계값을 고르십시오.

---

## 15. 흔한 함정과 대응

| 함정 | 증상 | 대응 |
|---|---|---|
| **정확도만** 보고 결정 | 불균형에서 과대평가 | Precision/Recall/F1/PR-AUC 병기 |
| ROC-AUC만 보고 만족 | 운영 임계값에서 성능 저하 | PR-AUC, Precision@k, 제약 기반 지표 함께 |
| 미보정 확률로 비용 최적화 | 임계값·비용 산출 왜곡 | Calibration(Platt/Isotonic) 후 결정 |
| 임계값 고정(0.5) 고집 | 비용·유병률 변화 무시 | 운영별 임계값 조정·모니터링 |
| 리포트 평균 방식 오용 | 다수 클래스 성능만 반영 | micro/macro/weighted 병기 |
| 테스트 데이터 재사용 | 과적합된 지표 | **Train/Val/Test 분리** 또는 **CV** |

---

## 16. 미니 체크리스트

- [ ] 비즈니스 비용/제약 정의(예: FPR 한도, 검토 인력 k)
- [ ] 적절한 **지표 묶음** 선정(Precision/Recall/F1/ROC-AUC/PR-AUC 등)
- [ ] **임계값 최적화**(F1 최대, 비용 최소, 제약 만족)
- [ ] **Calibration** 점검(Brier, Calibration curve)
- [ ] **멀티 평균 방식** 병기(micro/macro/weighted)
- [ ] 재현 가능한 평가(CV/시드/누수 방지)

---

## 부록 A) 수식 요약

- \( \mathrm{Accuracy}=\frac{TP+TN}{TP+FP+FN+TN} \)
- \( \mathrm{Precision}=\frac{TP}{TP+FP} \), \( \mathrm{Recall}=\frac{TP}{TP+FN} \)
- \( \mathrm{F1}=2\cdot\frac{PR}{P+R} \), \( F_\beta=(1+\beta^2)\frac{PR}{\beta^2P+R} \)
- \( \mathrm{FPR}=\frac{FP}{FP+TN},\ \mathrm{TPR}=\mathrm{Recall} \)
- \( \mathrm{Specificity}=\frac{TN}{TN+FP},\ \mathrm{G\text{-}mean}=\sqrt{\mathrm{TPR}\cdot\mathrm{TNR}} \)
- \( \mathrm{MCC}=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}} \)

---

## 부록 B) 실무 리포트 템플릿(요약 예)

1) 데이터: 기간/전처리/불균형 비율
2) 모델: 버전/특징/학습 방식
3) **지표표**: Accuracy, Precision, Recall, F1, ROC-AUC, PR-AUC(AP), MCC(±CI)
4) **임계값**: F1 최대/Youden’s J/비용 최소/제약 만족 임계값 및 해당 지표
5) **운영지표**: Precision@k, Recall@FPR≤α
6) **Calibration**: Brier, 곡선
7) **설명**: 중요도/에러분석(군집별)
8) 결론: 선택 사유, 리스크, 모니터링 계획

---

### 최종 정리

- **정확도 하나로는 부족**합니다. 최소한 **Precision, Recall, F1, ROC-AUC**를 함께 보되, **불균형**이면 **PR-AUC/Precision@k**를 반드시 포함하세요.
- 임계값은 **F1/Youden/비용/제약** 관점에서 **데이터·업무 현실에 맞게 최적화**해야 합니다.
- 랭킹 품질(ROC-AUC)과 **확률 보정(Calibration)**, 그리고 **운영 구간 성능**(Precision@k, Recall@FPR≤α)을 **동시에** 챙기는 것이 실무의 핵심입니다.
