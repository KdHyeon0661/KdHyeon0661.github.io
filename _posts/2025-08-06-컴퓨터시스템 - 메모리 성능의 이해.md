---
layout: post
title: 컴퓨터시스템 - 메모리 성능의 이해
date: 2025-08-06 15:20:23 +0900
category: 컴퓨터시스템
---
# 메모리 성능의 이해

## 메모리 계층의 큰 그림

- **계층**: 레지스터 → L1D/L1I → L2 → L3/LLC(소켓 공유) → DRAM(메모리 컨트롤러) → 스토리지(SSD/HDD).
- **두 축**
  - **지연(latency)**: 위로 갈수록 짧다(L1≪DRAM).
  - **대역(bandwidth)**: 상위 캐시가 폭넓다(코어 근처 포트/버스 병렬).
- **핵심 교훈**: **자주/가까운** 데이터를 캐시에 붙이고, **DRAM 왕복을 최소화**하라.

ASCII 개념도:
```
[레지스터] (수 사이클, 극소 용량)
   ↓
[L1D/L1I] (수~10 사이클, 32~64KB/코어)
   ↓
[L2] (10~20+ 사이클, 256KB~2MB/코어)
   ↓
[L3/LLC] (수십~100+ 사이클, 수~수십MB/소켓)
   ↓
[DRAM] (수백 ns, 수~수천 GB)
   ↓
[SSD/HDD] (μs~ms, TB)
```

**지역성(Locality)**
- **시간**: 방금 쓴 데이터는 곧 또 쓴다.
- **공간**: 어떤 주소를 접근하면 그 이웃도 곧 접근한다(캐시 라인 단위).

---

## 성능 모델: AMAT·CPI로 보는 병목

### 평균 메모리 접근 시간(AMAT)

$$
\text{AMAT} = T_{L1} + m_1\Big(T_{L2} + m_2\big(T_{L3} + m_3 T_{\text{Mem}}\big)\Big)
$$

- \(T_{Lk}\): 레벨 \(k\) 히트 시간, \(m_k\): 그 레벨 미스율, \(T_{\text{Mem}}\): DRAM 왕복 지연.
- **해석 포인트**: \(m_1, m_2, m_3\) 중 하나라도 커지면 총 지연이 기하급수적으로 커진다.

### CPI 분해(메모리 스톨 포함)

$$
\text{CPI} \approx \text{CPI}_{\text{base}}
+ \frac{\text{MemAccesses}}{\text{Instr}}
\times \frac{\text{Misses}}{\text{Access}}
\times \frac{\text{MissPenalty}}{\text{MLP}}
$$

- **MLP**(Memory-Level Parallelism): 동시에 outstanding 가능한 미스 수(클수록 페널티 분모에 들어가 완화).
- **중요**: **포인터 추적**처럼 직렬 의존이면 MLP≈1 → DRAM 지연이 **그대로** 노출된다.

---

## 로컬리티와 접근 패턴

- **좋음**: 연속 순회(스트리밍), 블록 단위 재사용(타일링), SoA 열 연산.
- **나쁨**: 큰 스트라이드, 랜덤 포인터 추적, 자주 변하는 인덱스/분기.

```c
// 행-주도(row-major) 순회(좋음) vs 열-주도(나쁠 수 있음)
#ifndef N
#define N 4096
#endif

int sum_row(int (*a)[N]) {
    int s=0;
    for (int i=0;i<N;i++)
        for (int j=0;j<N;j++)
            s += a[i][j];  // 연속 접근 (stride 1)
    return s;
}
int sum_col(int (*a)[N]) {
    int s=0;
    for (int j=0;j<N;j++)
        for (int i=0;i<N;i++)
            s += a[i][j];  // 큰 스트라이드 → 캐시/TLB 미스↑
    return s;
}
```

---

## 캐시 구조 핵심: 블록·연관도·치환·쓰기 정책

- **라인(블록) 크기**: 보통 64B. 공간 지역성 활용 vs **오염(pollution)** 트레이드오프.
- **연관도**: Direct ↔ Set-Assoc(일반) ↔ Fully. 낮으면 **충돌(conflict)** ↑, 높으면 면적/지연 코스트.
- **치환**: LRU 근사(PLRU 등). 세트 스래싱엔 **패딩/오프셋/데이터 재배치**.
- **쓰기 정책**
  - *Write-back + Write-allocate*(기본): 쓰기 재사용에 유리.
  - *Write-through + No-allocate*: 오염 줄고 단순하지만 대역 소모↑.
  - **비휘발(non-temporal) 저장**: 스트리밍 결과를 캐시 오염 없이 DRAM으로.

---

## 3C 미스 모델(+1C)

1. **Compulsory**(콜드): 처음 보는 데이터.
2. **Capacity**: 용량 부족으로 교체 발생.
3. **Conflict**: 같은 세트로 몰려 충돌.
4. **Coherence**(멀티코어): 타 코어 쓰기가 무효화 유발 → **폴스 셰어링**.

---

## TLB와 가상 메모리

- **TLB**: 가상→물리 변환 캐시.
- **TLB reach**:
  $$
  \text{Reach} = \text{entries} \times \text{page\_size}
  $$
  워킹셋이 reach보다 크면 **TLB 미스**와 **페이지 워크**가 성능을 잠식.
- **Huge Page**(2MB/1GB): reach 확대(장점), 단편화/관리/NUMA 배치 주의.
- **페이지 컬러링**: 물리 인덱스-세트 충돌을 피하기 위한 페이지 색상 배치 개념(현대 OS/하드웨어에선 암묵적 관리 + 특정 워크로드에서 수동 조정 여지).

---

## DRAM/행 버퍼/채널 병렬성

- 구조: **채널→랭크→뱅크→행(row)→열(col)**, 각 뱅크에 **행 버퍼(row buffer)**.
- **Row hit**: 같은 행 연속 접근(빠름). **Row miss**: precharge→activate(느림).
- 스케줄러는 **FR-FCFS** 경향(행 히트 우선).
- **권장**: 데이터 배치를 채널/뱅크에 **분산**하고, 순회는 가능하면 **행 지역성**있게.

---

## NUMA(Non-Uniform Memory Access)

- **로컬 노드**는 빠르고, **원격 노드**는 지연/대역 손해.
- **First-touch** 정책: 페이지는 **처음 쓴 스레드의 노드**에 할당.
- **실전**: 초기화를 **스레드 분할**로 실행해 first-touch를 보장.

```c
// OpenMP: 초기화도 병렬로 first-touch
#pragma omp parallel for

for (long i=0;i<n;i++) a[i] = 0;  // 각 스레드가 자기 파티션을 터치
```

셸 바인딩:
```sh
# 특정 노드로 CPU/메모리 바인딩

numactl --cpunodebind=0 --membind=0 ./app
```

---

## 메모리 레벨 병렬성(MLP)과 프리패치

- **MLP↑**: 독립 로드 여러 개를 앞서 요청(언롤/벡터화), HW 프리패처가 따라오게.
- **포인터 추적**: 선형 프리패치 곤란 → 소프트웨어 프리패치/파이프라이닝.

```c
void sum_prefetch(const float *a, int n) {
    float s=0;
    for (int i=0;i<n;i+=16) {
        __builtin_prefetch(&a[i+64], 0, 1); // 읽기, 약한 지역성
        for (int k=0;k<16 && i+k<n;k++) s += a[i+k];
    }
    (void)s;
}
```

**프리패치 거리(d)** 근사:
$$
d \approx \left\lceil \frac{L_{\text{mem}}}{C_{\text{iter}}} \right\rceil
$$
- \(L_{\text{mem}}\): 메모리 왕복 사이클, \(C_{\text{iter}}\): 반복당 소비 사이클.
- **현실**: 후보 {16,32,64,128} 등 스윕→**중앙값**으로 선택.

---

## 데이터 레이아웃: AoS vs SoA, 정렬, 패딩, AoSoA

- **AoS(배열-의-구조체)**: 객체 단위 접근이 잦으면 편리.
- **SoA(구조체-의-배열)**: **열 단일 연산**/벡터화/대역 효율에 유리.
- **AoSoA**: 타일 단위 SoA 묶음(캐시·벡터 균형).
- **정렬(alignment)**: 32/64바이트 정렬 시 벡터 로드/스토어 효율↑.
- **패딩**: 세트 충돌/폴스 셰어링 회피용.

```c
// SoA 예시: 열 단일 연산은 SoA가 유리
typedef struct { float *x, *y, *z; } vec3_soa;
void norm_soa(vec3_soa v, int n) {
  #pragma omp simd
  for (int i=0;i<n;i++)
    v.x[i] = v.x[i]*v.x[i] + v.y[i]*v.y[i] + v.z[i]*v.z[i];
}
```

---

## 타일링(블로킹)으로 재사용 극대화

행렬 곱(ijk)을 예로 **블록** 단위로 A/B를 L1/L2에 붙잡고 C에 누적:

```c
void gemm_blocked(int n, float *restrict A, float *restrict B, float *restrict C, int Bsz) {
  for (int ii=0; ii<n; ii+=Bsz)
    for (int jj=0; jj<n; jj+=Bsz)
      for (int kk=0; kk<n; kk+=Bsz) {
        int im = (ii+Bsz<n)? ii+Bsz : n;
        int jm = (jj+Bsz<n)? jj+Bsz : n;
        int km = (kk+Bsz<n)? kk+Bsz : n;
        for (int i=ii;i<im;i++)
          for (int j=jj;j<jm;j++) {
            float acc = C[i*n+j];              // 레지스터 누적
            for (int k=kk;k<km;k++)
              acc += A[i*n+k] * B[k*n+j];      // 블록 내 재사용
            C[i*n+j] = acc;
          }
      }
}
```

**블록 크기** 경험칙(대략):
$$
B \;\approx\; \sqrt{\frac{\alpha \cdot C_{\text{cache}}}{
\text{element\_size}\times \text{(동시에 잡을 행렬 수)}}}
$$
- \(\alpha\): 여유 계수(라인/연관도/메타데이터 감안). → 후보 32~256 테스트로 결정.

---

## 불필요한 메모리 참조 줄이기(필수 습관)

- **스칼라 치환**: 같은 원소 **한 번만 로드**하고 레지스터 재사용.
- **루프 불변 호이스팅**: 스케일·테이블 포인터 등은 루프 밖.
- **Dead store 제거**: 곧 덮을 메모리에는 쓰지 않기.

```c
for (int i=0;i<n;i++) {
    float xi = x[i], yi = y[i];     // 두 번 읽지 않기
    y[i] = yi + a * xi;
}
```

---

## 동시성: 코히어런시·폴스 셰어링

- **MESI/MOESI**: 같은 라인을 서로 쓰면 무효화/소유권 전이.
- **폴스 셰어링**: 다른 변수라도 **같은 라인(예: 64B)**에 있으면 충돌.

```c
// 라인 패딩으로 폴스 셰어링 회피
typedef struct { _Alignas(64) long v; } padded_long;
padded_long cnt[128];
```

샤딩 패턴:
```c
// 스레드 로컬로 누적 → 드물게 병합
_Thread_local long local_cnt = 0;
void hit(){ local_cnt++; }
long total(/* thread handles... */){ /* 병합 */ }
```

---

## Roofline으로 “왜 안 빨라지는지” 판단

- **연산집약도(AI)**:
$$
\text{AI} = \frac{\text{FLOPs}}{\text{Bytes moved}}
$$

- **상한**:
$$
\text{Perf} \le \min(\text{Peak FLOPs},\;\text{AI}\times \text{Peak BW})
$$

- **SAXPY** 예: \(y \leftarrow a x + y\). 요소당 2 FLOP, 메모리 이동 24B(읽기 16B + 쓰기 8B) →
  \(\text{AI}\approx 2/24\approx 0.083 \text{ FLOP/B}\) → **메모리 바운드**.
  → 해결은 연산 늘리기가 아니라 **대역/로컬리티** 최적화(타일/벡터화/비휘발 저장).

---

## 측정·검증: 절차와 도구

### 환경 고정/워밍업/통계

- CPU governor=performance, 고정 클럭(가능 시), 터보 상태 기록.
- 코어 고정(taskset/numactl), 백그라운드 부하 최소화.
- 여러 번 실행 → **중앙값** + 표준편차 또는 95% 신뢰구간.

### 타이머/카운터/프로파일

```c
// 최소 측정 틀 (CLOCK_MONOTONIC)
#include <time.h>

double now(){ struct timespec t; clock_gettime(CLOCK_MONOTONIC,&t);
              return t.tv_sec + t.tv_nsec*1e-9; }
```

```sh
# perf로 주요 카운터 수집 (Linux)

perf stat -e cycles,instructions,IPC,\
L1-dcache-loads,L1-dcache-load-misses,\
LLC-loads,LLC-load-misses,\
dTLB-load-misses,branches,branch-misses \
./app
```

- **IPC 낮고 LLC miss↑** → 메모리 바운드(타일/SoA/프리패치).
- **dTLB miss↑** → Huge Page/워크셋/패턴 점검.
- **branch-misses↑** → 분기 패턴/브랜치리스.

### 대역/패턴 벤치

- **STREAM**: 메모리 대역 상한 확인.
- **Cachegrind/VTune/Advisor**: 캐시 시뮬·MLP·벤터화 인사이트.

---

## 랩 템플릿 모음

### 행/열 순회 비교(체감용)

```c
#include <time.h>
#include <stdio.h>
#include <stdlib.h>
#ifndef N
#define N 4096
#endif

static double now_s(){ struct timespec t; clock_gettime(CLOCK_MONOTONIC,&t);
  return t.tv_sec + t.tv_nsec*1e-9; }

long long sum_row(int *a){
  long long s=0;
  for(int i=0;i<N;i++)
    for(int j=0;j<N;j++)
      s += a[i*N+j];
  return s;
}
long long sum_col(int *a){
  long long s=0;
  for(int j=0;j<N;j++)
    for(int i=0;i<N;i++)
      s += a[i*N+j];
  return s;
}
int main(){
  int *a = aligned_alloc(64, sizeof(int)*N*N);
  for (long i=0;i<(long)N*N;i++) a[i]=i&1;
  double t0=now_s(); volatile long long s1=sum_row(a);
  double t1=now_s(); volatile long long s2=sum_col(a);
  double t2=now_s();
  printf("row=%.3fs col=%.3fs diff=%lld\n", t1-t0, t2-t1, (long long)(s1-s2));
  free(a);
}
```

### stride/working-set 스윕(캐시·TLB 계단)

```c
// stride_bench.c
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

static double now(){ struct timespec t; clock_gettime(CLOCK_MONOTONIC,&t);
  return t.tv_sec + t.tv_nsec*1e-9; }

int main(){
  const size_t MAX = 256*1024*1024; // 256MB
  int *a = aligned_alloc(64, MAX);
  size_t N = MAX/sizeof(int);
  for (size_t i=0;i<N;i++) a[i]=i;

  for (size_t ws=8*1024; ws<=MAX; ws*=2) {           // working set
    for (size_t stride=64; stride<=8192; stride*=2){ // 바이트
      size_t step = stride/sizeof(int);
      size_t n = ws/sizeof(int);
      volatile int s=0;
      double t0=now();
      for (int rep=0; rep<16; ++rep)
        for (size_t i=0;i<n; i+=step) s += a[i];
      double t1=now();
      double iters = (double)16*(n/step);
      printf("ws=%zuKB stride=%zuB ns/iter=%.2f\n", ws/1024, stride, 1e9*(t1-t0)/iters);
    }
  }
  free(a);
}
```
- **ns/iter** 상승 지점이 **L1/L2/LLC/TLB 임계**로 보이는 “계단”을 만든다.

### 프리패치 거리 스윕

```c
float sum_pf(const float *a, int n, int dist){
  float s=0;
  for (int i=0;i<n;i++){
    __builtin_prefetch(&a[i+dist], 0, 1);
    s += a[i];
  }
  return s;
}
```
- 셸 스크립트로 `dist ∈ {16,32,64,128}` 반복 실행 → 중앙값 비교.

### 폴스 셰어링 데모(OpenMP)

```c
// false_sharing.c
#include <stdio.h>
#include <omp.h>

typedef struct { _Alignas(64) long v; } padded_long;
#ifndef T
#define T 8
#endif

padded_long cnt[T];

int main(){
  long iters = 100000000;
  double t0 = omp_get_wtime();
  #pragma omp parallel num_threads(T)
  {
    int tid = omp_get_thread_num();
    for (long i=0;i<iters;i++) cnt[tid].v++;  // 각자 다른 라인 → 충돌 적음
  }
  double t1 = omp_get_wtime();
  printf("no-sharing: %.3fs total=%ld\n", t1-t0, (long)(cnt[0].v+cnt[1].v));
}
```
- 패딩 제거 후 동일 실험을 해보면 성능 급락(코히어런시 트래픽↑)을 관찰 가능.

---

## 셸/시스템 레시피

```sh
# 고정 클럭(가능 시), governor=performance

sudo cpupower frequency-set -g performance
# 코어 고정

taskset -c 4 ./app
# NUMA 바인딩

numactl --cpunodebind=0 --membind=0 ./app
# perf 계수

perf stat -e cycles,instructions,IPC,\
L1-dcache-loads,L1-dcache-load-misses,\
LLC-loads,LLC-load-misses,dTLB-load-misses \
./app
# STREAM 빌드/실행(예)

gcc -O3 -fopenmp stream.c -o stream && OMP_NUM_THREADS=8 ./stream
```

---

## 자주 쓰는 최적화 패턴 요약

- **행 순회(Stride=1)**, **타일링**(L1/L2 용량 기반), **레지스터 누적**
- **SoA/AoSoA**, **정렬(aligned_alloc)**, **패딩**(세트 충돌·폴스 셰어링 회피)
- **스칼라 치환/불변 호이스팅/Dead store 제거**
- **프리패치**(거리 스윕), **비휘발 저장**(스트리밍 결과)
- **NUMA first-touch** + 스레드/데이터 바인딩
- **Huge Page**로 TLB miss 감축(워크로드별 on/off 측정)

---

## 바로 적용 체크리스트

- [ ] **행-주도** 순서 및 **스트라이드 1**인가?
- [ ] **타일링** 블록 크기를 후보(32~256)로 **스윕**해 최적점 찾았는가?
- [ ] **스칼라 치환/불변 호이스팅**으로 중복 로드를 제거했는가?
- [ ] **SoA/AoSoA + 정렬/패딩**으로 벡터화·충돌을 도왔는가?
- [ ] **프리패치 거리**를 2~4개 후보로 **측정 기반** 선정했는가?
- [ ] **Huge Page/NUMA first-touch** 적용으로 TLB/원격 접근을 줄였는가?
- [ ] **폴스 셰어링** 없는가(라인 패딩/샤딩/큐 분리)?
- [ ] `perf`/프로파일로 **IPC/미스율/대역** 개선을 수치로 확인했는가?
- [ ] 결과를 **중앙값+표준편차(또는 신뢰구간)** 로 보고했는가?
- [ ] 코드/빌드/환경을 **재현성 있게 문서화**했는가?

---

## 맺음말

메모리 성능은 **지연·대역·로컬리티**의 삼각형 위에서 결정된다.
가장 강력한 레버는 **데이터·순회·타일**이다. 그 다음이 **TLB/NUMA/코히어런시**와 같은 시스템적 제어다.
본 문서의 모델(AMAT/CPI/MLP)과 랩 템플릿, 체크리스트를 그대로 적용해
**측정→수정→재측정** 루프를 돌리면, 워크로드별 최적 지점을 빠르게 찾아낼 수 있다.
