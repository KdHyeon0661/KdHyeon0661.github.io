---
layout: post
title: Docker - exec vs attach
date: 2025-01-28 20:20:23 +0900
category: Docker
---
# Docker 네트워크 심화: macvlan과 host 연결, Overlay와 Consul 서비스 디스커버리

Docker 네트워킹은 컨테이너 기반 애플리케이션의 아키텍처 설계에 있어 핵심적인 요소입니다. 이번 가이드에서는 실제 운영 환경에서 자주 마주치는 두 가지 고급 네트워킹 시나리오를 깊이 있게 다루어 보겠습니다. 첫 번째는 macvlan을 사용하여 컨테이너를 물리 네트워크에 직접 노출시키는 방법과 호스트와의 통신 문제 해결, 두 번째는 overlay 네트워크와 Consul을 결합한 서비스 디스커버리 구축에 대해 알아보겠습니다.

## Macvlan: 컨테이너를 물리 네트워크에 직접 노출하기

### Macvlan의 기본 개념과 활용 시나리오

Macvlan은 Docker 컨테이너에 고유한 MAC 주소와 IP 주소를 부여하여 물리 네트워크(L2 계층)에 직접 연결할 수 있게 해주는 네트워크 드라이버입니다. 이 방식은 컨테이너를 외부 장비에서 실제 물리 호스트처럼 인식하게 만들어 줍니다.

**주요 활용 사례:**
- IP 카메라, IoT 장비와 같은 네트워크 장비 에뮬레이션
- 기존 네트워크 인프라와의 완전한 통합 필요 시
- VLAN 기반의 네트워크 분리 환경
- 특정 서비스에 공인 IP 직접 할당 필요 시

### Macvlan의 기본적인 제약 사항 이해하기

많은 사용자들이 macvlan을 처음 사용할 때 겪는 가장 큰 혼란은 동일한 물리 NIC(예: eth0)를 사용하는 호스트와 macvlan 컨테이너 간에 직접적인 통신이 불가능하다는 점입니다. 이는 macvlan의 설계적 특성으로, 커널 수준에서 호스트의 상위 인터페이스와 macvlan 하위 인터페이스 간의 프레임 전달을 허용하지 않기 때문입니다.

이러한 제약은 보안과 네트워크 격리 관점에서는 장점이 될 수 있지만, 호스트에서 실행되는 모니터링 에이전트나 관리 도구가 컨테이너와 통신해야 하는 경우 문제가 됩니다.

### 호스트와 Macvlan 컨테이너 간 통신 문제의 올바른 해결 방법

호스트와 macvlan 컨테이너가 통신할 수 있도록 하려면 호스트 측에도 macvlan 인터페이스를 생성하여 동일한 서브넷에 연결하는 방법을 사용해야 합니다.

#### 단계별 구현 가이드

**1. Macvlan 네트워크 생성**

```bash
docker network create -d macvlan \
  --subnet=192.168.1.0/24 \
  --gateway=192.168.1.1 \
  -o parent=eth0 \
  lan-macvlan
```

이 명령은 `eth0` 인터페이스를 기반으로 하는 macvlan 네트워크를 생성합니다. 서브넷은 `192.168.1.0/24`, 게이트웨이는 `192.168.1.1`로 설정됩니다.

**2. Macvlan 컨테이너 실행**

```bash
docker run -d --name nginx-container \
  --network lan-macvlan \
  --ip 192.168.1.100 \
  nginx:alpine
```

정적 IP 주소 `192.168.1.100`을 할당하여 컨테이너를 실행합니다. 이 IP 주소는 물리 네트워크에서 직접 접근 가능합니다.

**3. 호스트에 Macvlan 인터페이스 생성**

```bash
# 호스트에서 root 권한으로 실행
sudo ip link add macvlan-host link eth0 type macvlan mode bridge
sudo ip addr add 192.168.1.240/24 dev macvlan-host
sudo ip link set macvlan-host up
```

이 단계가 가장 중요합니다. 호스트에 `macvlan-host`라는 macvlan 인터페이스를 생성하고, 컨테이너와 동일한 서브넷(`192.168.1.0/24`)에 IP 주소를 할당합니다.

**4. 통신 확인**

```bash
# 호스트에서 컨테이너로 통신 테스트
ping -c 3 192.168.1.100

# 컨테이너에서 호스트로 통신 테스트
docker exec -it nginx-container sh -c "ping -c 3 192.168.1.240"
```

이제 호스트와 macvlan 컨테이너가 서로 통신할 수 있습니다. 외부 클라이언트는 `192.168.1.100:80`으로 직접 접근할 수 있습니다.

### VLAN을 이용한 네트워크 분리

프로덕션 환경에서는 VLAN을 사용하여 네트워크를 논리적으로 분리하는 것이 일반적입니다. Docker macvlan도 VLAN 서브인터페이스를 parent로 사용할 수 있습니다.

```bash
# 호스트에 VLAN 인터페이스 생성
sudo ip link add link eth0 name eth0.30 type vlan id 30
sudo ip link set eth0.30 up

# VLAN 기반 macvlan 네트워크 생성
docker network create -d macvlan \
  --subnet=192.168.30.0/24 \
  --gateway=192.168.30.1 \
  -o parent=eth0.30 \
  vlan30-macvlan
```

이렇게 구성하면 VLAN 30에 속하는 macvlan 네트워크가 생성됩니다. 호스트의 기본 네트워크(eth0)와 VLAN 30 네트워크는 L3 라우팅을 통해 통신하게 됩니다.

### Docker Compose와의 통합

Macvlan 네트워크를 Docker Compose와 함께 사용할 때는 네트워크를 외부에서 먼저 생성한 후 참조하는 패턴을 사용하는 것이 안정적입니다.

**외부 네트워크 생성:**
```bash
docker network create -d macvlan \
  --subnet=192.168.1.0/24 \
  --gateway=192.168.1.1 \
  -o parent=eth0 \
  production-macvlan
```

**docker-compose.yml:**
```yaml
version: "3.9"
services:
  web-server:
    image: nginx:alpine
    networks:
      production:
        ipv4_address: 192.168.1.101

networks:
  production:
    external: true
    name: production-macvlan
```

### 운영 시 고려사항

**보안 고려사항:**
- Macvlan 컨테이너는 물리 네트워크상의 실체 IP를 가지므로, 방화벽, IDS/IPS, NAC(Network Access Control) 정책의 관리 대상에 포함되어야 합니다.
- 불필요한 서비스 포트는 노출하지 않고, 최소 권한 원칙을 준수해야 합니다.

**성능 최적화:**
- Macvlan은 캡슐화 오버헤드가 없어 일반적으로 물리 네트워크의 MTU(Maximum Transmission Unit) 값을 그대로 사용할 수 있습니다.
- 대부분의 이더넷 환경에서 MTU는 1500바이트입니다.

**Docker Desktop 사용자 참고:**
- Macvlan은 리눅스 커널 기능에 의존하므로, Docker Desktop for Mac/Windows에서는 제한적으로 지원됩니다.
- 프로덕션 환경에서는 리눅스 호스트를 사용하는 것이 권장됩니다.

## Overlay 네트워크와 Consul을 이용한 서비스 디스커버리

### Overlay 네트워크의 기본 개념

Overlay 네트워크는 여러 Docker 호스트에 걸쳐 가상 네트워크를 생성하는 기술로, VXLAN(Virtual Extensible LAN) 프로토콜을 기반으로 합니다. 이는 Docker Swarm 모드와 통합되어 있어, Swarm 클러스터 내에서 서비스 간 통신을 용이하게 합니다.

### Consul을 이용한 서비스 디스커버리

Docker Swarm의 내장 DNS는 기본적인 서비스 디스커버리를 제공하지만, 더 복잡한 시나리오나 Swarm 외부 환경에서의 서비스 디스커버리가 필요할 때는 Consul과 같은 전문적인 서비스 디스커버리 도구가 유용합니다.

**Consul의 주요 기능:**
- 서비스 디스커버리: `*.service.consul` 도메인을 통한 서비스 조회
- 상태 확인: 정기적인 헬스체크를 통한 불량 인스턴스 감지
- Key/Value 저장소: 환경 설정과 구성 데이터 관리
- 멀티 데이터센터 지원: 지리적으로 분산된 환경에서의 서비스 연동

### Overlay 네트워크와 Consul 통합 아키텍처

#### 1. Docker Swarm 클러스터 초기화 및 Overlay 네트워크 생성

```bash
# Docker Swarm 초기화
docker swarm init

# Attachable 옵션을 가진 overlay 네트워크 생성
docker network create -d overlay --attachable app-overlay-network
```

`--attachable` 옵션은 Swarm 서비스뿐만 아니라 일반 컨테이너도 이 네트워크에 연결할 수 있게 해줍니다.

#### 2. Consul 클러스터 배포

**consul-stack.yml:**
```yaml
version: "3.9"
services:
  consul-server:
    image: hashicorp/consul:1.18
    command: agent -server -bootstrap-expect=1 -ui -client=0.0.0.0 -bind=0.0.0.0
    ports:
      - "8500:8500"   # 웹 UI 및 HTTP API
      - "8600:8600/tcp" # DNS (TCP)
      - "8600:8600/udp" # DNS (UDP)
    networks:
      - app-overlay-network
    volumes:
      - consul-data:/consul/data
    deploy:
      placement:
        constraints:
          - node.role == manager

volumes:
  consul-data:

networks:
  app-overlay-network:
    external: true
    name: app-overlay-network
```

Consul 스택 배포:
```bash
docker stack deploy -c consul-stack.yml consul
```

#### 3. 서비스 등록을 위한 Registrator 설정

Registrator는 Docker 이벤트를 감지하여 컨테이너를 Consul에 자동으로 등록하고 제거하는 도구입니다.

**registrator-service.yml:**
```yaml
version: "3.9"
services:
  registrator:
    image: gliderlabs/registrator:latest
    command: -internal -cleanup consul://consul-server:8500
    volumes:
      - /var/run/docker.sock:/tmp/docker.sock
    networks:
      - app-overlay-network
    deploy:
      mode: global  # 모든 노드에 배포

networks:
  app-overlay-network:
    external: true
    name: app-overlay-network
```

#### 4. 애플리케이션 서비스 배포

**application-stack.yml:**
```yaml
version: "3.9"
services:
  web-service:
    image: nginx:alpine
    networks:
      - app-overlay-network
    environment:
      - SERVICE_NAME=web-service
      - SERVICE_TAGS=public
    deploy:
      replicas: 3
      restart_policy:
        condition: any
      update_config:
        parallelism: 1
        delay: 10s

  api-service:
    image: hashicorp/http-echo
    command: -text="API Service Response"
    networks:
      - app-overlay-network
    environment:
      - SERVICE_NAME=api-service
      - SERVICE_TAGS=internal
    deploy:
      replicas: 2
      restart_policy:
        condition: any

networks:
  app-overlay-network:
    external: true
    name: app-overlay-network
```

애플리케이션 배포:
```bash
docker stack deploy -c registrator-service.yml registrator
docker stack deploy -c application-stack.yml applications
```

### 서비스 디스커버리 테스트

Consul의 DNS 인터페이스를 통해 서비스를 조회할 수 있습니다:

```bash
# 임시 컨테이너에서 DNS 쿼리 테스트
docker run --rm --network app-overlay-network \
  alpine sh -c "apk add bind-tools && dig web-service.service.consul"

# 특정 서비스의 모든 인스턴스 조회
docker run --rm --network app-overlay-network \
  alpine sh -c "apk add bind-tools && dig web-service.service.consul ANY"
```

Consul 웹 UI는 `http://<manager-node-ip>:8500`에서 접근할 수 있습니다.

### Consul Connect를 이용한 서비스 간 보안 통신

Consul Connect는 서비스 간 통신에 mTLS(mutual TLS)를 적용하여 제로 트러스트(Zero Trust) 보안 모델을 구현할 수 있게 해줍니다.

**connect-enabled-service.yml:**
```yaml
version: "3.9"
services:
  frontend:
    image: myapp/frontend:latest
    networks:
      - app-overlay-network
    environment:
      - SERVICE_NAME=frontend
      - CONSUL_HTTP_ADDR=consul-server:8500
    configs:
      - source: frontend-service-config
        target: /etc/consul/service.hcl

  backend:
    image: myapp/backend:latest
    networks:
      - app-overlay-network
    environment:
      - SERVICE_NAME=backend
      - CONSUL_HTTP_ADDR=consul-server:8500
    configs:
      - source: backend-service-config
        target: /etc/consul/service.hcl

configs:
  frontend-service-config:
    content: |
      service {
        name = "frontend"
        port = 8080
        connect {
          sidecar_service {}
        }
      }

  backend-service-config:
    content: |
      service {
        name = "backend"
        port = 9000
        connect {
          sidecar_service {
            proxy {
              upstreams = [
                {
                  destination_name = "database"
                  local_bind_port = 5432
                }
              ]
            }
          }
        }
      }

networks:
  app-overlay-network:
    external: true
    name: app-overlay-network
```

## 보안 및 운영 모범 사례

### 네트워크 보안 강화

**컨테이너 수준 보안:**
```yaml
services:
  secured-service:
    image: nginx:alpine
    user: "1000:1000"  # 비루트 사용자
    read_only: true  # 읽기 전용 루트 파일 시스템
    tmpfs:
      - /tmp
      - /run
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
```

**Overlay 네트워크 암호화:**
```bash
docker network create -d overlay \
  --opt encrypted \
  --attachable \
  secure-overlay
```

### 성능 최적화

**MTU 튜닝:**
Overlay 네트워크는 VXLAN 캡슐화로 인해 일반적으로 50바이트 정도의 오버헤드가 발생합니다. 따라서 유효 MTU는 다음과 같이 계산할 수 있습니다:

```
유효_MTU ≈ 물리적_MTU - 50
```

예를 들어, 물리적 MTU가 1500바이트인 경우 overlay 네트워크의 유효 MTU는 약 1450바이트가 됩니다. 애플리케이션에서 이 값을 고려하여 패킷 크기를 조정해야 합니다.

MTU 테스트:
```bash
# 패킷 단편화 테스트
ping -M do -s 1472 192.168.1.1
```

### 모니터링과 문제 해결

**네트워크 진단 도구:**
```bash
# 네트워크 상태 확인
docker network ls
docker network inspect <network-name>

# Netshoot 컨테이너를 이용한 진단
docker run --rm -it --network <network-name> nicolaka/netshoot sh
```

**Consul 상태 확인:**
```bash
# Consul 멤버십 확인
docker exec $(docker ps -q -f name=consul) consul members

# 서비스 상태 확인
curl http://localhost:8500/v1/catalog/services
```

## 문제 해결 가이드

### 일반적인 문제와 해결 방법

**호스트와 Macvlan 컨테이너 간 통신 불가:**
- 증상: 호스트에서 macvlan 컨테이너로 ping이 전달되지 않음
- 원인: Macvlan의 설계적 특성
- 해결: 호스트에 별도의 macvlan 인터페이스를 생성하고 동일 서브넷에 IP 할당

**Overlay 네트워크 성능 저하:**
- 증상: 네트워크 지연 증가, 패킷 손실
- 원인: MTU 단편화, 네트워크 과부하
- 해결: 애플리케이션 MTU 조정, 네트워크 대역폭 모니터링

**Consul 서비스 등록 실패:**
- 증상: 서비스가 Consul에 등록되지 않음
- 원인: Registrator 구성 오류, 네트워크 연결 문제
- 해결: Registrator 로그 확인, Consul 연결 테스트

### 진단 명령어 모음

```bash
# 네트워크 인터페이스 확인
ip link show
ip addr show

# 라우팅 테이블 확인
ip route show

# ARP 테이블 확인
ip neigh show

# DNS 해결 테스트
nslookup web-service.service.consul 127.0.0.1 -port=8600

# 네트워크 연결 테스트
nc -zv <host> <port>
```

## 실제 구현 예제

### 완전한 Macvlan 구현 예제

```bash
#!/bin/bash
# macvlan-setup.sh

# 네트워크 구성 변수
SUBNET="192.168.1.0/24"
GATEWAY="192.168.1.1"
PARENT_INTERFACE="eth0"
MACVLAN_NETWORK="production-macvlan"

# 1. Macvlan 네트워크 생성
echo "Creating macvlan network..."
docker network create -d macvlan \
  --subnet=$SUBNET \
  --gateway=$GATEWAY \
  -o parent=$PARENT_INTERFACE \
  $MACVLAN_NETWORK

# 2. 호스트 Macvlan 인터페이스 생성
echo "Creating host macvlan interface..."
sudo ip link add macvlan-host link $PARENT_INTERFACE type macvlan mode bridge
sudo ip addr add 192.168.1.254/24 dev macvlan-host
sudo ip link set macvlan-host up

# 3. 컨테이너 실행
echo "Starting containers..."
docker run -d --name web-app \
  --network $MACVLAN_NETWORK \
  --ip 192.168.1.100 \
  nginx:alpine

docker run -d --name api-service \
  --network $MACVLAN_NETWORK \
  --ip 192.168.1.101 \
  hashicorp/http-echo -text="API Service"

# 4. 통신 테스트
echo "Testing connectivity..."
ping -c 2 192.168.1.100
ping -c 2 192.168.1.101

echo "Setup completed."
```

### Consul 기반 서비스 메시 구현

```yaml
# service-mesh.yml
version: "3.9"

services:
  # Consul 서버
  consul-server:
    image: hashicorp/consul:1.18
    command: agent -server -bootstrap-expect=1 -ui -client=0.0.0.0 -bind=0.0.0.0
    ports:
      - "8500:8500"
      - "8600:8600/tcp"
      - "8600:8600/udp"
    volumes:
      - consul-data:/consul/data
    networks:
      - service-mesh

  # Registrator
  registrator:
    image: gliderlabs/registrator:latest
    command: -internal -cleanup -deregister on-success consul://consul-server:8500
    volumes:
      - /var/run/docker.sock:/tmp/docker.sock
    depends_on:
      - consul-server
    networks:
      - service-mesh
    deploy:
      mode: global

  # 프론트엔드 서비스
  frontend:
    image: nginx:alpine
    environment:
      - SERVICE_NAME=frontend
      - SERVICE_TAGS=public,web
    ports:
      - "8080:80"
    networks:
      - service-mesh
    deploy:
      replicas: 2

  # 백엔드 API 서비스
  backend:
    image: hashicorp/http-echo
    command: -text="Backend Service"
    environment:
      - SERVICE_NAME=backend
      - SERVICE_TAGS=internal,api
    networks:
      - service-mesh
    deploy:
      replicas: 2

volumes:
  consul-data:

networks:
  service-mesh:
    driver: overlay
    attachable: true
```

## 결론

Docker 네트워킹은 컨테이너 기반 애플리케이션 아키텍처의 핵심 요소로, 올바른 네트워크 설계는 시스템의 성능, 보안, 유지보수성을 크게 향상시킵니다.

**Macvlan을 통한 물리 네트워크 통합**은 컨테이너를 기존 네트워크 인프라에 완전히 통합해야 할 때 가장 효과적인 솔루션입니다. 호스트와의 통신 문제는 별도의 macvlan 인터페이스 생성으로 해결할 수 있으며, VLAN을 활용하면 네트워크 논리적 분리를 구현할 수 있습니다.

**Overlay 네트워크와 Consul의 조합**은 분산 시스템에서의 서비스 디스커버리와 통신 관리에 강력한 플랫폼을 제공합니다. Docker Swarm의 내장 기능을 넘어서는 유연성과 고급 기능이 필요할 때, Consul은 검증된 솔루션으로 자리매김하고 있습니다.

운영 환경에서는 보안을 최우선으로 고려해야 합니다. 컨테이너의 최소 권한 원칙 준수, 네트워크 트래픽 암호화, 정기적인 보안 감사는 필수적입니다. 성능 측면에서는 MTU 튜닝과 네트워크 모니터링을 통해 최적의 사용자 경험을 제공할 수 있습니다.

이러한 네트워킹 기술들을 프로젝트의 요구사항에 맞게 조합하고, Docker Compose나 Stack 파일을 통해 인프라를 코드로 관리하면 재현성 높고 관리하기 쉬운 시스템을 구축할 수 있습니다. 각 기술의 특성을 이해하고 적절히 활용함으로써 현대적인 마이크로서비스 아키텍처의 네트워킹 요구사항을 효과적으로 충족시킬 수 있습니다.
