---
layout: post
title: 기계학습 - 고객 세분화
date: 2025-08-20 20:25:23 +0900
category: 기계학습
---
# — 비지도 학습

## 세분화의 목적과 성공 기준

### 목적(비즈니스 연결)

- **타깃팅/개인화**: 온보딩·추천·혜택·메시지의 **적합도** 극대화
- **가치 극대화**: LTV/ARPU 향상, 업셀·크로스셀 기회 발굴, 이탈 감소
- **제품 전략**: 핵심 사용 패턴과 페인포인트가 다른 **고객군별 니즈** 파악
- **운영 효율**: 지원/품질 자원 배분(프리미엄 케어 vs 셀프서비스)

### 성공 기준

- **구분성**: 군집 간 **행동/가치 분포가 통계적으로 유의** (예: Kruskal–Wallis)
- **규모·기여**: 각 세그가 충분한 크기·매출·성장성 보유
- **실행 가능성**: 세그 → **캠페인/정책**으로 바로 연결 가능
- **안정성/재현성**: 재표본·시간이 지나도 **분류 일관성**(안정성 지표 ↑)
- **KPI Lift**: 전환·리텐션·NPS·ARPU 등 **사후 검증 상승**이 존재

---

## 데이터 설계와 피처 엔지니어링

### 소스와 관찰 창(window)

- **거래/과금**: 주문·환불·할인·마진
- **사용/행동**: 세션 수·체류·기능 사용 다양성·기기/채널
- **CRM/캠페인**: 노출·클릭·구매·쿠폰 반응
- **지원/품질**: 티켓·처리시간·불만율·반품
- **메타**: 지역·유입 채널·가입일·플랜·구독 상태

> **아이덴티티 통합**(ID resolution) 후, 예: “최근 **180일** 피처로 세분화, 이후 **30일** 성과 검증”처럼 **관찰/검증 창**을 명시하세요.

### 대표 피처 묶음(예시)

- **RFM**: Recency(최종활동-오늘), Frequency(활동/구매 횟수), Monetary(매출/마진)
- **참여도**: DAU/WAU, 세션 빈도·평균 체류·기능 다양성(고유 이벤트 수)
- **구매 행태**: 카테고리 분포(TF–IDF), 가격 민감도(할인 비중/탄력), 반품률
- **고객가치**: LTV(또는 6/12개월 매출), 마진 기반 가치
- **여정/상태**: 온보딩 완료 여부, 플랜, 남은 기간, 이탈 시그널
- **품질/지원**: CS 건수·지연·불만율
- **생애주기 보정**: **가입 후 경과일/활동기간으로 정규화**(tenure bias 제거)

### 전처리 원칙

- 결측: 비즈니스 의미로 대치(0/“미사용”), 그렇지 않으면 `median/most_frequent`
- 스케일: 거리 기반 알고리즘은 **StandardScaler/RobustScaler**
- 분포: 왜도 큰 지표는 `log1p`/Yeo–Johnson
- 범주형: 원-핫(상위 N + “기타”), 혼합형은 **K-Prototypes/Gower**
- 이상치: 윈저라이징/클리핑(상·하위 1–2%)
- **누수 방지**: 모든 변환은 **Train에서 `fit` → Val/Test/배치에서 `transform`**

---

## 알고리즘 선택 가이드(강점/유의점)

| 알고리즘 | 거리/가정 | 강점 | 유의점/언제 쓰나 |
|---|---|---|---|
| **K-Means / MiniBatchKMeans** | 유클리드, 구형 | 빠름·확장성·해석 쉬움 | K 필요, 스케일/이상치 민감 |
| **GMM (Gaussian Mixture)** | 타원형, 확률 | **소프트 할당**(확률)·BIC로 K | 공분산 불안정·초기화 민감 |
| **계층적(워드)** | SSE 최소 병합 | **덴드로그램**·소규모 | O(n²) 비용, 커팅 높이 결정 |
| **DBSCAN/HDBSCAN** | 밀도 | 임의모양·노이즈 분리 | ε/MinPts 튜닝, 밀도 차이 문제 |
| **스펙트럴** | 그래프 라플라시안 | 비선형 구조 | kNN·스케일 파라 민감 |
| **K-Prototypes/Gower** | 혼합형(수치+범주) | 실무 친화 | 추가 라이브러리 필요 |
| **AE + KMeans** | 잠재공간 | 고차원 복잡 패턴 | 학습·재현성 관리 필요 |

> **대규모/스트리밍**: MiniBatchKMeans, HDBSCAN, Spark KMeans.
> **시계열**: HMM/시퀀스 클러스터(고급).

---

## K(군집 수)와 내부 평가

### 내부 지표

- **실루엣(Silhouette)** \(s(i)\):
  $$
  a(i)=\text{동일 군집 평균거리},\quad b(i)=\text{가장 가까운 타군집 평균거리},\quad
  s(i)=\frac{b(i)-a(i)}{\max\{a(i),b(i)\}}\in[-1,1].
  $$
- **Davies–Bouldin(DB)** (↓ 좋음):
  $$
  \mathrm{DB}=\frac{1}{K}\sum_{i}\max_{j\ne i}\frac{S_i+S_j}{M_{ij}}
  $$
  (\(S_i\): 군집 i 내 분산, \(M_{ij}\): 중심 거리)
- **Calinski–Harabasz(CH)** (↑ 좋음)

### K 선택·안정성

- **엘보**(SSE/관성 감소 완만화 지점), **Gap Statistic**:
  $$
  \mathrm{Gap}(K)=\mathbb{E}_{\text{ref}}\{\log W_K\}-\log W_K
  $$
- **GMM의 BIC/AIC**(통계 기준):
  $$
  \mathrm{BIC}=-2\log L + p\log n
  $$
- **안정성**: 부트스트랩 재학습 후 **ARI/Jaccard**로 라벨 일관성 측정

---

## 외부 검증(운영 관점)

- 세그별 **리텐션/전환/ARPU/NPS** 분포 차이(검정: ANOVA/Kruskal–Wallis)
- **A/B**로 타깃 전략의 **Lift** 검증(유의성·효과크기)
- **해석 가능성**: 프로파일링 KPI가 **명확히 구분**되어야 실행 가능

---

## 시각화·해석

- **PCA(50) → t-SNE/UMAP(2D)**로 **형태** 파악(거리 해석은 신중)
- 세그 **레이더/막대**로 피처 분위수 비교, **텍스트 프로파일** 자동 생성
- **중요 피처**: 각 세그 vs 전체의 표준화 차이(Z-score)

---

## — **RFM 기반 K-Means**

> 예제 가정: `transactions.csv` [customer_id, order_id, order_date, amount]

```python
import pandas as pd, numpy as np
from sklearn.preprocessing import StandardScaler, FunctionTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.metrics import silhouette_score
from datetime import timedelta

# 데이터 로드 & RFM 피처

tx = pd.read_csv("transactions.csv", parse_dates=["order_date"])
ref_date = tx["order_date"].max() + timedelta(days=1)

rfm = (tx.groupby("customer_id")
         .agg(last=("order_date","max"),
              F=("order_id","nunique"),
              M=("amount","sum"))
         .assign(R=lambda d: (ref_date - d["last"]).dt.days)
         .drop(columns=["last"])
         .reset_index()[["customer_id","R","F","M"]])

# 전처리 + KMeans 파이프라인

pipe = Pipeline([
    ("log", FunctionTransformer(lambda X: np.log1p(X))),   # 왜도 완화
    ("scaler", StandardScaler()),
    ("km", KMeans(n_clusters=5, n_init="auto", random_state=42))
])

X = rfm[["R","F","M"]].values
pipe.fit(X)
rfm["segment"] = pipe.named_steps["km"].labels_

# K 튜닝(실루엣 + 최소 군집 크기 제약)

def select_k(X, ks=range(2,10), min_size_ratio=0.03):
    best=(None,-1,None)
    for k in ks:
        km = Pipeline([
            ("log", FunctionTransformer(lambda X: np.log1p(X))),
            ("scaler", StandardScaler()),
            ("km", KMeans(n_clusters=k, n_init="auto", random_state=42))
        ]).fit(X)
        labels = km.named_steps["km"].labels_
        # 최소 군집 비율 제약(너무 작은 세그 방지)
        sizes = pd.Series(labels).value_counts(normalize=True)
        if (sizes < min_size_ratio).any():
            continue
        Z = km.named_steps["scaler"].transform(np.log1p(X))
        sil = silhouette_score(Z, labels)
        if sil > best[1]:
            best=(k, sil, km)
    return best

k, sil, km_best = select_k(X)
if k:
    rfm["segment"] = km_best.named_steps["km"].labels_

# 세그 프로파일(중앙값/합계)

profile = (rfm.groupby("segment")
           .agg(n=("customer_id","nunique"),
                R_median=("R","median"),
                F_median=("F","median"),
                M_median=("M","median"),
                M_sum=("M","sum"))
           .sort_values("M_sum", ascending=False))
print(profile)

# 새 고객 실시간 스코어링

def score_new(customers_df):  # columns ["R","F","M"]
    return pipe.predict(customers_df[["R","F","M"]].values)

# 모델/파이프라인 저장(배치/서빙)

import joblib
joblib.dump(pipe, "rfm_kmeans.pkl")
```

---

## **혼합형 데이터** 세분화 — K-Prototypes / Gower

```python
# !pip install kmodes gower

import pandas as pd, numpy as np
from kmodes.kprototypes import KPrototypes
import gower
from sklearn.cluster import AgglomerativeClustering

# 예: 수치[R,F,M], 범주[gender, region, device]

df = pd.read_csv("customer_features.csv")
X = df[["R","F","M","gender","region","device"]]
cat_idx = [3,4,5]

# K-Prototypes

kproto = KPrototypes(n_clusters=6, init='Huang', n_init=5, random_state=42)
labels_kp = kproto.fit_predict(X, categorical=cat_idx)
df["segment_kp"] = labels_kp

# Gower + 계층적(소규모 데이터에 적합)

D = gower.gower_matrix(X)  # 거리행렬
clu = AgglomerativeClustering(n_clusters=6, affinity="precomputed", linkage="average")
df["segment_gower"] = clu.fit_predict(D)
```

---

## **GMM**으로 소프트 세분화(확률 기반)

```python
import numpy as np, pandas as pd
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import silhouette_score

Xr = np.log1p(rfm[["R","F","M"]].values)
sc = StandardScaler().fit(Xr)
Xz = sc.transform(Xr)

best = (None, np.inf, None)  # (k, BIC, model)
for k in range(2,10):
    gmm = GaussianMixture(n_components=k, covariance_type="full", reg_covar=1e-6, random_state=42)
    gmm.fit(Xz)
    bic = gmm.bic(Xz)
    if bic < best[1]:
        best = (k, bic, gmm)

k, bic, gmm = best
proba = gmm.predict_proba(Xz)          # 각 세그 확률
rfm["segment_gmm"] = proba.argmax(1)
rfm["confidence"] = proba.max(1)       # 낮은 고객은 ‘경계 고객’으로 취급 가능
```

---

## — 노이즈 분리/임의 모양

```python
# !pip install hdbscan

import numpy as np, pandas as pd, hdbscan
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

Xn = StandardScaler().fit_transform(np.log1p(rfm[["R","F","M"]].values))
Y2 = PCA(n_components=10, random_state=42).fit_transform(Xn)  # 고차원 노이즈 축소

clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=10, cluster_selection_epsilon=0.0)
labels = clusterer.fit_predict(Y2)
rfm["segment_hdb"] = labels     # -1은 노이즈(이상치/무정형)
rfm["stability"] = clusterer.probabilities_
```

---

## 통계 검정·안정성·해석 자동화

### 군집 간 분포 차이 검정(예: Kruskal–Wallis)

```python
from scipy.stats import kruskal
features = ["R","F","M"]
for f in features:
    samples = [rfm[rfm["segment"]==s][f].values for s in sorted(rfm["segment"].unique())]
    stat, p = kruskal(*samples)
    print(f, "p-value:", p)
```
p-value가 작으면 세그 간 **해당 피처 분포가 유의하게 다름**.

### 부트스트랩 안정성(Adjusted Rand Index)

```python
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
rng = np.random.RandomState(42)

def bootstrap_ari(X, k=5, B=20, sample_ratio=0.8):
    aris=[]
    for _ in range(B):
        idx = rng.choice(len(X), int(sample_ratio*len(X)), replace=True)
        km = KMeans(n_clusters=k, n_init="auto", random_state=rng.randint(0,99999)).fit(X[idx])
        labels_sub = km.labels_
        # 원본 전체에 재할당(최근접 중심)
        centers = km.cluster_centers_
        assign = np.argmin(((X[:,None,:]-centers[None,:,:])**2).sum(2), axis=1)
        aris.append(adjusted_rand_score(assign, pipe.named_steps["km"].labels_))
    return np.mean(aris), np.std(aris)

Xz = pipe.named_steps["scaler"].transform(np.log1p(X))
print("Bootstrap ARI mean,std:", bootstrap_ari(Xz, k=pipe.named_steps["km"].n_clusters))
```

---

## 캠페인·액션 디자인(세그별 전략 템플릿)

### 프로파일링 리포트 항목

- **규모/가치**: n, 매출·마진 합/중앙값, ARPU, LTV
- **행동**: R/F/M 중앙값·분위수, 기능 다양성, 채널/기기 분포
- **마케팅 반응**: 오픈/클릭/전환, 쿠폰 사용
- **리스크**: 이탈 확률, CS 이슈율, 반품율
- **라벨링 예**
  - S0: **고가치 충성**(R↓, F↑, M↑)
  - S1: **할인 민감 탐색형**(F↑, 할인 반응↑)
  - S2: **잠재 고가치 온보딩**(가입 초기·사용↑)
  - S3: **휴면 복귀 후보**(R↑, F↓)
  - S4: **지원 이슈 多**(CS↑, NPS↓)

### 액션 예시

- S0: 멤버십/얼리액세스, 추천 리워드
- S1: 번들/쿠폰/가격 앵커링, 유사 상품 추천
- S2: 기능 온보딩·튜토리얼·가이드 메일
- S3: 카트 회수·재참여 리마인더·복귀 혜택 A/B
- S4: VIP 상담/우선 라우팅, 리팩토링/UX 개선 티켓 연계

> **루프**: 가설 → 타깃 액션 → A/B → Lift 측정 → 지식 축적.

---

## 운영·거버넌스(MLOps)

- **재학습 주기**: 월/분기(사업 속도에 맞춤), **MiniBatch**로 신규 고객 스코어
- **드리프트 감지**: 피처 분포(PSI), 실루엣/세그 비중 변화, KPI 저하
- **세그 라벨 일치**: 재학습 후 **헝가리안 매칭**으로 이전 라벨과 대응
  - 비용행렬: 교차 분포의 **음수 상호정보량** 또는 1−Jaccard
- **버전·재현성**: 데이터 snapshot, 코드·파라미터·시드, 아티팩트 저장
- **공정성·프라이버시**: 민감 속성 최소화, 동의/삭제권, 익명화, 목적 외 사용 금지

---

## 흔한 함정과 해결

| 함정 | 증상 | 해결책 |
|---|---|---|
| 스케일 미적용 | K-Means가 엉뚱한 축으로 분할 | **표준화/로그 변환** |
| K 과다/과소 | 미니세그/혼합세그 다수 | 내부지표 + **최소 세그 크기** 제약 |
| 이상치 영향 | 경계 왜곡·센터 튐 | 윈저라이징·로버스트 스케일·DBSCAN |
| 텐뉴어 바이어스 | 오래된 고객만 “우수” | **기간 대비 정규화** |
| 해석 불가 | 실행 연결 실패 | 프로파일링·라벨링·액션 템플릿 |
| 일회성 분석 | 시간이 지나 무의미 | 정기 재학습·드리프트 모니터링 |

---

## 체크리스트

- [ ] 관찰/검증 창 명시(누수 금지)
- [ ] 전처리 파이프라인(결측·로그·스케일·인코딩) 코드화
- [ ] 알고리즘·K 결정(내부지표+비즈니스 제약)
- [ ] 세그 **프로파일링 리포트** 생성 자동화
- [ ] 액션/캠페인 설계 → A/B → Lift 측정
- [ ] 재현성(버전/시드/스냅샷)·모니터링·재학습 계획

---

## 부록 — 선택 수식 모음

- **Silhouette**:
  $$
  s(i)=\frac{b(i)-a(i)}{\max\{a(i),b(i)\}}
  $$
- **Davies–Bouldin**:
  $$
  \mathrm{DB}=\frac{1}{K}\sum_{i}\max_{j\ne i}\frac{S_i+S_j}{M_{ij}}
  $$
- **Calinski–Harabasz**:
  $$
  \mathrm{CH}=\frac{\text{between-cluster dispersion} /(K-1)}
  {\text{within-cluster dispersion} /(n-K)}
  $$
- **Gap Statistic**:
  $$
  \mathrm{Gap}(K)=\mathbb{E}_{\text{ref}}\{\log W_K\}-\log W_K
  $$
- **GMM-BIC**:
  $$
  \mathrm{BIC}=-2\log L + p\log n
  $$
- **Adjusted Rand Index**(요지): 라벨 일치의 확률 보정 지표(−1~1)

---

## 요약

- **탄탄한 피처(RFM·행동·가치) + 적절 전처리**가 성패를 좌우
- **K-Means/GMM/HDBSCAN** 등 문제 특성에 맞는 알고리즘 선택
- **내부지표 + 안정성 + 비즈니스 Lift**로 다각도 평가
- **프로파일링→액션→A/B→학습**의 반복으로 **성과**에 연결
- **운영·거버넌스**(드리프트, 라벨 매핑, 프라이버시)까지 고려해야 **장기적 성과**가 유지됨
