---
layout: post
title: 기계학습 - 고객 세분화
date: 2025-08-20 20:25:23 +0900
category: 기계학습
---
# 고객 세분화 - 비지도 학습으로 만드는 실행 가능한 세그먼트


## 1. 세분화의 목적과 성공 기준

### 1.1 세분화는 왜 하는가? (비즈니스 연결)

고객 세분화는 **“데이터 기반 페르소나 설계”**로 볼 수 있다.  
결국 목표는 **“각기 다른 고객군에 서로 다른 의사결정을 하겠다”**는 선언이다.

기본 목적을 다시 정리하면 다음과 같다.

#### 목적(비즈니스 연결)

- **타깃팅/개인화**
  - 온보딩·추천·혜택·메시지의 **적합도** 극대화
  - 같은 쿠폰이라도 세그마다 타이밍·금액·채널을 달리 설계
- **가치 극대화**
  - LTV/ARPU 향상
  - 업셀·크로스셀 기회 발굴
  - 이탈(Churn) 감소
- **제품 전략**
  - 핵심 사용 패턴과 페인포인트가 다른 **고객군별 니즈** 파악
  - “헤비 유저 vs 초보자”, “모바일 중심 vs 데스크톱 중심” 등
- **운영 효율**
  - 지원/품질 자원 배분(프리미엄 케어 vs 셀프 서비스)
  - 고비용 세그에 대해 프로세스 재설계·자동화 우선순위 결정

즉, **“어떤 고객에게 어떤 액션을 다르게 할 것인가?”** 가 없다면  
세분화는 **분석 놀이**에 불과하다.

### 1.2 성공 기준: 좋은 세그먼트란 무엇인가?

#### 성공 기준

- **구분성**
  - 군집 간 **행동/가치 분포가 통계적으로 유의**하게 다르다.
  - 예: 각 세그의 매출 분포에 대해 Kruskal–Wallis 검정
- **규모·기여**
  - 각 세그가 충분한 **크기**와 **매출/성장 잠재력**을 가진다.
  - 너무 작은 “1% 미만 세그”가 여러 개 생기면 운영이 힘들다.
- **실행 가능성**
  - 세그 → **캠페인/정책/제품 기능**으로 바로 연결 가능해야 한다.
  - “설명은 되지만 아무것도 못 하는 세그”는 나쁜 세그다.
- **안정성/재현성**
  - 재표본·시간이 지나도 **분류가 크게 흔들리지 않는다.**
  - 안정성 지표(Adjusted Rand Index 등)가 일정 수준 이상
- **KPI Lift**
  - 세그에 기반한 타깃 액션이
  - 전환·리텐션·NPS·ARPU 등에서 **실질적인 상승**을 보이는지  
    (A/B 테스트, 홀드아웃 검증 등으로 사후 확인)

> 요약  
> 좋은 세그는 **“통계적으로 구분되고, 비즈니스에서 쓸 수 있고, 시간이 지나도 버티고, 실제 성과를 올리는 세그”**다.

---

## 2. 데이터 설계와 피처 엔지니어링

세분화의 품질은 **알고리즘**보다 **데이터/피처 설계**가 지배한다.  
여기서 한 번 삐끗하면 이후 단계에서 아무리 고급 모델을 써도 효과가 없다.

### 2.1 데이터 소스와 관찰 창(window)

#### 소스와 관찰 창(window)

대표적인 데이터 소스는 다음과 같다.

- **거래/과금**
  - 주문·환불·할인·마진
  - 예: 주문 테이블(주문ID, 고객ID, 주문일, 금액, 마진, 쿠폰ID…)
- **사용/행동**
  - 세션 수·체류시간·기능 사용 다양성·기기/채널
  - 예: 로그 테이블(고객ID, 이벤트타입, 타임스탬프, 기기)
- **CRM/캠페인**
  - 노출·클릭·구매·쿠폰 반응
- **지원/품질**
  - 티켓 수·처리시간·불만율·반품 사유
- **메타 정보**
  - 지역(국가/도시), 유입 채널, 가입일, 플랜 타입, 구독 상태 등

> **아이덴티티 통합(ID resolution)**  
> 여러 시스템(웹/앱/콜센터/오프라인 POS 등)의 ID를  
> **하나의 고객ID로 통합**해야 한다.  
> 가능한 한 “최근 **180일** 데이터를 관찰 창으로 사용하고,  
> 이후 **30일/60일**을 성과 검증 창으로 잡는다”처럼  
> **관찰/검증 창**을 명시해야 한다.

**예시**

- 관찰 창: 2024-01-01 ~ 2024-06-30 (180일)
- 검증 창: 2024-07-01 ~ 2024-07-31 (30일)
- 세분화: 관찰 창 데이터를 바탕으로 군집화
- 검증: 각 세그의 **2024-07월 리텐션/매출** 차이를 평가

### 2.2 대표 피처 묶음(예시)

#### RFM 기반 피처

- **R(Recency)**: 최종 활동일/구매일로부터 경과일
- **F(Frequency)**: 활동/구매 횟수
- **M(Monetary)**: 매출 또는 마진 합계

#### 참여도(Engagement)

- DAU/WAU, 월간 세션 수
- 평균 체류시간, 세션당 이벤트 수
- 기능 사용 다양성(고유 이벤트 타입 수)
- 기기/채널 다양성(모바일/웹, 이메일/푸시 등)

#### 구매 행태

- 카테고리 분포(TF–IDF, 비율)
- 가격 민감도(할인 비중, 쿠폰 의존도)
- 반품률(반품 주문 수 / 전체 주문 수)
- 브랜드 선호 지수(특정 브랜드 매출 비중)

#### 고객가치

- 과거 **6/12개월 매출** 또는 **마진 기반 LTV 추정치**
- 월평균 결제 금액
- ARPU, ARPPU(유료 고객만)

#### 여정/상태

- 온보딩 완료 여부(튜토리얼/프로필/결제수단 등록 등)
- 플랜 타입(무료/유료/프리미엄)
- 구독 상태(활성/일시중지/해지)
- 이탈 시그널(마지막 N일간 활동 없음 등)

#### 품질/지원

- CS 티켓 수, 이슈 유형(결제/품질/배송 등)
- 평균 처리시간, 재문의율
- 불만/부정 피드백 비율

#### 생애주기 보정

- **가입 후 경과일/활동기간으로 정규화**
- 예: “6개월 된 고객의 10회 구매” vs “1개월 된 고객의 10회 구매”
  - 후자가 훨씬 더 공격적 패턴 → **기간 대비 빈도**가 중요

### 2.3 전처리 원칙

#### 전처리 원칙

- **결측치 처리**
  - 의미 있는 0/“미사용”으로 대치 가능한 경우 그렇게 처리
  - 그렇지 않으면 `median`/`most_frequent` 등
- **스케일링**
  - 거리 기반 알고리즘(K-Means, GMM 등)은  
    **StandardScaler/RobustScaler** 사용
- **분포 변환**
  - 왜도가 큰 매출/세션 수 등은 `log1p`/Yeo–Johnson
- **범주형 처리**
  - 원-핫 인코딩(상위 N + “기타”)
  - 혼합형(수치+범주)은 **K-Prototypes/Gower 거리** 고려
- **이상치 처리**
  - 상/하위 1–2% 윈저라이징 또는 클리핑
- **누수 방지**
  - 모든 변환은 **Train에서 `fit` → Val/Test/배치에서 `transform`**
  - 세분화 모델도 관찰 창 데이터로만 학습, 검증 창은 사후 평가용

---

## 3. 알고리즘 선택 가이드(강점/유의점)

여기서는 **대표 군집 알고리즘들의 특성·장단점**을 정리한다.

### 3.1 알고리즘 비교 표

| 알고리즘 | 거리/가정 | 강점 | 유의점/언제 쓰나 |
|---|---|---|---|
| **K-Means / MiniBatchKMeans** | 유클리드 거리, 구형 군집 가정 | 빠름·확장성 좋음·해석 쉬움(중심) | 군집 수 K 필요, 스케일/이상치 민감 |
| **GMM (Gaussian Mixture)** | 타원형 분포, 가우시안 가정 | **소프트 할당**(개인별 군집 확률), BIC로 K 선택 | 공분산 행렬 불안정, 초기화 민감 |
| **계층적 군집(워드 링크 등)** | SSE 최소 병합 | **덴드로그램**으로 계층 구조 시각화, 소규모 데이터 | O(n²) 비용, 컷팅 높이 결정 필요 |
| **DBSCAN/HDBSCAN** | 밀도 기반 | 임의 모양 군집·노이즈 분리, K 불필요 | ε/MinPts 튜닝 민감, 밀도 차이 큰 경우 어려움 |
| **스펙트럴 군집화** | 그래프 라플라시안 기반 | 복잡/비선형 구조 캡처 | kNN 그래프·스케일 파라미터 튜닝 필요 |
| **K-Prototypes / Gower** | 수치+범주 혼합 | 실무형 CRM 데이터에 적합 | 추가 라이브러리, 거리해석 고려 필요 |
| **AE + K-Means** | 잠재 공간(Representation) | 고차원 복잡 패턴 학습 가능 | 신경망 학습 비용·재현성 관리 필요 |

> **대규모/스트리밍**  
> - MiniBatchKMeans, HDBSCAN, 분산 환경(Spark KMeans 등)  
> **시계열·경로 기반 세분화**  
> - HMM, 시퀀스 클러스터링, 마르코프 체인 기반 모델 등 고급 기법 고려

### 3.2 K-Means 기반 세분화의 수학적 직관

K-Means는 **군집 내 제곱합 최소화** 문제를 푸는 알고리즘이다.

- 데이터: $$x_1, \dots, x_n \in \mathbb{R}^d$$
- 군집 수: $$K$$
- 목표:

  $$
  \min_{\{C_k\}} \sum_{k=1}^K \sum_{x_i \in C_k} \|x_i - \mu_k\|^2
  $$

  여기서 $$\mu_k$$ 는 군집 $$C_k$$ 의 중심(평균)이다.

**해석**

- 각 군집은 “구형 클라우드”로 모델링된다.
- 중심 $$\mu_k$$ 는 세그먼트를 요약하는 대표 포인트.

실무상:

- **초기화**: K-Means++로 초기 중심 선택
- **반복**: 할당(각 포인트 → 가장 가까운 중심), 중심 재계산

---

## 4. K(군집 수)와 내부 평가

### 4.1 내부 지표

내부 지표는 **라벨(정답)** 없이 군집 구조를 평가하는 지표다.

#### 실루엣(Silhouette)

각 포인트 $$i$$ 에 대해

- $$a(i)$$ : 동일 군집 내 다른 포인트들과의 **평균 거리**
- $$b(i)$$ : 포인트 $$i$$ 가 속하지 않은 군집들 중  
  가장 가까운 군집의 평균 거리

정의:

$$
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}} \in [-1, 1].
$$

- 1에 가까울수록: 잘 분리된 군집
- 0 근처: 경계에 위치
- 음수: 다른 군집에 속하는 것이 더 나은 포인트

#### Davies–Bouldin(DB) 지수

군집 $$i$$ 의 평균 반지름 $$S_i$$, 중심 간 거리 $$M_{ij}$$ 라고 하면

$$
\mathrm{DB} = \frac{1}{K}\sum_{i}\max_{j\ne i}\frac{S_i + S_j}{M_{ij}}
$$

- 작을수록(↓) 좋은 군집 구조

#### Calinski–Harabasz(CH) 지수

Between-cluster vs Within-cluster 분산 비율을 기반으로 한다.

$$
\mathrm{CH} = 
\frac{\text{between-cluster dispersion} /(K-1)}
{\text{within-cluster dispersion} /(n-K)}
$$

- 클수록(↑) 좋은 구조

### 4.2 K 선택·안정성

#### 엘보(Elbow) 방법

- K에 따른 SSE(관성) 감소를 그려서  
  **기울기가 완만해지는 지점**을 찾는다.

#### Gap Statistic

참조 분포(예: 동일 범위에서 균일 샘플링)와 비교:

$$
\mathrm{Gap}(K) = \mathbb{E}_{\text{ref}}\{\log W_K\} - \log W_K
$$

- $$W_K$$ : K 군집에서의 군집 내 제곱합
- Gap이 충분히 크고, 이후 증가량이 작아지는 K 선택

#### GMM의 BIC/AIC

GMM에서는 모델 우도와 파라미터 수를 고려하는 BIC를 사용:

$$
\mathrm{BIC} = -2\log L + p\log n
$$

- $$L$$ : 데이터에 대한 최대우도
- $$p$$ : 파라미터 수
- $$n$$ : 샘플 수
- BIC가 가장 작은 K를 후보로 선택

#### 안정성 평가

- 같은 K에 대해 **재표본(부트스트랩)** 후 군집화를 반복
- 라벨 일관성을 **Adjusted Rand Index(ARI)**, Jaccard 등으로 측정
- ARI 평균이 높고 분산이 작을수록 안정적

---

## 5. 외부 검증(운영 관점)

세분화의 진짜 평가는 **운영·비즈니스 지표**로 한다.

#### 외부 검증 지표

- 세그별 **리텐션/전환/ARPU/NPS** 분포 차이 확인
  - 예: 각 세그의 30일 리텐션 비교
  - ANOVA, Kruskal–Wallis 등으로 통계적 유의성 검정
- **A/B 테스트**로 타깃 전략의 **Lift** 검증
  - 세그별 다른 오퍼 → Control vs Treatment 비교
- **해석 가능성**
  - 세그별 프로파일(KPI, 행동 패턴)이 **명확히 구분**되어야  
    실제 현업 팀이 액션을 설계할 수 있다.

---

## 6. 시각화·해석

시각화는 **복잡한 고차원 분포**를  
사람이 이해할 수 있는 수준으로 요약해 주는 역할을 한다.

#### 시각화·해석 방법

- **PCA(예: 50차원) → t-SNE/UMAP(2D)**  
  - 고차원 구조를 2D로 시각화
  - **군집 간 상대적 위치/형태** 파악
  - 단, 거리 해석은 신중(투영 과정에서 왜곡)
- 세그별 **레이더/막대 그래프**
  - 주요 피처 분위수(예: 25/50/75%) 비교
- **텍스트 프로파일 자동 생성**
  - “S0: 최근 활동 빈도가 높고, 매출이 상위 10%이며, CS 이슈가 적은 고가치 충성 고객”
- **중요 피처 추출**
  - 각 세그 vs 전체에서 표준화 차이(Z-score) 기반 중요도 산출

---

## 7. RFM 기반 K-Means: 엔드 투 엔드 예제

여기서는 단일 테이블 `transactions.csv`가 있다고 가정한다.

> 예제 가정: `transactions.csv` 컬럼  
> `[customer_id, order_id, order_date, amount]`  
> - 북미/유럽권 이커머스의 주문 데이터라고 상정

### 7.1 RFM 피처 생성 + KMeans 파이프라인

```python
import pandas as pd, numpy as np
from sklearn.preprocessing import StandardScaler, FunctionTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.metrics import silhouette_score
from datetime import timedelta

# 1. 데이터 로드 & RFM 피처 생성

tx = pd.read_csv("transactions.csv", parse_dates=["order_date"])

# 세분화 기준일: 마지막 주문 다음날
ref_date = tx["order_date"].max() + timedelta(days=1)

rfm = (
    tx.groupby("customer_id")
      .agg(
          last=("order_date", "max"),        # 마지막 주문일
          F=("order_id", "nunique"),         # 주문 횟수
          M=("amount", "sum"),               # 총 구매 금액
      )
      .assign(R=lambda d: (ref_date - d["last"]).dt.days)  # Recency (일)
      .drop(columns=["last"])
      .reset_index()[["customer_id", "R", "F", "M"]]
)

print(rfm.head())

# 2. 전처리 + KMeans 파이프라인 정의
#    - log1p로 왜도 완화
#    - StandardScaler로 표준화

pipe = Pipeline([
    ("log", FunctionTransformer(lambda X: np.log1p(X))),   # (R, F, M)에 log1p 적용
    ("scaler", StandardScaler()),
    ("km", KMeans(n_clusters=5, n_init="auto", random_state=42))
])

X = rfm[["R", "F", "M"]].values
pipe.fit(X)

rfm["segment"] = pipe.named_steps["km"].labels_
print(rfm["segment"].value_counts())
```

### 7.2 K 자동 선택(실루엣 + 최소 군집 크기 제약)

```python
def select_k(X, ks=range(2, 10), min_size_ratio=0.03):
    """
    ks 중에서 실루엣 기준으로 가장 좋은 K를 선택.
    단, 최소 군집 크기 비율(min_size_ratio)보다 작은 군집이 있으면 제외.
    """
    best = (None, -1, None)  # (K, silhouette, model)
    for k in ks:
        km_pipe = Pipeline([
            ("log", FunctionTransformer(lambda X: np.log1p(X))),
            ("scaler", StandardScaler()),
            ("km", KMeans(n_clusters=k, n_init="auto", random_state=42))
        ]).fit(X)

        labels = km_pipe.named_steps["km"].labels_
        sizes = pd.Series(labels).value_counts(normalize=True)

        # 너무 작은 세그가 있는지 체크
        if (sizes < min_size_ratio).any():
            continue

        Z = km_pipe.named_steps["scaler"].transform(np.log1p(X))
        sil = silhouette_score(Z, labels)
        if sil > best[1]:
            best = (k, sil, km_pipe)

    return best

k, sil, km_best = select_k(X)
print("Selected K:", k, "Silhouette:", sil)

if k:
    rfm["segment"] = km_best.named_steps["km"].labels_
```

### 7.3 세그 프로파일 요약

```python
profile = (
    rfm.groupby("segment")
       .agg(
           n=("customer_id", "nunique"),
           R_median=("R", "median"),
           F_median=("F", "median"),
           M_median=("M", "median"),
           M_sum=("M", "sum")
       )
       .sort_values("M_sum", ascending=False)
)

print(profile)
```

여기서 얻은 프로파일을 해석하면 예를 들어:

- S0: F↑, M↑, R↓ → 최근에 자주 많이 구매하는 **고가치 충성 고객**
- S1: F↑, M↓, R↑ → 과거에 자주 사다가 최근 활동이 줄어든 **이탈 위험 고객**
- S2: F↓, M↓, R↑ → 가입만 하고 거의 사용하지 않은 **저활성 고객**

### 7.4 새 고객 실시간 스코어링

```python
def score_new(customers_df):
    """
    customers_df: ['R','F','M'] 컬럼을 가진 DataFrame
    반환: segment 라벨 배열
    """
    return pipe.predict(customers_df[["R", "F", "M"]].values)

# 예시
new_customers = pd.DataFrame(
    {"R": [3, 60], "F": [5, 1], "M": [300.0, 20.0]},
    index=["cust_A", "cust_B"]
)
print(score_new(new_customers))
```

### 7.5 모델/파이프라인 저장

```python
import joblib
joblib.dump(pipe, "rfm_kmeans.pkl")
```

---

## 8. 혼합형 데이터 세분화 — K-Prototypes / Gower

실제 CRM 데이터는 **수치 + 범주**가 섞인 경우가 많다.  
예: `[R, F, M, gender, region, device]` 등.

이럴 때 단순 원-핫 + K-Means로 가는 대신,  
혼합형 전용 알고리즘을 쓰는 것이 깔끔할 수 있다.

### 8.1 K-Prototypes 예제

```python
# !pip install kmodes

import pandas as pd, numpy as np
from kmodes.kprototypes import KPrototypes

df = pd.read_csv("customer_features.csv")

# 예: 컬럼 구성
# [customer_id, R, F, M, gender, region, device]

X = df[["R", "F", "M", "gender", "region", "device"]]

# 범주형 컬럼 인덱스(0부터): 3,4,5
cat_idx = [3, 4, 5]

kproto = KPrototypes(
    n_clusters=6,
    init='Huang',
    n_init=5,
    max_iter=50,
    random_state=42
)

labels_kp = kproto.fit_predict(X, categorical=cat_idx)
df["segment_kp"] = labels_kp

print(df["segment_kp"].value_counts())
```

### 8.2 Gower 거리 + 계층적 군집(소규모 데이터)

```python
# !pip install gower

import gower
from sklearn.cluster import AgglomerativeClustering

X_mix = df[["R", "F", "M", "gender", "region", "device"]]

# 1. Gower 거리 행렬 계산 (O(n²) → 수천 명 이하에 적합)
D = gower.gower_matrix(X_mix)

# 2. 거리 기반 계층적 군집
clu = AgglomerativeClustering(
    n_clusters=6,
    affinity="precomputed",
    linkage="average"
)

df["segment_gower"] = clu.fit_predict(D)
print(df["segment_gower"].value_counts())
```

---

## 9. GMM으로 소프트 세분화(확률 기반)

K-Means는 **하드 할당**(각 고객은 딱 하나의 세그)에 속한다.  
GMM(Gaussian Mixture Model)은 **“각 세그일 확률”**을 함께 제공한다.

### 9.1 GMM 학습 + BIC로 K 선택

```python
import numpy as np, pandas as pd
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# RFM 기반
Xr = np.log1p(rfm[["R", "F", "M"]].values)
sc = StandardScaler().fit(Xr)
Xz = sc.transform(Xr)

best = (None, np.inf, None)  # (K, BIC, model)

for k in range(2, 10):
    gmm = GaussianMixture(
        n_components=k,
        covariance_type="full",
        reg_covar=1e-6,
        random_state=42
    )
    gmm.fit(Xz)
    bic = gmm.bic(Xz)
    print("k=", k, "BIC=", bic)
    if bic < best[1]:
        best = (k, bic, gmm)

k, bic, gmm = best
print("Selected K (GMM):", k, "BIC:", bic)

proba = gmm.predict_proba(Xz)        # 각 세그에 속할 사후 확률
rfm["segment_gmm"] = proba.argmax(1) # 가장 확률이 큰 세그
rfm["confidence"] = proba.max(1)     # 최대 확률 → 세그 소속 확신도
```

- `confidence`가 낮은 고객은 “경계 고객”으로 보고  
  타깃에서 제외하거나 별도 처리할 수 있다.

---

## 10. 노이즈 분리/임의 모양 군집 — HDBSCAN

DBSCAN/HDBSCAN은 **밀도 기반 군집**으로  
“임의 모양” 군집과 노이즈 분리를 잘해 준다.

### 10.1 HDBSCAN 기본 예제

```python
# !pip install hdbscan

import numpy as np, pandas as pd, hdbscan
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1. 스케일링 + 차원 축소
Xn = StandardScaler().fit_transform(np.log1p(rfm[["R","F","M"]].values))
Y2 = PCA(n_components=10, random_state=42).fit_transform(Xn)  # 노이즈 축소

# 2. HDBSCAN 군집화
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=50,
    min_samples=10,
    cluster_selection_epsilon=0.0
)

labels = clusterer.fit_predict(Y2)
rfm["segment_hdb"] = labels         # -1은 노이즈
rfm["stability"] = clusterer.probabilities_

print(rfm["segment_hdb"].value_counts())
```

- 라벨 `-1`: 어떤 군집에도 속하지 않는 **노이즈/이상치**
- `stability`: 해당 포인트가 자기 군집에 속하는 **신뢰도**

---

## 11. 통계 검정·안정성·해석 자동화

### 11.1 군집 간 분포 차이 검정 (Kruskal–Wallis)

```python
from scipy.stats import kruskal

features = ["R","F","M"]
segments = sorted(rfm["segment"].unique())

for f in features:
    samples = [rfm[rfm["segment"] == s][f].values for s in segments]
    stat, p = kruskal(*samples)
    print(f"[{f}] Kruskal-Wallis stat={stat:.3f}, p-value={p:.3e}")
```

- p-value가 충분히 작으면  
  “세그 간 해당 피처의 분포가 유의하게 다르다”고 볼 수 있다.

### 11.2 부트스트랩 안정성(Adjusted Rand Index)

```python
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score

rng = np.random.RandomState(42)

def bootstrap_ari(X, base_labels, k=5, B=20, sample_ratio=0.8):
    """
    K-Means 군집 결과의 안정성을 부트스트랩으로 측정.
    base_labels: 기준 군집 라벨 (전체 데이터에 대한 것)
    """
    aris = []
    n = len(X)
    for _ in range(B):
        idx = rng.choice(n, int(sample_ratio*n), replace=True)
        km = KMeans(
            n_clusters=k,
            n_init="auto",
            random_state=rng.randint(0, 99999)
        ).fit(X[idx])
        labels_sub = km.labels_

        # 전체에 대해 '최근접 중심'으로 다시 할당
        centers = km.cluster_centers_
        assign = np.argmin(((X[:, None, :] - centers[None, :, :])**2).sum(2), axis=1)

        aris.append(adjusted_rand_score(assign, base_labels))
    return np.mean(aris), np.std(aris)

# 기준 라벨: 앞에서 만든 pipe의 KMeans 결과
Xz = pipe.named_steps["scaler"].transform(np.log1p(X))
base_labels = pipe.named_steps["km"].labels_

mean_ari, std_ari = bootstrap_ari(Xz, base_labels, k=pipe.named_steps["km"].n_clusters)
print("Bootstrap ARI mean:", mean_ari, "std:", std_ari)
```

- ARI가 0.8 이상, 표준편차가 작다면 **상당히 안정적**인 군집 구조라고 볼 수 있다.

---

## 12. 캠페인·액션 디자인(세그별 전략 템플릿)

세분화의 목적은 결국 **행동(Action)** 이다.  
각 세그에 대해 **무엇을 할 것인가?** 를 명확히 적어야 한다.

### 12.1 프로파일링 리포트 항목

각 세그먼트에 대해 최소 다음 항목을 정리한다.

- **규모/가치**
  - 고객 수(n), 매출·마진 합, ARPU, LTV
- **행동**
  - R/F/M 중앙값·분위수
  - 기능 사용 다양성, 방문 빈도, 채널/기기 분포
- **마케팅 반응**
  - 이메일/푸시 오픈율·클릭률
  - 쿠폰 수신/사용 패턴
- **리스크**
  - 이탈 확률(별도 churn 모델이 있다면)
  - CS 이슈율, 반품율, 환불 비율
- **라벨링 예시**
  - S0: **고가치 충성 고객** (R↓, F↑, M↑)
  - S1: **할인 민감 탐색형 고객** (F↑, 할인 반응↑, 정가 구매↓)
  - S2: **잠재 고가치 온보딩 중 고객** (가입 초기, 사용량↑)
  - S3: **휴면/이탈 위험 고객** (R↑, F↓)
  - S4: **지원 이슈 많음** (CS↑, 불만율↑, NPS↓)

### 12.2 세그별 액션 예시

- **S0: 고가치 충성**
  - 멤버십/로열티 프로그램
  - 신제품/한정판 얼리액세스
  - 추천/친구 초대 리워드
- **S1: 할인 민감 탐색형**
  - 번들/세트 상품 제안
  - 적절 수준의 쿠폰/프로모션 (과도한 할인 의존 방지)
  - 유사 상품/대체 상품 추천
- **S2: 잠재 고가치 온보딩**
  - 기능 온보딩 튜토리얼
  - 첫 구매/첫 핵심 행동 유도 인센티브
  - 성공 사례/사용법 컨텐츠
- **S3: 휴면/이탈 위험**
  - 카트 회수 캠페인
  - 재참여 리마인더/리액티베이션 오퍼
  - 이탈 이유 설문/피드백 수집 후 제품 개선
- **S4: 지원 이슈 많음**
  - VIP 상담/우선 라우팅
  - FAQ/헬프센터 개선
  - 품질/배송/결제 등 특정 이슈에 대한 구조적 해결책 설계

> **운영 루프**  
> 1) 가설(세그 정의 + 액션)  
> 2) 캠페인/정책 실행  
> 3) A/B 또는 Before/After로 결과 측정  
> 4) 잘된 조합은 표준 플레이북으로 승격  
> 5) 세그/피처/액션을 지속적으로 업데이트

---

## 13. 운영·거버넌스(MLOps) 관점

세분화 모델도 결국 **운영되는 ML 시스템**이다.

#### 운영·거버넌스 요소

- **재학습 주기**
  - 월/분기 단위(비즈니스 변화 속도에 맞춤)
  - MiniBatchKMeans로 신규 고객만 서서히 업데이트하는 방식도 가능
- **드리프트 감지**
  - 피처 분포 변화(PSI 등)
  - 실루엣/세그 비중 변화
  - KPI(전환/리텐션) 저하
- **세그 라벨 일치**
  - 재학습 후 세그 번호가 바뀔 수 있으므로  
    **헝가리안 매칭**으로 이전 라벨과 대응
  - 비용 행렬: 교차 분포의 음수 상호정보량 또는 1−Jaccard
- **버전·재현성**
  - 데이터 스냅샷(관찰 창 범위 명시)
  - 코드·파라미터·랜덤 시드
  - 학습된 모델 아티팩트 저장
- **공정성·프라이버시**
  - 민감 속성(예: 특정 법적 보호 카테고리)을 최소한으로 사용
  - 동의/삭제권 등 개인정보 보호 규정을 준수
  - 세분화 목적 외 사용 금지(목적 제한 원칙)

---

## 14. 흔한 함정과 해결책

| 함정 | 증상 | 해결책 |
|---|---|---|
| 스케일링 미적용 | K-Means가 한 두 개의 큰 스케일 피처 기준으로만 분할 | **표준화/로그 변환** 필수 |
| K 과다/과소 | 미니 세그 다수 또는 서로 다른 군집이 섞인 세그 | 내부지표 + **최소 세그 크기** 제약으로 K 선택 |
| 이상치 영향 | 중심이 일부 극단값에 끌림 | 윈저라이징·로버스트 스케일·DBSCAN/HDBSCAN |
| 텐뉴어 바이어스 | 오래된 고객만 “우수 세그”로 보임 | 가입 기간 대비 정규화(기간당 매출/빈도) |
| 해석 불가 세그 | 현업에서 “이걸로 뭘 해야 할지 모르겠다” | 프로파일링·라벨링·액션 템플릿으로 재설계 |
| 일회성 분석 | 몇 개월 지나 세그가 현실과 맞지 않음 | 정기 재학습·드리프트 모니터링 체계 구축 |

---

## 15. 체크리스트

- [ ] 관찰/검증 창을 명시했고, 누수(검증 창 데이터 사용)는 없는가?
- [ ] 전처리 파이프라인(결측·로그·스케일·인코딩)을 코드로 정의했는가?
- [ ] 알고리즘과 K를 내부 지표 + 비즈니스 제약으로 합리적으로 선택했는가?
- [ ] 각 세그의 **프로파일링 리포트**를 만들었는가?
- [ ] 세그별 액션/캠페인 설계 → A/B → Lift 측정 계획이 있는가?
- [ ] 모델 버전/시드/데이터 스냅샷 등 재현성 확보를 했는가?
- [ ] 피처/세그/액션을 주기적으로 점검하는 운영 프로세스가 있는가?

---

## 16. 부록 — 선택 수식 모음

여기서는 위에서 등장한 핵심 수식들을 한 번에 정리한다.

- **Silhouette**:

  $$
  s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
  $$

- **Davies–Bouldin**:

  $$
  \mathrm{DB} = \frac{1}{K}\sum_{i}\max_{j\ne i}\frac{S_i + S_j}{M_{ij}}
  $$

- **Calinski–Harabasz**:

  $$
  \mathrm{CH} =
  \frac{\text{between-cluster dispersion} /(K-1)}
  {\text{within-cluster dispersion} /(n-K)}
  $$

- **Gap Statistic**:

  $$
  \mathrm{Gap}(K)=\mathbb{E}_{\text{ref}}\{\log W_K\}-\log W_K
  $$

- **GMM-BIC**:

  $$
  \mathrm{BIC}=-2\log L + p\log n
  $$

- **Adjusted Rand Index**(개념)
  - 무작위 일치 가능성을 보정한 라벨 일치 지표
  - 범위: $$[-1, 1]$$  
    - 1: 완전 일치  
    - 0: 무작위 수준  
    - <0: 무작위보다 나쁨

---

## 17. 요약

- **목적/성공 기준**
  - 세분화는 “데이터 기반 고객군 → 각기 다른 액션”이 목적이다.
  - 구분성, 규모·기여, 실행 가능성, 안정성, KPI Lift로 평가해야 한다.
- **데이터/피처**
  - RFM, 참여도, 구매 행태, 고객가치, 여정, 품질 지표 등  
    **풍부한 피처**가 세분화의 성패를 좌우한다.
  - 관찰/검증 창을 나누고, 누수/텐뉴어 바이어스를 관리해야 한다.
- **알고리즘**
  - K-Means, GMM, HDBSCAN, K-Prototypes, 스펙트럴 등  
    데이터 특성에 맞는 모델을 선택한다.
  - 내부 지표(실루엣, DB, CH, BIC 등)와 안정성(ARI)으로 K와 알고리즘을 튜닝한다.
- **해석/캠페인**
  - 세그별 프로파일링 리포트 → 명확한 라벨 →  
    액션 템플릿(온보딩, 업셀, 리액티베이션, VIP 케어 등)으로 연결한다.
  - A/B 테스트와 외부 KPI로 **실제 Lift**를 검증해야 한다.
- **운영**
  - 재학습 주기, 드리프트 감지, 세그 라벨 매핑, 버전 관리,  
    프라이버시·공정성까지 포함하는 **MLOps 관점**이 필요하다.

결국 “좋은 세분화”는 **분석과 엔지니어링, 비즈니스 의사결정**이  
한 번에 맞물려 돌아갈 때 만들어진다.  
이 글의 파이프라인을 실제 데이터에 적용하면서,  
자신의 도메인에 맞는 피처/알고리즘/액션 조합을 조금씩 조정해 보길 권한다.