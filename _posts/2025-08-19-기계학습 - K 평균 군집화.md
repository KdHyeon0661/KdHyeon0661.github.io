---
layout: post
title: 기계학습 - K-평균 군집화
date: 2025-08-19 14:25:23 +0900
category: 기계학습
---
# K-평균 군집화(K-Means Clustering)

## 개요

**K-평균(K-Means)**은 \(K\)개의 군집으로 데이터를 나누며, 각 군집의 **평균(centroid)** 에 데이터가 가깝게 모이도록 하는 **거리 기반** 비지도 학습 알고리즘이다.
핵심 특징:
- \(K\)를 **사전에 지정**.
- **유클리드 거리**를 기본으로, **군집 내 제곱합(SSE)** 최소화.
- **반복적(Assignment ↔ Update)** 으로 수렴(=Lloyd 알고리즘).

---

## 알고리즘(라이트한 시작)

### 목적 함수(Objective; Within-Cluster SSE)

$$
J(\{C_k,\mu_k\}_{k=1}^{K}) \;=\; \sum_{k=1}^{K}\sum_{x^{(i)}\in C_k} \left\|x^{(i)}-\mu_k\right\|_2^2
$$
- \(C_k\): \(k\)번째 군집의 데이터 집합
- \(\mu_k\): \(k\)번째 군집 중심점(평균)

### Lloyd의 두 단계 반복

1) **할당(Assignment)**
$$
c^{(i)} \;=\; \arg\min_{k\in\{1,\dots,K\}} \big\|x^{(i)}-\mu_k\big\|_2^2
$$

2) **갱신(Update)**
$$
\mu_k \;=\; \frac{1}{|C_k|}\sum_{x^{(i)}\in C_k} x^{(i)}
$$

3) **정지 조건**: 할당 불변 또는 \( \max_k \|\mu_k^{(t)}-\mu_k^{(t-1)}\| \) 가 임계 이하.

### 의사코드

```text
입력: 데이터 {x^(i)}_(i=1..n), 군집 수 K
초기화: μ_1,...,μ_K (무작위 또는 K-Means++)
반복:
  # Assignment
  C_k ← { x^(i) : k = argmin_j ||x^(i) - μ_j||^2 }
  # Update
  μ_k ← (1/|C_k|) Σ_{x^(i)∈C_k} x^(i),  for all k
수렴 시 종료, {C_k}, {μ_k} 반환
```

---

## 수학적 정당성

### 업데이트가 평균이 되는 이유(최적성)

한 군집 \(C_k\)의 항:
$$
J_k(\mu_k) \;=\; \sum_{x^{(i)}\in C_k} \|x^{(i)} - \mu_k\|_2^2
$$
편미분:
$$
\frac{\partial J_k}{\partial \mu_k} \;=\; 2\sum_{x^{(i)}\in C_k} (\mu_k - x^{(i)}) \;=\; 0
\quad \Rightarrow \quad
\mu_k = \frac{1}{|C_k|}\sum_{x^{(i)}\in C_k} x^{(i)}
$$

### 스케치

- **할당 단계**는 고정된 \(\mu_k\)에서 각 점을 가장 가까운 군집에 배치 → \(J\) **감소/불증가**.
- **갱신 단계**는 고정된 \(C_k\)에서 \(\mu_k\)를 평균으로 → \(J\) **최소**.
- \(J\ge 0\) 이고 유한한 할당 경우의 수 ⇒ **유한 단계 후 수렴**(지역 최적).

### 최적화 난이도

- 전역 최적화는 일반적으로 **NP-난해**(일반 \(d\), 일반 \(K\)).
- 따라서 **초기화**가 중요하며, 좋은 초기화는 더 낮은 \(J\)로 수렴하는 경향.

---

## 초기화: 무작위 vs **K-Means++**

### 무작위 초기화의 문제

- 지역 최적에 빠질 위험, 실루엣/관찰상 불안정.
- 관행: **여러 번 초기화(n_init)** 후 최적 \(J\) 선택.

### K-Means++ 절차(핵심 아이디어)

1) 임의로 한 점을 첫 중심으로 선택.
2) 각 점 \(x\)에 대해 현 중심들까지의 **최소 제곱거리** \(D(x)^2\) 계산.
3) \(D(x)^2\)에 비례한 확률로 다음 중심 샘플링.
4) \(K\)개 뽑을 때까지 반복 → 이후 Lloyd 반복.

- 장점: 중심들이 **멀리 퍼져** 시작 → 수렴 속도/품질 개선(기대 근사 보장).

---

## K 값 선택(모형 선택)

### 엘보(Elbow)

- \(K\) 증가에 따른 \(J\) 또는 스킷런 `inertia_`(=SSE) 감소 곡선에서 **기울기 급변점** 선택.

### 실루엣(Silhouette)

- 각 샘플 \(i\)에 대해
$$
a(i)=\text{같은 군집 내 평균거리},\quad
b(i)=\text{다른 군집까지의 평균거리 중 최소}
$$
$$
s(i)=\frac{b(i)-a(i)}{\max\{a(i),b(i)\}}\in[-1,1]
$$
- 평균 \( \bar{s} \) 가 클수록 좋음(1에 근접).

### CH/DBI (보조 지표)

- **Calinski–Harabasz (CH)**: 군집간 분산/군집내 분산 비율 → **클수록** 좋음.
- **Davies–Bouldin (DBI)**: 군집간 분리도/내부 산포 → **작을수록** 좋음.

---

## 전처리/스케일링/차원축소

- **표준화(Standardization)**:
  각 특징을 평균 0, 표준편차 1로.
  K-Means는 거리 기반이므로 **스케일에 민감**(연봉 vs 키).
- **이상치 처리**: 클리핑, 로버스트 스케일러(IQR), **Trimmed K-Means** 고려.
- **차원 축소**: PCA/Whitening으로 잡음 축 제거 → 더 둥근(등방성) 구조 유도.
- **카테고리/혼합형 데이터**: K-Means **부적합** → k-prototypes/가중 거리로 변형.

---

## 변형: 거리·손실 교체에 따른 친족 알고리즘

| 변형 | 손실/거리 | 중심 | 데이터 유형/특징 |
|---|---|---|---|
| **k-means** | L2(제곱거리) | 평균 | 연속형, 구형 클러스터 |
| **k-medians** | L1(맨해튼) | **중앙값** | 이상치에 강함 |
| **k-medoids (PAM)** | 일반 거리 | **실제 포인트** | 범용/로버스트 |
| **k-modes** | 해밍 등 | 범주별 최빈값 | 순수 범주형 |
| **k-prototypes** | 혼합 거리 | 혼합 중심 | 연속+범주 혼합 |

---

## 복잡도/수렴/엣지 케이스

- **시간 복잡도**: 대략 \(O(n K d \cdot t)\) (샘플 수 \(n\), 차원 \(d\), 반복 \(t\)).
- **수렴 기준**: 할당 불변, 중심 이동 \(\varepsilon\), 반복 상한.
- **빈 군집(Empty Cluster)**:
  - 가장 SSE 큰 군집을 둘로 쪼개기
  - 전체에서 **가장 먼 포인트**를 새 중심으로 리시드
  - K-Means++ 규칙으로 리시드
- **회전/이동/동일 스케일** 불변성: 유클리드 거리는 **평행이동/직교회전/일양 스케일**에 대해 최근접 관계 유지(일양 스케일은 임계값/수치에 주의).
- **특징별 이질 스케일**(cm vs 원)에는 민감 → **표준화 필수**.

---

## 대규모 데이터: Mini-Batch / 스트리밍

- **Mini-Batch K-Means**: 미니배치로 근사 업데이트(중심에 **지수 이동** 혹은 **카운트 기반** 평균 업데이트).
- 수렴 빠르고 메모리 절약, 약간의 근사 오차 허용.
- 스트리밍/온라인: **partial_fit**(scikit-learn `MiniBatchKMeans`)로 지속 업데이트.

---

## GMM/EM과의 관계(이론적 연결)

- 등방성 공분산 \(\sigma^2 I\), 동일 사전확률 가정의 **Gaussian Mixture Model(GMM)** 에서
  EM의 **E-step 책임도**가 **하드할당**(가장 가까운 평균에 1, 나머지 0)으로 수렴하면 K-Means와 동일.
- \(\sigma^2\to 0\) 극한에서 GMM 최대우도 ↔ K-Means SSE 최소화가 일치(직관적 연결).

---

## 파이썬 실습

### 표준 파이프라인 + KMeans++ + 다중 초기화

```python
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# 데이터

X, _ = make_blobs(n_samples=2000, centers=5, cluster_std=[0.5, 1.0, 1.2, 0.6, 0.8],
                  n_features=6, random_state=42)

# 스케일링(매우 중요)

scaler = StandardScaler()
Xs = scaler.fit_transform(X)

# KMeans (K-Means++ 초기화, 여러 시도 중 최적 선택)

k = 5
kmeans = KMeans(n_clusters=k, init="k-means++", n_init=20, max_iter=300, random_state=42)
labels = kmeans.fit_predict(Xs)

print("SSE(inertia):", kmeans.inertia_)
print("Silhouette:", silhouette_score(Xs, labels))
print("Centers shape:", kmeans.cluster_centers_.shape)
```

### 엘보 & 실루엣 곡선으로 K 선택

```python
from sklearn.metrics import silhouette_score

Ks = range(2, 11)
elbow = []
sil = []

for k in Ks:
    km = KMeans(n_clusters=k, init="k-means++", n_init=10, random_state=0)
    y = km.fit_predict(Xs)
    elbow.append(km.inertia_)
    sil.append(silhouette_score(Xs, y))

print("K\tInertia(SSE)\tSilhouette")
for k, e, s in zip(Ks, elbow, sil):
    print(f"{k}\t{e:.1f}\t\t{s:.4f}")
```

### Mini-Batch K-Means(대용량 근사)

```python
from sklearn.cluster import MiniBatchKMeans

mbk = MiniBatchKMeans(n_clusters=5, init="k-means++", batch_size=1024,
                      n_init=5, max_iter=200, random_state=42)
lbl_mbk = mbk.fit_predict(Xs)
print("MiniBatch Inertia:", mbk.inertia_)
```

### 혼합형 데이터 대안(k-prototypes 아이디어 스케치)

> 실제 구현은 `kmodes` 패키지 참고. 여기선 개념적 흐름 예시.

```python
# 연속형 특징 L2, 범주형 특징 해밍거리(불일치 개수)에 가중 합:
# = α * ||x_cont - μ_cont||_2^2 + (1-α) * Hamming(x_cat, mode_cat)
# μ_cont는 연속형 평균, mode_cat은 범주별 최빈값.

```

### Numpy로 from-scratch(학습용; 기본형)

```python
import numpy as np

def kmeans_numpy(X, K, n_init=10, max_iter=100, seed=42):
    rng = np.random.default_rng(seed)
    best_inertia, best_centers, best_labels = np.inf, None, None

    for _ in range(n_init):
        # K-Means++ 초기화(간단 버전)
        centers = [X[rng.integers(0, len(X))]]
        for _k in range(1, K):
            d2 = np.min(((X[:, None, :] - np.array(centers)[None, :, :])**2).sum(axis=2), axis=1)
            probs = d2 / d2.sum()
            centers.append(X[rng.choice(len(X), p=probs)])
        centers = np.array(centers)

        for _ in range(max_iter):
            # Assignment
            dist2 = ((X[:, None, :] - centers[None, :, :])**2).sum(axis=2)  # [n, K]
            labels = dist2.argmin(axis=1)

            # Update
            new_centers = np.array([X[labels == k].mean(axis=0) if np.any(labels == k) else centers[k]
                                    for k in range(K)])

            # Empty cluster 처리(간단): 비어있다면 가장 먼 점으로 교체
            for k in range(K):
                if not np.any(labels == k):
                    far_idx = dist2.max(axis=1).argmax()
                    new_centers[k] = X[far_idx]

            if np.allclose(new_centers, centers):
                centers = new_centers
                break
            centers = new_centers

        inertia = ((X - centers[labels])**2).sum()
        if inertia < best_inertia:
            best_inertia, best_centers, best_labels = inertia, centers.copy(), labels.copy()

    return best_centers, best_labels, best_inertia

# 사용 예시
# Xs: 표준화된 데이터

C, L, SSE = kmeans_numpy(Xs, K=5, n_init=5, max_iter=200, seed=0)
print("from-scratch SSE:", SSE)
```

### 실루엣 시각화(개념 코드)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_samples, silhouette_score

k = 5
km = KMeans(n_clusters=k, init="k-means++", n_init=10, random_state=0).fit(Xs)
labels = km.labels_
sil_vals = silhouette_samples(Xs, labels)
print("Silhouette mean:", sil_vals.mean())

# 간단 막대 플롯(클러스터별 분포 확인)

fig, ax = plt.subplots()
y_lower = 10
for c in range(k):
    vals = np.sort(sil_vals[labels == c])
    y_upper = y_lower + len(vals)
    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, vals)
    ax.text(-0.05, y_lower + 0.5 * len(vals), str(c))
    y_lower = y_upper + 10
ax.axvline(sil_vals.mean(), color="red", linestyle="--")
ax.set_xlabel("Silhouette value")
ax.set_ylabel("Samples")
plt.show()
```

---

## 자주 겪는 문제와 해결책

| 문제 | 증상 | 원인 | 대응 |
|---|---|---|---|
| 스케일 미스매치 | 한 특징이 지배 | 단위/범위 상이 | **표준화/정규화** |
| 빈 군집 | 일부 \(C_k=\varnothing\) | 초기화/구조 | **리시드**, 분할, K-Means++ |
| 지역 최적 | 반복마다 상이한 결과 | 무작위 초기화 | **n_init↑**, K-Means++ |
| 비구형 군집 | 경계에 왜곡 | SSE 가정(구형) | **GMM/DBSCAN/Spectral** 고려 |
| 이상치 민감 | 중심이 튐 | 평균 기반 | **k-medoids/k-medians/trimmed** |
| K 선택 난제 | 엘보 불명확 | 데이터 구조 미약 | **실루엣/CH/DBI**, 도메인 지식 |

---

## 이론 메모: 분산 분해(해석 도움)

총 제곱합(Total SS) \(\mathrm{TSS}\)를 군집 내(Within)와 군집 간(Between)으로:
$$
\mathrm{TSS} = \sum_{i}\|x^{(i)}-\bar{x}\|^2
= \underbrace{\sum_{k}\sum_{x^{(i)}\in C_k}\|x^{(i)}-\mu_k\|^2}_{\mathrm{WSS}=J}
+ \underbrace{\sum_{k}|C_k|\cdot\|\mu_k-\bar{x}\|^2}_{\mathrm{BSS}}
$$
- \(J=\mathrm{WSS}\) 최소화 ↔ \(\mathrm{BSS}\) 최대화(분리도 최대화 관점).

---

## 체크리스트(실전)

1) **스케일링**: `StandardScaler` 기본, 이상치 많으면 `RobustScaler`.
2) **초기화**: `init="k-means++"`, `n_init≥10`, `random_state` 고정.
3) **K 선택**: 엘보+실루엣(+CH/DBI) 병행, 도메인 제약 반영.
4) **진단**: 실루엣 평균, 클러스터 크기 불균형/경계 시각화.
5) **대규모**: `MiniBatchKMeans` + 적절한 `batch_size`.
6) **대안 고려**: 비구형/밀도차 크면 DBSCAN/HDBSCAN/Spectral/GMM.
7) **재현성**: 시드 고정, 데이터 셔플 기록, 전처리 파이프라인화.

---

## 요약

- K-Means는 **SSE 최소화**를 통해 **구형/등분산** 군집을 빠르게 찾는 알고리즘.
- **K-Means++**, **스케일링**, **다중 초기화**, **적절한 K 선택**이 성패를 좌우.
- **Mini-Batch**로 대규모 데이터 대응, **k-medians/medoids**로 로버스트화,
  비구형/밀도 차 문제는 **GMM/DBSCAN/Spectral** 등으로 보완하라.
