---
layout: post
title: 기계학습 - K-평균 군집화
date: 2025-08-19 14:25:23 +0900
category: 기계학습
---
# K-평균 군집화(K-Means Clustering)

## 1. 개요
**K-평균 군집화(K-Means Clustering)**는 비지도학습(unsupervised learning)에서 가장 널리 사용되는 **군집화(Clustering)** 알고리즘입니다.  
데이터를 **K개의 그룹(클러스터)**로 나누어, 각 그룹 내 데이터들이 서로 **가깝게 모이도록** 하는 것이 목표입니다.

K-평균은 다음과 같은 특징을 갖습니다.
- 군집의 개수 \(K\)를 **사전에 지정**해야 함.
- 군집의 중심점(centroid)을 반복적으로 갱신하며 군집화.
- 거리 기반 알고리즘(주로 **유클리드 거리**)을 사용.

---

## 2. 알고리즘 절차

### 2.1 단계별 과정
1. **K개의 중심점 초기화**
   - 데이터 중 임의로 K개의 점을 선택하거나, K-Means++ 알고리즘으로 더 안정적으로 초기화.
   
2. **군집 할당 단계(Assignment step)**
   - 각 데이터 포인트를 **가장 가까운 중심점**에 할당.
   $$
   c^{(i)} = \arg\min_{k \in \{1,\dots,K\}} \| x^{(i)} - \mu_k \|^2
   $$
   여기서:
   - \(x^{(i)}\) : \(i\)번째 데이터
   - \(\mu_k\) : \(k\)번째 군집의 중심점

3. **중심점 갱신 단계(Update step)**
   - 각 군집에 속한 데이터들의 평균을 계산하여 새로운 중심점으로 설정.
   $$
   \mu_k = \frac{1}{|C_k|} \sum_{x^{(i)} \in C_k} x^{(i)}
   $$

4. **수렴 조건 확인**
   - 중심점의 변화가 없거나, 변화량이 일정 기준 이하이면 종료.

---

## 3. 수학적 정의

### 3.1 목적 함수(Objective Function)
K-평균은 **군집 내 제곱합(SSE, Sum of Squared Errors)**을 최소화합니다.

$$
J = \sum_{k=1}^K \sum_{x^{(i)} \in C_k} \| x^{(i)} - \mu_k \|^2
$$

- \(C_k\) : \(k\)번째 군집의 데이터 집합
- \(\mu_k\) : 군집 \(k\)의 중심점
- 목표: \(J\)를 최소화하는 \(\mu_k\)와 데이터 할당 \(C_k\)를 찾기.

---

### 3.2 최적화 과정
- **Assignment step**은 \(\mu_k\)가 고정된 상태에서 \(J\)를 최소화하는 \(C_k\)를 찾는 문제.
- **Update step**은 \(C_k\)가 고정된 상태에서 \(\mu_k\)를 업데이트하는 문제이며, 이때 \(\mu_k\)는 군집의 평균이 최적해임을 증명할 수 있습니다.

#### 증명: 평균이 최적의 중심점임
목적 함수에서 한 군집 \(C_k\)에 대한 오차 항은:
$$
J_k = \sum_{x^{(i)} \in C_k} \| x^{(i)} - \mu_k \|^2
$$
\(\mu_k\)에 대해 편미분:
$$
\frac{\partial J_k}{\partial \mu_k} = \sum_{x^{(i)} \in C_k} 2(\mu_k - x^{(i)}) = 0
$$
따라서:
$$
\mu_k = \frac{1}{|C_k|} \sum_{x^{(i)} \in C_k} x^{(i)}
$$
즉, 군집의 평균이 최적의 중심점입니다.

---

## 4. 장단점

### 장점
- 구현이 간단하고 빠름 (\(O(n \cdot K \cdot t)\), t: 반복 횟수)
- 대규모 데이터셋 처리 가능.
- 해석이 쉬움.

### 단점
- K값을 미리 지정해야 함.
- 초기 중심점에 따라 결과가 달라질 수 있음 → K-Means++로 개선 가능.
- 구형(spherical) 클러스터에 적합하고, 복잡한 모양의 클러스터에는 부적합.
- 이상치(outlier)에 민감.

---

## 5. K값 선택 방법

1. **엘보(Elbow) 방법**
   - K에 따른 SSE 변화를 관찰하고, 감소폭이 완만해지는 지점을 선택.

2. **실루엣(Silhouette) 계수**
   - 군집 내 응집도와 군집 간 분리도를 고려.
   - 값이 1에 가까울수록 좋음.

---

## 6. 파이썬 예제

```python
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 샘플 데이터 생성
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)

# K-Means 모델 생성
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
kmeans.fit(X)

# 결과 예측
y_kmeans = kmeans.predict(X)

# 시각화
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
            s=200, c='red', marker='X')
plt.title("K-Means Clustering")
plt.show()
```

---

## 7. 확장 버전
- **K-Medoids** : 중심점 대신 실제 데이터 포인트를 대표점으로 사용.
- **Mini-Batch K-Means** : 대규모 데이터셋에 효율적인 미니배치 버전.
- **Spectral Clustering** : 복잡한 모양의 군집 탐지 가능.

---

## 📌 요약
- K-평균은 거리 기반 반복 알고리즘으로, 군집의 중심을 평균으로 갱신.
- 목적 함수는 **군집 내 제곱합(SSE)** 최소화.
- 장점: 빠르고 간단, 단점: K값 지정 필요, 이상치 민감.
- K-Means++ 초기화와 엘보/실루엣 계수로 성능 개선 가능.