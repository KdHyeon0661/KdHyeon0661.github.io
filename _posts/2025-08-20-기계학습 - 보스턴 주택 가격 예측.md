---
layout: post
title: ê¸°ê³„í•™ìŠµ - ë³´ìŠ¤í„´ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡
date: 2025-08-20 19:25:23 +0900
category: ê¸°ê³„í•™ìŠµ
---
# ë³´ìŠ¤í„´ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ (íšŒê·€)

## ëª©í‘œì™€ ì‚°ì¶œë¬¼

- **ë¬¸ì œ ìœ í˜•**: íšŒê·€(íƒ€ê¹ƒ `MEDV`, $1,000 ë‹¨ìœ„ ì¤‘ìœ„ê°€ê²©, ìƒí•œ=50)
- **í•µì‹¬ ì‚°ì¶œ**: ëˆ„ìˆ˜ ì—†ëŠ” **íŒŒì´í”„ë¼ì¸ + êµì°¨ê²€ì¦(CV)** ì„±ëŠ¥ ë¦¬í¬íŠ¸, **ì”ì°¨/ì—ëŸ¬ í”„ë¡œíŒŒì¼ë§**, **í•´ì„(ë¶€ë¶„ì˜ì¡´/ì¤‘ìš”ë„)**, **ìœ¤ë¦¬ ì ê²€**, **ë°°í¬ ìŠ¤í¬ë¦½íŠ¸**
- **ê¶Œì¥ ì§€í‘œ**: RMSE(í•´ì„ ìš©ì´), MAE(ì´ìƒì¹˜ì— ê°•ê±´), RÂ²(ë³´ì¡°)

$$
\text{RMSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n(\hat y_i-y_i)^2},\quad
\text{MAE}=\frac{1}{n}\sum_{i=1}^n|\hat y_i-y_i|,\quad
R^2=1-\frac{\sum(\hat y_i-y_i)^2}{\sum(y_i-\bar y)^2}.
$$

---

## ë°ì´í„°Â·í”¼ì²˜ í•œëˆˆì— ë³´ê¸°

| ë³€ìˆ˜ | ì„¤ëª… | íƒ€ì…/íŒíŠ¸ |
|---|---|---|
| CRIM | ë²”ì£„ìœ¨ | ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ â†’ `log1p` í›„ë³´ |
| ZN | 25,000 ftÂ² ì´ìƒ ì£¼ê±°ì§€ ë¹„ìœ¨ | ì—°ì† |
| INDUS | ë¹„ì†Œë§¤ ìƒì—…ì§€ ë¹„ìœ¨ | ì—°ì† |
| **CHAS** | ì°°ìŠ¤ê°• ì¸ì ‘(0/1) | ì´ì§„(ë²”ì£¼) |
| **NOX** | ì¼ì‚°í™”ì§ˆì†Œ ë†ë„ | ì—°ì† |
| **RM** | ì£¼ê±°ë‹¹ í‰ê·  ë°© ìˆ˜ | MEDVì™€ ê°•í•œ ì–‘ì˜ ìƒê´€ |
| AGE | 1940ë…„ ì´ì „ ê±´ì¶• ë¹„ìœ¨ | ì—°ì† |
| **DIS** | ê³ ìš© ì¤‘ì‹¬ì§€ê¹Œì§€ ê±°ë¦¬ | ì—°ì† |
| **RAD** | ê³ ì†ë„ë¡œ ì ‘ê·¼ì„± | **ëª…ëª©í˜• ì·¨ê¸‰** ê¶Œì¥ |
| **TAX** | ì¬ì‚°ì„¸ìœ¨ | ì—°ì† |
| **PTRATIO** | í•™ìƒ/êµì‚¬ ë¹„ | ì—°ì† |
| **B** | ì¸ì¢… ì§€í‘œ(ë¬¸ì œì ) | **ìœ¤ë¦¬ ì´ìŠˆ** |
| **LSTAT** | í•˜ìœ„ê³„ì¸µ ë¹„ìœ¨ | MEDVì™€ ê°•í•œ ìŒì˜ ìƒê´€ |
| **MEDV** | íƒ€ê¹ƒ(ì¤‘ìœ„ê°€ê²©, $1,000s) | **ìƒí•œ 50** â†’ ê²€ì—´ ì£¼ì˜ |

> **ê²€ì—´(Censoring)**: `MEDV`ê°€ 50ìœ¼ë¡œ ìº¡í•‘ë¨ â†’ ìƒí•œ ë¶€ê·¼ ì”ì°¨ ì™œê³¡. ëŒ€ì•ˆ: **ë¡œë²„ìŠ¤íŠ¸/í€€íƒ€ì¼ íšŒê·€**, (ì—°êµ¬ìš©) í† ë¹— ëª¨í˜•.

---

## EDA ì²´í¬ë¦¬ìŠ¤íŠ¸ & ì½”ë“œ ìŠ¤ë‹ˆí«

- ê²°ì¸¡/ì´ìƒì¹˜ ì ê²€(ì›í˜• CSVëŠ” ê²°ì¸¡ ê±°ì˜ ì—†ìŒ)
- ë¶„í¬ ì™œë„: `CRIM`, `LSTAT`, `TAX` â†’ `log1p`/Yeoâ€“Johnson í›„ë³´
- ìƒê´€: `RMâ†‘`, `LSTATâ†“`, `PTRATIOâ†“` â†” `MEDV`
- ì˜í–¥ì : ë ˆë²„ë¦¬ì§€, Cookâ€™s distance(ì°¸ê³ : `statsmodels`)
- íƒ€ê¹ƒ ìƒí•œ(=50) ì§‘ì¤‘ë„ í™•ì¸

```python
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
# seaborn ë“± ì‹œê°í™”ëŠ” ì„ íƒ

df = pd.read_csv("BostonHousing.csv")  # Kaggle ë“± ë™í˜• ìŠ¤í‚¤ë§ˆ ê°€ì •
print(df.describe().T)
print(df.isna().sum())

fig, ax = plt.subplots(1,2, figsize=(10,4))
ax[0].hist(df["MEDV"], bins=30); ax[0].set_title("MEDV distribution (cap@50)")
ax[1].scatter(df["RM"], df["MEDV"], s=8, alpha=0.6); ax[1].set_xlabel("RM"); ax[1].set_ylabel("MEDV")
plt.show()
```

---

## ì „ì²˜ë¦¬Â·íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì „ëµ

### ìˆ˜ì¹˜í˜•

- ê²°ì¸¡: `SimpleImputer(strategy="median")`
- ìŠ¤ì¼€ì¼: `StandardScaler`(ì„ í˜•Â·ì»¤ë„Â·ê±°ë¦¬ê¸°ë°˜ ëª¨ë¸ ìœ ë¦¬)
- ë³€í™˜: **ì¥ê¼¬ë¦¬**(`CRIM`, `LSTAT`, `TAX`) â†’ `log1p` ë˜ëŠ” `PowerTransformer('yeo-johnson')`

### ë²”ì£¼í˜•

- `CHAS`, `RAD` â†’ **One-Hot**(ëª…ëª©í˜•)
- ê³ ì • í¬ê¸° OHE: `handle_unknown="ignore"`

### ìƒí˜¸ì‘ìš©/ë¹„ì„ í˜• í™•ì¥(ì„ í˜• ê°•í™”ë¥¼ ì›í•  ë•Œ)

- ë‹¤í•­: `RM^2`, `LSTAT^2`
- ìƒí˜¸ì‘ìš©: `RMÃ—LSTAT`, `DISÃ—NOX`(ë„ë©”ì¸ ê°€ì„¤)
- íƒ€ê¹ƒ ë¡œê·¸í™”: ê°€ê²© ë¶„í¬ í•œìª½ ì¹˜ìš°ì¹¨ ì‹œ **`TransformedTargetRegressor`**ë¡œ `log1p(MEDV)` í•™ìŠµ â†’ `expm1` ì—­ë³€í™˜

---

## í‰ê°€ ì„¤ê³„(ëˆ„ìˆ˜ ë°©ì§€)

- **5-Fold KFold**(íšŒê·€), ì‹œë“œ ê³ ì •
- ëª¨ë“  ë³€í™˜ì€ **íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ì—ì„œ `fit`** â†’ **CVë§ˆë‹¤ ë…ë¦½ í•™ìŠµ**
- ë³´ê³ : **RMSE, MAE, RÂ²ì˜ í‰ê· Â±í‘œì¤€í¸ì°¨**
- ëª¨ë¸ ì„ íƒ/íŠœë‹: **Validation** í˜¹ì€ **ë‚´ì¬ CV**(Grid/Random search)

---

## ëª¨ë¸ë§ ë¡œë“œë§µ

1) **Baseline**: OLS(ê°€ì •/ì”ì°¨ ì ê²€)
2) **ê·œì œ ì„ í˜•**: Ridge/Lasso/ElasticNet(+ë‹¤í•­/ìƒí˜¸ì‘ìš©)
3) **íŠ¸ë¦¬/ë¶€ìŠ¤íŒ…**: RandomForest, **HistGradientBoostingRegressor(HGBR)**, (ì˜µì…˜) GradientBoostingRegressor
4) **ê²€ì—´Â·ì´ìƒì¹˜ ëŒ€ì‘**: HuberRegressor, **Quantile Regression**(GBRì˜ `loss='quantile'`)
5) **ì„¤ëª…**: Permutation Importance, PDP/ICE

---

## íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ(ê·œì œ ì„ í˜•Â·ë¡œê·¸ íƒ€ê¹ƒ)

```python
from sklearn.model_selection import KFold, cross_validate
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import ElasticNet
from sklearn.compose import TransformedTargetRegressor
import numpy as np, pandas as pd

df = pd.read_csv("BostonHousing.csv")
y = df["MEDV"].values
X = df.drop(columns=["MEDV"])

num_cols = ["CRIM","ZN","INDUS","NOX","RM","AGE","DIS","TAX","PTRATIO","B","LSTAT"]
cat_cols = ["CHAS","RAD"]

num_pipe = Pipeline([
    ("imp", SimpleImputer(strategy="median")),
    ("sc",  StandardScaler()),
])
cat_pipe = Pipeline([
    ("imp", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer([
    ("num", num_pipe, num_cols),
    ("cat", cat_pipe, cat_cols)
])

reg = ElasticNet(alpha=0.01, l1_ratio=0.3, random_state=42, max_iter=10000)

pipe = Pipeline([
    ("prep", preprocess),
    ("reg",  reg)
])

model = TransformedTargetRegressor(
    regressor=pipe, func=np.log1p, inverse_func=np.expm1
)

cv = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_validate(model, X, y, cv=cv,
                        scoring=["neg_root_mean_squared_error","neg_mean_absolute_error","r2"],
                        n_jobs=-1)
print("ElasticNet+log-target | RMSE:", -scores["test_neg_root_mean_squared_error"].mean())
print("MAE :", -scores["test_neg_mean_absolute_error"].mean())
print("R2  :",  scores["test_r2"].mean())
```

> í•„ìš” ì‹œ `PolynomialFeatures`ë¥¼ **ìˆ˜ì¹˜ ì»¬ëŸ¼ì—ë§Œ** ì ìš©í•˜ëŠ” ì„œë¸Œ `ColumnTransformer`ë¥¼ êµ¬ì„±í•´ ë‹¤í•­/ìƒí˜¸ì‘ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”.

---

## ë¶€ìŠ¤íŒ… ë² ì´ìŠ¤ë¼ì¸(HistGradientBoosting)

```python
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.model_selection import cross_validate

hgb = HistGradientBoostingRegressor(
    learning_rate=0.07,
    max_depth=None,      # ìë™
    max_bins=255,
    l2_regularization=0.0,
    random_state=42
)

# HGBRëŠ” ìŠ¤ì¼€ì¼ ì˜ì¡´ë„ ë‚®ì§€ë§Œ, ê²°ì¸¡/OHE ì²˜ë¦¬ë¥¼ ìœ„í•´ preprocess ìœ ì§€

pipe_hgb = Pipeline([
    ("prep", preprocess),
    ("reg",  hgb)
])

scores = cross_validate(pipe_hgb, X, y, cv=cv,
                        scoring=["neg_root_mean_squared_error","neg_mean_absolute_error","r2"],
                        n_jobs=-1)
print("HGBR | RMSE:", -scores["test_neg_root_mean_squared_error"].mean())
print("MAE :", -scores["test_neg_mean_absolute_error"].mean())
print("R2  :",  scores["test_r2"].mean())
```

**íŠœë‹ íŒíŠ¸**
- ë³µì¡ë„ ì œì–´: `max_depth`, `min_samples_leaf`(ë˜ëŠ” `min_samples_leaf` ëŒ€ì²´ íŒŒë¼ë¯¸í„°), `l2_regularization`
- í•™ìŠµë¥ â€“ì¶”ì •ê¸° ìˆ˜ íŠ¸ë ˆì´ë“œì˜¤í”„: `learning_rate`â†“, ë°˜ë³µ ìˆ˜â†‘
- ì´ìƒì¹˜ ëŒ€ì‘: **`GradientBoostingRegressor(loss='absolute_error' or 'huber')`** ë˜ëŠ” **Quantile**(í•˜ë‹¨ ì°¸ì¡°)

---

## ë¹ ë¥¸ Random Search(ë¶€ìŠ¤íŒ…)

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

param_dist = {
    "reg__learning_rate": uniform(0.02, 0.15),
    "reg__max_leaf_nodes": randint(15, 63),
    "reg__min_samples_leaf": randint(10, 50),
    "reg__l2_regularization": uniform(0.0, 0.2)
}

search = RandomizedSearchCV(
    estimator=pipe_hgb,
    param_distributions=param_dist,
    n_iter=50,
    cv=cv,
    scoring="neg_root_mean_squared_error",
    random_state=42,
    n_jobs=-1
)
search.fit(X, y)
print("Best RMSE:", -search.best_score_)
print("Best params:", search.best_params_)
best_model = search.best_estimator_
```

---

## ê²€ì—´Â·ì´ìƒì¹˜ ëŒ€ì‘(ë¡œë²„ìŠ¤íŠ¸Â·í€€íƒ€ì¼)

### Huber/ì ˆëŒ€ê°’ ì†ì‹¤

- `HuberRegressor`: ì´ìƒì¹˜ì— ê°•ê±´
- `GradientBoostingRegressor(loss='absolute_error' or 'huber')`: í° ì˜¤ì°¨ì— ì™„ë§Œ

```python
from sklearn.ensemble import GradientBoostingRegressor
gbr = GradientBoostingRegressor(loss="huber", alpha=0.9, random_state=42)
pipe_gbr = Pipeline([("prep", preprocess), ("reg", gbr)])
scores = cross_validate(pipe_gbr, X, y, cv=cv, scoring="neg_mean_absolute_error")
print("GBR-Huber | MAE:", -scores["test_score"].mean())
```

### Quantile Regression (ìƒÂ·í•˜ìœ„ ë¶„ìœ„ ì˜ˆì¸¡)

ê²€ì—´ ìƒë‹¨ ê·¼ì²˜ì˜ ì¡°ê±´ë¶€ ë¶„ìœ„ìˆ˜ë¥¼ ì§ì ‘ ëª¨ë¸ë§:

```python
gbr_q = GradientBoostingRegressor(loss="quantile", alpha=0.9, random_state=42)
pipe_gbr_q = Pipeline([("prep", preprocess), ("reg", gbr_q)])
pipe_gbr_q.fit(X, y)
# 90% ë¶„ìœ„ ê°€ê²© ì¶”ì • ê°€ëŠ¥

```

> **ì˜ë„**: â€œìƒí•œ ìº¡í•‘â€ êµ¬ê°„ì—ì„œ í‰ê·  ì˜ˆì¸¡ ëŒ€ì‹  ë¶„ìœ„ ì˜ˆì¸¡ìœ¼ë¡œ ì˜ì‚¬ê²°ì •(ìœ„í—˜í•œ ê³ í‰ê°€/ì €í‰ê°€ íšŒí”¼)

---

## ì”ì°¨Â·ì—ëŸ¬ ë¶„ì„(í”„ë¡œíŒŒì¼ë§)

- **ì”ì°¨ í”Œë¡¯**: ì˜ˆì¸¡ê°’ vs ì”ì°¨(ë¹„ì„ í˜•/ì´ë¶„ì‚° íƒì§€)
- **ì„¸ê·¸ë¨¼íŠ¸ë³„ ì—ëŸ¬**: `RM`, `LSTAT`, `PTRATIO` ë¶„ìœ„ ê·¸ë£¹ë³„ MAE/RMSE
- **ì§€ë¦¬/ì •ì±…ì  ê·¸ë£¹**(ì˜ˆ: `RAD`, `CHAS`)ë³„ ì—ëŸ¬ ì°¨ì´

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np, matplotlib.pyplot as plt

best_model.fit(X, y)
y_hat = best_model.predict(X)
resid = y_hat - y

fig, ax = plt.subplots(1,2, figsize=(10,4))
ax[0].scatter(y_hat, resid, s=8, alpha=0.5); ax[0].axhline(0, c="r")
ax[0].set_xlabel("Pred"); ax[0].set_ylabel("Residual")
ax[1].hist(resid, bins=40); ax[1].set_title("Residual distribution")
plt.show()

def seg_mae(col, q=4):
    bins = pd.qcut(X[col], q, duplicates="drop")
    return df.assign(pred=y_hat).groupby(bins).apply(
        lambda g: mean_absolute_error(g["MEDV"], g["pred"])
    )
print(seg_mae("LSTAT"))
```

---

## í•´ì„(ì¤‘ìš”ë„Â·ë¶€ë¶„ì˜ì¡´)

- **Permutation Importance**: ëª¨ë¸ ë¶ˆê°€ì§€ë¡ ì , ì‹ ë¢°ë„â†‘
- **PDP/ICE**: ë‹¨ì¡°/ë¹„ì„ í˜• ê´€ê³„ ì‹œê°í™”(í•´ì„ì€ â€œë¶€ë¶„â€ íš¨ê³¼ì„ì„ ëª…ì‹œ)

```python
from sklearn.inspection import permutation_importance, PartialDependenceDisplay

best_model.fit(X, y)
r = permutation_importance(best_model, X, y, n_repeats=10, random_state=42, n_jobs=-1)
importances = r.importances_mean
feat_names = (best_model.named_steps["prep"]
              .get_feature_names_out())
top = np.argsort(importances)[::-1][:10]
for i in top:
    print(feat_names[i], importances[i])

PartialDependenceDisplay.from_estimator(best_model, X, features=["RM","LSTAT","PTRATIO"])
plt.show()
```

> **ê²½í–¥(ë°ì´í„° íŠ¹ì´ì„± ì£¼ì˜)**: `RMâ†‘ â†’ MEDVâ†‘`, `LSTATâ†‘ â†’ MEDVâ†“`, `PTRATIOâ†‘ â†’ MEDVâ†“`

---

## ìœ¤ë¦¬Â·ê³µì •ì„± ì ê²€(ë¯¼ê° ë³€ìˆ˜ ì˜í–¥)

- **ë¯¼ê°/ë…¼ìŸ ë³€ìˆ˜ ì œê±° ë¹„êµ**: `B`(ì¸ì¢… ì§€í‘œ), `RAD`(ì •ì±…ì ), `CHAS`(ì§€ì—­)
- **ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥Â·ì”ì°¨** ë¹„êµ â†’ ì‹¤ë¬´ ë°°í¬ë³¸ì—ì„œ ì œê±°/ëŒ€ì²´ ê³ ë ¤
- **í”„ë¡œì‹œì €**: ë°ì´í„° ê°€ìš©ì„±Â·ë²•ì  ê·œì œÂ·ë‚´ë¶€ ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜

```python
sens_cols = ["B"]  # ì˜ˆì‹œ: ë¯¼ê° ë³€ìˆ˜
X_sans = X.drop(columns=sens_cols)

m_with  = best_model
m_sans  = Pipeline([("prep", ColumnTransformer([
                        ("num", num_pipe, [c for c in num_cols if c not in sens_cols]),
                        ("cat", cat_pipe, cat_cols)])),
                    ("reg", HistGradientBoostingRegressor(random_state=42))])

# ê°„ë‹¨ ì„±ëŠ¥ ë¹„êµ(CV)

sc_with = cross_validate(m_with , X,      y, cv=cv, scoring="neg_root_mean_squared_error")
sc_sans = cross_validate(m_sans , X_sans, y, cv=cv, scoring="neg_root_mean_squared_error")
print("With  sensitive | RMSE:", -sc_with ["test_score"].mean())
print("Sans sensitive | RMSE:", -sc_sans["test_score"].mean())
```

> **ì›ì¹™**: ì˜ˆì¸¡ ì •í™•ë„ë§Œìœ¼ë¡œ ë¯¼ê° ë³€ìˆ˜ ì‚¬ìš©ì„ ì •ë‹¹í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. **ë²•/ìœ¤ë¦¬/ì¡°ì§ ê·œë²”**ì„ ìš°ì„ í•˜ê³ , ëŒ€ì²´ íŠ¹ì„±Â·ê·œë²” ì¤€ìˆ˜í˜• ëª¨ë¸ë§ì„ íƒí•˜ì‹­ì‹œì˜¤.

---

## ì˜ˆì‹œ

- **joblib**ë¡œ íŒŒì´í”„ë¼ì¸ ì§ë ¬í™” â†’ ë¡œë”© ì˜ˆì¸¡
- **ì…ë ¥ ê²€ì¦**(ìŠ¤í‚¤ë§ˆ, ëˆ„ë½/ë²”ìœ„)Â·**ë¡œê·¸**(ì¶”ë¡ /ì”ì°¨)Â·**ëª¨ë‹ˆí„°ë§**(ë°ì´í„° ë“œë¦¬í”„íŠ¸)

```python
import joblib
best_model.fit(X, y)
joblib.dump(best_model, "boston_price_model.joblib")

# ---- inference ----

model = joblib.load("boston_price_model.joblib")
sample = X.iloc[[0]]
pred = model.predict(sample)[0]
print("Pred price($1,000s):", pred)
```

---

## ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒìœ¼ë¡œ ëŒ€ì²´

```python
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing(as_frame=True)
Xc, yc = data.data, data.target  # ë‹¨ìœ„: $100,000s

num_cols_c = Xc.columns.tolist()   # ì „ë¶€ ìˆ˜ì¹˜
preprocess_c = ColumnTransformer([("num", num_pipe, num_cols_c)])
hgb_c = HistGradientBoostingRegressor(random_state=42)
pipe_c = Pipeline([("prep", preprocess_c), ("reg", hgb_c)])

cv = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_validate(pipe_c, Xc, yc, cv=cv,
                        scoring=["neg_root_mean_squared_error","neg_mean_absolute_error","r2"])
print("California | RMSE:", -scores["test_neg_root_mean_squared_error"].mean())
print("MAE:", -scores["test_neg_mean_absolute_error"].mean(), " R2:", scores["test_r2"].mean())
```

---

## í”í•œ í•¨ì • Â· ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•¨ì • | ì¦ìƒ | ì²˜ë°© |
|---|---|---|
| **ë°ì´í„° ëˆ„ìˆ˜** | CV ì„±ëŠ¥ ê³¼ëŒ€ | ëª¨ë“  ë³€í™˜ì„ **Pipeline**ì— ë„£ê³  CV ë‚´ì—ì„œ `fit` |
| **ê²€ì—´ ë¬´ì‹œ** | ìƒí•œ 50 ê·¼ì²˜ ì˜ˆì¸¡ ì™œê³¡ | ë¡œë²„ìŠ¤íŠ¸/í€€íƒ€ì¼,(ì—°êµ¬) í† ë¹—, ëª©í‘œ ì„¸ê·¸ ë¶„ë¦¬ |
| **ë‹¤ì¤‘ê³µì„ ì„±** | ê³„ìˆ˜ ë¶ˆì•ˆì • | Ridge/ElasticNet, ì°¨ì› ì¶•ì†Œ(PCA), ë³€ìˆ˜ ì„ íƒ |
| **ìŠ¤ì¼€ì¼ ë¯¸ì í•©** | ì„ í˜•/ì»¤ë„ ì„±ëŠ¥ ì €í•˜ | `StandardScaler` |
| **ìœ¤ë¦¬ ë¯¸ê³ ë ¤** | ë°°í¬ ë¦¬ìŠ¤í¬ | ë¯¼ê° ë³€ìˆ˜ ì œê±°/ëŒ€ì²´, ì˜í–¥Â·ì •ë‹¹ì„± ë¬¸ì„œí™” |
| **í•´ì„ ë‚¨ìš©** | ì¸ê³¼ ì˜¤í•´ | PDP/Permutationì€ **ì˜ˆì¸¡ ê´€ê³„**ì¼ ë¿, ì¸ê³¼ ì•„ë‹˜ |

**ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] EDAÂ·ê²€ì—´ í™•ì¸, ìœ¤ë¦¬ ê³ ì§€
- [ ] íŒŒì´í”„ë¼ì¸Â·CVÂ·ì‹œë“œ ê³ ì •
- [ ] Base(ElasticNet) vs HGBR ë¹„êµ
- [ ] ì”ì°¨/ì„¸ê·¸ ì—ëŸ¬Â·ì¤‘ìš”ë„Â·PDP
- [ ] ë¯¼ê° ë³€ìˆ˜ ì œê±° ë¹„êµÂ·ì˜ì‚¬ê²°ì •
- [ ] ì§ë ¬í™”Â·ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸Â·ëª¨ë‹ˆí„°ë§

---

## ğŸ”š ìš”ì•½

- Boston Housingì€ ì—­ì‚¬ì  í•œê³„ê°€ ëšœë ·í•©ë‹ˆë‹¤. **êµìœ¡ìš©**ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³ , ì‹¤ë¬´Â·ë°ëª¨ëŠ” **ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ** ë“±ìœ¼ë¡œ ëŒ€ì²´í•˜ì‹­ì‹œì˜¤.
- **ëˆ„ìˆ˜ ì—†ëŠ” íŒŒì´í”„ë¼ì¸ + KFold CV**ë¡œ **RMSE/MAE/RÂ²**ë¥¼ ë³´ê³ , **ElasticNet+ë¡œê·¸íƒ€ê¹ƒ**ê³¼ **HGBR**ì„ ë¹„êµí•´ ì‹¤ìš©ì  ë² ì´ìŠ¤ë¼ì¸ì„ í™•ë¦½í•˜ì„¸ìš”.
- **ê²€ì—´/ì´ìƒì¹˜**ëŠ” ë¡œë²„ìŠ¤íŠ¸/í€€íƒ€ì¼ íšŒê·€ë¡œ, **í•´ì„**ì€ Permutation/PDPë¡œ ë³´ì¡°í•˜ë©°, **ìœ¤ë¦¬Â·ê³µì •ì„±** ì ê²€ì„ ì ˆì°¨í™”í•˜ì„¸ìš”.
- ë§ˆì§€ë§‰ìœ¼ë¡œ **ì§ë ¬í™”Â·ëª¨ë‹ˆí„°ë§**ê¹Œì§€ í¬í•¨í•´ì•¼ â€œì—°ìŠµâ†’ìš´ì˜â€ì´ ì™„ê²°ë©ë‹ˆë‹¤.
