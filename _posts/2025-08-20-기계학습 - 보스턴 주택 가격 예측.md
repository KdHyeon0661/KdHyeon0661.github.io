---
layout: post
title: 기계학습 - 보스턴 주택 가격 예측
date: 2025-08-20 19:25:23 +0900
category: 기계학습
---
# 🏠 보스턴 주택 가격 예측 (회귀) — 실무형 가이드

> **중요 고지**  
> 고전 “Boston Housing” 데이터셋은 1970년대 자료로 **윤리·공정성 이슈**(예: 인종 관련 변수)와 **품질 한계**가 지적되어 `scikit-learn`에서는 더 이상 기본 제공되지 않습니다(1.2부터 `load_boston` 제거).  
> 교육·연습 목적이라면 **데이터의 역사적 한계**를 분명히 인지하고 사용하세요.  
> 실무 대체재: `fetch_california_housing` 또는 자체 부동산 데이터.

---

## 1) 문제 정의와 목표 지표

- **문제 유형**: 회귀(연속형 타깃 `MEDV`: 주택 중위가격, $1000 단위)
- **대표 성능 지표**: `RMSE`(해석 용이, 큰 오차 패널티), `MAE`(이상치에 강건), `R²`(설명력)

> **권장**: `RMSE` + `MAE`를 함께 보고, 리포트에 `R²`를 보조로 제시.

---

## 2) 특징(Feature) 개요

전통적 컬럼(예시):

| 변수 | 설명 | 유형/힌트 |
|---|---|---|
| CRIM | 범죄율 | 오른쪽 꼬리 → `log1p` 고려 |
| ZN | 25,000 평방피트 이상 주거지 비율 | 연속 |
| INDUS | 비소매상업지 비율 | 연속 |
| **CHAS** | 찰스강 인접(0/1) | **이진 범주형** |
| **NOX** | 일산화질소 농도 | 연속 |
| **RM** | 주거당 평균 방 수 | 강한 양의 상관 |
| AGE | 1940년 이전 건축 비율 | 연속 |
| **DIS** | 고용 중심지까지 거리 | 연속 |
| **RAD** | 방사형 고속도로 접근성 | **범주형 취급 권장** |
| **TAX** | 재산세율 | 연속 |
| **PTRATIO** | 학생-교사 비율 | 연속 |
| **B** | 인종 관련 지표(문제적) | 윤리적으로 주의 |
| **LSTAT** | 하위계층 비율 | 강한 음의 상관 |
| **MEDV** | **타깃**(중위가격, $1,000s) | 타깃(상한 50: **검열(censoring) 유의**) |

> **주의**: `MEDV`는 50으로 **상한(capping)** 되어 있어 상한 부근에서 **잔차 왜곡**이 발생합니다. 필요하다면 **토빗(Tobit)**, **Quantile Regression**, **로버스트 회귀**를 고려할 수 있습니다.

---

## 3) 탐색적 데이터 분석(EDA) 체크리스트

- 결측치 확인(원본 Boston CSV는 보통 결측 거의 없음)
- 분포/왜도: `CRIM`, `LSTAT`, `TAX` 등은 **log 변환** 후보
- 상관분석: `RM ↑`, `LSTAT ↓`가 `MEDV`와 강한 상관  
- 이상치/영향점: 레버리지, **Cook’s distance**로 영향 관측치 점검  
- 타깃 분포: **`MEDV` 상한 50** 부근 데이터 확인

```python
import pandas as pd, numpy as np
import seaborn as sns, matplotlib.pyplot as plt

df = pd.read_csv("BostonHousing.csv")  # Kaggle 등에서 동일 스키마 가정
print(df.isna().sum())
sns.histplot(df["MEDV"], kde=True); plt.title("MEDV distribution"); plt.show()
corr = df.corr(numeric_only=True)
plt.figure(figsize=(10,8)); sns.heatmap(corr, vmin=-1, vmax=1, cmap="coolwarm", annot=False); plt.show()
```

---

## 4) 전처리 전략

### (1) 수치형
- 결측: `median` 대치(견고)
- **비정규/꼬리**: `PowerTransformer(yeo-johnson)` 또는 `log1p`(예: `CRIM`, `LSTAT`, `TAX`)
- 스케일링: **`StandardScaler`**(선형계열·거리기반 모델에 유리)

### (2) 범주형
- `CHAS`(0/1), `RAD`(정수지만 **등간 의미 불명**) → **One-Hot** 추천

### (3) 특성 엔지니어링
- **다항/상호작용**: `RM^2`, `LSTAT^2`, `RM×LSTAT` 등 (선형 회귀 성능↑)
- **거리-오염 상호작용**: `DIS×NOX` 등 도메인 가설 테스트
- **로그 타깃**: 가격 분포가 한쪽으로 치우치면 **`log1p(MEDV)`**로 학습 후 `expm1` 역변환(TransformedTargetRegressor)

---

## 5) 평가 설정

- **Train/Validation/Test** 혹은 **K-Fold**(회귀는 `KFold`)  
- **시드 고정** + **파이프라인**으로 **누수 방지**  
- 보고: **CV 평균 ± 표준편차** (RMSE, MAE, R²)

---

## 6) 모델링 로드맵

- **Baseline**: 선형회귀(OLS) → 잔차·가정 점검  
- **규제 선형**: **Ridge/Lasso/ElasticNet** (다항/상호작용과 함께)  
- **트리/부스팅**: **RandomForest**, **GradientBoosting/HistGBDT**, (가능하면 XGBoost/LightGBM)  
- **체크**: 과적합(학습 성능만 과도하게↑) 시 규제·깊이 제한·학습률 조정

---

## 7) Scikit-learn 파이프라인 예시

### 7.1 규제 선형(ElasticNet) + 로그 타깃 + 다항(선택)
```python
from sklearn.model_selection import KFold, cross_validate
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.linear_model import ElasticNet
from sklearn.compose import TransformedTargetRegressor
import numpy as np, pandas as pd

df = pd.read_csv("BostonHousing.csv")
y = df["MEDV"].values
X = df.drop(columns=["MEDV"])

# 변수 타입 분리
num_cols = ["CRIM","ZN","INDUS","NOX","RM","AGE","DIS","TAX","PTRATIO","B","LSTAT"]
cat_cols = ["CHAS","RAD"]  # 범주형 취급

# 선택적 로그변환 대상
log_transform = FunctionTransformer(lambda x: np.log1p(x), feature_names_out="one-to-one")

num_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("power",   None),              # 여기에 PowerTransformer를 넣어도 됨
    ("scaler",  StandardScaler())
])

cat_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("ohe",     OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer([
    ("num", num_pipe, num_cols),
    ("cat", cat_pipe, cat_cols)
])

# 다항·상호작용(선택) — 수치 컬럼에만 적용하려면 ColumnTransformer 내부에 구성
poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)

base_model = ElasticNet(alpha=0.01, l1_ratio=0.3, random_state=42, max_iter=10000)

pipe = Pipeline([
    ("prep", preprocess),
    # ("poly", poly),  # 필요 시 활성화
    ("reg",  base_model)
])

# 타깃 로그-역변환 래퍼 (RMSE/MAE는 원 단위로 계산됨)
model = TransformedTargetRegressor(
    regressor=pipe,
    func=np.log1p, inverse_func=np.expm1
)

cv = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_validate(model, X, y, cv=cv,
                        scoring=["neg_root_mean_squared_error","neg_mean_absolute_error","r2"])
print("RMSE:", -scores["test_neg_root_mean_squared_error"].mean())
print("MAE :", -scores["test_neg_mean_absolute_error"].mean())
print("R2  :",  scores["test_r2"].mean())
```

### 7.2 부스팅 베이스라인 (HistGradientBoosting)
```python
from sklearn.ensemble import HistGradientBoostingRegressor

hgb = HistGradientBoostingRegressor(
    learning_rate=0.07,
    max_depth=None,
    max_bins=255,
    l2_regularization=0.0,
    random_state=42
)

pipe_hgb = Pipeline([
    ("prep", preprocess),  # HGBDT는 스케일 필요성 낮지만 결측·OHE는 유지
    ("reg",  hgb)
])

scores = cross_validate(pipe_hgb, X, y, cv=cv,
                        scoring=["neg_root_mean_squared_error","neg_mean_absolute_error","r2"])
print("HGB RMSE:", -scores["test_neg_root_mean_squared_error"].mean())
print("HGB MAE :", -scores["test_neg_mean_absolute_error"].mean())
print("HGB R2  :",  scores["test_r2"].mean())
```

> **팁**: 부스팅류는 `learning_rate`↓, `n_estimators`↑, `max_depth`·`min_samples_leaf`로 복잡도 제어.  
> 이상치/장꼬리 대응을 위해 **`MAE`(또는 Huber) 손실**을 지원하는 모델도 고려.

---

## 8) 하이퍼파라미터 튜닝 (빠른 Random Search 예시)

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

param_dist = {
    "reg__learning_rate": uniform(0.02, 0.15),
    "reg__max_leaf_nodes": randint(15, 63),
    "reg__min_samples_leaf": randint(10, 50),
    "reg__l2_regularization": uniform(0.0, 0.2)
}

rs = RandomizedSearchCV(
    estimator=pipe_hgb,
    param_distributions=param_dist,
    n_iter=50,
    cv=5,
    scoring="neg_root_mean_squared_error",
    random_state=42,
    n_jobs=-1
)
rs.fit(X, y)
print("Best RMSE:", -rs.best_score_)
print("Best params:", rs.best_params_)
best_model = rs.best_estimator_
```

---

## 9) 모델 해석과 진단

- **선형계열**: 계수(sign/크기), VIF(다중공선성), 잔차 진단(정규성·이분산성·비선형)
- **트리/부스팅**: `feature_importances_`, **Permutation Importance**, **Partial Dependence Plot**(PDP), ICE

```python
from sklearn.inspection import PartialDependenceDisplay
import matplotlib.pyplot as plt

best_model.fit(X, y)
feat = ["RM","LSTAT","PTRATIO"]
fig, ax = plt.subplots(figsize=(10,4))
PartialDependenceDisplay.from_estimator(best_model, X, features=feat, ax=ax)
plt.show()
```

> **해석 포인트(경향)**  
> `RM`(방 수 ↑) → 가격 ↑, `LSTAT`(하위계층 비율 ↑) → 가격 ↓, `PTRATIO`(학생-교사 비율 ↑) → 가격 ↓.  
> 다만 시대·지역 편향, 변수 정의의 윤리적 문제로 **해석·전이(transfer)** 에 유의.

---

## 10) 흔한 함정과 대응

- **데이터 누수**: 스케일링/다항변환을 **CV 밖에서 fit** → ❌. **파이프라인**으로 CV 내부에서 처리.
- **검열 효과**: `MEDV=50` 상한 구간 → 잔차 왜곡. **로버스트/퀀타일 회귀** 고려.
- **다중공선성**: `RAD/TAX/INDUS` 등 상관 ↑ → Ridge/ElasticNet, 차원축소, 변수 선택.
- **스케일 이슈**: 선형·커널 모델은 **스케일 필수**.
- **윤리·공정성**: B 같은 변수는 **사용/해석에 각별한 주의**. 실제 서비스엔 제거·변환·감사 필요.

---

## 11) (대안) 캘리포니아 주택 예측

실무·데모에는 다음이 안전하고 현대적입니다.

```python
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing(as_frame=True)
X, y = data.data, data.target  # target 단위: $100,000s
# 동일한 파이프라인/튜닝 전략 적용 가능
```

---

## ✅ 정리

- Boston Housing은 회귀 입문 예제로 유명하지만 **데이터 윤리/품질 한계**가 분명합니다.  
- **파이프라인 + CV**로 누수 없이 **RMSE/MAE/R²**를 평가하고, **규제 선형 + 부스팅**을 비교해 보세요.  
- **로그 타깃/변수 변환, 다항·상호작용**으로 선형 성능을 끌어올리고, **부스팅**으로 비선형 패턴을 포착합니다.  
- 결과 해석 시 **변수 편향·검열·시대성**을 반드시 고려하세요.  
- 실제 프로젝트에는 **캘리포니아 주택** 등 더 적합한 대체 데이터를 권장합니다.