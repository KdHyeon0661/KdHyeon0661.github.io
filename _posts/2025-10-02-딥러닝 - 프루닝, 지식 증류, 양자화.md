---
layout: post
title: 딥러닝 - 프루닝, 지식 증류, 양자화
date: 2025-10-02 14:25:23 +0900
category: 딥러닝
---
# 프루닝/지식 증류/양자화(PTQ·QAT) 개관  
**모델 경량화·가속화의 3축: Pruning · Knowledge Distillation · Quantization(PTQ/QAT)**

## 0. 큰 그림(결론부터)
- **Pruning(프루닝)**: 불필요한 가중치/채널을 제거해 **파라미터/연산**을 줄인다.  
  - *Unstructured* (개별 가중치 0), *Structured* (채널/필터/헤드 제거), *N:M* (정형 희소)  
  - **속도효과**는 **구조화 프루닝**이 즉효, 미구조 희소는 **전용 커널** 없으면 효과 제한.
- **Knowledge Distillation(지식 증류)**: 큰 **Teacher**의 지식을 작은 **Student**로 이전해 **정확도 손실 최소**.  
  - 로짓 증류, 중간 특성(Feature) 증류, 어텐션/히트맵 증류 등.
- **Quantization(양자화)**: FP32 → INT8/FP16/BF16 등 **정밀도 낮춰** 메모리/대역폭 감소, **스루풋↑**.  
  - **PTQ**: 학습 없이 **교정(calibration)** 만,  
  - **QAT**: 학습 동안 **가짜양자화(FakeQuant)** 로 정확도 보전.

> **현업 레시피**  
> 1) **증류**로 작은 모델 정확도 확보 → 2) **구조화 프루닝**으로 FLOPs/채널 줄임 → 3) **QAT** 혹은 **PTQ** 로 INT8 변환.  
> (문제·예산·하드웨어에 따라 순서/조합 조정)

---

## 1. Pruning (프루닝)

### 1.1 개념·분류
- **Unstructured Pruning**: 가중치 단위로 0.  
  - 예: **Magnitude Pruning**(작은 절댓값부터 제거).  
  - 장점: 성능 보존 쉬움. 단점: 일반 커널에선 속도 이득 제한.
- **Structured Pruning**: **채널/필터/헤드/Block** 단위 제거.  
  - 예: Conv의 **출력 채널** 삭제 → 다음 레이어 **입력 채널**도 줄어 듦.  
  - 장점: **실질 속도/메모리 이득**. 단점: 정확도 영향 큼 → **재학습/미세튜닝** 필요.
- **N:M 정형 희소**: 매 M개 중 N개만 비제로 (예: 2:4).  
  - 특정 하드웨어 커널에서 **가속**.

### 1.2 수식 직관
- **크기 기반 L₀ 제약(완화)**:
$$
\min_{\theta}\ \mathcal{L}(\theta)\quad \text{s.t.}\ \|\theta\|_0 \le K
$$
- 실전은 **프루닝 마스크** $$\mathbf{m}\in\{0,1\}^{|\theta|}$$ 로
$$
\min_{\theta,\mathbf{m}}\ \mathcal{L}(\theta\odot \mathbf{m})\ \ \text{s.t.}\ \ \|\mathbf{m}\|_0\le K
$$
- **Magnitude Heuristic**: $$m_i=\mathbf{1}\{|\theta_i|\ge \tau\}$$

### 1.3 절차(One-shot vs Iterative)
- **One-shot**: 한 번에 목표 희소도까지 → **빠르나** 정확도 손실↑  
- **Iterative**: (Prune → Fine-tune) × N → **느리지만** 정확도 보전↑  
- **Sparsity 스케줄**: 선형/지수/폴리노믹(훈련 step에 따라 점진적 증가)

### 1.4 PyTorch 내장(미구조·구조) 예제

#### (A) Unstructured L1 Magnitude (layer-wise / global)
```python
import torch, torch.nn as nn
import torch.nn.utils.prune as prune

model = nn.Sequential(
    nn.Linear(512, 512),
    nn.ReLU(),
    nn.Linear(512, 10)
)

# 레이어별 30% 제거
prune.l1_unstructured(model[0], name="weight", amount=0.3)
# 마스크는 model[0].weight_mask 로 보관, 실제 가중치는 model[0].weight_orig
# 영구화(마스크 적용한 텐서를 진짜 weight로 고정)
prune.remove(model[0], "weight")

# Global pruning (여러 모듈 묶어 전체에서 동일 임계)
parameters_to_prune = [
    (model[0], "weight"),
    (model[2], "weight"),
]
prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.5)
for m, n in parameters_to_prune:
    prune.remove(m, n)
```

#### (B) Structured LN-structured (필터/채널 제거)
- Conv2d의 **출력 채널** 기준(차원 `dim=0`) L1/L2 norm 기반
```python
import torch.nn.utils.prune as prune

conv = nn.Conv2d(64, 128, 3, padding=1)
# 출력 채널 30% 제거
prune.ln_structured(conv, name="weight", amount=0.3, n=2, dim=0)  # L2 norm, dim=0 → out_channels
prune.remove(conv, "weight")
```

> **주의**: 단순 마스킹만 하면 모양(shape)은 유지 → 진짜 속도이득 위해서는 **채널 삭제에 대응하는 후속 레이어 재작성(슬라이싱)** 필요. 아래 커스텀 파이프라인.

### 1.5 커스텀 구조화 프루닝 파이프라인(Conv 채널 제거 후 그래프 수정)
```python
import torch, torch.nn as nn
import numpy as np

def prune_conv_out_channels(conv: nn.Conv2d, keep_idx):
    # keep_idx: 남길 out_channel 인덱스 리스트
    new_out = len(keep_idx)
    new_conv = nn.Conv2d(conv.in_channels, new_out, conv.kernel_size,
                         stride=conv.stride, padding=conv.padding, dilation=conv.dilation,
                         groups=conv.groups, bias=(conv.bias is not None), padding_mode=conv.padding_mode)
    with torch.no_grad():
        new_conv.weight.copy_(conv.weight.data[keep_idx])
        if conv.bias is not None:
            new_conv.bias.copy_(conv.bias.data[keep_idx])
    return new_conv

def prune_conv_in_channels(conv: nn.Conv2d, keep_idx):
    # 다음 레이어의 입력 채널을 동일 인덱스로 유지
    new_in = len(keep_idx)
    new_conv = nn.Conv2d(new_in, conv.out_channels, conv.kernel_size,
                         stride=conv.stride, padding=conv.padding, dilation=conv.dilation,
                         groups=1, bias=(conv.bias is not None), padding_mode=conv.padding_mode)
    with torch.no_grad():
        new_conv.weight.copy_(conv.weight.data[:, keep_idx])
        if conv.bias is not None:
            new_conv.bias.copy_(conv.bias.data)
    return new_conv

# 예시: Conv-BN-ReLU-Conv … 블록에서 첫 Conv의 out_channel 30% 제거 → 다음 Conv의 in_channel도 슬라이스
class Block(nn.Module):
    def __init__(self, c):
        super().__init__()
        self.conv1 = nn.Conv2d(c, 64, 3, padding=1)
        self.bn1   = nn.BatchNorm2d(64)
        self.act   = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)

    def forward(self, x):
        x = self.act(self.bn1(self.conv1(x)))
        return self.conv2(x)

blk = Block(32)
# 중요도: 필터 L1 norm
with torch.no_grad():
    w = blk.conv1.weight.data.view(64, -1).abs().sum(1)  # (out_channels,)
keep = torch.topk(w, k=int(64*0.7)).indices.cpu().numpy().tolist()

blk.conv1 = prune_conv_out_channels(blk.conv1, keep)
blk.bn1   = nn.BatchNorm2d(len(keep))
blk.conv2 = prune_conv_in_channels(blk.conv2, keep)
```

> 팁  
> - 체인 전체에 대해 **의존 그래프**를 따라 **입/출 채널**을 일관성 있게 슬라이싱.  
> - 프루닝 후 **미세튜닝**(수~수십 epoch)으로 정확도 회복.  
> - BatchNorm은 새 채널 수에 맞게 **재초기화**하거나 러닝통계 **리셋** 후 재수렴.

### 1.6 글로벌 스케줄(Iterative Pruning + Fine-tune)
```python
target_sparsity = 0.8
stages = 4
per_stage = 1 - (1 - target_sparsity) ** (1/stages)  # 단계별 제거 비율
for s in range(stages):
    # 1) 중요도 산출 → 채널/가중치 제거
    # 2) 5~20 epoch 미세튜닝(작은 LR, Cosine/Step)
    # 3) 검증. 다음 스테이지 진행
    pass
```

---

## 2. Knowledge Distillation (지식 증류)

### 2.1 기본 아이디어
- **Teacher**(큰 모델)의 **소프트 타깃**과 **로짓 분포**가 담은 **암묵적 지식**을 **Student**(작은 모델)에게 전달.  
- 특히 하드 라벨이 제공하지 못하는 **클래스 간 유사성** 정보를 학습.

### 2.2 로짓 기반 증류(Temperature Scaling)
- Teacher 로짓 $$\mathbf{z}^{(t)}$$, Student 로짓 $$\mathbf{z}^{(s)}$$, 온도 $$T>0$$.
- 소프트 타깃:
$$
\mathbf{q} = \mathrm{softmax}\!\left(\frac{\mathbf{z}^{(t)}}{T}\right),\quad
\mathbf{p} = \mathrm{softmax}\!\left(\frac{\mathbf{z}^{(s)}}{T}\right)
$$
- 증류 손실(일반적 정의):
$$
\mathcal{L}_\text{KD} = \alpha\,T^2\,\mathrm{KL}(\mathbf{q}\,\|\,\mathbf{p}) + (1-\alpha)\,\mathrm{CE}(y,\ \mathrm{softmax}(\mathbf{z}^{(s)}))
$$
- **왜 \(T^2\)?**: 로짓을 \(1/T\)로 스케일하면 **그라디언트 크기**가 \(1/T^2\) 비례로 줄어들기 때문에 **보상**.

> 하이퍼팁: $$T\in[2, 4, 8]$$, $$\alpha\in[0.3, 0.7]$$ 탐색. 라벨 스무딩과 중복되지 않도록 주의.

### 2.3 중간 특성(Feature) 증류
- 특정 레이어의 활성 $$\mathbf{h}^{(t)}$$ 와 $$\mathbf{h}^{(s)}$$ 를 **공간/채널 정렬** 후
$$
\mathcal{L}_\text{feat} = \beta \cdot \|\phi(\mathbf{h}^{(s)}) - \psi(\mathbf{h}^{(t)})\|_2^2
$$
- 채널 수 다르면 **1×1 Conv** 로 정렬(프로젝션).

### 2.4 PyTorch 증류 루프(로짓 + 선택적 피쳐 증류)
```python
import torch, torch.nn as nn, torch.nn.functional as F

class KDLoss(nn.Module):
    def __init__(self, alpha=0.5, T=4.0):
        super().__init__()
        self.alpha, self.T = alpha, T
        self.ce = nn.CrossEntropyLoss()

    def forward(self, z_s, z_t, y):
        T = self.T
        # KL(q || p) with log_softmax for stability
        p = F.log_softmax(z_s / T, dim=-1)
        q = F.softmax(z_t / T, dim=-1)
        kd = F.kl_div(p, q, reduction='batchmean') * (T*T)
        ce = self.ce(z_s, y)
        return self.alpha * kd + (1 - self.alpha) * ce

# 학습 루프 스케치
teacher.eval()
kd_crit = KDLoss(alpha=0.6, T=4.0)
feat_crit = nn.MSELoss()

for xb, yb in train_loader:
    with torch.no_grad():
        z_t, feat_t = teacher(xb, return_feat=True)
    z_s, feat_s = student(xb, return_feat=True)

    loss = kd_crit(z_s, z_t, yb)
    # 선택: 중간 특성 증류
    loss += 0.1 * feat_crit(proj_s(feat_s), proj_t(feat_t).detach())

    opt.zero_grad(set_to_none=True)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)
    opt.step()
```

> 팁  
> - Teacher는 **freeze + eval()**(BN 통계 고정).  
> - Student는 **AdamW + Cosine + Warmup** 등 안정 세팅.  
> - **데이터 증강**은 Teacher/Student **동일** 입력에 적용(=같은 배치).  
> - Detection/Segmentation의 증류는 **로짓·Feature·Attention map** 병행(손실 가중 튜닝 필요).

---

## 3. Quantization (양자화)

### 3.1 개념·기초수식
- **Uniform affine quantization**(일반적):
$$
q = \mathrm{clip}\!\left(\left\lfloor \frac{x}{s} \right\rceil + z,\ q_{\min}, q_{\max}\right), \quad \hat{x} = s\,(q - z)
$$
- $$s$$: **scale**, $$z$$: **zero-point**, $$q_{\min}, q_{\max}$$: INT 범위(예: INT8 → -128~127 또는 0~255)
- **Symmetric**(대칭, z=0) vs **Asymmetric**(z≠0)  
- **Per-tensor** vs **Per-channel**(가중치에 많이 사용)

> 직관: **작은 s** → 정밀↑(단위 당 해상도↑) but **포화(clip)** 위험↑.  
> **Calib(교정)** 단계에서 **최적 s,z** 추정(최소 제곱/히스토그램/Percentile).

### 3.2 PTQ (Post-Training Quantization)

#### 3.2.1 Dynamic PTQ(가중치만 양자화; LSTM/Linear에 효과)
```python
import torch, torch.nn as nn
from torch.ao.quantization import quantize_dynamic

model = MyLSTMOrMLP()
qmodel = quantize_dynamic(model, {nn.Linear, nn.LSTM}, dtype=torch.qint8)
# 가중치 INT8 + 활성은 FP (런타임 quant/dequant)
```

#### 3.2.2 Static PTQ(FX Graph Mode; Conv에 효과, 캘리브레이션 필요)
1) **QConfig** & **prepare**: Observer 삽입  
2) **Calibration**: 소량의 대표 데이터로 통계 수집  
3) **convert**: INT8로 치환

```python
import torch
import torch.ao.quantization as tq
from torch.ao.quantization.fx import prepare_fx, convert_fx

model = MyConvNet().eval()
example = torch.randn(1,3,224,224)

# ① QConfig mapping (x86: fbgemm, ARM: qnnpack)
qconfig = tq.get_default_qconfig("fbgemm")
qmap = tq.QConfigMapping().set_global(qconfig)

# ② prepare (observer 삽입)
mp = prepare_fx(model, qmap, example)

# ③ calibration (대표 데이터 몇 백~수천 샘플 추론)
with torch.no_grad():
    for xb, _ in calib_loader:
        mp(xb)

# ④ convert (INT8 커널로 치환)
mp_int8 = convert_fx(mp)
mp_int8.eval()
```

> 팁  
> - **모듈 결합(Fusion)**: `Conv+BN(+ReLU)` 사전 결합하면 양자화 정확도/성능↑  
> - **Observer 선택**: MinMax vs Histogram vs Percentile → outlier 민감성 조절  
> - **Per-channel weight**: Conv/Linear 가중치에 유리(정확도↑)

### 3.3 QAT (Quantization-Aware Training)
- 학습 중 **FakeQuant**(양자화-역양자화) 삽입, backward는 **STE(Straight-Through Estimator)** 로 근사:
$$
\tilde{x} = s\left(\mathrm{clip}\!\left(\left\lfloor \frac{x}{s} \right\rceil + z,\ q_{\min},q_{\max}\right)-z\right),\quad
\frac{\partial \tilde{x}}{\partial x}\approx 1\ \text{(clip 구간 내)}
$$
- **정확도 보전**이 PTQ보다 좋음(특히 작은 INT8 범위에서 민감한 모델).

#### QAT 절차(FX)
```python
import torch
import torch.ao.quantization as tq
from torch.ao.quantization.fx import prepare_qat_fx, convert_fx

model = MyConvNet().train()

# (권장) Conv-BN-ReLU fuse (fx 기반 모델에 따라 auto/수동)
# model = fuse_modules(model, [["conv","bn","relu"], ...])  # 예시(구버전 API)

qconfig = tq.get_default_qat_qconfig("fbgemm")
qmap = tq.QConfigMapping().set_global(qconfig)

example = torch.randn(1,3,224,224)
mp = prepare_qat_fx(model, qmap, example)  # FakeQuant + Observer 삽입

# ① 수 에폭 학습(AMP도 가능하나 수치 확인)
optimizer = torch.optim.AdamW(mp.parameters(), lr=1e-3, weight_decay=1e-2)
for ep in range(10):
    for xb, yb in train_loader:
        optimizer.zero_grad(set_to_none=True)
        logits = mp(xb)
        loss = torch.nn.functional.cross_entropy(logits, yb, label_smoothing=0.1)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(mp.parameters(), 1.0)
        optimizer.step()

mp.eval()
mp_int8 = convert_fx(mp)  # INT8로 확정
```

> 팁  
> - QAT 중 초반에는 **observer 활성**(스케일 학습), 후반엔 **freeze** 가능.  
> - QAT 전 **PTQ**로 시작 → 정확도가 크게 떨어지면 QAT로 보정.  
> - **Bias correction / Cross-layer equalization** 등 사전 처리로 PTQ 성능↑.

### 3.4 수치 안정·성능 팁
- **Outlier**가 크면 **대역**이 낭비 → Histogram/Percentile observer, 클리핑  
- **대칭(symmetric) vs 비대칭(asymmetric)**:  
  - 가중치(대칭·per-channel), 활성(비대칭·per-tensor) 조합이 흔한 강한 기본값  
- **백엔드 선택**: x86=**fbgemm**, ARM=**qnnpack**  
- **실측**: 단순 FLOPs/파라미터 감소 ≠ 지연 감소. **실장치에서 latency 측정**.

---

## 4. 통합 레시피(조합 전략)

### 4.1 Distill → Prune → (PTQ/QAT)
1) **증류**: Student를 **Teacher 근접 정확도**로  
2) **구조화 프루닝**: 채널/FN의 **연산 축소** (iterative + fine-tune)  
3) **PTQ**: 먼저 시도 → 성능 저하 시 **QAT**로 보정

### 4.2 Prune ↔ Quant 순서
- **양자화 후 프루닝**: 스케일/제로포인트 영향으로 중요도 지표 왜곡 가능  
- **프루닝 후 양자화(권장)**: 구조가 고정된 뒤 양자화 파라미터 추정

### 4.3 N:M 희소 + INT8
- 지원 하드웨어(특정 GPU/가속기)에서 **병렬 가속**.  
- 학습 때 **N:M 마스크** 제약 + QAT 병행 시 정확도 유지 가능.

---

## 5. 예제 미니 프로젝트

### 5.1 CIFAR-10 소형 CNN: 프루닝 → PTQ
```python
import torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision as tv, torchvision.transforms as T
import torch.ao.quantization as tq
from torch.ao.quantization.fx import prepare_fx, convert_fx

# 1) 데이터
tfm = T.Compose([T.RandomCrop(32, padding=4), T.RandomHorizontalFlip(), T.ToTensor()])
train_set = tv.datasets.CIFAR10(root="./data", train=True, download=True, transform=tfm)
test_set  = tv.datasets.CIFAR10(root="./data", train=False, download=True, transform=T.ToTensor())
train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)
test_loader  = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=4)

# 2) 모델
class SmallCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1, bias=False), nn.BatchNorm2d(64), nn.ReLU(),
            nn.Conv2d(64,64,3,padding=1,bias=False), nn.BatchNorm2d(64), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64,128,3,padding=1,bias=False), nn.BatchNorm2d(128), nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(128,10)
        )
    def forward(self, x): return self.net(x)

def train(model, epochs=20):
    opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)
    for ep in range(epochs):
        model.train()
        for xb, yb in train_loader:
            opt.zero_grad(set_to_none=True)
            logits = model(xb.cuda())
            loss = F.cross_entropy(logits, yb.cuda(), label_smoothing=0.1)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()

def evaluate(model):
    model.eval(); c=t=0
    with torch.no_grad():
        for xb, yb in test_loader:
            p = model(xb.cuda()).argmax(1).cpu()
            c += (p==yb).sum().item(); t += yb.size(0)
    return c/t

device = "cuda"
model = SmallCNN().cuda()
train(model, epochs=10)
base_acc = evaluate(model)
print("FP32 acc:", base_acc)

# 3) 구조화 프루닝 (첫 Conv 64→45 채널로)
with torch.no_grad():
    w = model.net[0].weight.data.view(64, -1).abs().sum(1)
    keep = torch.topk(w, k=45).indices.cpu().tolist()
# conv1 out prune + bn/conv2 in 대응 (간단 슬라이스; 실무는 그래프 전파로 일반화)
def slice_out(m, keep):
    new = nn.Conv2d(3, len(keep), 3, padding=1, bias=False)
    new.weight.data.copy_(m.weight.data[keep])
    return new
def slice_bn(bn, keep):
    new = nn.BatchNorm2d(len(keep))
    new.weight.data.copy_(bn.weight.data[keep]); new.bias.data.copy_(bn.bias.data[keep])
    new.running_mean.data.copy_(bn.running_mean.data[keep]); new.running_var.data.copy_(bn.running_var.data[keep])
    return new
def slice_in(m, keep):
    new = nn.Conv2d(len(keep), 64, 3, padding=1, bias=False)
    new.weight.data.copy_(m.weight.data[:, keep])
    return new

model.net[0] = slice_out(model.net[0], keep)
model.net[1] = slice_bn(model.net[1], keep)
model.net[3] = slice_in(model.net[3], keep)

# 프루닝 후 미세튜닝
train(model, epochs=5)
acc_pruned = evaluate(model)
print("Pruned acc:", acc_pruned)

# 4) PTQ(FX) — 캘리브레이션 후 convert
example = torch.randn(1,3,32,32).cuda()
qconfig = tq.get_default_qconfig("fbgemm")
qmap = tq.QConfigMapping().set_global(qconfig)
mp = prepare_fx(model.eval(), qmap, example)

with torch.no_grad():
    for i, (xb, _) in enumerate(train_loader):
        mp(xb.cuda())
        if i>50: break  # 소량 캘리브레이션

mp_int8 = convert_fx(mp)
acc_int8 = evaluate(mp_int8)
print("INT8 acc:", acc_int8)
```

> 관찰  
> - **프루닝 전/후/INT8** 정확도 비교.  
> - 실장치에서 **Latency** 비교(엔진·백엔드에 따라 차이).

### 5.2 Distillation: ResNet-Teacher → SmallCNN-Student
```python
import torchvision.models as M

teacher = M.resnet18(weights=None, num_classes=10).cuda()
# teacher를 먼저 학습했다고 가정(혹은 사전학습 weight)
student = SmallCNN().cuda()

kd_crit = KDLoss(alpha=0.6, T=4.0)
opt = torch.optim.AdamW(student.parameters(), lr=3e-4, weight_decay=1e-2)

teacher.eval()
for ep in range(10):
    student.train()
    for xb, yb in train_loader:
        xb, yb = xb.cuda(), yb.cuda()
        with torch.no_grad():
            z_t = teacher(xb)
        z_s = student(xb)
        loss = kd_crit(z_s, z_t, yb)
        opt.zero_grad(set_to_none=True)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)
        opt.step()

print("Student KD acc:", evaluate(student))
```

### 5.3 QAT로 정확도 회복(필요 시)
```python
import torch.ao.quantization as tq
from torch.ao.quantization.fx import prepare_qat_fx, convert_fx

student.train()
qmap = tq.QConfigMapping().set_global(tq.get_default_qat_qconfig("fbgemm"))
example = torch.randn(1,3,32,32).cuda()
mp = prepare_qat_fx(student, qmap, example)

opt = torch.optim.AdamW(mp.parameters(), lr=1e-4, weight_decay=1e-2)
for ep in range(5):
    mp.train()
    for xb, yb in train_loader:
        xb, yb = xb.cuda(), yb.cuda()
        opt.zero_grad(set_to_none=True)
        logits = mp(xb)
        loss = F.cross_entropy(logits, yb, label_smoothing=0.1)
        loss.backward(); opt.step()

mp.eval()
mp_int8 = convert_fx(mp)
print("QAT INT8 acc:", evaluate(mp_int8))
```

---

## 6. 디버깅 & 주의 체크리스트

- **프루닝**
  - 속도 안 준다? → **구조화**로 전환 혹은 **희소 커널** 사용.  
  - 채널 불일치/형상 에러 → **그래프 추적**으로 의존 레이어 일괄 슬라이스.  
  - 정확도 급락 → **Iterative** + **더 긴 Fine-tune**, **라벨 스무딩** 완화, **LR↓**.

- **증류**
  - Teacher/Student 입력 불일치? → 같은 증강/정규화 **공유**.  
  - $$\alpha, T$$ 민감 → 작은 그리드 서치.  
  - Feature 증류 시 채널 정렬(1×1 Conv) 누락 주의.

- **PTQ/QAT**
  - 캘리브레이션 데이터 **대표성** 부족 → 다양한 배치로 보완.  
  - Outlier로 스케일 과대 → Histogram/Percentile observer, 클립 전략.  
  - Conv-BN **Fuse** 누락 → 정확도/성능 저하.  
  - 백엔드 미설정 → x86=**fbgemm**, ARM=**qnnpack**.  
  - INT8 성능인데 지연 개선 미미 → **실장치 프로파일링**, **배치=1** 최적화, **스레드/메모리 바운드** 확인.

---

## 7. 빠른 공식·정의(암기 카드)
- **Magnitude Pruning 마스크**  
  $$m_i=\mathbf{1}\{|\theta_i|\ge \tau\}$$
- **KD 손실**  
  $$\mathcal{L}_\text{KD}=\alpha T^2\,\mathrm{KL}\!\big(\mathrm{softmax}(\tfrac{z_t}{T})\ \|\ \mathrm{softmax}(\tfrac{z_s}{T})\big)+(1-\alpha)\,\mathrm{CE}(y,\ \mathrm{softmax}(z_s))$$
- **Uniform Quantization**  
  $$q=\mathrm{clip}\!\Big(\big\lfloor x/s\big\rceil + z,\ q_{\min}, q_{\max}\Big),\quad \hat{x}=s\,(q-z)$$
- **QAT STE 근사**  
  $$\frac{\partial \tilde{x}}{\partial x}\approx 1\ \text{(clip 범위 내)}$$

---

## 8. 마무리
- **증류**로 작은 모델의 표현력을 끌어올리고,  
- **구조화 프루닝**으로 **실제 연산량/지연**을 낮추며,  
- **PTQ/QAT**로 **메모리/대역폭/캐시 효율**을 극대화하자.  
프로젝트의 제약(정확도 허용 오차, 배포 하드웨어, 배치 크기, 지연 SLA)에 맞춰 **레시피 순서**를 조정하면, 거대한 모델도 **상용 수준**으로 날씬하게 만들 수 있다.
