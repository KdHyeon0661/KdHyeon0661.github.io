---
layout: post
title: 기계학습 - 비지도학습
date: 2025-08-18 23:25:23 +0900
category: 기계학습
---
# 비지도학습(Unsupervised Learning)

## 1. 개념과 문제 설정

- **비지도학습**은 레이블 없이 \(X=\{x_i\}_{i=1}^n,\ x_i\in\mathbb{R}^d\) 만 주어졌을 때 **잠재 구조**(군집, 저차원 다양체, 밀도, 이상치)를 찾는 방법이다.
- 대표 과업: **군집화(Clustering)**, **차원 축소(Dimensionality Reduction)**, **이상치 탐지(Anomaly/Outlier Detection)**, **밀도추정(Density Estimation)**, **주제 모델링(Topic Modeling)**, **행렬분해(Matrix Factorization)**.

---

## 2. 지도학습과의 비교(정리 표)

| 구분 | 지도학습 | 비지도학습 |
|---|---|---|
| 데이터 | \((x,y)\) | \(x\)만 존재 |
| 목표 | \(x\to y\) 예측 | 구조/패턴/분포 파악 |
| 전형 알고리즘 | 회귀/분류(SVM, LR) | K-Means, GMM, PCA, t-SNE, DBSCAN, IsolationForest |
| 평가 | 지도 지표(Accuracy 등) | **내부/외부** 군집 지표, 재현성, 시각·업무타당성 |

---

## 3. 주요 목적과 대표 알고리즘

### 3.1 군집화(Clustering)

#### (A) K-Means (하드 군집, 구형 클러스터)
- 목적함수(Within-Cluster Sum of Squares, WCSS):
$$
J(\{\mu_k\},\{C_k\})=\sum_{k=1}^K \sum_{x_i\in C_k}\|x_i-\mu_k\|_2^2,\quad \mu_k=\frac{1}{|C_k|}\sum_{x_i\in C_k}x_i.
$$
- **알터네이팅 미니마이제이션**: (할당) 최근접 중심 ←→ (업데이트) 군집 평균.
- **단조감소**: 각 단계가 \(J\)를 줄임 → 지역해 수렴.
- **초깃값**: k-means++ 권장(기하평균적으로 좋은 기대값).
- **가정**: 군집이 **구형·균등 분산·비슷한 크기**일 때 적합.

#### (B) 가우시안 혼합 모델(GMM, 소프트 군집, 타원형)
- **잠재변수** \(z_i\in\{1,\dots,K\}\), 혼합:
$$
p(x)=\sum_{k=1}^K \pi_k\,\mathcal{N}(x\mid \mu_k,\Sigma_k),\ \sum_k \pi_k=1,\ \pi_k\ge 0.
$$
- **EM 알고리즘**:
  E-step(책임도 \(r_{ik}\)):
  $$
  r_{ik}=\frac{\pi_k\,\mathcal{N}(x_i\mid \mu_k,\Sigma_k)}{\sum_{j}\pi_j\,\mathcal{N}(x_i\mid \mu_j,\Sigma_j)}.
  $$
  M-step:
  $$
  N_k=\sum_i r_{ik},\quad \mu_k=\frac{1}{N_k}\sum_i r_{ik}x_i,\quad
  \Sigma_k=\frac{1}{N_k}\sum_i r_{ik}(x_i-\mu_k)(x_i-\mu_k)^\top,\quad
  \pi_k=\frac{N_k}{n}.
  $$
- **관계**: K-Means는 \(\Sigma_k=\sigma^2 I,\ \sigma^2\to 0\) 극한에서 GMM의 하드할당 근사.

#### (C) 계층적 군집(Hierarchical, 덴드로그램)
- **병합형(Agglomerative)**: 점→군집을 Bottom-up 융합(링키지: single/complete/average/ward).
- 덴드로그램 컷으로 \(K\) 선택.

#### (D) 밀도 기반(DBSCAN/HDBSCAN)
- DBSCAN 핵심: \(\epsilon\) 이웃 내 점 수가 `minPts` 이상 → **핵심점**.
- 장점: **임의 모양** 군집, **노이즈 자동 식별**.
- 단점: \(\epsilon\) 튜닝 민감, 가변밀도 데이터에서 어려움 → HDBSCAN 권장.

#### (E) 스펙트럴 클러스터링
- 유사도 그래프 \(W\), 차수행렬 \(D=\mathrm{diag}(W\mathbf{1})\), 라플라시안 \(L=D-W\) 또는 \(L_{\text{sym}}=D^{-1/2}LD^{-1/2}\).
- \(L\)의 **작은 고유값 대응 고유벡터**를 뽑아 임베딩 후 K-Means.
- **비볼록 구조**(두 반달 등)에 강함. 커널/유사도 선택 중요.

---

### 3.2 차원 축소(Dimensionality Reduction)

#### (A) PCA (선형, 분산 최대화)
- 데이터 중심화 \(X\in\mathbb{R}^{n\times d}\), 공분산 \(S=\frac{1}{n}X^\top X\).
- 첫 주성분:
$$
w_1=\arg\max_{\|w\|=1} w^\top S w \quad \Rightarrow \quad S w_1=\lambda_1 w_1.
$$
- 등가: **재구성 오차 최소화**(랭크-\(m\) 근사), SVD \(X=U\Sigma V^\top\) 에서 \(V_m\) 채택.

#### (B) Kernel PCA (비선형)
- 커널 \(K_{ij}=k(x_i,x_j)\) 중심화 후 고유분해 → 고차원 특징공간에서 선형 PCA.

#### (C) t-SNE / UMAP (시각화 특화)
- t-SNE: 고차원 근접확률 \(p_{ij}\) vs 저차원 \(q_{ij}\)의 **KL 발산** 최소화. 지역구조 보존·글로벌 거리 왜곡 가능.
- UMAP: Riemann 유사 그래프의 topological 구조 근사, 전반적으로 빠르고 대규모에 유리.

#### (D) NMF (비음수 행렬분해, 해석성)
- \(X\approx WH,\ W,H\ge 0\). **부분 기반** 설명(이미지 부품, 주제). L1/L2 규제와 함께 사용.

---

### 3.3 이상치 탐지(Anomaly Detection)

- **Isolation Forest**: 무작위 분할 트리 깊이로 고립 정도 측정(얕게 분리되면 이상치).
- **One-Class SVM**: 원점에서 마진 최대의 결정경계로 대부분 데이터를 포함.
- **LOF(Local Outlier Factor)**: 지역밀도로 이상치 스코어.
- **DBSCAN**: 군집 외 점을 노이즈로 식별.

---

## 4. 거리/유사도·스케일링·전처리

| 데이터/과업 | 권장 거리/유사도 | 비고 |
|---|---|---|
| 연속, 저차원 | 유클리드/맨해튼 | K-Means, DBSCAN 기본 |
| 고차원 희소(텍스트) | **코사인 거리** | 방향성 비교, 정규화 필수 |
| 범주형/혼합형 | **Gower/Hamming** | K-Modes/K-Prototypes, 혹은 거리 행렬 기반 알고리즘 |
| 커널 기반 | RBF/폴리 커널 | 스펙트럴/KernelPCA/OC-SVM |

- **스케일링 필수**: K-Means/DBSCAN/OC-SVM 등 거리 기반은 `StandardScaler` 권장.
- **차원의 저주**: 고차원에서 거리가 비슷해짐 → PCA/UMAP 등으로 축소 후 군집.

---

## 5. 모델 선택과 평가

### 5.1 내부(레이블 미사용) 지표
- **실루엣(Silhouette)**:
$$
s_i=\frac{b_i-a_i}{\max(a_i,b_i)}, \quad a_i:\text{자기군집 평균거리},\ b_i:\text{가장 가까운 타군집 평균거리}.
$$
- **Davies–Bouldin (DBI)**: 낮을수록 좋음.
- **Calinski–Harabasz (CHI)**: 높을수록 좋음.

### 5.2 외부(레이블 있을 때만) 지표
- **ARI(Adjusted Rand Index)**, **NMI(Normalized Mutual Information)**.

### 5.3 \(K\)/하이퍼파라미터 선택
- **엘보(elbow)**, **Gap Statistic**, **BIC/AIC**(GMM), **HDBSCAN**(매개변수 적음).
- 다중 기준 + 도메인지식 병행 권장(단일 지표 맹신 금물).

---

## 6. 수학적 기반 핵심 요약

- **K-Means 단조 수렴**: 할당/업데이트 각 단계가 \(J\) 비증가 ⇒ 유한 반복 후 고정점(지역해).
- **EM(GMM) 수렴성**: 각 E/M 단계가 완화된 우도 하한 증가 ⇒ 우도 단조 증가(지역해 가능).
- **PCA=최대분산=최소재구성오차**: 고유값 문제/랭크-\(m\) SVD 근사 동치.
- **스펙트럴**: 그래프 컷 문제의 이완(relaxation)이 라플라시안 고유벡터로 귀결.

---

## 7. 실전 워크플로우(체크리스트)

1. **스케일링**: `StandardScaler`(또는 RobustScaler).
2. **차원 축소(선택)**: PCA(보통 2~50D) → 군집/이상치.
3. **초기 탐색**: MiniBatchKMeans / HDBSCAN / GMM(BIC) 병행.
4. **평가**: 실루엣/DBI/CHI + 시각화(t-SNE/UMAP).
5. **안정성**: 여러 시드/부트스트랩/부분샘플로 일관성 확인.
6. **업무 타당성**: 군집 프로파일링(통계/특성중앙값), 도메인 검증.
7. **배포**: 파이프라인으로 전처리+모델 고정, 훈련 분포 이동 감지.

---

## 8. 파이썬 실전 예제

### 8.1 Pipeline: PCA+KMeans, K 선택, 내부 지표
```python
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.pipeline import Pipeline
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# 1. 데이터 생성(가변 분산, 약간의 중첩)
X, _ = make_blobs(n_samples=5000, centers=5, cluster_std=[1.0, 2.0, 1.5, 0.7, 2.5], random_state=42)

# 2. 파이프라인: 스케일링→PCA→KMeans
def fit_eval_k(k):
    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=10, random_state=0)),
        ("km", MiniBatchKMeans(n_clusters=k, random_state=0, batch_size=2048))
    ])
    labels = pipe.fit_predict(X)
    sil = silhouette_score(X, labels)             # 원공간 거리로 평가
    ch  = calinski_harabasz_score(X, labels)
    db  = davies_bouldin_score(X, labels)
    return pipe, (sil, ch, db)

best = None
for k in range(2, 11):
    pipe, (sil, ch, db) = fit_eval_k(k)
    print(f"K={k:2d} | Sil={sil:.3f}  CH={ch:.1f}  DBI={db:.3f}")
    if (best is None) or (sil > best[0]):
        best = (sil, k, pipe)

print("Best by silhouette:", best[1])
best_pipe = best[2]
```

### 8.2 GMM + BIC로 군집 수 선택
```python
from sklearn.mixture import GaussianMixture

X_scaled = StandardScaler().fit_transform(X)
bic_scores = []
models = []
for k in range(2, 11):
    gmm = GaussianMixture(n_components=k, covariance_type="full", random_state=0)
    gmm.fit(X_scaled)
    bic_scores.append(gmm.bic(X_scaled))
    models.append(gmm)

best_k = np.argmin(bic_scores) + 2
print("BIC-best K:", best_k)
best_gmm = models[best_k - 2]
labels_gmm = best_gmm.predict(X_scaled)
```

### 8.3 DBSCAN: eps 추정(최근접 k-거리), 이상치 포함
```python
import numpy as np
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN

Xz = StandardScaler().fit_transform(X)
k = 10
nbrs = NearestNeighbors(n_neighbors=k).fit(Xz)
dists, _ = nbrs.kneighbors(Xz)
k_dists = np.sort(dists[:, -1])  # 각 점의 k번째 이웃 거리
# 보통 k-distance plot의 '무릎' 지점이 eps 후보
eps = float(np.percentile(k_dists, 90))  # 간단한 휴리스틱
db = DBSCAN(eps=eps, min_samples=k).fit(Xz)
labels_db = db.labels_
outlier_ratio = np.mean(labels_db == -1)
print("DBSCAN clusters:", len(set(labels_db)) - (1 if -1 in labels_db else 0),
      " | outliers ratio:", round(outlier_ratio, 3))
```

### 8.4 이상치 탐지: Isolation Forest / One-Class SVM
```python
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM

Xn = StandardScaler().fit_transform(X)

iso = IsolationForest(contamination=0.02, random_state=0)
is_out = (iso.fit_predict(Xn) == -1)

oc = OneClassSVM(kernel="rbf", gamma="scale", nu=0.02)
oc_out = (oc.fit_predict(Xn) == -1)

print("IF outliers:", is_out.mean(), " | OC-SVM outliers:", oc_out.mean())
```

### 8.5 시각화용 임베딩: t-SNE/UMAP (주의 포함)
```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# t-SNE는 시드·perplexity·학습률에 민감, 대규모면 UMAP 권장
ts = TSNE(n_components=2, perplexity=30, learning_rate="auto", init="pca", random_state=0)
Y = ts.fit_transform(Xn)

plt.scatter(Y[:,0], Y[:,1], s=6)
plt.title("t-SNE embedding (for visualization only)")
plt.show()
```

---

## 9. 알고리즘 선택 가이드

| 상황 | 추천 |
|---|---|
| 구형·균등 분산·큰 데이터 | **MiniBatchKMeans** |
| 타원형·겹침·확률 모델 | **GMM(EM)** + BIC |
| 임의 모양·노이즈 포함 | **HDBSCAN/DBSCAN** |
| 복잡한 비볼록 구조 | **스펙트럴 클러스터링** |
| 고차원 희소(텍스트) | 코사인 거리 + **K-Means/ミiniBatch** / **NMF**(주제) |
| 시각화 | **PCA(빠름)** → **t-SNE/UMAP(가시화)** |
| 이상치 | **IsolationForest / LOF / OC-SVM** |

---

## 10. 실무 팁과 함정

1. **스케일링** 누락 금지(거리 기반 왜곡).
2. **차원 축소 후 군집**: 특히 고차원 텍스트·이미지.
3. K-Means는 **불균형 군집/비등분산**에서 성능 저하 → GMM/DBSCAN 고려.
4. t-SNE 결과는 **전역 거리 비교 금지**(국소구조 중심), 하이퍼 민감.
5. **안정성 검증**: 여러 시드·부분샘플·시간대 반복으로 군집 일관성 체크.
6. 프로파일링: 군집별 **중앙값/분포/대표샘플**로 의미 부여(업무 해석).
7. 배포: `Pipeline`으로 전처리(스케일링/PCA)와 군집 모델 **일체화**.

---

## 11. 추가 확장 주제(요약)

- **K-Medoids(파미이즈)**: 평균 대신 Medoid(실제 샘플) 중심 → 이상치에 강함.
- **K-Prototypes**: 연속+범주 혼합.
- **Topic Modeling**: **LDA**, **NMF**(해석 쉬움).
- **Graph Community**: Louvain/Leiden(모듈러리티 최적화).
- **Self-Supervised**: SimCLR/MAE 등 표현학습 → 이후 간단 군집.

---

## 12. 간단 수학 증명 스케치(두 가지)

- **K-Means 단조감소**:
  (i) 고정된 중심에서 최근접 할당은 \(J\) 최소.
  (ii) 고정된 할당에서 각 군집 평균은 \(\sum\|x-\mu\|^2\) 최소.
  두 단계를 교대로 수행 → \(J\) 비증가, 유한 집합에서 고정점 수렴.

- **PCA 최적성(최대분산=최소오차)**:
  \(X=U\Sigma V^\top\) 에서 상위 \(m\)개 특이벡터 \(V_m\)가 분산 \(\sum_{i\le m}\sigma_i^2\) 최대이자
  Frobenius 오차 \(\|X-X_m\|_F^2=\sum_{i>m}\sigma_i^2\) 최소임.

---

## 13. 요약

- 비지도학습은 **레이블 없이** 구조/패턴/이상치를 드러낸다.
- **군집**: K-Means(빠름/구형) ↔ GMM(타원/확률) ↔ DBSCAN/HDBSCAN(임의 모양/노이즈) ↔ 스펙트럴(비볼록).
- **차원 축소**: PCA(선형/빠름) ↔ KernelPCA/UMAP/t-SNE(비선형/시각화).
- **이상치**: IsolationForest/OC-SVM/LOF.
- **거리/스케일링/차원의 저주**에 유의, 내부/외부 지표와 도메인지식으로 **안정성 + 해석**을 확보하라.
