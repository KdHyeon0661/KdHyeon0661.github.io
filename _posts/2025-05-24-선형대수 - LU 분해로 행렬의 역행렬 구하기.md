---
layout: post
title: 선형대수 - LU 분해로 행렬의 역행렬 구하기
date: 2025-05-24 19:20:23 +0900
category: 선형대수
---
# LU 분해로 행렬의 역행렬(Inverse Matrix) 구하기

**핵심 요지**
- 역행렬은 $$AX=I$$ 의 해 $$X=A^{-1}$$ 로 정의됩니다.
- 한 번의 **LU 분해**로 여러 오른쪽 항을 빠르게 푸는 것이 가능하므로, $$AX=I$$ 의 각 열(단위벡터)을 차례로 풀어 **역행렬의 각 열**을 얻을 수 있습니다.
- 수치해석 관점에서 **역행렬을 직접 구해 곱하는 것**은 일반적으로 비추천이며, 보통은 **$A^{-1}b$ 대신 LU로 $Ax=b$ 를 푸는** 방식이 안정적·효율적입니다. 다만, **여러 차례 역연산을 재사용**하거나 **이론·분석적 필요**가 있을 때 LU로 역행렬을 구성할 수 있습니다.

---

## 전제: LU와 피벗팅

대부분의 실전 구현은 **부분 피벗팅**을 사용합니다. 즉, **치환행렬** $$P$$ 에 대해

$$
PA = LU
$$

가 되도록 하며, 여기서
- $$L$$: 하삼각행렬(보통 단위 대각),
- $$U$$: 상삼각행렬,
- $$P$$: 행 교환을 기록하는 치환행렬입니다.

역행렬 조건 $$AX=I$$ 에 **좌측에서 $$P$$ 를 곱하면**:

$$
PAX=PI \quad\Longrightarrow\quad (LU)X = P.
$$

따라서 **각 열**에 대해
$$
(LU)\,x_i = p_i
$$
를 풉니다. 여기서 $$p_i$$ 는 $$P$$ 의 $$i$$번째 열이고, 일반적으로는 **단위벡터가 행 교환된 형태**입니다.

이 과정을 두 단계로 나누면,

$$
\begin{aligned}
Ly_i &= p_i \quad (\text{전방 대입})\\
Ux_i &= y_i \quad (\text{후방 대입})
\end{aligned}
$$

모든 열 $$i=1,\dots,n$$ 에 대해 구한 $$x_i$$ 들을 **열 방향으로 쌓으면** $$X=[x_1\ \cdots\ x_n]=A^{-1}$$ 입니다.

---

## 알고리즘(개요)

1) $$PA=LU$$ 를 구한다(부분 피벗팅).
2) $$P$$ 의 각 열 $$p_i$$ 에 대해
   a. 전방 대입으로 $$Ly_i=p_i$$ 를 푼다.
   b. 후방 대입으로 $$Ux_i=y_i$$ 를 푼다.
3) $$A^{-1}=[x_1\ \cdots\ x_n]$$.

**계산량**
- LU 분해: 대략 $$\tfrac{2}{3}n^3$$ FLOPs.
- 삼각해 풀이(전·후방 대입): 열당 대략 $$n^2$$ FLOPs, 전체 열 $$n$$ 개에 대해 $$\approx 2n^3$$ FLOPs.
- 총합은 상수계수를 무시하면 **$$O(n^3)$$**.
  역행렬을 한 번만 쓰는 것이면 **직접 역행렬을 구하는 것보다** $$Ax=b$$ 를 바로 푸는 편이 보통 더 안전·간단합니다.

---

## 손계산 간단 예시

행렬
$$
A=\begin{bmatrix}
2 & 3\\
1 & 4
\end{bmatrix}
$$
에 대해, 부분 피벗팅 없이도 $$A=LU$$ 가 가능합니다(이 작은 예에서는 피벗팅 불필요).

한편, **피벗팅을 쓰는 일반 구현**에서는 $$PA=LU$$ 가 만들어지고, 위의 절차대로 $$p_i$$ 를 오른쪽 항으로 두어 풉니다.

---

## PyTorch로 구현하기

PyTorch에는 고수준 해법이 준비되어 있어 **역행렬을 직접 만들지 않고**도 $$AX=I$$ 를 한 번에 풀 수 있습니다.

### 고수준: `torch.linalg.solve` 로 한 번에

$$A^{-1} = \text{solve}(A, I)$$ 는 내부적으로 **LU(또는 적절한 분해)** 와 삼각해를 사용합니다.

```python
import torch
torch.set_printoptions(precision=6, sci_mode=False)
DT = torch.float64

def inverse_via_solve(A: torch.Tensor) -> torch.Tensor:
    n = A.shape[0]
    I = torch.eye(n, dtype=A.dtype, device=A.device)
    # 내부적으로 LU 등으로 AX=I를 풉니다.
    return torch.linalg.solve(A, I)

# 예제

A = torch.tensor([[2., 3.],
                  [1., 4.]], dtype=DT)
Ainv = inverse_via_solve(A)
print("A^{-1} via solve:\n", Ainv)
print("A @ A^{-1}:\n", A @ Ainv)
print("||I - AA^{-1}||:", torch.linalg.norm(torch.eye(2, dtype=DT) - A @ Ainv))
```

**장점**: 간단하고, **역행렬을 직접 형식적으로 구하지 않으면서**도 결과적으로 동일한 $$A^{-1}$$ 를 얻습니다(수치적으론 더 안전).

---

### 교육용: 명시적 LU + 삼각해로 열별 구성

PyTorch에는 버전에 따라 `torch.linalg.lu_factor/lu_solve` 또는 구식 `torch.lu/lu_solve` 가 있을 수 있습니다. 아래는 **둘 다 지원**하는 예입니다.

```python
import torch
DT = torch.float64

def inverse_via_lu(A: torch.Tensor) -> torch.Tensor:
    A = A.to(DT)
    n = A.shape[0]
    I = torch.eye(n, dtype=A.dtype, device=A.device)

    # 1) LU factorization with pivots
    # 신버전 (권장): torch.linalg.lu_factor / lu_solve
    if hasattr(torch.linalg, "lu_factor"):
        lu, pivots = torch.linalg.lu_factor(A)
        # 2) AX=I의 각 열을 풉니다. (LU X = P I = P)
        # torch.linalg.lu_solve는 여러 RHS를 한 번에 받을 수 있습니다.
        Ainv = torch.linalg.lu_solve(lu, pivots, I)
        return Ainv

    # 2) 구버전 대체: torch.lu / torch.lu_solve
    # torch.lu는 P, L, U 또는 (LU packed, pivots)를 반환
    LU, pivots = torch.lu(A)  # packed LU
    # torch.lu_solve는 LU-packed + pivots를 받아 RHS를 풉니다.
    Ainv = torch.lu_solve(I, LU, pivots)
    return Ainv

# 예제

A = torch.tensor([[2., 3.],
                  [1., 4.]], dtype=DT)
Ainv_lu = inverse_via_lu(A)
print("A^{-1} via LU:\n", Ainv_lu)
print("A @ A^{-1}:\n", A @ Ainv_lu)
print("||I - AA^{-1}||:", torch.linalg.norm(torch.eye(2, dtype=DT) - A @ Ainv_lu))
```

**설명**
- 내부적으로는 $$PA=LU$$ 를 만들고, `lu_solve` 가 **전·후방 대입과 피벗 적용**을 알아서 수행합니다.
- 단위행렬 $$I$$ 를 한 번에 RHS로 넘기면 **열별 반복 없이** 역행렬 전체를 효율적으로 구합니다.

---

### 배치(Batch) 역행렬

여러 개의 정방행렬을 한 번에 역변환해야 하는 경우(예: 시계열 각 시점, 미니배치):

```python
import torch
DT = torch.float64

# B개의 (n x n) 행렬

B, n = 4, 3
A = torch.randn(B, n, n, dtype=DT)

# 방법 1: solve로 일괄

I = torch.eye(n, dtype=DT).expand(B, n, n)
Ainv = torch.linalg.solve(A, I)

# 방법 2: (버전에 따라) lu_factor/lu_solve 배치도 지원

if hasattr(torch.linalg, "lu_factor"):
    lu, piv = torch.linalg.lu_factor(A)
    Ainv2 = torch.linalg.lu_solve(lu, piv, I)
    print("batch max diff:", (Ainv - Ainv2).abs().max())
```

---

## 수치 안정성과 실전 조언

1) **직접 역행렬 곱은 가급적 피함**
   $$A^{-1}b$$ 가 필요하더라도, **역행렬을 구성한 뒤 곱하기**보다는
   **`solve(A, b)`** 로 바로 푸는 편이 잔차·오차 증폭에서 유리합니다.

2) **정방·가역 여부 확인**
   역행렬이 존재하려면 $$\det(A)\neq 0$$, 즉 $$\mathrm{rank}(A)=n$$ 이어야 합니다. 수치적으로는 **작은 피벗·큰 조건수**가 경고 신호입니다.

3) **조건수(컨디셔닝) 진단**
   특이값 분해로 $$\kappa_2(A)=\sigma_{\max}/\sigma_{\min}$$ 을 추정합니다. 큰 조건수는 **오차 증폭**을 의미합니다.

```python
import torch
DT = torch.float64

def kappa2(A: torch.Tensor) -> float:
    s = torch.linalg.svdvals(A.to(DT))
    return float((s.max() / s.min()).item())

A = torch.tensor([[2., 3.],[1., 4.]], dtype=DT)
print("cond_2(A):", kappa2(A))
```

4) **대칭 양정(SPD)라면 Cholesky 우선**
   $$A=A^\top,\ A\succ 0$$ 인 경우 **Cholesky** 가 더 빠르고 안정적입니다. 역행렬이 꼭 필요하면 역시 $$AX=I$$ 를 **Cholesky 삼각해**로 풉니다.

```python
import torch
DT = torch.float64

def inverse_via_cholesky(A: torch.Tensor) -> torch.Tensor:
    # SPD 가정(검사는 별도로)
    L = torch.linalg.cholesky(A.to(DT))
    n = A.shape[0]
    I = torch.eye(n, dtype=DT, device=A.device)
    # L L^T X = I  =>  L Y = I,  L^T X = Y
    # PyTorch의 triangular_solve로 전·후방 대입
    Y, _ = torch.triangular_solve(I, L, upper=False)
    X, _ = torch.triangular_solve(Y, L.transpose(-2, -1), upper=True)
    return X

# 예제 (SPD 만들기: A = M^T M + eps I)

M = torch.randn(4, 4, dtype=DT)
A_spd = M.T @ M + 1e-6 * torch.eye(4, dtype=DT)
Ainv_spd = inverse_via_cholesky(A_spd)
print("||I - AA^{-1}|| (SPD):", torch.linalg.norm(torch.eye(4, dtype=DT) - A_spd @ Ainv_spd))
```

5) **정규화/스케일링**
   열 스케일 격차가 큰 행렬은 **스케일링(표준화)** 후에 푸는 것이 수치적으로 유리할 때가 많습니다.

6) **랭크부족/특이 근접 시 대응**
   - 역행렬은 존재하지 않거나 극도로 불안정.
   - 의사역행렬(펜로즈)로 대체하거나, 릿지 정규화 $$A+\lambda I$$ 로 안정화합니다.

---

## 검증: 단위행렬 근접성과 잔차

역행렬을 얻었다면 다음을 확인하세요.

$$
\|I - AA^{-1}\|_2,\qquad \|I - A^{-1}A\|_2
$$

```python
import torch
DT = torch.float64

def inverse_check(A, Ainv):
    n = A.shape[0]
    I = torch.eye(n, dtype=A.dtype, device=A.device)
    err_left  = torch.linalg.norm(I - A @ Ainv)
    err_right = torch.linalg.norm(I - Ainv @ A)
    return float(err_left), float(err_right)

A = torch.tensor([[2., 3.],
                  [1., 4.]], dtype=DT)
Ainv = torch.linalg.solve(A, torch.eye(2, dtype=DT))
eL, eR = inverse_check(A, Ainv)
print("||I - AA^{-1}||:", eL, "  ||I - A^{-1}A||:", eR)
```

잔차가 머신 정밀도 수준(더블에서 대략 $$10^{-14}\sim10^{-12}$$)이면 정상입니다. 조건수가 크면 잔차가 커질 수 있습니다.

---

## 예제: 여러 우변과 역행렬 재사용의 이점

한 번의 LU 분해로 **여러 $$b_k$$** 에 대해 빠르게 풉니다.

```python
import torch
DT = torch.float64

A = torch.randn(500, 500, dtype=DT)
B = torch.randn(500, 10,  dtype=DT)   # 10개의 오른쪽 항(벡터 묶음)

# 권장: 역행렬을 만들지 말고 바로 solve

X = torch.linalg.solve(A, B)          # 내부적으로 분해+삼각해

# 정말 역행렬이 필요할 때만:

Ainv = torch.linalg.solve(A, torch.eye(500, dtype=DT))
X_via_inv = Ainv @ B
print("diff:", torch.linalg.norm(X - X_via_inv))
```

대부분의 경우 **`solve(A,B)`** 가 더 빠르고 안정적입니다. 역행렬을 만들어 곱하는 방식은 **오차 증폭**과 **불필요한 계산**을 유발할 수 있습니다.

---

## 자주 묻는 질문(FAQ)

**Q1. 왜 역행렬을 직접 구하는 게 비추천인가요?**
- 수치오차가 불필요하게 증폭됩니다. $$A^{-1}b$$ 를 원하면 **`solve(A,b)`** 가 더 안전합니다.

**Q2. 그래도 역행렬이 꼭 필요할 땐?**
- LU(또는 SPD라면 Cholesky)로 $$AX=I$$ 를 풀어 **구성**하세요.
- 또는 의사역행렬(특히 비정방·랭크부족) $$A^{+}$$ 가 필요한 상황인지 확인하세요.

**Q3. 피벗팅 없이도 되나요?**
- 특정 행렬에선 가능하지만, 일반적으로 **부분 피벗팅**이 안전합니다. 라이브러리는 자동으로 처리합니다.

**Q4. 정방이 아닌 경우 역행렬은?**
- 정의상 **없습니다**. 대신 **의사역행렬** $$A^{+}$$ 을 사용합니다.

---

## 요약

- 역행렬은 $$AX=I$$ 의 해이며, **LU 분해**로 각 열을 푸는 방식으로 구성할 수 있습니다.
- PyTorch에서는 `torch.linalg.solve(A, I)` 가 **가장 간단하고 안전한 방법**입니다(내부적으로 LU/삼각해 사용).
- 실전에서는 **역행렬을 직접 만들기보다** `solve` 로 문제를 푸는 것이 일반적으로 **더 낫다**는 점을 기억하세요.
- SPD라면 **Cholesky** 경로가 가장 안정적이며, 랭크부족·조건수 악화 시에는 **SVD/의사역행렬** 또는 **릿지** 정규화를 검토하세요.
