---
layout: post
title: 컴퓨터시스템 - 메모리 계층 구조
date: 2025-08-06 19:20:23 +0900
category: 컴퓨터시스템
---
# 메모리 계층 구조

## 큰 그림: 왜 ‘계층’인가?

CPU는 나노초 단위로 연산하지만 모든 데이터를 그 속도로 **공급(feed)** 할 수 없다. 그래서 **작고 빠름 ↘ 크고 느림** 으로 층을 쌓고, **자주 쓰는 데이터는 가까이** 두어 평균 접근 시간을 줄인다(지역성 가설).

```
레지스터
  ↓
L1I/L1D (코어 전용)
  ↓
L2 (코어 전용/반공유)
  ↓
L3/LLC (소켓 공유)
  ↓
DRAM (메인 메모리)
  ↓
스토리지(SSD/HDD) / 스왑
```

**지역성(Locality)**
- **시간 지역성**: 최근 사용한 데이터는 곧 다시 쓸 확률이 높다.
- **공간 지역성**: 어떤 주소를 접근했다면 그 주변도 접근할 확률이 높다.

이 가정이 캐시·프리패치·타일링의 근거다.

---

## 성능 축 세 가지: 지연·대역·용량

대략적 **서열**(플랫폼별 수치 차이는 있으나 경향은 동일):

| 계층 | 용량 | 지연(대략) | 대역 | 특징 |
|---|---:|---:|---:|---|
| 레지스터 | 수십 KB/코어 | 수 사이클 | 매우 큼 | 컴파일러/스케줄러가 관리 |
| L1D/I | 32~128 KB | ~4–10 사이클 | 매우 큼 | 라인 64B, 분리형이 일반적 |
| L2 | 256 KB–2 MB | 10~20+ 사이클 | 큼 | 코어 가까움 |
| L3/LLC | 수~수십 MB | 30~100+ 사이클 | 중간 | 소켓 공유 |
| DRAM | 수~수천 GB | 수백 ns | 중간 | 채널/뱅크 병렬성 |
| SSD/HDD | TB↑ | μs–ms | 낮음~중간 | OS 페이지 캐시 영향 큼 |

**교훈**: **상위 캐시 크기·라인·세트 구조**에 맞춘 접근 패턴이 전체 성능을 좌우한다.

---

## 캐시 기본기: 라인·매핑·교체·쓰기·포용성

### 라인(cache line)

- **라인(보통 64B)** 단위로 읽어 온다 → **공간 지역성** 활용.
- 필요한 4B 때문에 64B를 가져오므로 **오염(pollution)** 가능.

### 매핑

- **직접 매핑**: 한 주소 → 정확히 한 세트 슬롯. 충돌 쉬우나 단순/빠름.
- **집합 연관(set-associative)**: 세트마다 **N-way**(N 슬롯). 일반적.
- **완전 연관**: 어디든 배치 가능. 비용 큼(주로 TLB/LLC 일부에서).

### 교체 정책

- LRU 근사(PLRU 등). **충돌(conflict)** 을 피하려면 **패딩/오프셋**·데이터 배치가 유효.

### 쓰기 정책

- **Write-back + Write-allocate(기본)**: 쓰기 시 라인 유지, 더티로 표시→나중 반영.
- **Write-through**: 즉시 메모리 반영(단순/일관성 용이, 대역 소모↑).
- **No-write-allocate**: 미스 시 라인을 들고 오지 않고 바로 메모리로(스트리밍 쓰기 유리).
- **비휘발(non-temporal) 저장**: 한 번 쓰고 곧 안 보는 큰 버퍼는 **캐시 우회/오염 최소화**.

### 포용성(inclusivity)

- **Inclusive**: L3 ⊇ L2 ⊇ L1 (디버그·스noop 필터 용이).
- **Exclusive/Non-inclusive**: 용량 효율/유연성. 플랫폼마다 다름.

---

## 주소 변환과 TLB

CPU는 **가상 주소**를 사용, 하드웨어가 **물리 주소**로 변환한다. 이 경로의 캐시가 **TLB**(Translation Lookaside Buffer).

- **TLB reach**
  \[
  \text{Reach} \approx \text{(엔트리 수)} \times \text{(페이지 크기)}
  \]
  워킹셋이 reach를 넘으면 **TLB miss**↑ → 페이지 테이블 워크 비용.
- **Huge Page(2MB/1GB)**: reach 확대·TLB miss 감소. 단, **단편화·관리·NUMA 배치** 주의.
- L1D는 종종 **VIPT**(Virtually Indexed, Physically Tagged) → 인덱스는 VA, 태그는 PA. **동의어(alias)** 문제를 하드웨어/OS로 해결.

---

## DRAM: 구조와 지연의 정체

DRAM은 **채널→랭크→뱅크→행(row)→열(col)** 로 구성. **뱅크**마다 **행 버퍼(row buffer)** 존재.

- **Row hit**: 같은 행을 연속 접근 → **빠름**.
- **Row miss**: 다른 행 필요 → **precharge→activate** 오버헤드.
- 스케줄러는 대개 **FR-FCFS**(행 히트 우선). **행 지역성**과 **뱅크 분산**이 중요.

**암시**: DRAM도 캐시처럼 **지역성**과 **병렬성**이 핵심이다.

---

## NUMA(Non-Uniform Memory Access)

멀티 소켓/칩렛 시스템은 메모리가 **노드**로 나뉜다.

- **로컬 노드** 접근이 빠르고, **원격 노드**는 지연·대역 페널티.
- **first-touch**: 페이지를 **처음 쓴** 스레드의 노드에 할당.
- **전략**: 스레드-데이터 **바인딩** + **초기화의 병렬화**.

```c
// OpenMP: first-touch — 각 스레드가 자기 파티션을 먼저 쓴다
#pragma omp parallel for

for (long i=0; i<n; i++) a[i] = 0;
```

셸로 바인딩:
```sh
# numactl: 노드 바인딩/메모리 정책

numactl --cpunodebind=0 --membind=0 ./app
```

---

## 성능 모델: AMAT & CPI(MLP 포함)

### AMAT(평균 메모리 접근 시간)

\[
\text{AMAT} = T_{L1} + m_1\Big(T_{L2} + m_2\big(T_{L3} + m_3 T_\text{Mem}\big)\Big)
\]
- \(m_k\): 레벨 \(k\) 미스율, \(T\): 해당 레벨 지연/페널티.

### CPI 근사(메모리 스톨 포함)

\[
\text{CPI} \approx \text{CPI}_\text{base} \;+\; \underbrace{
\frac{\text{메모리 접근수}}{\text{명령수}}
\times \frac{\text{미스수}}{\text{접근수}}
\times \frac{\text{미스 페널티}}{\text{MLP}}
}_{\text{메모리 스톨 기여}}
\]
- **MLP**(Memory-Level Parallelism): 동시에 outstanding 가능한 미스 수.
  포인터 추적처럼 직렬 의존이 크면 MLP≈1 → 페널티가 **그대로** 드러난다.

---

## 미스 분류: 3C + 1C

- **Compulsory**: 처음 보는 데이터라서 미스(콜드 미스).
- **Capacity**: 캐시 용량 부족 → 교체로 미스.
- **Conflict**: 같은 세트에 몰려 충돌로 미스.
- **Coherence**: 타 코어의 쓰기로 라인 무효화(특히 **폴스 셰어링**).

---

## 프리패칭과 MLP

### 하드웨어 프리패처

- 순차/스트라이드/근접 패턴을 학습·선제 로드.
- **너무 불규칙**하거나 **쓰기가 섞인** 워크로드에서는 한계.

### 소프트웨어 프리패치(거리 선택)

```c
void sum_prefetch(const float *a, int n){
  float s=0;
  for (int i=0; i<n; i+=16){
    __builtin_prefetch(&a[i+64], 0, 1); // 읽기, 약한 지역성
    for (int k=0; k<16 && i+k<n; ++k) s += a[i+k];
  }
  (void)s;
}
```

**거리(d) 추정**:
루프 한 반복당 소비 사이클을 \(C_\text{iter}\), 메모리 왕복 지연을 \(L\)사이클이라 하면
\[
d \approx \left\lceil \frac{L}{C_\text{iter}} \right\rceil
\]
실전에서는 **측정 기반**으로 2–4 값 후보를 스윕.

### 포인터 추적(MLP↑)

```c
// 소프트웨어 파이프라이닝으로 '두 단계 앞' 노드를 먼저 당겨오기
struct Node { struct Node* next; int v; };
int sum_list(struct Node* p){
  int s=0;
  struct Node* q=p ? p->next : 0;
  while (p){
    __builtin_prefetch(q ? q->next : 0, 0, 1);
    s += p->v;
    p = q; if (q) q = q->next;
  }
  return s;
}
```

---

## 캐시 친화적 코드 패턴

### 행 우선 vs 열 우선

```c
// 행 우선(좋음)
for (int i=0;i<N;i++)
  for (int j=0;j<N;j++)
    s += A[i][j];

// 열 우선(스트라이드 큼 → 미스↑)
for (int j=0;j<N;j++)
  for (int i=0;i<N;i++)
    s += A[i][j];
```

### 타일링(블로킹)

```c
void mm_block(int n, float *A, float *B, float *C, int Bsz){
  for (int ii=0; ii<n; ii+=Bsz)
    for (int jj=0; jj<n; jj+=Bsz)
      for (int kk=0; kk<n; kk+=Bsz){
        int im = (ii+Bsz<n)? ii+Bsz : n;
        int jm = (jj+Bsz<n)? jj+Bsz : n;
        int km = (kk+Bsz<n)? kk+Bsz : n;
        for (int i=ii;i<im;i++)
          for (int j=jj;j<jm;j++){
            float acc = C[i*n+j];     // 레지스터 누적
            for (int k=kk;k<km;k++)
              acc += A[i*n+k]*B[k*n+j];
            C[i*n+j] = acc;
          }
      }
}
```
- **블록 크기**는 L1/L2에 맞춘다. 경험적으로 32–256 범위를 스윕.

### SoA / AoS / AoSoA

```c
// SoA: 필드별로 연속 → 벡터화/대역 효율↑
typedef struct { float *x, *y, *z; } vec3_soa;

void norm_soa(vec3_soa v, int n){
  #pragma omp simd
  for (int i=0;i<n;i++){
    float dx=v.x[i], dy=v.y[i], dz=v.z[i];
    v.x[i] = dx*dx + dy*dy + dz*dz;
  }
}
```
- **AoSoA**(Structure of Arrays of Structs): 타일 단위로 SoA 묶음 → 캐시/벡터 균형.

### 스칼라 치환(불필요한 메모리 참조 제거)

```c
for (int i=0;i<n;i++){
  float xi=x[i], yi=y[i];  // 동일 원소 재사용
  y[i] = yi + a*xi;
}
```

### 비휘발 저장(스트리밍 결과)

- **한 번 쓰고 다시 안 볼** 큰 결과 버퍼 → **non-temporal store**(플랫폼 힌트)로 캐시 오염↓.
- x86 SSE/AVX: `_mm_stream_ps/_mm256_stream_ps` 등.

---

## 일관성(Coherence)과 폴스 셰어링

- 코어 간 일관성(MESI/MOESI)으로 **같은 라인**에 대한 쓰기는 무효화/소유권 전이를 유발.
- **폴스 셰어링**: 다른 변수라도 **같은 캐시 라인**(예: 64B)에 있으면 쓰기 충돌.

```c
typedef struct { _Alignas(64) long v; } padded_long;
padded_long counters[128]; // 스레드당 1개: 라인 단위 패딩으로 분리
```

**샤딩** 패턴:
```c
// per-thread shard에 누적 후, 드물게 병합
thread_local long local_cnt = 0;
void hit(){ local_cnt++; }
long total(){ long s=0; for(each thread) s+=thread[i].local_cnt; return s; }
```

---

## 측정과 진단

### perf 카운터(리눅스)

```sh
perf stat -e cycles,instructions,IPC,\
L1-dcache-loads,L1-dcache-load-misses,\
LLC-loads,LLC-load-misses,\
dTLB-load-misses,branch-misses \
./app
```
- **IPC 낮고 LLC miss↑** → 메모리 바운드 가능성.
- **dTLB miss↑** → TLB reach 부족/랜덤 접근.
- **branch-misses↑** → 분기 패턴 개선/브랜치리스.

### 프로파일러

- **VTune/Advisor**: 메모리 대역, 벡터화, MLP 분석.
- **Valgrind Cachegrind**: 캐시 시뮬로 패턴 비교.
- **STREAM**: 메모리 대역 측정(복사/스케일/합/삼중연산).

STREAM 빌드/실행(예):
```sh
gcc -O3 -fopenmp stream.c -o stream
OMP_NUM_THREADS=8 ./stream
```

---

## 흔한 병목 → 처방 매핑

증상 | 지표/징후 | 1차 처방 | 2차 처방
---|---|---|---
IPC<1, LLC miss↑ | backend bound | **타일링/연속 접근/SoA** | 프리패치, 알고리즘 재구성
dTLB miss↑ | 페이지 워크↑ | Huge Page | 페이지 컬러링/할당기 튜닝
Row miss↑ | DRAM bank 충돌 | 행 지역성↑ | 채널/뱅크 분산, 데이터 배치
폴스 셰어링 | 코히어런시 트래픽↑ | 패딩/샤딩 | 락 없는 구조, 큐 분리
세트 스래싱 | 특정 세트 충돌↑ | 패딩/오프셋 | 해시/인덱스 재설계

---

## 실험 템플릿

### 행/열 순회 비교

```c
#include <time.h>
#include <stdio.h>
#include <stdlib.h>

static double now_s(){ struct timespec t; clock_gettime(CLOCK_MONOTONIC,&t);
  return t.tv_sec + t.tv_nsec*1e-9; }

long long sum_row(int *a, int n){
  long long s=0;
  for(int i=0;i<n;i++)
    for(int j=0;j<n;j++)
      s += a[i*n+j];
  return s;
}
long long sum_col(int *a, int n){
  long long s=0;
  for(int j=0;j<n;j++)
    for(int i=0;i<n;i++)
      s += a[i*n+j];
  return s;
}
int main(int argc,char**argv){
  int n=(argc>1)?atoi(argv[1]):4096;
  int *a=aligned_alloc(64, n*n*sizeof(int));
  for (long i=0;i<(long)n*n;i++) a[i]=i&1;
  double t0=now_s(); volatile long long s1=sum_row(a,n);
  double t1=now_s(); volatile long long s2=sum_col(a,n);
  double t2=now_s();
  printf("row=%.3fs col=%.3fs diff=%lld\n", t1-t0, t2-t1, (long long)(s1-s2));
  free(a);
}
```

### 캐시/LLC 용량 탐지(계단 그래프)

```c
// stride_bench.c — stride/working-set 스윕으로 캐시 '계단' 관찰
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

static double now(){ struct timespec t; clock_gettime(CLOCK_MONOTONIC,&t);
  return t.tv_sec + t.tv_nsec*1e-9; }

int main(){
  const size_t max = 256*1024*1024; // 256MB
  int *a = aligned_alloc(64, max);
  for (size_t i=0;i<max/sizeof(int);++i) a[i]=i;

  for (size_t ws=8*1024; ws<=max; ws*=2){        // working set
    for (size_t stride=64; stride<=4096; stride*=2){ // 바이트
      size_t step = stride/sizeof(int);
      size_t n = ws/sizeof(int);
      volatile int s=0;
      double t0=now();
      for (int rep=0; rep<16; ++rep)
        for (size_t i=0;i<n; i+=step) s += a[i];
      double t1=now();
      double iters = (double)16*(n/step);
      printf("ws=%zuKB stride=%zuB  ns/iter=%.2f\n",
             ws/1024, stride, 1e9*(t1-t0)/iters);
    }
  }
  free(a);
}
```
- **ns/iter**가 급증하는 지점이 **캐시/TLB 임계**를 시사.

### 프리패치 거리 스윕

```c
float sum_pf(const float *a, int n, int dist){
  float s=0;
  for (int i=0;i<n;i++){
    __builtin_prefetch(&a[i+dist], 0, 1);
    s += a[i];
  }
  return s;
}
```
- 스크립트로 `dist ∈ {16,32,64,128}` 스윕 → 중앙값 비교.

---

## 프로덕션 팁: 시스템/할당기/NUMA

### Huge Page

```sh
# 전역 HugeTLB 예약(예: 2MB 페이지 2048개)

echo 2048 | sudo tee /proc/sys/vm/nr_hugepages
# 프로세스에서 사용: libhugetlbfs or mmap(MAP_HUGETLB)

```
- **THP(Transparent Huge Pages)** 는 자동이지만 예측성/NUMA 제어가 어려울 수 있다(워크로드에 따라 on/off 측정).

### NUMA 배치/확인

```sh
numactl --hardware
numactl --cpunodebind=0 --membind=0 ./app
```
- **초기화 루프**를 병렬로 돌려 **first-touch** 보장.

### 할당기 선택

- glibc malloc vs **jemalloc/tcmalloc**: 큰 워킹셋·다스레드에서 단편화·락 경합 차이.
- **배열·버퍼는 aligned_alloc**(64B 이상)으로 정렬 → 스트라이드/벡터화 유리.

---

## 수학 메모(요약)

- **AMAT**:
\[
\text{AMAT} = T_{L1} + m_1\big(T_{L2} + m_2(T_{L3} + m_3T_\text{Mem})\big)
\]
- **CPI(MLP 고려)**:
\[
\Delta\text{CPI}_\text{mem} \approx \frac{\text{Access/Instr}\times \text{Miss/Access}\times \text{Penalty}}{\text{MLP}}
\]
- **TLB reach**:
\[
\text{Reach} \approx N_\text{entries}\times \text{page\_size}
\]
- **프리패치 거리**:
\[
d \approx \Big\lceil \frac{L}{C_\text{iter}} \Big\rceil
\]

---

## 바로 적용 체크리스트

- [ ] **행(스트라이드 1)** 순회인가? (언어 레이아웃과 일치?)
- [ ] **타일링/블로킹**으로 L1/L2에 붙여두는가?
- [ ] **SoA/AoSoA + 정렬/패딩**으로 벡터화·충돌을 돕는가?
- [ ] **스칼라 치환/루프 불변 호이스팅**으로 중복 로드를 줄였는가?
- [ ] **프리패치**가 유효한 패턴인가? 거리 튜닝 했는가?
- [ ] **Huge Page/NUMA first-touch** 적용했는가?
- [ ] **폴스 셰어링** 없는가(라인 패딩/샤딩)?
- [ ] `perf`/프로파일로 **IPC/미스율/대역**을 수치로 확인했는가?
- [ ] **STREAM/마이크로벤치**로 메모리 상한을 파악했는가?
- [ ] 최적화 전/후를 **중앙값 + 신뢰구간**으로 보고했는가?

---

## 맺음말

메모리 계층은 **지연·대역·용량**의 상충을 **지역성**으로 다루는 장치다.
성능의 다수는 **데이터가 어디에 있고 어떤 순서로 접근되는가**에서 결정된다.
따라서 **데이터 레이아웃/순회 순서/타일 크기/프리패치 거리/NUMA 배치**를 **측정→개선→재측정**으로 조율하라.
이 문서의 코드 템플릿과 체크리스트를 **당장 적용**해 워크로드에 맞는 최적 지점을 찾아가기 바란다.
