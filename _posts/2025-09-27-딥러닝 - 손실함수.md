---
layout: post
title: 딥러닝 - 손실함수
date: 2025-09-27 16:25:23 +0900
category: 딥러닝
---
# 1.5 손실함수 기초  
**MSE · BCE/BCEWithLogits · Cross-Entropy · 라벨 스무딩 · 클래스 불균형 가중치(Weighting)**

## A. 손실함수란?
- 학습의 목표를 수치로 표현한 **평가 함수**. 모델 파라미터 $$\theta$$ 가 예측 $$\hat{y}=f_\theta(x)$$ 를 만들었을 때, 정답 $$y$$ 와의 **오차 비용**을 산출한다.
- **경사하강(역전파)** 과정에서 손실의 **미분가능성**과 **수치안정성**이 중요하다.
- 실무에서 중요한 파라미터:
  - `reduction`: `"mean"`(기본) / `"sum"` / `"none"`  
  - **가중치**: 불균형 데이터에서 클래스/샘플별 가중치  
  - **입력 형태**: **로짓(logits)** vs **확률(probabilities)** — *대부분의 손실은 로짓 입력을 기대*

---

## B. 회귀: 평균제곱오차 MSE (Mean Squared Error)

### B-1. 정의
- 회귀(연속값 예측)에서 가장 기본적인 손실:
  $$
  \mathrm{MSE}(\hat{y}, y) \;=\; \frac{1}{N}\sum_{i=1}^{N} \|\hat{y}_i - y_i\|_2^2
  $$
- 미분:
  $$
  \frac{\partial \mathrm{MSE}}{\partial \hat{y}} \;=\; \frac{2}{N}(\hat{y} - y)
  $$

### B-2. 특징
- 큰 오차에 **제곱 페널티** → 이상치(outlier)에 민감.
- 정규분포 가정(등분산 잡음)에서 **최우추정(MLE)** 와 일치.

### B-3. PyTorch 예제 (주택 가격 예측 가정)
```python
import torch, torch.nn as nn
torch.manual_seed(0)

X = torch.randn(1024, 10)             # 10개 피처
true_w = torch.randn(10, 1)
y = X @ true_w + 0.5*torch.randn(1024, 1)  # 노이즈 포함 타깃

model = nn.Sequential(nn.Linear(10, 64), nn.ReLU(), nn.Linear(64, 1))
opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
mse = nn.MSELoss()

for epoch in range(200):
    pred = model(X)
    loss = mse(pred, y)
    opt.zero_grad(); loss.backward(); opt.step()
```

> 팁  
> - **스케일**: 타깃 스케일이 크면 MSE 값이 커져 최적화가 어려울 수 있다 → **표준화** 또는 **MAE/Huber** 같은 대안 고려.

---

## C. 이진 분류: BCE & **BCEWithLogits** (안정형)

### C-1. BCE(이진 크로스엔트로피) 정의
- 확률 $$\hat{p}\in(0,1)$$, 라벨 $$y\in\{0,1\}$$:
  $$
  \mathrm{BCE}(\hat{p}, y) \;=\; -\Big[y\log(\hat{p}) + (1-y)\log(1-\hat{p})\Big]
  $$
- **문제**: $$\hat{p}=\sigma(z)$$ 로 시그모이드를 직접 적용한 뒤 로그를 취하면 **수치 불안정**(saturate/overflow) 가능.

### C-2. 안정형: BCEWithLogits (로짓 입력)
- 로짓 $$z\in\mathbb{R}$$ 에 대해:
  $$
  \mathrm{BCEWithLogits}(z,y)
  \;=\; \max(z,0) - z\,y + \log\!\big(1+e^{-|z|}\big)
  $$
- 내부적으로 **시그모이드 + BCE** 를 **안정적으로** 합친 형태 → **반드시 로짓을 입력**한다.

### C-3. 클래스 불균형(이진)에서 `pos_weight`
- **양성(1) 수가 극히 적을 때** 양성 항만 추가 가중:
  $$
  \ell(z,y) = -\big[\,\underbrace{w_p}_{\text{pos\_weight}} \, y\log\sigma(z) + (1-y)\log(1-\sigma(z))\big]
  $$
- PyTorch `BCEWithLogitsLoss(pos_weight=tensor([...]))` 는 양성 항에만 가중을 준다.  
  비율 가이드: `pos_weight ≈ (N_neg / N_pos)`.

### C-4. PyTorch 예제 (이진·심한 불균형: 사기탐지)
```python
import torch, torch.nn as nn
torch.manual_seed(0)

N, D = 5000, 20
X = torch.randn(N, D)
y = (torch.rand(N) < 0.03).float()  # 3%만 양성(사기)

model = nn.Sequential(nn.Linear(D, 64), nn.ReLU(), nn.Linear(64, 1))
opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)

pos_weight = torch.tensor([(1 - y.mean()) / y.mean()])  # ~ 0.97/0.03 ≈ 32.3
crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

for epoch in range(20):
    z = model(X).squeeze(1)      # 로짓
    loss = crit(z, y)
    opt.zero_grad(); loss.backward(); opt.step()
```

> 팁  
> - **결과 임계값**(0.5)이 최적이 아닐 수 있다 → **PR-AUC/F1** 기준으로 최적 임계값을 선택.  
> - 이진이지만 라벨이 **멀티라벨**(여러 클래스 독립 이진)이라면 **차원별 BCEWithLogits** 사용.

---

## D. 멀티라벨(다중 라벨) 분류: 독립 BCE의 벡터화

### D-1. 정의
- 샘플 하나에 여러 라벨이 동시에 True일 수 있는 문제(예: 이미지 태그).
- **각 라벨을 독립 이진**으로 처리 → 출력 차원 $$K$$ 의 로짓 $$\mathbf{z}\in\mathbb{R}^{K}$$, 타깃 $$\mathbf{y}\in\{0,1\}^K$$:
  $$
  \mathrm{BCEWithLogits}(\mathbf{z},\mathbf{y}) \;=\; \sum_{k=1}^{K} \mathrm{BCEWithLogits}(z_k, y_k)
  $$

### D-2. PyTorch 예제
```python
import torch, torch.nn as nn
torch.manual_seed(0)

B, D, K = 64, 100, 5   # 5개 태그(멀티라벨)
X = torch.randn(B, D)
Y = (torch.rand(B, K) > 0.7).float()  # 30% 확률로 각 태그 True

model = nn.Sequential(nn.Linear(D, 128), nn.ReLU(), nn.Linear(128, K))
z = model(X)  # (B, K)
crit = nn.BCEWithLogitsLoss()  # 로짓 입력

loss = crit(z, Y)
loss.backward()
```

> 팁  
> - 클래스별 불균형이 다르면 `pos_weight` 를 **라벨별 벡터**로 제공할 수 있다(크기 K).  
> - 출력층에서 `Sigmoid`를 **따로 쓰지 말고** 로짓을 그대로 손실에 넣는 것이 원칙.

---

## E. 다중분류(One-of-K): Cross-Entropy (Softmax-CE)

### E-1. 정의 (안정형)
- 로짓 $$\mathbf{z}\in\mathbb{R}^{K}$$, 정답 클래스 $$y\in\{0,\dots,K-1\}$$:
  $$
  \ell(\mathbf{z}, y) \;=\; \log\!\Big(\sum_{j=1}^{K}e^{z_j}\Big) - z_y
  $$
- 수치안정성( LogSumExp ):
  $$
  \log\!\sum_{j}e^{z_j} \;=\; m \;+\; \log\!\sum_{j}e^{z_j - m},\quad m=\max_j z_j
  $$

### E-2. 그라디언트
- 소프트맥스 확률 $$p_j=\frac{e^{z_j}}{\sum_k e^{z_k}}$$:
  $$
  \frac{\partial \ell}{\partial z_j} \;=\; p_j - \mathbb{1}\{j=y\}
  $$
- **해석**: 예측확률과 one-hot 라벨의 **차이**.

### E-3. PyTorch 예제
```python
import torch, torch.nn as nn
torch.manual_seed(0)

N, D, K = 1024, 50, 7
X = torch.randn(N, D)
W_true = torch.randn(D, K)
y = (X @ W_true).argmax(dim=1)  # 선형으로 생성한 라벨

model = nn.Sequential(nn.Linear(D, 128), nn.ReLU(), nn.Linear(128, K))
opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
ce = nn.CrossEntropyLoss()  # **로짓 입력**, 내부적으로 log-softmax + NLL

for epoch in range(200):
    logits = model(X)
    loss = ce(logits, y)
    opt.zero_grad(); loss.backward(); opt.step()
```

> 주의  
> - **소프트맥스를 미리 적용하지 않는다.** `CrossEntropyLoss` 는 **로짓**을 기대한다.  
> - 확률을 따로 보고 싶으면 `torch.softmax(logits, dim=-1)` 를 *추론 시*에만 사용.

---

## F. 라벨 스무딩(Label Smoothing)

### F-1. 개념과 동기
- One-hot 라벨은 정답 클래스 확률을 1로 강제 → **과신(over-confidence)** 과 **일반화 저하**.  
- 스무딩으로 타깃을 **조금 분산**시켜 모델이 지나치게 확신하지 않게 한다.

### F-2. 스무딩 타깃
- $$K$$ 클래스, 스무딩 파라미터 $$\varepsilon\in[0,1)$$ 일 때:
  $$
  \mathbf{t}_k \;=\;
  \begin{cases}
  1-\varepsilon & k=y\\[2pt]
  \dfrac{\varepsilon}{K-1} & k\ne y
  \end{cases}
  $$
- 손실(소프트 타깃):
  $$
  \ell(\mathbf{z}, \mathbf{t}) \;=\; -\sum_{k=1}^{K} t_k \,\log p_k,
  \quad p_k=\mathrm{softmax}(\mathbf{z})_k
  $$

### F-3. 효과
- **로짓 마진** 과도증가를 억제 → **캘리브레이션** 향상, **일반화** 개선.  
- 너무 큰 $$\varepsilon$$ 는 **구분 능력**을 약화시킬 수 있음(보통 0.05~0.2 사이 탐색).

### F-4. PyTorch 예제 (내장)
```python
import torch, torch.nn as nn
torch.manual_seed(0)

logits = torch.randn(8, 5)             # (B, K)
targets = torch.randint(0, 5, (8,))    # 정수 라벨
ce_smooth = nn.CrossEntropyLoss(label_smoothing=0.1)
loss = ce_smooth(logits, targets)      # 내부적으로 스무딩 적용
loss.backward()
```

### F-5. 수동 구현(검증용)
```python
def cross_entropy_with_soft_targets(logits, soft_targets):
    log_p = torch.log_softmax(logits, dim=-1)
    return -(soft_targets * log_p).sum(dim=-1).mean()

def label_smooth_one_hot(targets, num_classes, epsilon=0.1):
    with torch.no_grad():
        y = torch.zeros(targets.size(0), num_classes, device=targets.device)
        y.fill_(epsilon / (num_classes - 1))
        y.scatter_(1, targets.unsqueeze(1), 1 - epsilon)
    return y

B, K = 8, 5
logits = torch.randn(B, K)
targets = torch.randint(0, K, (B,))
soft_t = label_smooth_one_hot(targets, K, epsilon=0.1)
loss = cross_entropy_with_soft_targets(logits, soft_t)
```

---

## G. 클래스 불균형 가중치(Class Weighting)

### G-1. 왜 필요한가?
- 데이터가 **한 클래스에 치우친** 경우, 평균 손실이 **다수 클래스** 중심으로 계산되어 소수 클래스에 둔감해진다.
- 해결: 손실에 **클래스별 가중치**를 곱해 **균형**을 맞춘다.

### G-2. 다중분류에서의 가중치
- `CrossEntropyLoss(weight=class_weight)` 에서 `class_weight` 는 길이 $$K$$ 텐서.  
  손실:  
  $$
  \ell = -\, w_y \,\log p_y
  $$
- 보통 **역빈도**로 설정:
  $$
  w_k \;\propto\; \frac{1}{\text{freq}(k)}
  $$

### G-3. PyTorch 예제 (가중치 계산)
```python
import torch, torch.nn as nn
torch.manual_seed(0)

# 불균형 라벨 예: 0: 70%, 1: 20%, 2: 10%
y = torch.cat([
    torch.zeros(700, dtype=torch.long),
    torch.ones(200, dtype=torch.long),
    torch.full((100,), 2, dtype=torch.long)
])
counts = torch.bincount(y)                # tensor([700, 200, 100])
weights = 1.0 / counts.float()            # 역빈도
weights = weights / weights.sum() * len(weights)  # 스케일(선택)

logits = torch.randn(len(y), 3)
ce_w = nn.CrossEntropyLoss(weight=weights)
loss = ce_w(logits, y)
loss.backward()
```

> 팁  
> - **이진**의 경우 `BCEWithLogitsLoss(pos_weight=...)` 가 간편.  
> - **다중분류**는 `weight=` 로 클래스별 가중.  
> - 더 강력한 방법으로 **샘플링 전략**(e.g., `WeightedRandomSampler`) 도 함께 고려.

---

## H. `reduction`과 샘플별 가중(Per-sample Weight)

### H-1. `reduction='none'`
- 손실을 샘플/토큰 단위로 받아 후처리(마스킹/가중) 가능.

```python
import torch, torch.nn as nn

logits = torch.randn(6, 4)
targets = torch.tensor([0,1,2,1,0,3])
ce = nn.CrossEntropyLoss(reduction='none')
loss_per_sample = ce(logits, targets)  # (6,)

weights = torch.tensor([1., 2., 1., 0.5, 1., 3.])  # 샘플별 가중
loss = (loss_per_sample * weights).mean()
loss.backward()
```

### H-2. 시퀀스 마스킹 (NLP에서 `ignore_index`)
- 패딩 토큰은 학습에서 제외해야 한다.
```python
import torch, torch.nn as nn

logits = torch.randn(4, 10, 30522)  # (B, L, V)
targets = torch.randint(0, 30522, (4, 10))
targets[targets < 10] = -100  # 예: -100을 ignore_index로 사용
crit = nn.CrossEntropyLoss(ignore_index=-100)

loss = crit(logits.view(-1, logits.size(-1)), targets.view(-1))
loss.backward()
```

---

## I. 수치안정성 체크리스트

1. **이진/멀티라벨**: `BCEWithLogitsLoss` 사용(로짓 입력).  
2. **다중분류**: `CrossEntropyLoss` 사용(로짓 입력).  
3. Softmax/ Sigmoid 를 **이중 적용**하지 않는다(출력층에서 확률로 바꾸지 말고 로짓 그대로).  
4. 큰 로짓 값에서의 overflow는 **LogSumExp** 로 처리(프레임워크가 내부 처리).  
5. FP16/AMP 사용 시 **손실 스케일링**은 옵티마이저가 내부 처리(GradScaler).  
6. 매우 불균형이면 **가중치/샘플러/임계값 최적화** 병행.

---

## J. 실전 미니 프로젝트: 네 가지 문제 한 번에

### J-1. 데이터 & 모델
- 회귀(주가 예측 느낌의 합성), 이진(사기탐지), 멀티라벨(이미지 태그), 다중분류(뉴스 주제) — **각각 손실을 바꿔가며** 동일한 학습 루프 재사용.

```python
import torch, torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
torch.manual_seed(42)

# 1) 회귀
Xr = torch.randn(1024, 16); wr = torch.randn(16, 1)
Yr = Xr @ wr + 0.3*torch.randn(1024, 1)

# 2) 이진(불균형)
Xb = torch.randn(2000, 20)
Yb = (torch.rand(2000) < 0.05).float()

# 3) 멀티라벨(K=4)
Xm = torch.randn(800, 32)
Ym = (torch.rand(800, 4) > 0.7).float()

# 4) 다중분류(K=5)
Xd = torch.randn(1500, 24)
Wd = torch.randn(24, 5)
Yd = (Xd @ Wd).argmax(dim=1)

def make_loader(X, Y, batch=128, shuffle=True):
    ds = TensorDataset(X, Y) if Y.ndim>1 else TensorDataset(X, Y.unsqueeze(-1))
    return DataLoader(ds, batch_size=batch, shuffle=shuffle)

# 간단 모델들
Reg = nn.Sequential(nn.Linear(16, 64), nn.ReLU(), nn.Linear(64, 1))
Bin = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 1))     # 로짓 1
Mul = nn.Sequential(nn.Linear(32, 64), nn.ReLU(), nn.Linear(64, 4))     # 로짓 4
Clf = nn.Sequential(nn.Linear(24, 64), nn.ReLU(), nn.Linear(64, 5))     # 로짓 5

def train_epoch(model, loader, crit, opt):
    model.train()
    total=0; n=0
    for xb, yb in loader:
        xb = xb.squeeze(1) if xb.ndim==3 else xb
        yb = yb.squeeze(-1)
        z = model(xb).squeeze(-1) if isinstance(crit, nn.BCEWithLogitsLoss) or isinstance(crit, nn.MSELoss) else model(xb)
        loss = crit(z, yb if isinstance(crit, nn.MSELoss) else (yb if z.ndim==1 else yb.long().squeeze()))
        opt.zero_grad(); loss.backward(); opt.step()
        total += loss.item()*len(xb); n += len(xb)
    return total/n
```

### J-2. 손실별 학습
```python
# 회귀: MSE
lr1 = 1e-3
opt = torch.optim.AdamW(Reg.parameters(), lr=lr1)
mse = nn.MSELoss()
regr_loader = make_loader(Xr, Yr, batch=128)
for ep in range(50): train_epoch(Reg, regr_loader, mse, opt)

# 이진: BCEWithLogits + pos_weight
pos_weight = torch.tensor([(1 - Yb.mean()) / Yb.mean()])
opt = torch.optim.AdamW(Bin.parameters(), lr=1e-3)
bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
bin_loader = make_loader(Xb, Yb, batch=256)
for ep in range(20): train_epoch(Bin, bin_loader, bce, opt)

# 멀티라벨: (B,4) 로짓에 BCEWithLogits
opt = torch.optim.AdamW(Mul.parameters(), lr=1e-3)
bce_multi = nn.BCEWithLogitsLoss()
mul_loader = DataLoader(TensorDataset(Xm, Ym), batch_size=128, shuffle=True)
for ep in range(30):
    Mul.train()
    for xb, yb in mul_loader:
        z = Mul(xb)           # (B,4) 로짓
        loss = bce_multi(z, yb)
        opt.zero_grad(); loss.backward(); opt.step()

# 다중분류: CE + 라벨 스무딩
opt = torch.optim.AdamW(Clf.parameters(), lr=1e-3)
ce_smooth = nn.CrossEntropyLoss(label_smoothing=0.1)
clf_loader = make_loader(Xd, Yd, batch=256)
for ep in range(40):
    Clf.train()
    total=0; n=0
    for xb, yb in clf_loader:
        logits = Clf(xb)
        loss = ce_smooth(logits, yb.squeeze().long())
        opt.zero_grad(); loss.backward(); opt.step()
```

---

## K. 자주 하는 실수 & 예방법

1. **CE에 Softmax를 중복 적용**  
   - 잘못: `softmax(logits)` 후 `CrossEntropyLoss` → 소프트맥스 이중 적용.  
   - 정답: **로짓**을 바로 `CrossEntropyLoss` 에 넣는다.

2. **BCE에 Sigmoid를 중복 적용**  
   - 잘못: `sigmoid(z)` 후 `BCEWithLogitsLoss` → 시그모이드 이중 적용.  
   - 정답: **로짓** `z` 를 그대로 `BCEWithLogitsLoss` 에 넣는다.

3. **불균형에서 단순 정확도만 보고 모델 선택**  
   - 정답: **PR-AUC, F1, 리콜/정밀도**를 함께 본다. 임계값 튜닝 필수.

4. **라벨 스무딩을 모든 곳에 남용**  
   - 스무딩은 과신 억제에 유용하지만, **극소수 클래스 탐지**에서는 민감도(리콜) 저하 가능 → 데이터/업무 목적에 따라 조절.

5. **`ignore_index` 미설정으로 패딩까지 학습**  
   - NLP/시퀀스에서 패딩 토큰을 **반드시 무시**해야 한다.

---

## L. 수식 요약(빠른 참조)

- **MSE**
  $$
  \mathrm{MSE}=\frac{1}{N}\sum_i (\hat{y}_i - y_i)^2,\quad
  \frac{\partial \mathrm{MSE}}{\partial \hat{y}}=\frac{2}{N}(\hat{y}-y)
  $$

- **BCE (확률 입력)**
  $$
  \mathrm{BCE}(\hat{p},y)=-\Big[y\log\hat{p}+(1-y)\log(1-\hat{p})\Big]
  $$

- **BCEWithLogits (로짓 입력)**
  $$
  \ell(z,y)=\max(z,0)-zy+\log(1+e^{-|z|})
  $$

- **Cross-Entropy (다중분류, 안정형)**
  $$
  \ell(\mathbf{z},y)=\log\sum_j e^{z_j}-z_y,\quad
  \frac{\partial \ell}{\partial z_j}=p_j-\mathbb{1}\{j=y\}
  $$

- **Label Smoothing 타깃**
  $$
  t_k=\begin{cases}
  1-\varepsilon & k=y\\
  \varepsilon/(K-1) & k\ne y
  \end{cases},\quad
  \ell=-\sum_k t_k \log p_k
  $$

- **클래스 가중(다중분류)**
  $$
  \ell=-\,w_y \log p_y
  $$

- **BCE 양성 가중(이진)**
  $$
  \ell=-\big[w_p\,y\log\sigma(z)+(1-y)\log(1-\sigma(z))\big]
  $$

---

## M. 선택과 조합 가이드

- **회귀**: MSE(기본) → 이상치 많으면 **MAE/Huber** 검토.  
- **이진**: **BCEWithLogits** (+ `pos_weight` 불균형 보정).  
- **멀티라벨**: **BCEWithLogits** (출력 차원=라벨 수).  
- **다중분류**: **CrossEntropy** (+ `label_smoothing`/`class_weight`).  
- **불균형**: 가중치 + 샘플러(예: `WeightedRandomSampler`) + 임계값 튜닝.

---

## N. 연습 과제 (권장)

1. 다중분류에서 **`label_smoothing`** 값을 {0, 0.05, 0.1, 0.2} 로 바꾸어 **정확도/캘리브레이션** 비교.  
2. 이진 불균형 문제에서 `pos_weight` 를 **N_neg/N_pos** 와 **sqrt(N_neg/N_pos)** 로 바꾸어 F1/PR-AUC 비교.  
3. 멀티라벨에서 **라벨별 `pos_weight`** 를 실제 빈도에 맞춰 벡터로 세팅해 성능 변화를 측정.  
4. 회귀에서 타깃 스케일을 **10배** 키워 MSE 학습 속도 변화를 관찰(스케일링의 중요성).

---

### 결론
- 손실함수는 **과제의 본질**을 수학적으로 정의한다.  
- **로짓 입력의 안정형 손실**(BCEWithLogits, CrossEntropy)이 기본.  
- **라벨 스무딩/가중치** 등은 **일반화/불균형** 문제를 다루는 **첫 번째 도구**다.  
- 문제 특성에 맞게 **지표/임계값**까지 포함해 **엔드투엔드로** 설계하자.