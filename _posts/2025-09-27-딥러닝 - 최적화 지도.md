---
layout: post
title: 딥러닝 - 최적화 지도
date: 2025-09-27 18:25:23 +0900
category: 딥러닝
---
# 1.7 최적화 지도
**SGD/모멘텀/Nesterov · RMSProp · Adam/AdamW · 학습률 스케줄링(Cosine/Step/OneCycle) · 워밍업/쿨다운 · 그라디언트 클리핑 · 기울기 축적**

## A. 기본 형태: 경사하강의 뼈대

### A-1. 목적과 한 스텝
손실 $$\mathcal{L}(\theta)$$ 를 줄이기 위해, **그라디언트** $$g_t=\nabla_\theta \mathcal{L}(\theta_t)$$ 를 이용해 파라미터를 갱신한다.
$$
\theta_{t+1} \;=\; \theta_t \;-\; \eta_t \, g_t
$$
- $$\eta_t$$: 학습률(Learning Rate, LR). 일정할 수도 있고 **스케줄**로 시간에 따라 변한다.
- **미니배치**: $$g_t \approx \frac{1}{B}\sum_{i\in\mathcal{B}}\nabla_\theta \ell_i(\theta_t)$$

### A-2. Weight Decay(가중치 감쇠)
$$
\theta_{t+1} \;=\; \theta_t \;-\; \eta_t \, g_t \;-\; \eta_t\,\lambda \,\theta_t
$$
- $$\lambda$$: L2 정규화 계수.
- **AdamW** 는 **감쇠를 모멘텀/적응항과 분리**(decoupled)하여 일반화 및 제어성이 좋아졌다(뒤에서 상세).

---

## B. Optimizer 총정리

### B-1. SGD(Plain) / Momentum / Nesterov

#### (1) SGD
$$
\theta_{t+1}=\theta_t-\eta_t g_t
$$
- **장점**: 단순/메모리 효율. 큰 배치/잘 정규화된 문제에서 강력.
- **단점**: 손실 표면이 **길쭉(anisotropic)** 하면 진동/느린 수렴.

#### (2) Momentum
$$
\begin{aligned}
v_{t+1} &= \mu v_t + g_t\\
\theta_{t+1} &= \theta_t - \eta_t v_{t+1}
\end{aligned}
$$
- $$\mu \in [0,1)$$: 관성(보통 0.9). **평균화된 기울기**로 골짜기 벽 흔들림↓.

#### (3) Nesterov (NAG)
**한 발 앞서 보기(lookahead)**:
$$
\begin{aligned}
\tilde{\theta}_t &= \theta_t - \eta_t \mu v_t\\
v_{t+1} &= \mu v_t + \nabla_\theta \mathcal{L}(\tilde{\theta}_t)\\
\theta_{t+1} &= \theta_t - \eta_t v_{t+1}
\end{aligned}
$$
- **더 미리** 경사 방향을 본다 → 곡률이 큰 곳에서 안정.

**PyTorch 설정**
```python
torch.optim.SGD(params, lr, momentum=0.9, nesterov=True, dampening=0.0)
```
> `nesterov=True` 는 momentum>0, dampening=0일 때만 유효.

**권장값(초기 가이드)**: `lr=0.1`(큰 배치), `momentum=0.9`, `weight_decay=1e-4`.

---

### B-2. RMSProp (적응적 스텝: 축별 스케일링)
$$
\begin{aligned}
s_{t+1} &= \rho s_t + (1-\rho)\, g_t^2\\
\theta_{t+1} &= \theta_t - \eta_t \frac{g_t}{\sqrt{s_{t+1}}+\epsilon}
\end{aligned}
$$
- **축별로 스케일**을 조절하여 지그재그 완화.
- $$\rho\sim 0.9$$, $$\epsilon\sim 10^{-8}$$. RNN/강화학습에서 전통적으로 많이 사용.

**PyTorch**
```python
torch.optim.RMSprop(params, lr=1e-3, alpha=0.99, eps=1e-8, momentum=0.0, centered=False)
```
> `alpha` 는 위 수식의 $$\rho$$에 해당(명칭만 다름). `centered=True` 는 분산 중심화(추정 비용↑).

---

### B-3. Adam / AdamW (1차·2차 모멘트 적응)

#### (1) Adam
$$
\begin{aligned}
m_{t+1} &= \beta_1 m_t + (1-\beta_1) g_t \\
v_{t+1} &= \beta_2 v_t + (1-\beta_2) g_t^2 \\
\hat m_{t+1} &= \frac{m_{t+1}}{1-\beta_1^{t+1}},\quad
\hat v_{t+1} = \frac{v_{t+1}}{1-\beta_2^{t+1}} \\
\theta_{t+1} &= \theta_t - \eta_t \frac{\hat m_{t+1}}{\sqrt{\hat v_{t+1}}+\epsilon}
\end{aligned}
$$
- **장점**: 튜닝 쉬움, 초기 학습 빠름, 널리 쓰이는 기본값: `β1=0.9, β2=0.999, eps=1e-8`.
- **단점**: L2 정규화(`weight_decay`)가 모멘텀 항과 얽히면 **의도한 규제와 다르게 작동** 가능.

#### (2) AdamW (Decoupled Weight Decay)
$$
\theta_{t+1} \;=\; \theta_t \;-\; \eta_t\frac{\hat m_{t+1}}{\sqrt{\hat v_{t+1}}+\epsilon} \;-\; \eta_t\lambda\,\theta_t
$$
- **감쇠를 분리**하여 **진짜 L2 규제**를 구현.
- **권장**: 오늘날 대부분의 Transformer/비전 모델의 기본.

**PyTorch**
```python
torch.optim.AdamW(params, lr=1e-3, betas=(0.9,0.999), eps=1e-8, weight_decay=0.01)
```

#### (3) 어떤 걸 쓸까?
- **비전/언어·대부분**: **AdamW**(+Cosine/OneCycle, Warmup).
- **큰 배치·단순 백본**: **SGD+Momentum/Nesterov** 가 여전히 강함.
- **RNN/강화학습 구형 코드**: RMSProp 관성.

---

## C. 학습률 스케줄링

### C-1. 왜 스케줄인가?
- 초기에 **큰 LR** 로 빠른 진입, 후반에 **작은 LR** 로 정밀 수렴.
- **가속**과 **일반화**를 동시에 잡기 위한 **온도 조절 장치**.

### C-2. Step Decay
$$
\eta_t = \eta_0 \cdot \gamma^{\left\lfloor\frac{t}{T}\right\rfloor}
$$
- 간단/효과적. 갑작스런 점프가 생겨 **진동**이 있을 수도.

**PyTorch**
```python
scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=30, gamma=0.1)
```

### C-3. Cosine Annealing
$$
\eta_t = \eta_{\min} + \tfrac{1}{2}(\eta_0-\eta_{\min})\big(1+\cos(\pi\,t/T_{\max})\big)
$$
- **부드럽게 감소**, 종종 더 나은 일반화.

**PyTorch**
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=100, eta_min=1e-6)
```

**Warm Restarts(적용 예시)**: 주기적으로 다시 크게 키워 **새 지역 탐색**
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2)
```

### C-4. OneCycle (CLR의 진화)
- **상승(warmup)** → **최대 LR** → **감소(anneal)** 의 한 주기를 **한 번**만.
- 모멘텀은 역상(inverse)으로 조절(작→크).

**핵심 파라미터**
- `max_lr`: 정점 LR
- `pct_start`: 상승 비율(보통 0.3~0.45)
- `anneal_strategy`: "cos" 권장
- `div_factor`: 초기 LR = `max_lr / div_factor`
- `final_div_factor`: 마지막 LR = `initial_lr / final_div_factor`

**PyTorch**
```python
steps_per_epoch = len(train_loader)
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    opt, max_lr=3e-4, epochs=num_epochs, steps_per_epoch=steps_per_epoch,
    pct_start=0.3, anneal_strategy='cos', div_factor=25.0, final_div_factor=1e4
)
```

---

## D. 워밍업 / 쿨다운

### D-1. Warmup (초반 안전장치)
- 큰 배치/AdamW/Transformer에서 **초반 불안정** 방지:
$$
\eta_t = \eta_{\text{start}} + (\eta_{\text{target}}-\eta_{\text{start}})\frac{t}{T_\text{warmup}},\quad t\le T_\text{warmup}
$$
- 보통 **연속형 스케줄**(Cosine/OneCycle)과 **조합**.

**PyTorch(수동 구현 예)**
```python
def linear_warmup_factor(step, warmup_steps):
    return min(1.0, (step+1) / warmup_steps)

warmup_steps = 1000
for step, (xb, yb) in enumerate(train_loader):
    factor = linear_warmup_factor(step, warmup_steps)
    for g in opt.param_groups:
        g['lr'] = base_lr * factor
    # forward/backward/step...
```

### D-2. Cooldown
- 학습 말미에 LR을 **추가로 낮춰** 마지막 수렴을 돕는다(예: 마지막 1~5 에폭 `eta_min` 고정).

---

## E. 그라디언트 클리핑 & 기울기 축적

### E-1. 클리핑(폭주 방지)
- **노름 기반**
$$
g \leftarrow g \cdot \min\Big(1, \frac{\tau}{\|g\|_2}\Big)
$$
- **값 기반**: 각 성분을 `[-c, c]` 로 제한.

**PyTorch**
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
# 또는
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```
> AMP 사용 시 **`scaler.unscale_(opt)` 후** 클리핑.

### E-2. 기울기 축적(Gradient Accumulation)
GPU 메모리 제약으로 작은 배치만 가능할 때, **여러 스텝 그라디언트를 합산** 후 한 번 업데이트:
- **유효 배치**:
  $$B_{\text{eff}} = B_{\text{per\_step}} \times \text{accum\_steps} \times \text{num\_gpus}$$

**패턴**
```python
accum_steps = 4
opt.zero_grad(set_to_none=True)
for step, (xb, yb) in enumerate(train_loader, start=1):
    with autocast():
        loss = crit(model(xb), yb) / accum_steps
    scaler.scale(loss).backward()
    if step % accum_steps == 0:
        scaler.unscale_(opt)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(opt); scaler.update()
        opt.zero_grad(set_to_none=True)
```
> **BatchNorm 주의**: per-step 배치가 너무 작으면 BN 통계 불안정 → **SyncBN/LN/GN** 또는 큰 유효 배치 확보.

---

## F. 실전: 하나의 스크립트로 옵티마이저·스케줄을 바꿔가며 실험

### F-1. 합성 데이터(이진·약간 비선형)
```python
import torch, torch.nn as nn, math
from torch.utils.data import TensorDataset, DataLoader
torch.manual_seed(0)

def make_data(n=4000, noise=0.4):
    t = torch.rand(n)*2*math.pi
    r = 2 + torch.randn(n)*noise
    X = torch.stack([r*torch.cos(t), r*torch.sin(t)], dim=1)
    y = (t > math.pi).long()
    perm = torch.randperm(n)
    return X[perm], y[perm]

X, y = make_data()
n = len(X); n_tr = int(n*0.8)
Xtr, ytr = X[:n_tr], y[:n_tr]
Xva, yva = X[n_tr:], y[n_tr:]
tr_dl = DataLoader(TensorDataset(Xtr, ytr), batch_size=128, shuffle=True)
va_dl = DataLoader(TensorDataset(Xva, yva), batch_size=512)
```

### F-2. 모델 & 공용 루프
```python
class MLP(nn.Module):
    def __init__(self, d=2, h=128, k=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d,h), nn.GELU(),
            nn.Linear(h,h), nn.GELU(),
            nn.Linear(h,k)
        )
    def forward(self, x): return self.net(x)

def eval_acc(model, dl):
    model.eval(); c=t=0
    with torch.no_grad():
        for xb,yb in dl:
            p = model(xb).argmax(1)
            c += (p==yb).sum().item(); t += yb.numel()
    return c/t

def train_one(model, opt, scheduler=None, epochs=40, clip=1.0):
    ce = nn.CrossEntropyLoss()
    for ep in range(epochs):
        model.train()
        for xb, yb in tr_dl:
            opt.zero_grad(set_to_none=True)
            logits = model(xb)
            loss = ce(logits, yb)
            loss.backward()
            if clip: torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
            opt.step()
            if scheduler and not isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):
                scheduler.step()  # per-step형 스케줄러인 경우
        if scheduler and isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):
            pass  # OneCycle은 위에서 per-step으로 이미 step됨
        if scheduler and isinstance(scheduler, (torch.optim.lr_scheduler.StepLR,
                                               torch.optim.lr_scheduler.CosineAnnealingLR)):
            scheduler.step()  # per-epoch형인 경우
        if (ep+1) % 10 == 0:
            print(f"ep{ep+1:02d} va_acc={eval_acc(model, va_dl):.3f}")
```

### F-3. 조합 실험 예시
```python
# 1. SGD + Momentum + StepLR
m1 = MLP()
opt1 = torch.optim.SGD(m1.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4, nesterov=True)
sch1 = torch.optim.lr_scheduler.StepLR(opt1, step_size=20, gamma=0.1)
train_one(m1, opt1, scheduler=sch1, epochs=40)

# 2. AdamW + CosineAnnealing
m2 = MLP()
opt2 = torch.optim.AdamW(m2.parameters(), lr=3e-3, weight_decay=1e-2)
sch2 = torch.optim.lr_scheduler.CosineAnnealingLR(opt2, T_max=40, eta_min=1e-6)
train_one(m2, opt2, scheduler=sch2, epochs=40)

# 3. AdamW + OneCycle
m3 = MLP()
opt3 = torch.optim.AdamW(m3.parameters(), lr=1e-3, weight_decay=0.01)
sch3 = torch.optim.lr_scheduler.OneCycleLR(opt3, max_lr=3e-3,
        epochs=40, steps_per_epoch=len(tr_dl), pct_start=0.3,
        anneal_strategy='cos', div_factor=25.0, final_div_factor=1e4)
# train_one 내부에서 per-step step을 호출하도록 루프를 약간 변형해야 함(위 루프의 if 분기 참고).
```

> 관찰 포인트
> - 데이터/배치·모델 크기에 따라 **SGD vs AdamW** 우열이 달라질 수 있음.
> - 같은 옵티마이저라도 **스케줄러**가 크게 좌우(특히 OneCycle/Cosine).
> - **weight_decay** 로 일반화 갭을 조절.

---

## G. 하이퍼파라미터 레시피(현업 가이드)

| 상황 | Optimizer | LR(초기) | WD | 비고 |
|---|---|---:|---:|---|
| 소형 CNN/MLP, 데이터 적음 | AdamW | 1e-3 | 1e-2 | Cosine, warmup 500 steps |
| ResNet류, 대용량 이미지 | SGD+Nesterov | 0.1*(B/256) | 1e-4 | StepLR(30,60,80) or Cosine |
| Transformer(언어/비전) | AdamW | 1e-4~3e-4 | 0.05~0.1 | Linear warmup 1–10k steps + Cosine |
| 초거대 배치(>8k) | AdamW | 선형 스케일: $$\eta\propto B$$ | 0.01~0.1 | **필수: Warmup** |
| RNN/강화학습 구형 | RMSProp | 1e-3 | 0 | eps 1e-8, alpha 0.99 |

**추가 팁**
- **No-decay 그룹**: bias, LayerNorm/BatchNorm의 `weight` 는 `weight_decay=0` 권장(AdamW).
```python
decay, no_decay = [], []
for n,p in model.named_parameters():
    if p.ndimension()==1 or n.endswith("bias"): no_decay.append(p)
    else: decay.append(p)
opt = torch.optim.AdamW([
    {"params": decay, "weight_decay": 0.01},
    {"params": no_decay, "weight_decay": 0.0},
], lr=3e-4)
```
- **Linear Scaling Rule**: 배치가 $$k$$ 배 커지면 LR을 **대략 $$k$$ 배** 키우고 **warmup** 추가.
- **β2 조정**: 극단적 잡음/작은 배치에서 `β2=0.98~0.995` 로 바꿔보면 종종 안정.

---

## H. 자주 겪는 문제와 해결

1) **손실 NaN/Inf**
- LR 과대 → 줄이기 / Warmup 추가
- 손실 수치안정: BCEWithLogits/CE(로짓 입력)
- AMP: `scaler.unscale_()` 후 클리핑, `eps` 키우기(AdamW `eps=1e-8→1e-6`)

2) **수렴은 빠른데 일반화 나쁨**
- WD↑, 라벨 스무딩, 증강↑, 스케줄 완만(Cosine)
- AdamW→SGD 전환 검토(특히 비전)

3) **진동/불안정**
- Momentum↓(0.9→0.8), β1↓(0.9→0.8), β2↑(0.99→0.999)
- Clip norm(1.0), Cosine/OneCycle로 부드럽게

4) **학습이 너무 느림**
- OneCycle로 **초반 가속**
- Mixed Precision(AMP), 큰 배치(+누적), `pin_memory/persistent_workers`

5) **BN 불안정(작은 배치)**
- SyncBN/GroupNorm/LayerNorm, 또는 **유효 배치** 키우기(누적)

---

## I. 수식 요약(치트시트)

- **SGD**
  $$
  \theta_{t+1}=\theta_t-\eta_t g_t
  $$

- **Momentum**
  $$
  v_{t+1}=\mu v_t+g_t,\quad \theta_{t+1}=\theta_t-\eta_t v_{t+1}
  $$

- **Nesterov**
  $$
  v_{t+1}=\mu v_t+\nabla \mathcal{L}(\theta_t-\eta_t \mu v_t),\quad \theta_{t+1}=\theta_t-\eta_t v_{t+1}
  $$

- **RMSProp**
  $$
  s_{t+1}=\rho s_t+(1-\rho)g_t^2,\quad \theta_{t+1}=\theta_t-\eta_t \frac{g_t}{\sqrt{s_{t+1}}+\epsilon}
  $$

- **Adam**
  $$
  m_{t+1}=\beta_1 m_t+(1-\beta_1)g_t,\quad v_{t+1}=\beta_2 v_t+(1-\beta_2)g_t^2\\
  \hat m=\frac{m}{1-\beta_1^t},\ \hat v=\frac{v}{1-\beta_2^t},\
  \theta_{t+1}=\theta_t-\eta_t \frac{\hat m}{\sqrt{\hat v}+\epsilon}
  $$

- **AdamW(Decoupled WD)**
  $$
  \theta_{t+1}=\theta_t-\eta_t \frac{\hat m}{\sqrt{\hat v}+\epsilon}-\eta_t\lambda\,\theta_t
  $$

- **Cosine**
  $$
  \eta_t = \eta_{\min} + \tfrac{1}{2}(\eta_0-\eta_{\min})(1+\cos(\pi t/T_{\max}))
  $$

- **OneCycle(개념)**
  상승: $$\eta \uparrow$$ (모멘텀 ↓) → 정점 → 감소: $$\eta \downarrow$$ (모멘텀 ↑)

- **Clip-norm**
  $$
  g \leftarrow g \cdot \min\Big(1, \tfrac{\tau}{\|g\|_2}\Big)
  $$

- **유효 배치**
  $$
  B_{\text{eff}} = B \times \text{accum\_steps} \times \text{num\_gpus}
  $$

---

## J. 마무리
- **Optimizer 선택**은 데이터/모델/배치 크기의 함수다.
  - 기본은 **AdamW + Cosine(or OneCycle) + Warmup**
  - 큰 배치/비전 백본은 **SGD+Nesterov** 여전히 강력
- **스케줄**과 **정규화(Weight Decay)**, **클리핑/누적**이 **성능과 안정성**을 좌우한다.
- 본 장의 템플릿을 복사-붙여넣기하여 **옵티마이저·스케줄·클리핑·누적**을 빠르게 조합/튜닝하자.
