---
layout: post
title: DB 심화 - 페이지 처리의 중요성
date: 2025-11-01 21:25:23 +0900
category: DB 심화
---
# 효율적인 페이지 처리 구현 방법

> **핵심 요약**  
> 페이지 처리(pagination)는 사용자 경험과 시스템 성능 모두에 영향을 미치는 중요한 기능입니다. 잘못 구현하면 데이터베이스 부하가 급증하고, 사용자 경험이 나빠지며, 데이터 일관성이 보장되지 않을 수 있습니다. 올바른 해법은 **부분범위처리(Stopkey)**, **Keyset(커서 기반) 페이지네이션**, **적절한 Array Fetch**, 그리고 **읽기 일관성 확보**의 조합입니다.

---

## 테스트 환경 구성

```sql
-- 주문 테이블 생성
DROP TABLE orders PURGE;
CREATE TABLE orders (
  order_id     NUMBER PRIMARY KEY,
  customer_id  NUMBER NOT NULL,
  created_at   DATE   NOT NULL,
  status       VARCHAR2(10) NOT NULL,
  amount       NUMBER(12,2) NOT NULL
);

-- 200만 건 샘플 데이터 삽입
INSERT /*+ APPEND */ INTO orders
SELECT level,
       MOD(level, 500000)+1,
       (TRUNC(SYSDATE) - MOD(level, 365)) + (MOD(level, 86400)/86400),
       CASE WHEN MOD(level, 9)=0 THEN 'CANCEL' ELSE 'OK' END,
       ROUND(DBMS_RANDOM.VALUE(10, 1000), 2)
FROM dual CONNECT BY level <= 2000000;
COMMIT;

-- 고객별 시간순 정렬을 위한 인덱스
CREATE INDEX ix_orders_cust_time ON orders(customer_id, created_at DESC, order_id DESC);

-- 통계 수집
BEGIN
  DBMS_STATS.GATHER_TABLE_STATS(USER, 'ORDERS', cascade=>TRUE);
END;
/
```

---

## 페이지 처리의 중요성과 도전 과제

### 성능 측면의 도전
페이지 처리는 단순히 데이터를 나누어 보여주는 것 이상의 의미를 갖습니다. 잘못 구현된 페이지 처리는 다음과 같은 문제를 발생시킵니다:

1. **성능 저하**: OFFSET 방식의 페이지네이션은 페이지 번호가 높아질수록 앞선 행들을 모두 읽고 버리는 비효율적인 작업을 수행합니다.
2. **자원 낭비**: 불필요한 I/O와 CPU 사용으로 인해 데이터베이스 서버의 부하가 증가합니다.
3. **네트워크 비용**: 작은 Array Size로 인한 과도한 Fetch Call로 네트워크 왕복 시간이 증가합니다.

### 데이터 일관성 문제
데이터가 실시간으로 변경되는 환경에서 페이지 처리는 추가적인 복잡성을 가집니다:

1. **중복/누락 현상**: 페이지 이동 중 새로운 데이터가 삽입되거나 기존 데이터가 삭제되면 OFFSET 기반 페이지네이션에서 중복 또는 누락이 발생할 수 있습니다.
2. **읽기 일관성**: 여러 페이지에 걸쳐 동일한 데이터 스냅샷을 유지하지 못하면 사용자에게 혼란을 줄 수 있습니다.

### 성능 모델 수식
페이지 1장을 조회하는 데 소요되는 시간은 다음 공식으로 근사할 수 있습니다:

$$
\text{응답시간} \approx \underbrace{\text{네트워크 왕복 시간} \times \text{Fetch Call 횟수}}_{\text{네트워크 오버헤드}} + \underbrace{\text{CPU} + \text{I/O} + \text{대기 시간}}_{\text{데이터베이스 처리}}
$$

효율적인 페이지 처리는 이 두 요소 모두를 최소화하는 방향으로 설계되어야 합니다.

---

## 페이지네이션 기법 비교

### OFFSET-FETCH 방식
**SQL 패턴**: `ORDER BY ... OFFSET :skip ROWS FETCH NEXT :take ROWS ONLY`

**장점**:
- 구현이 간단하고 직관적입니다.
- 임의의 페이지에 직접 접근이 가능합니다.

**단점**:
- OFFSET 값이 커질수록 앞선 행들을 모두 읽고 버리는 비효율이 발생합니다.
- 데이터 변경 시 중복/누락 문제가 발생할 수 있습니다.

**적합한 상황**:
- 데이터 양이 적고 정적인 목록
- 전체 레코드 수가 많지 않은 경우

### Keyset(커서 기반) 페이지네이션
**SQL 패턴**: `WHERE (정렬_키) < :마지막_키 ORDER BY ... FETCH FIRST :가져올_행수`

**장점**:
- 항상 상위 N개 행만 읽어 성능이 일정하게 유지됩니다.
- 데이터 변경에 대한 내성이 강합니다.
- 부분범위처리(Stopkey)와 잘 조화됩니다.

**단점**:
- 첫 페이지를 제외하고는 마지막 키 정보가 필요합니다.
- 정렬 키 설계에 신경을 써야 합니다.

**적합한 상황**:
- OLTP 환경의 대규모 목록
- 무한 스크롤 구현
- 실시간 데이터 조회

### ROWID 기반 페이지네이션
**SQL 패턴**: `WHERE ROWID > :마지막_ROWID ORDER BY ROWID`

**장점**:
- 구현이 매우 간단하고 빠릅니다.

**단점**:
- 비즈니스 정렬 기준을 반영할 수 없습니다.

**적합한 상황**:
- 내부 관리 도구
- 데이터 보정 작업

---

## Keyset 페이지네이션의 수학적 이점

### OFFSET 방식의 비효율성
OFFSET `k` 페이지를 조회할 때 데이터베이스가 처리해야 하는 작업량:

$$
\text{I/O 작업량}(k) \approx \text{처음 } (k \times \text{페이지 크기} + \text{페이지 크기}) \text{ 행을 읽는 데 필요한 I/O}
$$

페이지 번호가 증가할수록 이 작업량은 선형적으로 증가합니다.

### Keyset 방식의 효율성
Keyset 방식은 항상 동일한 양의 작업만 수행합니다:

$$
\text{I/O 작업량}_{\text{Keyset}} \approx \text{상위 } \text{페이지 크기} \text{ 행을 읽는 데 필요한 I/O}
$$

이 차이는 데이터 양이 많고 페이지 번호가 높을수록 더욱 두드러집니다.

---

## Keyset 페이지네이션 구현 가이드

### 설계 원칙
1. **정렬 키 선택**: 시간순, ID 순 등 단조롭게 증가 또는 감소하는 컬럼을 선택합니다.
2. **유일성 보장**: 동점(tie) 상황을 방지하기 위해 기본 키를 정렬 조건의 마지막에 추가합니다.
3. **인덱스 설계**: 필터 조건과 정렬 조건을 모두 만족하는 복합 인덱스를 생성합니다.
4. **부분범위처리 활용**: `FETCH FIRST :take ROWS ONLY` 구문으로 Stopkey 최적화를 유도합니다.

### 첫 페이지 조회 예제
```sql
SELECT /*+ index(o ix_orders_cust_time) */
       o.order_id, o.created_at, o.amount, o.status
FROM   orders o
WHERE  o.customer_id = :customer_id
ORDER  BY o.created_at DESC, o.order_id DESC
FETCH FIRST 20 ROWS ONLY;   -- Stopkey 최적화 적용
```

### 다음 페이지 조회 예제
```sql
SELECT /*+ index(o ix_orders_cust_time) */
       o.order_id, o.created_at, o.amount, o.status
FROM   orders o
WHERE  o.customer_id = :customer_id
  AND (o.created_at, o.order_id) < (:last_created_at, :last_order_id)
ORDER  BY o.created_at DESC, o.order_id DESC
FETCH FIRST 20 ROWS ONLY;   -- Stopkey 최적화 적용
```

**실행 계획 분석**: 위 쿼리들은 `INDEX RANGE SCAN`과 `STOPKEY` 연산자를 사용하여 최적의 성능을 발휘합니다. 필요한 컬럼이 모두 인덱스에 포함되어 있다면(커버링 인덱스) 테이블 접근까지 제거될 수 있습니다.

---

## 읽기 일관성 보장 전략

### 문제 상황
여러 페이지에 걸쳐 데이터를 조회할 때, 페이지 간의 데이터 일관성이 보장되지 않으면 사용자 경험이 나빠질 수 있습니다. 새로운 데이터의 삽입이나 기존 데이터의 삭제로 인해 중복 또는 누락이 발생할 수 있습니다.

### 해결 방안

#### 트랜잭션 기반 일관성
동일한 데이터베이스 세션에서 트랜잭션을 시작한 후 모든 페이지를 조회하면 일관된 스냅샷을 유지할 수 있습니다.

#### Flashback Query 활용
첫 페이지 조회 시점의 시스템 변경 번호(SCN)를 저장하고, 이후 모든 페이지 조회에 동일한 SCN을 사용합니다.

```sql
-- SCN 값 저장
VAR scn NUMBER;
SELECT CURRENT_SCN INTO :scn FROM v$database;

-- 첫 페이지 조회 (특정 SCN 기준)
SELECT /*+ index(o ix_orders_cust_time) */
       o.order_id, o.created_at, o.amount
FROM   orders AS OF SCN :scn
WHERE  customer_id = :customer_id
ORDER  BY created_at DESC, order_id DESC
FETCH FIRST 20 ROWS ONLY;

-- 다음 페이지 조회 (동일 SCN 사용)
SELECT /* 다음 페이지 */
       o.order_id, o.created_at, o.amount
FROM   orders AS OF SCN :scn
WHERE  customer_id = :customer_id
  AND (created_at, order_id) < (:last_created_at, :last_order_id)
ORDER  BY created_at DESC, order_id DESC
FETCH FIRST 20 ROWS ONLY;
```

**주의사항**: Flashback Query는 UNDO 데이터의 보존 기간에 의존합니다. 장기간 보존이 필요한 리포트나 감사 용도의 페이지에만 사용하는 것이 좋습니다.

---

## 인덱스 설계 최적화

### Stopkey 최적화를 위한 조건
부분범위처리(Stopkey)가 효과적으로 작동하려면 다음 조건을 만족해야 합니다:

1. **WHERE 절과 ORDER BY 절이 단일 복합 인덱스로 처리 가능**해야 합니다.
2. **정렬 방향이 인덱스 구성과 일치**해야 합니다(오름차순/내림차순).
3. **커버링 인덱스**를 사용하면 추가적인 테이블 접근을 제거할 수 있습니다.

### 인덱스 설계 예시
- **고객별 최신순**: `(customer_id, created_at DESC, order_id DESC)`
- **테넌트별 이벤트**: `(tenant_id, created_at DESC, id DESC)`
- **카테고리별 가격순**: `(category_id, price ASC, id ASC)`

**Tie-breaker 중요성**: 동점을 방지하고 완전한 순서를 보장하기 위해 항상 기본 키를 정렬 조건의 마지막에 포함시킵니다.

---

## 프로그래밍 언어별 구현 예제

### Java (JDBC)
```java
// 첫 페이지 조회
String firstPageSql = """
SELECT order_id, created_at, amount, status
FROM   orders
WHERE  customer_id = ?
ORDER  BY created_at DESC, order_id DESC
FETCH FIRST ? ROWS ONLY
""";

try (PreparedStatement ps = conn.prepareStatement(firstPageSql)) {
  ps.setInt(1, customerId);
  ps.setInt(2, pageSize);
  // 페이지 크기 이상으로 Fetch Size 설정 (일반적으로 500-2000 권장)
  ps.setFetchSize(Math.max(pageSize, 1000));
  
  try (ResultSet rs = ps.executeQuery()) {
    long lastId = 0;
    Timestamp lastTimestamp = null;
    
    while (rs.next()) {
      long orderId = rs.getLong("order_id");
      Timestamp createdAt = rs.getTimestamp("created_at");
      BigDecimal amount = rs.getBigDecimal("amount");
      String status = rs.getString("status");
      
      // 마지막 행 정보 저장 (다음 페이지용)
      lastId = orderId;
      lastTimestamp = createdAt;
      
      // 비즈니스 로직 처리
    }
    
    // lastTimestamp와 lastId를 다음 페이지 요청에 사용
  }
}

// 다음 페이지 조회
String nextPageSql = """
SELECT order_id, created_at, amount, status
FROM   orders
WHERE  customer_id = ?
AND   (created_at, order_id) < (?, ?)
ORDER  BY created_at DESC, order_id DESC
FETCH FIRST ? ROWS ONLY
""";
```

### Python (python-oracledb)
```python
import oracledb

connection = oracledb.connect(user="username", password="password", dsn="dsn")
cursor = connection.cursor()

# Array Size 설정 (페이지 크기보다 크게 설정)
cursor.arraysize = max(page_size, 1000)

# 첫 페이지 조회
cursor.execute("""
  SELECT order_id, created_at, amount, status
  FROM   orders
  WHERE  customer_id = :cust_id
  ORDER  BY created_at DESC, order_id DESC
  FETCH FIRST :take ROWS ONLY
""", cust_id=customer_id, take=page_size)

rows = cursor.fetchall()
if rows:
    last_row = rows[-1]
    last_id, last_ts = last_row[0], last_row[1]
    
    # 다음 페이지 조회 시 last_ts와 last_id 사용
```

### C# (ODP.NET)
```csharp
using Oracle.ManagedDataAccess.Client;

using (var cmd = new OracleCommand(@"
  SELECT order_id, created_at, amount, status
  FROM orders
  WHERE customer_id = :cust_id
  ORDER BY created_at DESC, order_id DESC
  FETCH FIRST :take ROWS ONLY", connection))
{
    cmd.Parameters.Add(":cust_id", OracleDbType.Int32).Value = customerId;
    cmd.Parameters.Add(":take", OracleDbType.Int32).Value = pageSize;
    
    // Fetch Size 설정 (바이트 단위, 일반적으로 4-16MB 권장)
    cmd.FetchSize = 8 * 1024 * 1024;
    
    using (var reader = cmd.ExecuteReader())
    {
        while (reader.Read())
        {
            long orderId = reader.GetInt64(0);
            DateTime createdAt = reader.GetDateTime(1);
            decimal amount = reader.GetDecimal(2);
            string status = reader.GetString(3);
            
            // 비즈니스 로직 처리
        }
    }
}
```

### Node.js (node-oracledb)
```javascript
const result = await connection.execute(
  `
  SELECT order_id, created_at, amount, status
  FROM orders
  WHERE customer_id = :cust_id
  ORDER BY created_at DESC, order_id DESC
  FETCH FIRST :take ROWS ONLY
  `,
  { cust_id: customerId, take: pageSize },
  { 
    prefetchRows: 500,      // 서버가 미리 가져오는 행 수
    fetchArraySize: 2000    // 클라이언트가 한 번에 받는 행 수
  }
);

// 결과 처리
for (const row of result.rows) {
  const [orderId, createdAt, amount, status] = row;
  // 비즈니스 로직 처리
}
```

---

## 총 레코드 수 계산 전략

대규모 테이블에서 정확한 총 레코드 수를 계산하는 것은 비용이 많이 드는 작업입니다. 다음과 같은 전략을 고려해 보십시오:

### 근사치 표시 방식
- 첫 페이지 응답에 "5000+ 건"과 같은 근사치를 표시합니다.
- 통계 정보나 샘플링을 활용한 근사치 계산을 사용합니다.

### 비동기 계산 방식
- 첫 페이지는 근사치로 응답합니다.
- 정확한 수치는 백그라운드 작업으로 계산하여 이후 요청 시 반영합니다.

### 요약 테이블 활용
- 자주 조회되는 필터 조건별로 요약 테이블을 유지합니다.
- 파티션 프루닝이 가능한 경우 파티션 단위로 카운트를 관리합니다.

---

## 성능 측정과 검증

### SQL 트레이스 활성화
```sql
-- 세션 통계 수집 레벨 설정
ALTER SESSION SET statistics_level = ALL;

-- 상세 트레이스 활성화
ALTER SESSION SET events '10046 trace name context forever, level 8';

-- 테스트 쿼리 실행
SELECT order_id, created_at, amount, status
FROM orders
WHERE customer_id = 12345
ORDER BY created_at DESC, order_id DESC
FETCH FIRST 20 ROWS ONLY;

-- 트레이스 비활성화
ALTER SESSION SET events '10046 trace name context off';
```

### TKPROF 분석 포인트
트레이스 파일을 분석할 때 다음 지표에 주목하십시오:

1. **Fetch Call 횟수**: Keyset 방식으로 전환 후 현저히 감소해야 합니다.
2. **경과 시간(Elapsed Time)**: 특히 Execute와 Fetch 시간이 감소해야 합니다.
3. **논리적/물리적 읽기**: 부분범위처리가 적용되면 읽기 횟수가 감소합니다.
4. **대기 이벤트**: 정렬, 디스크 I/O, SQL*Net 관련 대기가 줄어들어야 합니다.

---

## 일반적인 실패 패턴과 해결책

### 1. OFFSET-FETCH 패턴의 과도한 사용
**문제**: 페이지 번호가 높아질수록 성능이 기하급수적으로 저하됩니다.  
**해결**: **Keyset 페이지네이션으로 전환**하고 적절한 인덱스를 설계하세요.

### 2. 정렬 컬럼이 인덱스에 포함되지 않음
**문제**: 전체 테이블 스캔과 정렬 작업으로 인해 성능이 저하됩니다.  
**해결**: **필터 조건과 정렬 조건을 모두 포함하는 복합 인덱스**를 생성하세요.

### 3. Tie-breaker의 부재
**문제**: 동점 상황에서 순서가 불안정하여 중복/누락이 발생합니다.  
**해결**: 정렬 조건의 마지막에 **기본 키를 추가**하여 완전한 순서를 보장하세요.

### 4. 기본 Array Size 사용
**문제**: 과도한 Fetch Call로 인한 네트워크 오버헤드가 발생합니다.  
**해결**: `setFetchSize`, `arraysize`, `FetchSize` 등을 적절히 증가시키세요.

### 5. 불필요한 컬럼 조회
**문제**: I/O, 네트워크 트래픽, GC 부하가 증가합니다.  
**해결**: **실제로 필요한 컬럼만 조회**하고, 가능하면 커버링 인덱스를 활용하세요.

---

## 실전 튜닝 사례: 고객 주문 목록 API

### 문제 상황
- API 엔드포인트: `/customers/{id}/orders?page=N&size=50`
- 기존 구현: `OFFSET (N*50) FETCH 50` 방식
- 정렬을 위한 적절한 인덱스 부재
- 기본 Fetch Size(50) 사용
- 성능 지표: p95=1.6초, p99=4.8초

### 개선 조치
1. **인덱스 설계**: `(customer_id, created_at DESC, order_id DESC)` 복합 인덱스 생성
2. **페이지네이션 방식**: OFFSET에서 **Keyset 방식으로 전환**
3. **Fetch Size 조정**: JDBC `setFetchSize(1000)` 설정
4. **컬럼 최소화**: 실제 표시에 필요한 컬럼만 조회
5. **커버링 인덱스 검토**: 자주 조회되는 컬럼을 인덱스에 포함

### 개선 결과
- **Fetch Call 횟수**: 22회 → 1-2회로 감소
- **실행 시간**: 정렬과 테이블 스캔 감소로 대폭 개선
- **전체 응답 시간**: 약 1.6초 → 약 110ms로 개선
- **시스템 부하**: CPU와 I/O 사용량이 현저히 감소

---

## 결론

효율적인 페이지 처리는 현대적인 웹 애플리케이션과 API 설계의 핵심 요소입니다. 단순한 데이터 분할을 넘어서 성능, 사용자 경험, 데이터 일관성을 모두 고려한 설계가 필요합니다.

**핵심 원칙**:
1. **Keyset 페이지네이션**을 기본으로 사용하여 성능을 일정하게 유지하세요.
2. **부분범위처리(Stopkey)** 를 활용하여 불필요한 I/O를 제거하세요.
3. **적절한 Array Fetch 크기**를 설정하여 네트워크 왕복을 최소화하세요.
4. **읽기 일관성**을 보장하여 사용자에게 안정적인 경험을 제공하세요.

**실행 계획**:
- 필터 조건과 정렬 조건을 모두 만족하는 **복합 인덱스**를 설계하세요.
- 동점을 방지하기 위해 **기본 키를 tie-breaker**로 활용하세요.
- 실제 성능 개선 효과를 **SQL 트레이스와 실행 계획 분석**으로 검증하세요.

페이지 처리는 단순한 기능 구현이 아니라 시스템 아키텍처의 중요한 구성 요소입니다. 올바른 설계와 지속적인 최적화를 통해 확장성 높고 사용자 친화적인 애플리케이션을 구축할 수 있습니다.