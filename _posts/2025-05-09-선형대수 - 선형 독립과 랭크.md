---
layout: post
title: 선형대수 - 선형 독립과 랭크
date: 2025-05-09 22:20:23 +0900
category: 선형대수
---
# 선형 독립과 랭크 — 벡터의 독립성과 공간의 구조 이해하기 (전면 개정)

본 글은 선형 독립과 랭크를 **정의 → 기하학적 의미 → 판정법(랭크·행렬식·그람행렬) → 랭크–널리티 → 연립방정식 해의 구조 → 풀랭크의 의미(단사·전사) → PCA와 랭크 → 수치적 이슈와 방어전략 → PyTorch 실습** 순서로 정리합니다.  

---

## 0. 표기 규약(Notation)

- 스칼라: $$\alpha,\beta,c_i\in\mathbb{R}$$  
- 벡터(열벡터): $$\mathbf{v}_1,\dots,\mathbf{v}_k\in\mathbb{R}^n$$  
- 행렬: $$\mathbf{A}\in\mathbb{R}^{m\times n}$$  
- 전치: $$\mathbf{A}^\top$$, 노름: $$\lVert\cdot\rVert\equiv\lVert\cdot\rVert_2$$  
- 생성(span): $$\operatorname{span}\{\cdot\}$$, 영벡터: $$\mathbf{0}$$

---

## 1. 선형 독립(Linear Independence)

### 1.1 정의
벡터 집합 $$\{\mathbf{v}_1,\dots,\mathbf{v}_k\}$$ 가
$$
c_1\mathbf{v}_1+\cdots+c_k\mathbf{v}_k=\mathbf{0}
\ \Rightarrow\
c_1=\cdots=c_k=0
$$
을 만족하면 **선형 독립**, 아니면 **선형 종속**입니다.

### 1.2 기하학적 의미
- $$\mathbb{R}^2$$: 두 벡터가 같은 직선 위에 있지 않으면 독립(평면을 두 방향으로 채움).  
- $$\mathbb{R}^3$$: 세 벡터가 같은 평면에 갇히지 않으면 독립(공간을 세 방향으로 채움).  
- 일반적으로 독립이면 불필요한 중복 없이 “방향 축”을 제공합니다.

### 1.3 독립성 판정의 관점
1) 랭크 관점: 열행렬 $$\mathbf{V}=[\mathbf{v}_1\ \cdots\ \mathbf{v}_k]\in\mathbb{R}^{n\times k}$$ 에서  
$$
\operatorname{rank}(\mathbf{V})=k \ \Longleftrightarrow\ \{\mathbf{v}_i\}\ \text{독립}.
$$

2) 정사각의 경우( $$k=n$$ ): 행렬식이 0이 아니면 독립.  
$$
\det(\mathbf{V})\neq 0 \ \Longleftrightarrow\ \text{독립}.
$$

3) 그람(Gram) 행렬 관점: $$\mathbf{G}=\mathbf{V}^\top \mathbf{V}$$ 가 양정치(모든 고유값 양수)면 독립.  
$$
\mathbf{u}^\top \mathbf{G}\mathbf{u}=\lVert \mathbf{V}\mathbf{u}\rVert^2>0\ \ (\mathbf{u}\neq 0).
$$

---

## 2. 랭크(Rank)

### 2.1 정의와 동치
행렬 $$\mathbf{A}\in\mathbb{R}^{m\times n}$$ 의 **랭크**는
- 선형 독립한 열(또는 행)의 최대 개수  
- 열공간(또는 행공간)의 차원  
- SVD $$\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top$$ 의 **0이 아닌 특이값 개수**  
와 같습니다.

즉,
$$
\operatorname{rank}(\mathbf{A})=\dim\operatorname{Col}(\mathbf{A})=\dim\operatorname{Row}(\mathbf{A}).
$$

### 2.2 기본 성질
- 상계: $$\operatorname{rank}(\mathbf{A})\le \min(m,n).$$  
- 곱에 대한 상계: $$\operatorname{rank}(\mathbf{A}\mathbf{B})\le \min(\operatorname{rank}(\mathbf{A}),\operatorname{rank}(\mathbf{B})).$$  
- 기본행연산 불변: 랭크는 가우스 소거로도 보존됩니다.  
- 풀랭크: 열이 독립이면 **풀 컬럼 랭크**, 행이 독립이면 **풀 로우 랭크**.

---

## 3. 선형 독립과 랭크의 관계

| 조건 | 해석 |
|---|---|
| $$\operatorname{rank}(\mathbf{V})=k$$ | 열벡터 $$\mathbf{v}_1,\dots,\mathbf{v}_k$$ 가 독립 |
| $$\operatorname{rank}(\mathbf{V})<k$$ | 일부 열은 다른 열들의 선형결합(종속) |
| $$\operatorname{rank}(\mathbf{A})=n$$ | $$\mathbf{A}$$ 가 풀 컬럼 랭크(단사), 해가 유일(가능시) |
| $$\operatorname{rank}(\mathbf{A})=m$$ | $$\mathbf{A}$$ 가 풀 로우 랭크(전사), 모든 $$\mathbf{b}$$ 가 이미지 |

---

## 4. 랭크–널리티 정리와 네 가지 기본 부분공간

### 4.1 랭크–널리티 정리
열 수가 $$n$$ 인 $$\mathbf{A}\in\mathbb{R}^{m\times n}$$ 에 대해
$$
\operatorname{rank}(\mathbf{A})+\operatorname{nullity}(\mathbf{A})=n,
$$
여기서 $$\operatorname{nullity}(\mathbf{A})=\dim \mathcal{N}(\mathbf{A})$$,  
$$\mathcal{N}(\mathbf{A})=\{\mathbf{x}:\mathbf{A}\mathbf{x}=\mathbf{0}\}$$ 은 영공간입니다.

### 4.2 네 가지 부분공간
- 열공간 $$\operatorname{Col}(\mathbf{A})\subset\mathbb{R}^m$$  
- 행공간 $$\operatorname{Row}(\mathbf{A})=\operatorname{Col}(\mathbf{A}^\top)\subset\mathbb{R}^n$$  
- 영공간 $$\mathcal{N}(\mathbf{A})\subset\mathbb{R}^n$$  
- 좌영공간 $$\mathcal{N}(\mathbf{A}^\top)\subset\mathbb{R}^m$$

서로 직교쌍을 이룹니다(예: 행공간과 영공간은 서로 직교보완).

---

## 5. 연립방정식과 랭크

선형 시스템 $$\mathbf{A}\mathbf{x}=\mathbf{b}$$ 의 해 구조는  
계수행렬의 랭크와 확장행렬의 랭크로 판정합니다.

- **일관성(해 존재)**:  
  $$
  \operatorname{rank}(\mathbf{A})=\operatorname{rank}([\mathbf{A}\mid\mathbf{b}]).
  $$
- **유일해**: 위가 성립하고 $$\operatorname{rank}(\mathbf{A})=n$$.  
- **무수히 많은 해**: 위가 성립하지만 $$\operatorname{rank}(\mathbf{A})<n$$.  
- **해 없음**: $$\operatorname{rank}(\mathbf{A})<\operatorname{rank}([\mathbf{A}\mid\mathbf{b}])$$.

또한, 풀 컬럼 랭크면 단사(해의 유일성), 풀 로우 랭크면 모든 $$\mathbf{b}$$ 가 적어도 하나의 해를 가집니다(전사).

---

## 6. PCA(주성분 분석)와 랭크

데이터 행렬 $$\mathbf{X}\in\mathbb{R}^{n\times d}$$ (행=샘플, 열=특성)를 평균 제거 후 SVD 하면
$$
\mathbf{X}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top,
$$
비영 특이값 개수가 **유효 차원(랭크)**를 줍니다. 큰 특이값에 대응하는 열벡터 $$\mathbf{V}$$ 가 **주성분 축(정규직교 기저)** 이며,  
랭크가 낮을수록 데이터가 저차원 다양체에 더 가깝게 놓여 있음을 뜻합니다.

---

## 7. 수치적 이슈와 방어 전략

- 부동소수점 오차: 거의 종속인 벡터는 수치적으로 독립/종속 판정이 민감합니다.  
- 권장: float64 사용, 스케일 정규화, SVD 기반 랭크(임계값 설정).  
- 임계값 예: $$\text{tol} = \varepsilon \cdot \max(m,n)\cdot \sigma_{\max}$$ 형태(문제 스케일에 맞춤).

---

## 8. PyTorch 실습

아래 코드는 독립 실행 가능하며, 모두 PyTorch 만 사용합니다.

### 8.1 랭크로 선형 독립 판정
```python
import torch
torch.set_printoptions(precision=5, sci_mode=False)

def rank_svd(A, tol=None):
    A = A.double()
    U, S, Vh = torch.linalg.svd(A, full_matrices=False)
    if tol is None:
        tol = 1e-12 * max(A.size()) * float(S.max()) if S.numel() > 0 else 1e-12
    r = int((S > tol).sum().item())
    return r, S

def is_independent(vecs, tol=None):
    # vecs: list of 1D tensors(같은 길이). 열로 쌓아 랭크 비교
    V = torch.stack(vecs, dim=1).double()  # n x k
    r, S = rank_svd(V, tol)
    return (r == V.size(1)), r, S

v1 = torch.tensor([1., 2.])
v2 = torch.tensor([3., 1.])
v3 = v1 + v2  # 의도적으로 종속

print("독립성(두 벡터):", is_independent([v1, v2])[0])
print("독립성(세 벡터):", is_independent([v1, v2, v3])[0])
```

### 8.2 그람행렬로 판정(양정치 여부)
```python
import torch

def gram_matrix(V):
    # V: n x k, 열이 벡터
    return V.T @ V

V = torch.stack([torch.tensor([1., 1.]),
                 torch.tensor([1., -1.])], dim=1).double()

G = gram_matrix(V)
eigvals = torch.linalg.eigvalsh(G)  # 대칭이므로 실특성값
print("그람행렬 고유값:", eigvals)     # 모두 > 0 이면 독립
```

### 8.3 영공간/좌영공간 기저, 열공간 기저(SVD)
```python
import torch

def nullspace(A, tol=None):
    A = A.double()
    U, S, Vh = torch.linalg.svd(A, full_matrices=False)
    if tol is None:
        tol = 1e-12 * max(A.size()) * float(S.max()) if S.numel() > 0 else 1e-12
    mask = S <= tol
    if not torch.any(mask):
        return torch.empty(A.size(1), 0, dtype=A.dtype)
    return Vh[mask, :].T  # (n, k)

def left_nullspace(A, tol=None):
    # N(A^T): U의 열 중 S에 대응하는 0 특이값 방향
    A = A.double()
    U, S, Vh = torch.linalg.svd(A, full_matrices=False)
    if tol is None:
        tol = 1e-12 * max(A.size()) * float(S.max()) if S.numel() > 0 else 1e-12
    mask = S <= tol
    if not torch.any(mask):
        return torch.empty(A.size(0), 0, dtype=A.dtype)
    return U[:, mask]  # (m, k)

def colspace_orthonormal_basis(A, tol=None):
    A = A.double()
    U, S, Vh = torch.linalg.svd(A, full_matrices=False)
    if tol is None:
        tol = 1e-12 * max(A.size()) * float(S.max()) if S.numel() > 0 else 1e-12
    r = int((S > tol).sum().item())
    return U[:, :r]  # orthonormal basis of Col(A)

A = torch.tensor([[1., 2., 3.],
                  [2., 4., 6.],
                  [0., 1., 1.]], dtype=torch.float64)

N = nullspace(A)
LN = left_nullspace(A)
Qcol = colspace_orthonormal_basis(A)

print("Null(A) 기저(열):\n", N)
print("Left Null(A^T) 기저(열):\n", LN)
print("Col(A) 정규직교 기저 Q:\n", Qcol)
```

### 8.4 연립방정식의 해 구조 판정과 해 구하기
```python
import torch

def ranks_for_consistency(A, b, tol=None):
    A = A.double()
    b = b.reshape(-1, 1).double()
    Ab = torch.cat([A, b], dim=1)
    rA, _ = rank_svd(A, tol)
    rAb, _ = rank_svd(Ab, tol)
    n = A.size(1)
    return rA, rAb, n

def solve_linear(A, b, tol=None):
    # 일관성 검사 후: 유일해면 solve, 무한해면 최소노름 해, 불일치면 최소제곱해
    A = A.double()
    b = b.reshape(-1).double()
    rA, rAb, n = ranks_for_consistency(A, b, tol)
    if rA == rAb:
        if rA == n:
            x = torch.linalg.solve(A, b) if A.size(0) == n else torch.linalg.lstsq(A, b).solution
            status = "unique"
        else:
            x = torch.linalg.lstsq(A, b).solution  # 자유도 존재: 최소노름 해
            status = "infinitely many (returned a minimum-norm solution)"
    else:
        x = torch.linalg.lstsq(A, b).solution  # 불일치: 최소제곱해
        status = "inconsistent (returned a least-squares solution)"
    return x, status

# 사례 1: 유일해
A1 = torch.tensor([[2., 1.],
                   [1., 1.]], dtype=torch.float64)
b1 = torch.tensor([5., 3.], dtype=torch.float64)
x1, s1 = solve_linear(A1, b1)
print("사례1:", s1, "해 x =", x1)

# 사례 2: 무한해 (랭크 < n)
A2 = torch.tensor([[1., 2., 3.],
                   [2., 4., 6.]], dtype=torch.float64)
b2 = torch.tensor([6., 12.], dtype=torch.float64)
x2, s2 = solve_linear(A2, b2)
print("사례2:", s2, "대표 해 x =", x2)

# 사례 3: 불일치
A3 = torch.tensor([[1., 1.],
                   [1., 1.]], dtype=torch.float64)
b3 = torch.tensor([2., 3.], dtype=torch.float64)
x3, s3 = solve_linear(A3, b3)
print("사례3:", s3, "최소제곱해 x =", x3)
```

### 8.5 PCA 관점: 특이값으로 유효 차원 파악
```python
import torch

# 평균 제거된 데이터 행렬 X (n x d)
X = torch.tensor([[1., 2., 3.],
                  [2., 4., 6.],
                  [0., 1., 1.]], dtype=torch.float64)
X = X - X.mean(dim=0, keepdim=True)  # 센터링

U, S, Vh = torch.linalg.svd(X, full_matrices=False)
print("특이값 S:", S)

# 유효 랭크(임계 적용)
tol = 1e-12 * max(X.size()) * float(S.max()) if S.numel() > 0 else 1e-12
eff_rank = int((S > tol).sum().item())
print("유효 랭크:", eff_rank)

# 주성분 축(열): V = Vh^T
V = Vh.T[:, :eff_rank]
print("주성분 축(열벡터):\n", V)
```

### 8.6 거의 종속인 벡터의 수치 민감성
```python
import torch

# v2가 v1에 거의 평행
v1 = torch.tensor([1.0, 1.0, 1.0], dtype=torch.float64)
v2 = v1 + 1e-10 * torch.tensor([1.0, -2.0, 3.0], dtype=torch.float64)
V = torch.stack([v1, v2], dim=1)

r, S = rank_svd(V)
print("근종속 예: 랭크 =", r, " 특이값 =", S)
# 작은 특이값이 tol 기준으로 0 취급될 수 있음 → 독립/종속 판정이 tol에 민감
```

---

## 9. 추가 이론 포인트(요약)

1) 정사각행렬의 경우  
$$
\det(\mathbf{A})\neq 0 \ \Longleftrightarrow\ \operatorname{rank}(\mathbf{A})=n \ \Longleftrightarrow\ \mathbf{A}^{-1}\ \text{존재}.
$$

2) 랭크와 선형사상  
- 풀 컬럼 랭크: 선형사상 $$\mathbb{R}^n\to\mathbb{R}^m$$ 이 단사(커널이 0).  
- 풀 로우 랭크: 선형사상 이미지가 $$\mathbb{R}^m$$ 전체(전사).

3) 부분공간 차원 공식  
부분공간 $$U,W\subset\mathbb{R}^n$$ 에 대해
$$
\dim(U+W)=\dim U+\dim W-\dim(U\cap W).
$$

---

## 10. 연습문제(해설 힌트 포함)

1) 벡터  
$$
\mathbf{v}_1=(1,0,1)^\top,\ \mathbf{v}_2=(0,1,1)^\top,\ \mathbf{v}_3=(1,1,2)^\top
$$
가 선형종속임을 보이고, 독립인 부분집합으로 기저를 제시하시오.  
힌트: $$\mathbf{v}_3=\mathbf{v}_1+\mathbf{v}_2$$.

2) 행렬  
$$
\mathbf{A}=\begin{bmatrix}1&2\\2&4\\0&1\end{bmatrix}
$$
의 랭크를 SVD 또는 가우스 소거로 구하시오. 열공간의 차원과 기저는 무엇인가.  
힌트: 두 번째 열이 첫 번째 열의 배수가 아님을 확인.

3) 연립방정식 $$\mathbf{A}\mathbf{x}=\mathbf{b}$$ 에서  
$$
\mathbf{A}=\begin{bmatrix}1&1\\1&1\end{bmatrix},\ 
\mathbf{b}=\begin{bmatrix}2\\3\end{bmatrix}
$$
의 해 존재 여부를 랭크로 판정하고 최소제곱해를 구하시오.  
힌트: 확장행렬 랭크가 더 큽니다.

4) 데이터 행렬 $$\mathbf{X}\in\mathbb{R}^{n\times d}$$ 를 평균 제거 후 SVD 했을 때, 작은 특이값을 임계 아래로 자르면 어떤 의미의 차원 축소가 되는지 설명하고, 주성분 축에서의 좌표를 어떻게 구하는지 쓰시오.  
힌트: $$\mathbf{Z}=\mathbf{X}\mathbf{V}_k$$.

5) 거의 종속인 두 벡터 사례에서 tol 값에 따른 랭크 판정 변화를 실험하고, 스케일 정규화가 판정 안정성에 주는 영향을 논하시오.

---

## 11. 요약

- 선형 독립은 “오직 0 계수만으로 영벡터를 만드는가”로 정의되며, **열행렬의 랭크**로 판정할 수 있습니다.  
- 랭크는 행렬이 생성하는 공간의 차원으로, **0이 아닌 특이값 개수**와 같습니다.  
- 랭크–널리티 정리는 해 공간의 자유도와 직결되며, 연립방정식의 해 존재/유일성/다중해를 판정하는 핵심 도구입니다.  
- PCA에서 랭크는 데이터의 유효 차원을 뜻하며, 큰 특이값의 축이 정보가 많은 방향입니다.  
- 실무에서는 SVD 기반 판정, 적절한 임계값, float64 사용, 스케일 정규화로 수치적 안정성을 확보합니다.