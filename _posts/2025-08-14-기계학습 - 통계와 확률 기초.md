---
layout: post
title: 기계학습 - 통계와 확률 기초
date: 2025-08-14 17:20:23 +0900
category: 기계학습
---
# 통계와 확률 기초

## 기술 통계(Descriptive Statistics)

### (1) 중심 경향성

- **평균(Mean)**
  $$
  \bar{x}=\frac{1}{n}\sum_{i=1}^n x_i
  $$
- **중앙값(Median)**: 정렬 후 중앙. 이상치에 **견고**.
- **최빈값(Mode)**: 가장 자주 등장.

### (2) 산포(Dispersion)

- **표본 분산/표준편차(불편 추정)**
  $$
  s^2=\frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})^2,\qquad s=\sqrt{s^2}
  $$
- **범위**: \(\max-\min\)
- **사분위 범위(IQR)**: \(Q_3-Q_1\)
- **MAD(중위절대편차)**:
  $$
  \mathrm{MAD}=\mathrm{median}\big(|x_i-\mathrm{median}(x)|\big)
  $$

### (3) 분포 형태

- **왜도(Skewness)**: 비대칭성.
- **첨도(Kurtosis)**: 꼬리 두꺼움.

### (4) 파이썬: 요약 통계 + 견고 통계량

```python
import numpy as np, pandas as pd
rs = np.random.default_rng(42)
x = np.concatenate([rs.normal(0,1,500), rs.normal(8,0.5,5)])  # 이상치 군집
s = pd.Series(x)
desc = s.describe()
mad = (s - s.median()).abs().median()
iqr = s.quantile(0.75) - s.quantile(0.25)
print(desc.to_string(), "\nMAD:", mad, "IQR:", iqr)
```

---

## 추론 통계(Inferential Statistics)

### (1) 점추정과 구간추정

- **평균의 신뢰구간(정규/대표본 가정)**
  $$
  \bar X \pm z_{1-\alpha/2}\cdot \frac{s}{\sqrt{n}}
  $$
- **t-구간(모분산 미지·소표본)**
  $$
  \bar X \pm t_{n-1,1-\alpha/2}\cdot \frac{s}{\sqrt{n}}
  $$

### (2) 가설검정

- **귀무/대립**: \(H_0:\mu=\mu_0\), \(H_1:\mu\ne\mu_0\)
- **검정통계량**:
  $$
  T=\frac{\bar X-\mu_0}{s/\sqrt{n}}\ \sim\ t_{n-1}\ \ (\text{정규 가정})
  $$
- **p-value**: 관측치 이상 극단값이 나올 확률( \(H_0\) 하).
- **1종/2종 오류**: \(\alpha=\Pr(\text{기각}|H_0)\), \(\beta=\Pr(\text{채택}|H_1)\).
- **검정력**: \(1-\beta\) (진짜 효과를 잡아낼 확률).

### (3) 표본크기(평균 차이) 근사식

- 두 집단 평균 차이 \(\Delta=\mu_1-\mu_2\) 검출(양측)
  $$
  n \approx \left(\frac{z_{1-\alpha/2}+z_{1-\beta}}{\Delta/\sigma}\right)^2
  $$
  (집단당 표본수, 등분산·정규 가정)

---

## 확률의 기초

### (1) 공리와 기본 성질

- \(0\le P(A)\le 1\), \(P(S)=1\), 가산성.
- 여사건: \(P(A^c)=1-P(A)\).
- 배반: \(P(A\cup B)=P(A)+P(B)\) if \(A\cap B=\emptyset\).

### (2) 조건부 확률·독립

$$
P(A|B)=\frac{P(A\cap B)}{P(B)},\quad A\perp B\Rightarrow P(A\cap B)=P(A)P(B)
$$

### (3) 전확률·베이즈 정리

$$
P(B)=\sum_i P(B|A_i)P(A_i),\qquad
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$

---

## 확률변수와 분포 (요약표)

| 분포 | 지지집합 | 파라미터 | 평균 | 분산 | 용례 |
|---|---|---|---|---|---|
| 베르누이 \( \mathrm{Bern}(p) \) | \(\{0,1\}\) | \(p\) | \(p\) | \(p(1-p)\) | 성공/실패 |
| 이항 \( \mathrm{Bin}(n,p) \) | \(\{0..n\}\) | \(n,p\) | \(np\) | \(np(1-p)\) | 성공 횟수 |
| 포아송 \( \mathrm{Pois}(\lambda) \) | \(\mathbb{N}\) | \(\lambda\) | \(\lambda\) | \(\lambda\) | 드문 사건 |
| 기하 \( \mathrm{Geom}(p) \) | \(\mathbb{N}\) | \(p\) | \(1/p\) | \((1-p)/p^2\) | 최초 성공시행 |
| 균등 \(U(a,b)\) | \([a,b]\) | \(a,b\) | \((a+b)/2\) | \((b-a)^2/12\) | 무정보 |
| 정규 \(\mathcal{N}(\mu,\sigma^2)\) | \(\mathbb{R}\) | \(\mu,\sigma^2\) | \(\mu\) | \(\sigma^2\) | 오차/합 |
| 지수 \(\mathrm{Exp}(\lambda)\) | \([0,\infty)\) | \(\lambda\) | \(1/\lambda\) | \(1/\lambda^2\) | 대기시간 |
| 감마 \(\Gamma(k,\theta)\) | \([0,\infty)\) | \(k,\theta\) | \(k\theta\) | \(k\theta^2\) | 누적시간 |
| 베타 \(\mathrm{Beta}(\alpha,\beta)\) | \([0,1]\) | \(\alpha,\beta\) | \(\frac{\alpha}{\alpha+\beta}\) | \(\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\) | 확률의 분포 |

**근사관계**
- \(\mathrm{Bin}(n,p)\approx \mathcal{N}(np, np(1-p))\) (대표본).
- \(\mathrm{Bin}(n,p)\approx \mathrm{Pois}(\lambda=np)\) ( \(n\) 큼·\(p\) 작음).

---

## 기대값·분산·공분산·상관

### (1) 정의

- **기대값**: \(E[X]=\sum_x xP(X=x)\) 또는 \(E[X]=\int x f(x)\,dx\)
- **분산**: \( \mathrm{Var}(X)=E[(X-E[X])^2]=E[X^2]-E[X]^2 \)
- **공분산**: \( \mathrm{Cov}(X,Y)=E[(X-\mu_X)(Y-\mu_Y)] \)
- **상관**: \( \rho=\frac{\mathrm{Cov}(X,Y)}{\sigma_X\sigma_Y} \)

### (2) 전체 기댓값/분산의 법칙

$$
E[X]=E\big(E[X\mid Z]\big),\quad
\mathrm{Var}(X)=E\big(\mathrm{Var}(X\mid Z)\big)+\mathrm{Var}\big(E[X\mid Z]\big)
$$

---

## 베이즈 추론(Bayesian Inference)

### (1) 베타–베르누이(성공확률 추정)

사전 \(p\sim \mathrm{Beta}(\alpha,\beta)\), 데이터 \(k\)성공/\(n\)시행 → 사후
$$
p\mid \text{data}\ \sim\ \mathrm{Beta}(\alpha+k,\ \beta+n-k)
$$
- 사후평균: \(E[p\mid D]=\frac{\alpha+k}{\alpha+\beta+n}\)

```python
import numpy as np
from scipy.stats import beta
alpha,beta0 = 2,2
n,k = 50, 28
a_post, b_post = alpha+k, beta0+n-k
print("Posterior mean:", a_post/(a_post+b_post))
```

### (2) 정규–정규(평균 추정, 분산 알려짐)

사전 \(\mu\sim \mathcal{N}(\mu_0,\tau_0^2)\), 데이터 \(X_i\sim \mathcal{N}(\mu,\sigma^2)\) →
사후 \(\mu\mid D\sim \mathcal{N}(\mu_n,\tau_n^2)\)
$$
\tau_n^2=\left(\frac{n}{\sigma^2}+\frac{1}{\tau_0^2}\right)^{-1},\quad
\mu_n=\tau_n^2\left(\frac{n\bar X}{\sigma^2}+\frac{\mu_0}{\tau_0^2}\right)
$$

### (3) 나이브 베이즈(조건부 독립 가정)

$$
P(y\mid x)\ \propto\ P(y)\prod_{j} P(x_j\mid y)
$$
텍스트 분류에서 **멀티노미얼 NB**가 대표적.

---

## LLN·CLT 시뮬레이션

```python
import numpy as np
rs = np.random.default_rng(0)

# LLN: 표본평균이 모평균으로 수렴

for n in [10, 100, 1000, 10000]:
    x = rs.exponential(scale=1.0, size=n)
    print(n, "mean≈", x.mean())

# CLT: 비정규(지수) 합이 정규에 근접

m, n = 10000, 30
x = rs.exponential(1.0, size=(m,n))
sums = x.sum(axis=1)
normed = (sums - sums.mean())/sums.std()
print("CLT: mean≈", normed.mean(), "std≈", normed.std())
```

---

## A/B 테스트(빈도론) — 이항/연속 지표

### (1) 이항 지표(전환율) — 두 비율 z-검정

- 귀무: \(p_A=p_B\), 대립: \(p_A\ne p_B\)
- 풀드 비율 \(\hat p=\frac{x_A+x_B}{n_A+n_B}\)
$$
Z=\frac{\hat p_A-\hat p_B}{\sqrt{\hat p(1-\hat p)\left(\frac{1}{n_A}+\frac{1}{n_B}\right)}}
$$

### (2) 연속 지표(평균 체류시간) — t-검정

등분산 가정/비가정(웰치 t) 선택.

### (3) 검정력과 표본크기(두 비율 차 \(\Delta\))

$$
n\ \approx\ \frac{\big(z_{1-\alpha/2}\sqrt{2\bar p(1-\bar p)}+z_{1-\beta}\sqrt{p_A(1-p_A)+p_B(1-p_B)}\big)^2}{\Delta^2}
$$
(집단당 표본, \(\bar p=(p_A+p_B)/2\))

```python
import mpmath as mp
alpha, power = 0.05, 0.8
pA, pB = 0.10, 0.12
z_alpha = mp.sqrt(2)*mp.erfcinv(alpha)*1  # ≈1.96
z_beta  = mp.sqrt(2)*mp.erfcinv(2*(1-power))
pbar = (pA+pB)/2
num = (z_alpha*mp.sqrt(2*pbar*(1-pbar)) + z_beta*mp.sqrt(pA*(1-pA)+pB*(1-pB)))**2
n = num / (pB-pA)**2
print("per-arm n≈", mp.ceil(n))
```

### (4) 다중검정

- **FWER 제어**: 본페로니( \(\alpha/m\) ).
- **FDR 제어**: Benjamini–Hochberg(BH).
여러 지표/세그먼트 동시 비교 시 필수.

---

## 불확실성 추정 — 부트스트랩

- **아이디어**: 표본에서 **복원추출**로 재표본 → 지표 분포 근사.
- 신뢰구간(퍼센타일 방식): \([q_{\alpha/2}, q_{1-\alpha/2}]\)

```python
import numpy as np
rs = np.random.default_rng(1)
x = rs.normal(0,1, size=500)

def bootstrap_stat(x, B=2000):
    n = len(x); stats=[]
    for _ in range(B):
        idx = rs.integers(0,n,size=n)
        stats.append(np.mean(x[idx]))
    return np.array(stats)

bs = bootstrap_stat(x)
ci = np.quantile(bs, [0.025, 0.975])
print("mean CI (bootstrap):", ci)
```

---

## ML과의 접점

### (1) 위험(리스크) 최소화

- **참 위험**:
  $$
  \mathcal{R}(f)=\mathbb{E}_{(X,Y)\sim \mathcal{D}}[L(Y,f(X))]
  $$
- **경험 위험**:
  $$
  \hat{\mathcal{R}}(f)=\frac{1}{n}\sum_{i}L(y_i,f(x_i))
  $$

### (2) 편향–분산 분해(회귀, 제곱손실)

$$
\mathbb{E}\big[(Y-\hat f(X))^2\big] = \underbrace{\sigma^2}_{\text{노이즈}} + \underbrace{\mathrm{Bias}^2}_{\big(\mathbb{E}[\hat f]-f^\star\big)^2} + \underbrace{\mathrm{Var}}_{\mathbb{E}[(\hat f-\mathbb{E}\hat f)^2]}
$$

### (3) 확률적 예측의 보정(Calibration)

- **Brier 점수**(이진):
  $$
  \frac{1}{n}\sum_i (\hat p_i - y_i)^2
  $$
- 신뢰도 곡선(리라이어빌리티)로 **확률보정**(Platt/Isotonic) 확인.

---

## 상관 ≠ 인과 — 심슨의 역설(코드)

```python
import numpy as np, pandas as pd
rs = np.random.default_rng(7)

# 두 그룹에서 양(+)의 상관, 합치면 음(-)의 상관

g1_x = rs.normal(50,10,200); g1_y = 0.6*g1_x + rs.normal(0,8,200) + 20
g2_x = rs.normal(80,10,200); g2_y = 0.6*g2_x + rs.normal(0,8,200) - 40
df = pd.DataFrame({"x":np.r_[g1_x,g2_x],"y":np.r_[g1_y,g2_y],
                   "group":["A"]*200+["B"]*200})
print("corr(A):", df[df.group=="A"].corr().loc["x","y"])
print("corr(B):", df[df.group=="B"].corr().loc["x","y"])
print("corr(all):", df.corr().loc["x","y"])
```
> 그룹 혼합(잠재변수)로 전체 상관이 **방향전환** 가능 → **인과 추론 금물**, 계층/층화 분석 필요.

---

## 실전 체크리스트

- [ ] 기술 통계: 평균·중앙값·IQR·MAD로 **요약 + 이상치 견고성** 확인
- [ ] 시각화: 히스토그램/QQ-plot/잔차–적합 산점
- [ ] 신뢰구간과 가설검정 구분(효과크기·검정력 보고)
- [ ] A/B: 사전 전력분석(표본수), 다중검정 시 **BH/FDR**
- [ ] 부트스트랩으로 지표 **불확실성** 보고
- [ ] 베이즈 업데이트(의사결정에 사전정보 반영)
- [ ] ML 파이프라인: **누수 방지**, 교차검증 일관성
- [ ] 상관 ≠ 인과(심슨의 역설·교란) → 실험·도구변수·차분 등 고려

---

## 📌 요약

- **통계**는 데이터를 **요약(기술)**하고 **추론(신뢰구간·검정)**하는 언어, **확률**은 불확실성을 수량화하는 도구다.
- **분포**와 **기대–분산 법칙**은 모델 가정과 전처리 전략의 근간이며, **베이즈**는 사전 지식을 정식화한다.
- ML에서는 손실의 **기대위험**을 줄이는 과정에서 통계·확률이 전면에 등장하고, **편향–분산** 균형·보정·불확실성 보고가 실무 품질을 좌우한다.

---
### 부록 A: 분포 추가 공식

- 정규 pdf:
$$
f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big)
$$
- 포아송 pmf:
$$
P(X=k)=e^{-\lambda}\frac{\lambda^k}{k!}
$$
- 이항 pmf:
$$
P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}
$$

### 부록 B: 파이썬 단품 함수(요약)

```python
import numpy as np
from scipy import stats

# 신뢰구간(정규 근사)

def mean_ci(x, alpha=0.05):
    x = np.asarray(x); n = x.size
    m, s = x.mean(), x.std(ddof=1)
    z = stats.norm.ppf(1-alpha/2)
    hw = z*s/np.sqrt(n)
    return m-hw, m+hw

# 두 비율 z-검정

def prop_ztest(xA, nA, xB, nB):
    pA, pB = xA/nA, xB/nB
    p = (xA+xB)/(nA+nB)
    se = np.sqrt(p*(1-p)*(1/nA+1/nB))
    z = (pA-pB)/se
    pval = 2*(1-stats.norm.cdf(abs(z)))
    return z, pval
```
