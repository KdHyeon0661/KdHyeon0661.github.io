---
layout: post
title: 기계학습 - 타이타닉 생존자 예측
date: 2025-08-20 18:25:23 +0900
category: 기계학습
---
# 🚢 타이타닉 생존자 예측 (Binary Classification) — 끝까지 가는 실전 가이드

타이타닉 생존자 예측은 **이진 분류** 문제의 교과서 같은 예시입니다.  
목표는 승객의 속성(성별, 나이, 선실 등)으로 **Survived (0/1)** 를 예측하는 것.

---

## 1) 데이터 개요와 타깃

### 주요 컬럼
| 컬럼 | 설명 | 타입/예시 | 결측 |
|---|---|---|---|
| Survived | 생존 여부(타깃) | 0/1 | — |
| Pclass | 선실 등급(1=상,2=중,3=하) | 정수 | 거의 없음 |
| Name | 이름 (호칭 포함: Mr/Mrs/Miss/Master/…) | 문자열 | — |
| Sex | 성별 | male/female | — |
| Age | 나이 | 실수 | **많음** |
| SibSp | 함께 탑승한 형제/배우자 수 | 정수 | — |
| Parch | 함께 탑승한 부모/자녀 수 | 정수 | — |
| Ticket | 티켓 번호 | 문자열 | — |
| Fare | 운임 | 실수 | 일부 |
| Cabin | 객실 번호 (덱 글자 + 호수) | 문자열 | **대부분 결측** |
| Embarked | 탑승 항 | C/Q/S | 소수 |

> **레이블 불균형**: 대략 생존 38%, 사망 62% 수준으로 약한 불균형.

---

## 2) 문제 정의와 평가 지표

- 문제: **Binary Classification**
- 추천 지표: `Accuracy`, `F1`, `ROC-AUC` (리더보드 제출은 보통 Accuracy/정확도이지만 내부 검증에선 F1/ROC-AUC를 함께 보세요)
- 검증: **Stratified K-Fold** 또는 **Train/Validation split + 교차검증**

---

## 3) 전처리 & 특징 엔지니어링 (Feature Engineering)

### 3.1 결측치 처리
- **Age**: 중앙값/회귀로 보간(간단히는 **중앙값**)
- **Fare**: 중앙값
- **Embarked**: 최빈값
- **Cabin**: 대부분 비어있음 → **덱 문자만 추출**(첫 글자) 후 결측을 ‘U’(Unknown)로

### 3.2 범주형 인코딩
- **Sex, Pclass, Embarked, Title, CabinDeck, IsAlone** → **One-Hot Encoding**
- 트리 기반 모델은 원핫/순위 인코딩 모두 가능하나, **파이프라인 일관성**을 위해 원핫 추천

### 3.3 추천 파생 변수
- **Title**: `Name`에서 호칭 추출 → `{'Mr','Mrs','Miss','Master','Rare'}`로 묶기  
- **FamilySize**: `SibSp + Parch + 1`  
- **IsAlone**: `(FamilySize == 1)`  
- **CabinDeck**: `Cabin` 첫 문자(‘A’~‘G’), 결측은 ‘U’

> **주의: 데이터 누수 방지**  
> 티켓 그룹 크기 등 **전 데이터 통계가 필요한 파생변수**는 교차검증 폴드 안에서 계산하세요(파이프라인 내부 커스텀 변환기로 처리).

---

## 4) 베이스라인 파이프라인 설계

- **ColumnTransformer + Pipeline** 으로 전처리·모델을 하나로 묶기  
- 수치형은 **중앙값 대치 + (선형모델용) 스케일링**  
- 범주형은 **최빈 대치 + OneHotEncoder(handle_unknown='ignore')**  
- 모델: **Logistic Regression**, **Random Forest**, **HistGradientBoosting**(scikit-learn) 비교

```python
# !pip install scikit-learn pandas numpy

import re
import numpy as np
import pandas as pd

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier

# ---------- 1) 데이터 로드 ----------
train = pd.read_csv("train.csv")  # Kaggle 형식 가정 (Survived 포함)
# test = pd.read_csv("test.csv")  # 최종 제출용(옵션)

# ---------- 2) 커스텀 변환기: 파생변수 ----------
class TitanicFeatureBuilder(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.title_map_ = None
    def fit(self, X, y=None):
        # Title 빈도 기반 rare 처리용 맵 생성 (fit 데이터에서만)
        titles = X["Name"].str.extract(r",\s*([^\.]+)\.")[0].str.strip()
        titles = titles.replace({
            "Mlle":"Miss", "Ms":"Miss", "Mme":"Mrs", "Lady":"Rare", "the Countess":"Rare",
            "Capt":"Rare", "Col":"Rare", "Don":"Rare", "Dr":"Rare", "Major":"Rare",
            "Rev":"Rare", "Sir":"Rare", "Jonkheer":"Rare", "Dona":"Rare"
        })
        # 주된 타이틀만 남기고 나머지는 Rare (Optional: 빈도 기준  <15 등)
        common = {"Mr","Mrs","Miss","Master"}
        self.title_map_ = titles.apply(lambda t: t if t in common else "Rare").value_counts().index.tolist()
        return self
    def transform(self, X):
        X = X.copy()
        # Title
        titles = X["Name"].str.extract(r",\s*([^\.]+)\.")[0].str.strip()
        titles = titles.replace({
            "Mlle":"Miss", "Ms":"Miss", "Mme":"Mrs", "Lady":"Rare", "the Countess":"Rare",
            "Capt":"Rare", "Col":"Rare", "Don":"Rare", "Dr":"Rare", "Major":"Rare",
            "Rev":"Rare", "Sir":"Rare", "Jonkheer":"Rare", "Dona":"Rare"
        })
        X["Title"] = titles.apply(lambda t: t if t in {"Mr","Mrs","Miss","Master"} else "Rare")
        # Family features
        X["FamilySize"] = X["SibSp"] + X["Parch"] + 1
        X["IsAlone"]    = (X["FamilySize"] == 1).astype(int)
        # Cabin deck
        X["CabinDeck"]  = X["Cabin"].astype(str).str[0]
        X["CabinDeck"]  = X["CabinDeck"].replace({np.nan:"U", "n":"U"})
        X.loc[X["CabinDeck"].isin(["N","n"]), "CabinDeck"] = "U"
        # Pclass를 범주형으로 취급
        X["Pclass"] = X["Pclass"].astype("category")
        return X[[
            "Pclass","Sex","Age","SibSp","Parch","Fare","Embarked",
            "Title","FamilySize","IsAlone","CabinDeck"
        ]]

# ---------- 3) 전처리 파이프라인 ----------
num_cols  = ["Age","SibSp","Parch","Fare","FamilySize"]
cat_cols  = ["Pclass","Sex","Embarked","Title","IsAlone","CabinDeck"]

numeric_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler",  StandardScaler())  # 로지스틱 등 선형 모델에 유리
])

categorical_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("ohe",     OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer([
    ("num", numeric_pipe, num_cols),
    ("cat", categorical_pipe, cat_cols)
])

# ---------- 4) 전체 파이프라인 + 모델 ----------
X = train.drop(columns=["Survived"])
y = train["Survived"]

def evaluate_model(model, X, y, name):
    pipe = Pipeline([
        ("feat", TitanicFeatureBuilder()),
        ("prep", preprocess),
        ("clf",  model)
    ])
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_validate(
        pipe, X, y, cv=cv, n_jobs=-1,
        scoring=["accuracy","f1","roc_auc"]
    )
    print(f"[{name}] ACC: {scores['test_accuracy'].mean():.3f} ± {scores['test_accuracy'].std():.3f} | "
          f"F1: {scores['test_f1'].mean():.3f} ± {scores['test_f1'].std():.3f} | "
          f"AUC: {scores['test_roc_auc'].mean():.3f} ± {scores['test_roc_auc'].std():.3f}")

evaluate_model(LogisticRegression(max_iter=2000, class_weight=None), X, y, "LogReg")
evaluate_model(RandomForestClassifier(n_estimators=400, max_depth=None, random_state=42), X, y, "RandomForest")
evaluate_model(HistGradientBoostingClassifier(max_depth=None, learning_rate=0.08, random_state=42), X, y, "HGB")
```

> **설명**
> - `TitanicFeatureBuilder`는 **교차검증의 각 학습 폴드에서만** Title 통계를 학습 → **누수 방지**  
> - `ColumnTransformer`로 수치/범주형 **동시에** 처리 → 추후 추론 시 동일 규칙 적용

---

## 5) 하이퍼파라미터 튜닝 (RandomizedSearch 예)

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

rf = Pipeline([
    ("feat", TitanicFeatureBuilder()),
    ("prep", preprocess),
    ("clf",  RandomForestClassifier(random_state=42))
])

param_dist = {
    "clf__n_estimators": randint(300, 1000),
    "clf__max_depth": randint(3, 20),
    "clf__min_samples_split": randint(2, 20),
    "clf__min_samples_leaf": randint(1, 10),
    "clf__max_features": ["sqrt","log2", None]
}

rs = RandomizedSearchCV(
    rf, param_distributions=param_dist, n_iter=60, cv=5,
    scoring="roc_auc", n_jobs=-1, random_state=42
)
rs.fit(X, y)
print("Best AUC:", rs.best_score_)
print("Best params:", rs.best_params_)
best_model = rs.best_estimator_
```

> **팁**:  
> - 선형 모델(로지스틱)은 `class_weight='balanced'`를 시도해보세요.  
> - 부스팅류(HGB, XGBoost, LightGBM)는 `learning_rate` ↓, `n_estimators` ↑로 미세 조정.

---

## 6) 중요도/해석(Feature Importance)

트리 기반 모델은 `feature_importances_` 제공. One-Hot 이후 컬럼명은 인코더에서 가져옵니다.

```python
# 파이프라인에서 OHE 이후의 실제 피처 이름 추출
ohe = best_model.named_steps["prep"].named_transformers_["cat"].named_steps["ohe"]
cat_names = ohe.get_feature_names_out(cat_cols)
final_feature_names = np.r_[num_cols, cat_names]  # ColumnTransformer 순서에 맞춰 구성

clf = best_model.named_steps["clf"]
importances = getattr(clf, "feature_importances_", None)
if importances is not None:
    imp = pd.Series(importances, index=final_feature_names).sort_values(ascending=False)
    print(imp.head(15))
```

> **의미 있는 신호(경험적)**: `Sex`, `Pclass`, `Title`, `Age`, `Fare`, `FamilySize`, `IsAlone`, `Embarked`, `CabinDeck(U/상위덱)` 등

---

## 7) 최종 예측 & 제출 파일 생성 (선택)

```python
# train 전체로 재학습 후 test 예측
# test = pd.read_csv("test.csv")
best_model.fit(X, y)
# preds = best_model.predict(test)                 # 0/1
# 또는 확률 기준 임계값 조정:
# proba = best_model.predict_proba(test)[:,1]
# preds = (proba >= 0.5).astype(int)

# submission = pd.DataFrame({"PassengerId": test["PassengerId"], "Survived": preds})
# submission.to_csv("submission.csv", index=False)
```

> **임계값 튜닝**: 내부 검증에서 **F1/Recall/Precision** 균형을 보고 `0.45`/`0.55` 등 **업무 목적에 맞는 임계값**을 선택하세요.

---

## 8) 성능을 더 끌어올리는 팁

- **교차검증**: `StratifiedKFold(n_splits=5~10)`  
- **특징 강화**:  
  - 연령대 binning(Child/Adult/Senior), 운임 log 변환/구간화  
  - 상호작용: `Pclass × Sex`, `Title × Pclass`  
- **규제**: 로지스틱의 `C` 튜닝, 트리의 `max_depth/min_samples_*`  
- **앙상블**: 서로 다른 모델의 **확률 평균/스태킹**  
- **데이터 누수 금지**: 모든 전처리는 **학습 폴드에서 fit → 검증/테스트에 transform만**  
- **재현성**: 시드 고정, 환경/버전 기록

---

## 9) 체크리스트

- [ ] 파이프라인 내부에서 결측·인코딩·스케일링 수행  
- [ ] Stratified K-Fold로 검증  
- [ ] 적합한 지표(F1/ROC-AUC)와 임계값 설정  
- [ ] 하이퍼파라미터 탐색(Random/Optuna)  
- [ ] 피처 중요도/에러 케이스 분석(오분류 샘플 확인)  
- [ ] 최종 모델은 **Train 전량**으로 재학습 후 제출

---

### ✅ 정리
- 타이타닉은 **기본기 총정리**에 최적인 이진 분류 문제.  
- **파생변수(Title/Family/Deck)** + **견고한 전처리 파이프라인** + **적절한 검증/튜닝**만으로도 **강한 베이스라인**을 얻을 수 있습니다.  
- 그 위에 **부스팅·앙상블·임계값 최적화**를 얹어 점진적으로 성능을 개선하세요.