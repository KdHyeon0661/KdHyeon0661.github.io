---
layout: post
title: 기계학습 - 타이타닉 생존자 예측
date: 2025-08-20 18:25:23 +0900
category: 기계학습
---
# 타이타닉 생존자 예측 (Binary Classification)

## 데이터 개요와 타깃

### 주요 컬럼

| 컬럼 | 설명 | 타입/예시 | 결측 |
|---|---|---|---|
| Survived | 생존 여부(타깃) | 0/1 | — |
| Pclass | 선실 등급(1=상,2=중,3=하) | 정수 | 거의 없음 |
| Name | 이름 (호칭 포함: Mr/Mrs/Miss/Master/…) | 문자열 | — |
| Sex | 성별 | male/female | — |
| Age | 나이 | 실수 | 많음 |
| SibSp | 함께 탑승한 형제/배우자 수 | 정수 | — |
| Parch | 함께 탑승한 부모/자녀 수 | 정수 | — |
| Ticket | 티켓 번호 | 문자열 | — |
| Fare | 운임 | 실수 | 일부 |
| Cabin | 객실 번호 (덱 글자 + 호수) | 문자열 | 대부분 결측 |
| Embarked | 탑승 항 | C/Q/S | 소수 |

참고: 생존 비율은 대략 **38%**로 약한 불균형입니다.

---

## 문제 정의와 평가 지표

- 문제: **Binary Classification** (타깃 `Survived ∈ {0,1}`)
- 내부 검증 권장 지표: `Accuracy`, `F1`, `ROC-AUC`, `PR-AUC(AP)`
- 검증 전략: **Stratified K-Fold** 혹은 `Train/Validation split + 교차검증`

> 리더보드 제출 평가는 보통 Accuracy 중심이지만, **불균형** 고려를 위해 내부에서는 **F1/ROC-AUC**를 병기하십시오.

---

## 수학적 배경(요약)

로지스틱 회귀의 기본 손실(이진 크로스엔트로피)은
$$
\min_{\theta}\ \frac{1}{n}\sum_{i=1}^n \Big(-y_i\log \hat{p}_i - (1-y_i)\log(1-\hat{p}_i)\Big) + \lambda\Omega(\theta),
$$
여기서 \( \hat{p}_i = \sigma(\theta^\top x_i) \), \( \sigma(z)=\frac{1}{1+e^{-z}} \) 입니다.
클래스 불균형이 클 경우 `class_weight='balanced'`로 가중 손실을 사용하면
$$
\min_{\theta}\ \frac{1}{n}\sum_{i=1}^n w_{y_i}\, \ell(y_i, \hat{p}_i),
$$
로 바뀌며, \( w_k \propto 1/\text{freq}(k) \) 형태를 갖습니다.

임계값은 비용 기반으로
$$
\tau^\*=\frac{C_{FP}(1-\pi)}{C_{FN}\pi + C_{FP}(1-\pi)}
$$
처럼 선택할 수 있습니다(\(\pi=P(y{=}1)\)). 확률 보정이 안 된 모델은 보정 후 적용 권장(§ 11).

---

## EDA 체크리스트(핵심만)

- 결측: `Age` 다수, `Fare` 드물게, `Embarked` 소수, `Cabin` 대부분
- 분포/왜도: `Fare`는 **log 변환** 후보, `Age`는 중앙값 보간이 기본
- 상관: `Sex`, `Pclass`, `Title`, `Age`, `Fare`, `FamilySize`, `IsAlone` 등 타깃과 연관
- 그룹 차이: `Sex=female`, `Pclass=1` 생존률 높음 경향

EDA 예시(선택):
```python
import pandas as pd, seaborn as sns, matplotlib.pyplot as plt
df = pd.read_csv("train.csv")
sns.countplot(x="Survived", data=df); plt.show()
sns.histplot(df["Age"].dropna(), bins=30, kde=True); plt.show()
sns.boxplot(x="Pclass", y="Fare", data=df); plt.yscale("log"); plt.show()
```

---

## 전처리와 특징 엔지니어링

### 결측 처리 원칙

- 수치형: `median` 대치(견고)
- 범주형: `most_frequent` 대치
- `Cabin`: 결측 많음 → **덱 문자**만 추출하고 결측은 `'U'`로

### 추천 파생 변수

- **Title**: `Name`에서 호칭 추출 → `{'Mr','Mrs','Miss','Master','Rare'}`로 통합
- **FamilySize**: `SibSp + Parch + 1`
- **IsAlone**: `(FamilySize == 1)`
- **CabinDeck**: `Cabin` 첫 문자(‘A’~‘G’), 결측은 `'U'`
- **FarePerPerson**(선택): `Fare / FamilySize` (0 나눗셈 방지를 위해 `max(FamilySize,1)` 처리)
- **TicketPrefix**(선택): 티켓의 문자 프리픽스(상위 몇 종만 남기고 Rare)

> 주의: **누수 방지**를 위해 Rare 카테고리 경계, 통계 기반 파생은 **학습 폴드에서만 fit → 검증/테스트에는 transform** 하십시오(커스텀 변환기 활용).

---

## 베이스라인 파이프라인(누수 방지)

아래 파이프라인은
1) 커스텀 변환기에서 파생을 만들고,
2) 수치/범주형을 `ColumnTransformer`로 분리 전처리,
3) 분류기를 얹습니다.
검증은 **Stratified K-Fold**로 수행합니다.

```python
# !pip install scikit-learn pandas numpy

import re
import numpy as np
import pandas as pd

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier

# 데이터 로드

train = pd.read_csv("train.csv")  # Kaggle 형식 가정 (Survived 포함)

# 커스텀 변환기(파생 + 선택적 rare 처리)

class TitanicFeatureBuilder(BaseEstimator, TransformerMixin):
    def __init__(self, rare_min_count=10, use_ticket_prefix=False, add_fare_per_person=True):
        self.rare_min_count = rare_min_count
        self.use_ticket_prefix = use_ticket_prefix
        self.add_fare_per_person = add_fare_per_person
        self._title_vocab_ = None
        self._ticket_vocab_ = None

    def _extract_title(self, s: pd.Series) -> pd.Series:
        t = s.str.extract(r",\s*([^\.]+)\.")[0].str.strip()
        t = t.replace({
            "Mlle":"Miss", "Ms":"Miss", "Mme":"Mrs", "Lady":"Rare", "the Countess":"Rare",
            "Capt":"Rare", "Col":"Rare", "Don":"Rare", "Dr":"Rare", "Major":"Rare",
            "Rev":"Rare", "Sir":"Rare", "Jonkheer":"Rare", "Dona":"Rare"
        })
        t = t.apply(lambda x: x if x in {"Mr","Mrs","Miss","Master"} else "Rare")
        return t

    def _ticket_prefix(self, s: pd.Series) -> pd.Series:
        # 문자 프리픽스 추출(없으면 'NUM')
        pref = s.astype(str).str.replace(r"[\./]", " ", regex=True).str.split().str[0]
        pref = pref.apply(lambda x: x if not x.isdigit() else "NUM")
        return pref

    def fit(self, X, y=None):
        X = X.copy()
        # Title 사전(여기서는 고정 매핑이라 별도 드물지만, rare 기준이 있다면 학습 세트 통계로)
        titles = self._extract_title(X["Name"])
        self._title_vocab_ = titles.value_counts().index.tolist()

        if self.use_ticket_prefix:
            tp = self._ticket_prefix(X["Ticket"])
            vc = tp.value_counts()
            keep = set(vc[vc >= self.rare_min_count].index)
            self._ticket_vocab_ = keep
        return self

    def transform(self, X):
        X = X.copy()
        # 파생
        X["Title"] = self._extract_title(X["Name"])
        X["FamilySize"] = X["SibSp"] + X["Parch"] + 1
        X["IsAlone"] = (X["FamilySize"] == 1).astype(int)
        X["CabinDeck"] = X["Cabin"].astype(str).str[0].str.upper()
        X.loc[X["CabinDeck"].isin(["N", "n", " ", "nan"]), "CabinDeck"] = "U"
        X["Pclass"] = X["Pclass"].astype("category")

        if self.add_fare_per_person:
            denom = X["FamilySize"].clip(lower=1)
            X["FarePerPerson"] = X["Fare"] / denom

        if self.use_ticket_prefix:
            tp = self._ticket_prefix(X["Ticket"])
            if self._ticket_vocab_ is not None:
                X["TicketPrefix"] = tp.apply(lambda z: z if z in self._ticket_vocab_ else "Rare")
            else:
                X["TicketPrefix"] = tp
        else:
            X["TicketPrefix"] = "NA"

        keep_cols = [
            "Pclass","Sex","Age","SibSp","Parch","Fare","Embarked",
            "Title","FamilySize","IsAlone","CabinDeck","FarePerPerson","TicketPrefix"
        ]
        # 존재하지 않는 컬럼이 있을 수 있으니 필터
        keep_cols = [c for c in keep_cols if c in X.columns]
        return X[keep_cols]

# 전처리 파이프라인

num_cols = ["Age","SibSp","Parch","Fare","FamilySize","FarePerPerson"]
cat_cols = ["Pclass","Sex","Embarked","Title","IsAlone","CabinDeck","TicketPrefix"]
num_cols = [c for c in num_cols if c in train.columns or c in ["FamilySize","FarePerPerson"]]

numeric_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler",  StandardScaler())  # 선형 계열에 유리
])

categorical_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("ohe",     OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer([
    ("num", numeric_pipe, num_cols),
    ("cat", categorical_pipe, cat_cols)
], remainder="drop")

# 모델 비교 함수

def evaluate_model(model, X, y, name):
    pipe = Pipeline([
        ("feat", TitanicFeatureBuilder(use_ticket_prefix=True)),
        ("prep", preprocess),
        ("clf",  model)
    ])
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_validate(
        pipe, X, y, cv=cv, n_jobs=-1,
        scoring=["accuracy","f1","roc_auc","average_precision"]
    )
    print(f"[{name}] "
          f"ACC {scores['test_accuracy'].mean():.3f}±{scores['test_accuracy'].std():.3f} | "
          f"F1 {scores['test_f1'].mean():.3f}±{scores['test_f1'].std():.3f} | "
          f"AUC {scores['test_roc_auc'].mean():.3f}±{scores['test_roc_auc'].std():.3f} | "
          f"AP {scores['test_average_precision'].mean():.3f}±{scores['test_average_precision'].std():.3f}")

X = train.drop(columns=["Survived"])
y = train["Survived"]

evaluate_model(LogisticRegression(max_iter=2000, class_weight="balanced"), X, y, "LogReg(balanced)")
evaluate_model(RandomForestClassifier(n_estimators=500, random_state=42), X, y, "RandomForest")
evaluate_model(HistGradientBoostingClassifier(learning_rate=0.08, random_state=42), X, y, "HGB")
```

설명
- 모든 변환은 **학습 폴드에서만** `fit` → 검증/테스트에는 `transform`만 수행되어 **누수 방지**.
- `OneHotEncoder(handle_unknown="ignore")`로 미지의 범주가 나와도 안전.

---

## 임계값 최적화(F1 최대, 또는 제약 기반)

기본 임계값 0.5는 최적이 아닐 수 있습니다. 검증 세트의 **Precision–Recall 곡선**에서 **F1 최대** 임계값을 찾거나,
**FPR ≤ α** 같은 제약 하에서 Recall을 최대화하십시오.

```python
from sklearn.metrics import precision_recall_curve, f1_score, roc_curve
from sklearn.model_selection import StratifiedKFold

def best_threshold_by_f1(pipe, X, y):
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    ps, ys = [], []
    for tr, va in cv.split(X, y):
        pipe.fit(X.iloc[tr], y.iloc[tr])
        proba = pipe.predict_proba(X.iloc[va])[:,1]
        ps.append(proba); ys.append(y.iloc[va].values)
    p = np.concatenate(ps); yy = np.concatenate(ys)

    prec, rec, thr = precision_recall_curve(yy, p)
    f1s = 2 * prec * rec / (prec + rec + 1e-12)
    idx = np.nanargmax(f1s)
    return (thr[idx-1], f1s[idx])  # thr 길이는 prec/rec보다 1 작음

pipe_log = Pipeline([
    ("feat", TitanicFeatureBuilder(use_ticket_prefix=True)),
    ("prep", preprocess),
    ("clf",  LogisticRegression(max_iter=2000, class_weight="balanced"))
])
thr_star, f1_star = best_threshold_by_f1(pipe_log, X, y)
print("Best threshold by F1:", thr_star, "F1:", f1_star)
```

제약 예시(FPR ≤ 10%에서 Recall 최대):
```python
from sklearn.metrics import roc_curve
pipe_log.fit(X, y)
proba = cross_val_predict(pipe_log, X, y, method="predict_proba",
                          cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))[:,1]
fpr, tpr, thr = roc_curve(y, proba)
mask = fpr <= 0.10
thr_c = thr[mask][np.argmax(tpr[mask])]
print("Threshold @FPR≤10%:", thr_c, "Recall:", tpr[mask][np.argmax(tpr[mask])])
```

---

## 하이퍼파라미터 튜닝(RandomizedSearch)

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

rf_pipe = Pipeline([
    ("feat", TitanicFeatureBuilder(use_ticket_prefix=True)),
    ("prep", preprocess),
    ("clf",  RandomForestClassifier(random_state=42))
])

param_dist = {
    "clf__n_estimators": randint(400, 1000),
    "clf__max_depth": randint(4, 20),
    "clf__min_samples_split": randint(2, 20),
    "clf__min_samples_leaf": randint(1, 10),
    "clf__max_features": ["sqrt", "log2", None]
}

rs = RandomizedSearchCV(
    rf_pipe, param_distributions=param_dist, n_iter=60, cv=5,
    scoring="roc_auc", n_jobs=-1, random_state=42, verbose=1
)
rs.fit(X, y)
print("Best AUC:", rs.best_score_)
print("Best params:", rs.best_params_)
best_model = rs.best_estimator_
```

부스팅 계열(예: `HistGradientBoostingClassifier`)도 `learning_rate` ↓, `max_leaf_nodes`, `min_samples_leaf` 등으로 복잡도를 조정하세요.

---

## 모델 해석(Feature Importance, PDP), 오류 분석

Permutation Importance로 **전처리 이후**의 실제 피처 중요도를 봅니다.

```python
from sklearn.inspection import permutation_importance, PartialDependenceDisplay
import matplotlib.pyplot as plt

best_model.fit(X, y)
# ColumnTransformer 이후의 최종 피처 이름 추출

ohe = best_model.named_steps["prep"].named_transformers_["cat"].named_steps["ohe"]
cat_names = ohe.get_feature_names_out(["Pclass","Sex","Embarked","Title","IsAlone","CabinDeck","TicketPrefix"])
final_names = np.r_[num_cols, cat_names]

res = permutation_importance(best_model, X, y, scoring="roc_auc", n_repeats=10, random_state=42)
imp = pd.Series(res.importances_mean, index=final_names).sort_values(ascending=False)
print(imp.head(20))

# PDP(부분 의존)로 중요 변수의 효과 시각화

fig, ax = plt.subplots(figsize=(10,4))
PartialDependenceDisplay.from_estimator(best_model, X,
    features=[0, 1],  # 예: 수치형 인덱스(여기서는 final_names 순서에 맞게 조정 필요)
    ax=ax); plt.show()
```

오류 분석: 성별/등급별 혼동 경향
```python
from sklearn.metrics import confusion_matrix
pipe = best_model
pipe.fit(X, y)
pred = pipe.predict(X)
cm = pd.DataFrame(confusion_matrix(y, pred), index=["Actual0","Actual1"], columns=["Pred0","Pred1"])
print(cm)

tmp = train.assign(pred=pred)
print(tmp.groupby(["Sex","Pclass"])["Survived"].mean())
print(tmp.groupby(["Sex","Pclass"])["pred"].mean())
```

---

## 확률 보정(Calibration)과 Brier Score

비용 기반 임계값 선택, Precision@k 운영 등에서는 **확률 보정**이 중요합니다.

```python
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import brier_score_loss
import matplotlib.pyplot as plt

base = Pipeline([
    ("feat", TitanicFeatureBuilder(use_ticket_prefix=True)),
    ("prep", preprocess),
    ("clf",  RandomForestClassifier(n_estimators=600, random_state=42))
])

cal = CalibratedClassifierCV(base, method="isotonic", cv=5)
cal.fit(X, y)

proba_raw = cross_val_predict(base, X, y, method="predict_proba",
                              cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))[:,1]
proba_cal = cross_val_predict(cal, X, y, method="predict_proba",
                              cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))[:,1]

print("Brier(raw):", brier_score_loss(y, proba_raw))
print("Brier(cal):", brier_score_loss(y, proba_cal))

pt, pp = calibration_curve(y, proba_cal, n_bins=8)
plt.plot(pp, pt, "o-"); plt.plot([0,1],[0,1],"--",color="gray")
plt.xlabel("Predicted"); plt.ylabel("Observed"); plt.title("Calibration (Isotonic)"); plt.show()
```

---

## 앙상블(스태킹/보팅)로 한 단계 더

서로 다른 편향·분산 특성을 가진 모델을 조합해 성능을 끌어올릴 수 있습니다.

```python
from sklearn.ensemble import StackingClassifier, VotingClassifier

log_clf = Pipeline([
    ("feat", TitanicFeatureBuilder(use_ticket_prefix=True)),
    ("prep", preprocess),
    ("clf",  LogisticRegression(max_iter=2000, class_weight="balanced"))
])

rf_clf = Pipeline([
    ("feat", TitanicFeatureBuilder(use_ticket_prefix=True)),
    ("prep", preprocess),
    ("clf",  RandomForestClassifier(n_estimators=600, random_state=42))
])

hgb_clf = Pipeline([
    ("feat", TitanicFeatureBuilder(use_ticket_prefix=True)),
    ("prep", preprocess),
    ("clf",  HistGradientBoostingClassifier(learning_rate=0.08, random_state=42))
])

vote = VotingClassifier(
    estimators=[("log", log_clf), ("rf", rf_clf), ("hgb", hgb_clf)],
    voting="soft", n_jobs=-1
)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_validate(vote, X, y, cv=cv, scoring=["accuracy","f1","roc_auc","average_precision"], n_jobs=-1)
print("Voting | ACC:", scores["test_accuracy"].mean(),
      "F1:", scores["test_f1"].mean(), "AUC:", scores["test_roc_auc"].mean(),
      "AP:", scores["test_average_precision"].mean())
```

---

## 최종 학습과 제출 파일 생성

검증 설정·임계값 전략을 확정했다면, **훈련 전량**으로 재학습 후 테스트를 예측합니다.

```python
# test = pd.read_csv("test.csv")

final_model = rs.best_estimator_ if "rs" in globals() else hgb_clf
final_model.fit(X, y)

# 확률 → 임계값 적용(예: thr_star) 또는 기본 0.5
# proba = final_model.predict_proba(test)[:,1]
# preds = (proba >= thr_star).astype(int)

# submission = pd.DataFrame({"PassengerId": test["PassengerId"], "Survived": preds})
# submission.to_csv("submission.csv", index=False)

```

---

## 성능을 더 끌어올리는 팁

- 검증
  - `StratifiedKFold(n_splits=5~10)`로 안정화, 시드 고정
- 특징 강화
  - `Age` 구간화(Child/Adult/Senior), `Fare` 로그/구간화
  - 상호작용: `Pclass × Sex`, `Title × Pclass`
- 규제/모델
  - 로지스틱 `C` 튜닝, 트리 `max_depth/min_samples_*` 조절
  - 부스팅 학습률 ↓, 추정기 수 ↑
- 임계값
  - F1 최대, 혹은 FPR 제약 등 **업무 목적 기반**으로 결정
- 확률 보정
  - 비용 기반 의사결정, Precision@k 운용 시 **Isotonic/Platt** 적용
- 누수 방지
  - 모든 변환은 **파이프라인 내부**에서 `fit`/`transform` 분리
- 재현성
  - 시드, 라이브러리 버전, 데이터 스냅샷 기록

---

## 체크리스트

- [ ] `Pipeline`·`ColumnTransformer`로 전처리/모델 일체화
- [ ] `Stratified K-Fold` 정확히 설정
- [ ] 내부 지표 **Accuracy, F1, ROC-AUC, PR-AUC** 병기
- [ ] 임계값 전략(F1 최대, 비용 최소, 제약 만족 등) 확정
- [ ] 하이퍼파라미터 탐색(Random/Optuna 등)
- [ ] 중요도·PDP·오류분석으로 인사이트 확보
- [ ] 최종 모델을 **Train 전량**으로 재학습 후 제출 파일 생성

---

## 부록 A) 수식 요약

- 이진 크로스엔트로피:
  $$
  \mathcal{L}(\theta) = -\frac{1}{n}\sum_{i=1}^n \big[y_i\log \hat{p}_i + (1-y_i)\log(1-\hat{p}_i)\big].
  $$
- 로지스틱 시그모이드: \( \hat{p}_i=\sigma(z_i),\ z_i=\theta^\top x_i \).
- 정규화(가중) 손실: \( \sum w_{y_i}\ell(y_i,\hat{p}_i)+\lambda\Omega(\theta) \).
- F1:
  $$
  \mathrm{F1}=2\cdot\frac{\mathrm{Precision}\cdot\mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}.
  $$
- ROC 좌표: \( \mathrm{TPR}=\frac{TP}{TP+FN},\ \mathrm{FPR}=\frac{FP}{FP+TN} \).
- 비용 기반 최적 임계값:
  $$
  \tau^\*=\frac{C_{FP}(1-\pi)}{C_{FN}\pi + C_{FP}(1-\pi)}.
  $$

---

### 정리

타이타닉 문제는 **파생 변수의 힘**과 **견고한 평가 절차**의 중요성을 동시에 보여 줍니다.
**Title/Family/Deck/Prefix** 같은 간단한 파생과 **Pipeline·CV·임계값 최적화·보정**만으로도 강한 베이스라인을 얻을 수 있습니다.
그 위에 **튜닝·부스팅·앙상블**을 더해 점진적으로 성능을 개선하십시오.
