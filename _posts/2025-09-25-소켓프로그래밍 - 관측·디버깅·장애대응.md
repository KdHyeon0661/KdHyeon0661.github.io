---
layout: post
title: 소켓프로그래밍 - 관측·디버깅·장애대응
date: 2025-09-25 17:25:23 +0900
category: 소켓프로그래밍
---
## 실전 운영: 관측·디버깅·장애대응

> 목표: 운영 현장에서 **패킷(계층 2~4) 관측 → 소켓 상태/큐 관측 → 프로세스/시스템 콜 관측**을 연결해 문제를 **증상→원인→조치**로 빠르게 수렴한다.
> 핵심 도구: **tcpdump / dumpcap / Wireshark**, **ss / netstat / lsof**, **strace -e trace=network**, (보너스) `/proc` / `ip -s link` / `ethtool -S`.
> 메트릭: **연결 상태 분포**(ESTABLISHED/CLOSE_WAIT/TIME_WAIT 등), **큐 길이(Recv-Q/Send-Q)**, **재전송률/RTT**.
> 끝으로 **장애 시나리오 플레이북**(SYN flood, 포트 고갈, CLOSE_WAIT 누수)을 절차대로 정리한다.

---

### 빠른 관측 원칙 (3단 훑기)

1) **패킷 레벨**: tcpdump/와이어샤크로 **실제 구간**이 느린지/손실되는지 본다. (SYN/ACK/FIN/RST, 재전송, 윈도우 제로, 지연)
2) **소켓 레벨**: ss/netstat로 **상태 분포/큐/타이머**를 확인한다. (SYN-RECV 폭증? CLOSE_WAIT 누수? Recv-Q가 꽉 찼는가?)
3) **프로세스 레벨**: lsof/strace로 **누가 어떤 fd로 무엇을 하고 있는지**를 본다. (accept 처리 지연? send 블로킹? 타임아웃?)

> **규칙**: 측정은 **침습도 낮은 것부터**. dumpcap(커널에서 ring-buffer로 캡처) → tcpdump(텍스트 파싱) → strace(시스템콜 추적).

---

## `tcpdump` & Wireshark **필수 필터 레시피**

#### 안전 캡처 기본형 (회전/필터/노이즈 제거)

```bash
# SSH 제외, 특정 포트만, ring buffer(파일당 100MB, 총 10개)

sudo dumpcap -i eth0 -f 'tcp port 9000 and not port 22' \
  -b filesize:100000 -b files:10 -w /var/tmp/cap_9000.pcapng

# 짧게 확인(요약):

sudo tcpdump -i eth0 'tcp port 9000 and not port 22' -nn -tt -c 100
```
- **dumpcap**은 GUI 없이 커널 링버퍼로 **낮은 오버헤드** 캡처.
- **BPF 필터**는 `-f` (dumpcap) 또는 인자 마지막 따옴표(tcpdump).
- `not port 22`(자기 SSH 트래픽 제외), `-nn`(역해석 off), `-tt`(절대 타임스탬프).

#### 3-웨이 핸드셰이크, RST/FIN, 재전송

| 목적 | tcpdump BPF | Wireshark Display Filter |
|---|---|---|
| SYN (초기) | `tcp[tcpflags] & tcp-syn != 0 and tcp[tcpflags] & tcp-ack == 0` | `tcp.flags.syn==1 && tcp.flags.ack==0` |
| SYN-ACK | `tcp[tcpflags] & (tcp-syn|tcp-ack) == (tcp-syn|tcp-ack)` | `tcp.flags.syn==1 && tcp.flags.ack==1` |
| RST | `tcp[tcpflags] & tcp-rst != 0` | `tcp.flags.reset==1` |
| FIN | `tcp[tcpflags] & tcp-fin != 0` | `tcp.flags.fin==1` |
| 재전송 | *(tcpdump만으로 애매)* | `tcp.analysis.retransmission || tcp.analysis.fast_retransmission` |
| 윈도우 제로 | *(BPF 불가)* | `tcp.window_size_value == 0` |

예) 포트 9000의 핸드셰이크만:
```bash
sudo tcpdump -i eth0 'tcp port 9000 and (tcp[tcpflags] & (tcp-syn|tcp-ack) != 0)' -nn -tt
```

#### 지연/손실 패턴 빠르게 보기 (Wireshark)

- **재전송/중복 ACK**: `tcp.analysis.retransmission or tcp.analysis.duplicate_ack`
- **Out-of-Order**: `tcp.analysis.out_of_order`
- **제로윈도우/프로브**: `tcp.window_size_value==0 or tcp.analysis.zero_window_probe`
- **SACK**: `tcp.option_kind==5`

#### 특정 피어/서브넷/IPv6

```bash
# 특정 서버 IP와의 트래픽

sudo tcpdump -i eth0 'host 10.0.1.23 and tcp port 9000' -nn

# IPv6만

sudo tcpdump -i eth0 'ip6 and tcp port 9000' -nn

# NAT 뒤 클라이언트 대역만

sudo tcpdump -i eth0 'net 192.168.0.0/16 and tcp port 9000'
```

#### UDP/DNS/ICMP/패킷 크기

```bash
# DNS 질의/응답

sudo tcpdump -i eth0 'udp port 53' -vvv

# ICMP (핑/네트워크 오류)

sudo tcpdump -i eth0 'icmp or icmp6' -nn

# MTU 넘어 단편화 의심: 큰 프레임만

sudo tcpdump -i eth0 'tcp port 9000 and greater 1400' -nn
```

> **팁**: **패킷 타임라인**을 보면 “느림”의 정체가 구간별로 드러난다. SYN-ACK 지연(네트워크/방화벽) vs 서버 응답 지연(애플리케이션) vs 재전송(손실/혼잡).

---

## `ss` / `netstat` — **소켓 상태·큐·타이머** 해부

### 한눈에 요약

```bash
ss -s
```
주요 지표:
- `TCP:` 줄의 **estab**, **closed**, **synrecv**, **timewait**, **orphaned**
- `TCP timers`(RTO 등), **retrans**(재전송 카운터)
- 커널 전역 소켓 통계의 **스냅샷**.

### 상태별 나열/필터

```bash
# LISTEN 소켓 목록(프로세스/포트/백로그)

ss -tlpn

# ESTABLISHED 중 특정 포트

ss -tan 'sport = :9000 and state established'

# SYN-RECV(핸드셰이크 중)

ss -tan state syn-recv | wc -l

# CLOSE-WAIT(상대가 FIN을 보냈고, 내 프로세스가 아직 close 안 한 상태)

ss -tan state close-wait

# TIME-WAIT

ss -tan state time-wait

# 타이머/큐까지 상세(-i, -o)

ss -ti 'sport = :9000'
```

출력 해석 포인트:
- `Recv-Q` / `Send-Q` : **커널 수신/송신 큐** 크기(바이트). `Recv-Q`가 계속 쌓이면 **애플리케이션 읽기 지연**, `Send-Q`가 쌓이면 **상대가 느림 / 네트워크 혼잡**.
- `timer:(on,xxx)` : RTO/keepalive 타이머.
- `skmem:(r0,w0,f0,t0)` : 소켓 메모리 사용량(커널별 다름).
- `cwnd`, `rtt` : `-i`일 때 일부 시스템에서 보임.

### `/proc` 보조

```bash
cat /proc/net/sockstat
cat /proc/sys/net/core/somaxconn
cat /proc/sys/net/ipv4/tcp_max_syn_backlog
```
- **somaxconn**: `listen(backlog)` 상한.
- **tcp_max_syn_backlog**: 미완료(SYN) 큐 크기.
- **sockstat**: 전역 소켓 리소스 상황.

---

## `lsof` / `strace -e trace=network` — **프로세스 관점** 파헤치기

### `lsof`

```bash
# 포트 9000을 LISTEN 중인 프로세스

sudo lsof -nP -iTCP:9000 -sTCP:LISTEN

# 특정 PID가 가진 TCP 소켓 상태/상대IP

sudo lsof -nP -p <PID> -a -iTCP

# CLOSE_WAIT 소켓을 쥐고 있는 프로세스 찾기

sudo lsof -nP -iTCP -sTCP:CLOSE_WAIT
```

### `strace -e trace=network`

```bash
# PID에 네트워크 콜만 붙고, 시간/소요시간/쓰레드 따라가기

sudo strace -f -tt -T -y -p <PID> -e trace=network

# 서버를 직접 strace로 구동(개발환경)

strace -f -tt -T -y -e trace=network ./server :9000
```
- **읽기/쓰기 블로킹** 구간의 **소요시간**이 보인다.
- 반복되는 `EAGAIN` / `send` 부분쓰기 / `accept` 지연 등을 눈으로 확인.

> **팁**: strace는 침습적이다. **장시간** 생산서버에 붙이면 비용이 크다. **재현 환경/저부하 시간**에 사용하거나, **짧게** 붙고 떼자.

---

## 운영 메트릭 — **상태 분포/큐/재전송률**

### 상태 분포

- `ss -s`를 주기적으로 수집하여 **ESTABLISHED / SYN-RECV / TIME-WAIT / CLOSE-WAIT** 추이를 그린다.
- **이상치**:
  - **SYN-RECV 급증** → 핸드셰이크 지연(방화벽/로드밸런서/백로그 부족/SYN flood).
  - **CLOSE-WAIT 증가** → 애플리케이션이 `read==0`(상대 FIN)을 받고 **close()를 누락**.
  - **TIME-WAIT 많음** → 능동 종료가 많다(보통 클라이언트측). NAT/Ephemeral 포트 고갈 고려.

### 큐 길이(Recv-Q/Send-Q)

- `ss -tinp` 의 `Recv-Q/Send-Q`를 수집 → **상위 N 연결**을 정렬.
- **Recv-Q 상위**: 서버가 **읽기를 못 따라감**(CPU 바운드/락 경합/디스크/DB 대기) → **백프레셔 설계** 필요.
- **Send-Q 상위**: 상대가 느리거나(rwnd↓) 네트워크 혼잡(재전송↑).

### 재전송률/RTT(직관)

- `TCP_INFO` 또는 `ss -ti` / `netstat -s` 로 재전송 카운트/RTT를 본다.
- 단위시간 재전송률(근사):
  $$
  \text{RetransRate} = \frac{\Delta\text{RetransSegments}}{\Delta\text{SegmentsOut}}
  $$
  값이 **상승 추세**면 손실/혼잡 또는 **비정상 경로** 의심.

---

## **장애 시나리오 플레이북**

### 공통 프레임(각 시나리오에 적용)

1) **탐지(Detection)**: 알람/대시보드/사용자 신고/로그.
2) **증상(What)**: 구체적 수치(에러율, 지연 p95/p99, 드롭률).
3) **가설과 확인(Why)**: tcpdump/ss/lsof/strace로 가장 가능성 높은 원인부터 배제.
4) **완화(Mitigation)**: 즉시 적용 가능한 설정/우회/속도제한.
5) **근본원인(RCA)** & **영구대책**: 재발 방지(설계/코드/인프라).
6) **검증(Verification)**: 수치가 정상화되는지 확인.

---

### 시나리오 A — **SYN flood / 핸드셰이크 병목**

**증상**
- 신규 연결 지연/실패, 타임아웃 증가.
- `ss -s`의 `synrecv` 급증. `listen` 소켓의 **backlog** 포화.

**빠른 확인**
```bash
# SYN-RECV 개수

ss -tan state syn-recv | wc -l

# LISTEN/백로그 확인

ss -ltn | grep :9000

# 패킷 레벨: SYN만 폭주?

sudo tcpdump -i eth0 'tcp dst port 9000 and tcp[tcpflags] & tcp-syn != 0 and tcp[tcpflags] & tcp-ack == 0' -nn -c 100

# SYN cookies 카운터(리눅스)

cat /proc/net/netstat | egrep 'SyncookiesSent|SyncookiesRecv|SyncookiesFailed'
```

**즉시 완화(운영계)**
```bash
# SYN cookies ON

sudo sysctl -w net.ipv4.tcp_syncookies=1

# 백로그 상향

sudo sysctl -w net.core.somaxconn=4096
sudo sysctl -w net.ipv4.tcp_max_syn_backlog=8192

# SYNPROXY (nftables 예시, LB 앞단 게이트웨이에 구성 권장)
# *환경에 따라 다름 — 개념 예시*

sudo nft add table ip synproxy
sudo nft add chain ip synproxy input { type filter hook input priority 0\; }
sudo nft add rule ip synproxy input tcp dport 9000 ct state new,untracked synproxy mss 1460 sack-perm timestamp wscale 7
sudo nft add rule ip synproxy input tcp flags syn ct state new meter flood { ip saddr limit rate 50/second burst 100 } accept
sudo nft add rule ip synproxy input tcp flags syn drop
```
- **SYN cookies**: 핸드셰이크 확정 전 **상태**를 줄여 flood 버팀.
- **SYNPROXY**: 프록시가 **3-way를 대신 수행** 후 정상 연결만 뒤로 전달.

**원인 추적 & 영구대책**
- 방화벽/ACL/보안장비 **SYN rate-limit** 설정 재검토.
- 앞단 **L4 로드밸런서** 도입/강화, **애니캐스트**/CDN 등 에지 확장.
- 애플리케이션 측 **accept 루프**가 빠르게 돌아가는지(epoll ET, 드레인) 점검.
- 백로그 값을 서비스 특성에 맞게 상향. **`listen(backlog)` = `somaxconn` 캡** 주의.

**검증**
- `synrecv` 하락, 신규 연결 지연 개선.
- tcpdump에 **정상 SYN→SYN/ACK→ACK 비율** 회복.

---

### 시나리오 B — **포트 고갈(에페메랄 / NAT / TIME_WAIT 붐)**

**증상**
- 클라이언트 측 신규 연결 **ECONNREFUSED/ETIMEDOUT** 증가.
- `TIME_WAIT` 폭증, NAT 테이블 고갈.
- `ulimit -n` 초과(FD 고갈) 동반 가능.

**빠른 확인**
```bash
# 에페메랄 포트 범위(클라이언트 노드)

cat /proc/sys/net/ipv4/ip_local_port_range   # 예: 32768 60999

# TIME_WAIT 개수

ss -tan state time-wait | wc -l

# NAT 테이블(있다면)

sudo conntrack -S  # requires conntrack-tools

# 시스템 FD 한도

ulimit -n
```

**즉시 완화**
```bash
# 에페메랄 포트 범위 확장(클라이언트/중간 NAT)

sudo sysctl -w net.ipv4.ip_local_port_range="15000 65000"

# 커넥션 재사용(어플리케이션): Keep-Alive/커넥션 풀
#   - HTTP 클라이언트라면 Keep-Alive 켜기, MaxIdleConns/host 상향.

# NAT 테이블 상향 (게이트웨이)

sudo sysctl -w net.netfilter.nf_conntrack_max=1048576

# FD 한도 상향 (프로세스/서비스 유닛)

ulimit -n 1048576
```

> **주의**: 옛 커널의 `tcp_tw_reuse/tcp_tw_recycle`은 비권장/제거됨. **프로토콜 위배/경합** 부작용.
> 해결의 본질은 **연결 수명을 길게(Keep-Alive)** 하여 **새 연결 생성률**을 낮추는 것.

**원인 추적 & 영구대책**
- 클라이언트/프록시가 **짧은 요청마다 새 연결**을 만드는지 점검 → **풀/Keep-Alive**.
- NAT 하드웨어 용량/타임아웃 조정, 분산(여러 NAT 장비).
- 서버 측 **TIME_WAIT**는 보통 능동 종료를 의미 — 재설계로 **서버가 능동 종료 비율**을 줄이거나, **클라이언트가 먼저 종료**.

**검증**
- 실패율 하락, `TIME_WAIT` 감소, NAT 사용률 안정.
- 대시보드에서 **신규 연결/초**가 과도하지 않은지 확인.

---

### 시나리오 C — **CLOSE_WAIT 누수(애플리케이션 close() 누락)**

**증상**
- `CLOSE_WAIT` 소켓 수가 지속 증가. 서버 메모리/FD 고갈 위험.
- 지연/에러율 점진적 상승.

**빠른 확인**
```bash
# CLOSE_WAIT 목록/개수

ss -tan state close-wait | wc -l
sudo lsof -nP -iTCP -sTCP:CLOSE_WAIT | head

# 프로세스별 소켓 수 집계(예시)

sudo lsof -nP -iTCP -sTCP:CLOSE_WAIT | awk '{print $1,$2}' | sort | uniq -c | sort -nr | head
```

**해석**
- **CLOSE_WAIT** = 원격이 **FIN**을 보내고, 커널이 내 프로세스에 전달했으며, **내 프로세스가 아직 close()하지 않음**.
- 코드에서 `recv==0` (EOF)을 처리한 뒤 **반드시 close()** 해야 한다.

**즉시 완화**
- **프로세스 재시작**(임시).
- Keepalive를 짧게 조정(유휴 죽은 피어 감지) — 근본 해결은 아님.

**코드 점검 포인트**
- 길이-프리픽스 프로토콜에서 `recv_exact()`가 **EOF**를 에러로 넘기고 있는지.
- 예외 경로에서 `close()` 누락. RAII(예: `unique_fd`) 사용.
- 멀티스레드 경합으로 close 타이밍 놓침.

**검증**
- 배포 후 `CLOSE_WAIT` 수가 **안정/감소**. FD 사용량 정상화.

---

## 운영 실무 꿀팁 모음

- **캡처는 덤프캡**: `dumpcap -b filesize:... -b files:...`로 **회전 저장**. 후분석은 Wireshark.
- **SSH 제외**: `not port 22`를 항상 붙여의도치 않은 민감정보 캡처 억제.
- **ss 상위 N 연결**:
  ```bash
  ss -tinp | awk 'NR>1{print $2,$3,$5,$6,$7}' | head
  ```
- **RTT/재전송 스팟체크**:
  ```bash
  ss -ti 'sport = :9000' | egrep 'rtt|retrans'
  ```
- **인터페이스 드롭 확인**:
  ```bash
  ip -s link show eth0
  sudo ethtool -S eth0 | egrep 'rx_dropped|tx_dropped|errors'
  ```
- **ulimit/FD**: 서비스 유닛/컨테이너에 `LimitNOFILE=` 상향, 애플리케이션에서 **소켓 누수** 검출(주기적 FD 카운트 로깅).

---

## 미니 레시피 — “문제가 어디 구간인가?”

1) **SYN-ACK 지연** → tcpdump로 SYN↔SYN/ACK 간 시간 측정.
2) **수신 처리 지연** → `Recv-Q↑`, `tcp.window_size_value==0` (상대가 막힘), 애플리케이션 CPU/락 확인.
3) **송신 지연** → `Send-Q↑`, 재전송/중복ACK, RTT↑.
4) **즉시 RST/ECONNRESET** → 서버 `SO_LINGER=RST` 정책/프록시 RST/헬스체크 실패.

---

## 실습: **장애 시나리오 플레이북 연습**

### 준비

- 서버 포트 9000의 et-에코 서버(10장) 실행.
- 부하: 클라이언트 N개로 길이-프리픽스 프레임 전송.
- **netem**으로 네트워크 조건 주입:
```bash
# 50ms 지연, 3% 손실

sudo tc qdisc add dev eth0 root netem delay 50ms loss 3%
# 제거

sudo tc qdisc del dev eth0 root
```

### 시나리오 A: SYN flood 흉내(개념)

- 별도 호스트에서 빠르게 SYN만 보내는 도구(합법적 환경에서만).
- **목표**: `synrecv` 상승, backlog 포화 관측 → SYN cookies/SYNPROXY 적용 후 회복 확인.

### 시나리오 B: 포트 고갈

- 클라이언트에서 **짧은 연결**을 초당 수천 회 생성.
- **목표**: `TIME_WAIT` 폭증, 에페메랄 포트 범위 확인/상향, Keep-Alive 적용 후 **신규 연결/초** 감소와 성공률 상승 확인.

### 시나리오 C: CLOSE_WAIT 누수

- 서버 코드에서 `recv==0` 경로에 close 누락시킨 테스트 빌드 실행.
- **목표**: `CLOSE_WAIT` 증가, lsof로 해당 PID 식별 → 코드 수정 후 정상화.

---

## 치트시트 — 자주 쓰는 명령/필터

```bash
# 요약/상태

ss -s
ss -ltn
ss -tan state syn-recv
ss -tan state close-wait
ss -ti 'sport = :9000'

# lsof/strace

sudo lsof -nP -iTCP:9000 -sTCP:LISTEN
sudo lsof -nP -iTCP -sTCP:CLOSE_WAIT
sudo strace -f -tt -T -y -p <PID> -e trace=network

# tcpdump/dumpcap

sudo dumpcap -i eth0 -f 'tcp port 9000 and not port 22' -b filesize:100000 -b files:10 -w /var/tmp/cap.pcapng
sudo tcpdump -i eth0 'tcp port 9000 and tcp[tcpflags] & tcp-syn != 0' -nn -tt

# 커널/자원

cat /proc/net/sockstat
cat /proc/sys/net/core/somaxconn
cat /proc/sys/net/ipv4/tcp_max_syn_backlog
ulimit -n
```

Wireshark Display Filter:
```
tcp.flags.syn==1 && tcp.flags.ack==0
tcp.flags.reset==1
tcp.analysis.retransmission || tcp.analysis.fast_retransmission
tcp.window_size_value==0 || tcp.analysis.zero_window_probe
tcp.stream eq 5
```

---

## 간단 수식(메트릭 직관)

- 재전송률(구간 \(\Delta t\)):
  $$
  \text{RetransRate}=\frac{\Delta\text{RetransSegments}}{\Delta\text{SegmentsOut}}
  $$
- 큐 점유율(직관):
  $$
  \text{QueueOccupancy}=\frac{\text{Send-Q}}{\text{SNDBUF}},\quad
  \text{Backpressure}=\mathbf{1}\{\text{QueueOccupancy}>\theta\}
  $$

---

## 마무리

운영 문제는 **계층을 내려가며** 본다.
패킷(핸드셰이크/재전송/윈도우), 소켓(상태/큐/타이머), 프로세스(시스템 콜/FD) — 이 **세 시야**가 겹치는 지점이 **원인**이다.
이 장의 레시피와 플레이북을 팀 **RUNBOOK**으로 정착시키고, 알람(상태 분포/큐/재전송률)에 **명확한 임계값**을 부여하라.
그 순간 “네트워크가 느려요”라는 모호한 문장은 **재현 가능한 수치와 조치**로 바뀐다.
