---
layout: post
title: 딥러닝 - RAG 평가 체크리스트
date: 2025-10-07 20:25:23 +0900
category: 딥러닝
---
# RAG 평가 체크리스트 — **정확성 · 근거성(groundedness) · 지시 따르기(instruction-following)**  

## 0) 데이터 포맷 & 전체 평가 파이프라인

### 0.1 골드 세트 스키마(JSONL)

- `corpus.jsonl` — 색인 문서
```json
{"doc_id": "d1", "title": "FAISS 개요", "text": "FAISS는 벡터 검색 라이브러리로..."}
{"doc_id": "d2", "title": "BM25", "text": "BM25는 고전 정보검색 점수이며..."}
```

- `queries.jsonl` — 질문/정답/근거
```json
{"qid": "q1",
 "query": "FAISS는 무엇이며 HNSW는 어떤 인덱스인가?",
 "gold_answers": ["FAISS는 벡터 검색 라이브러리", "HNSW는 근사 최근접 탐색용 그래프 인덱스"],
 "gold_evidence": [["d1"], ["d1"]],               // 각 정답을 지지하는 문서 집합(여러 개 가능)
 "constraints": {"style":"bullet", "cite": true, "lang": "ko"}}
```

- `predictions.jsonl` — RAG 모델 결과(평가 입력)
```json
{"qid": "q1",
 "retrieved": ["d4","d1","d2"],                   // Top-k (순서 포함)
 "reranked":  ["d1","d2","d4"],                   // (선택) 재순위 결과
 "cited":     ["d1"],                              // 모델이 최종 응답에서 인용한 문서들
 "answer":    "- FAISS는 벡터 검색 라이브러리입니다 [#d1]\n- HNSW는 그래프 기반 ANN 인덱스입니다 [#d1]",
 "meta": {"latency_ms": 210, "tokens_ctx": 1200, "tokens_out": 110}}
```

> 최소 요건: `queries.jsonl` + `predictions.jsonl`. `gold_evidence`가 없으면 **근거성**은 NLI 기반 자동 판정(아래 3.2)으로 대체합니다.

---

## 1) **정확성(Answer Quality)** 체크리스트

**목표**: 답이 “사실적으로 맞는가?” + “질문 의도에 맞는 정보를 충분히 포함하는가?”

### 1.1 기준 지표

- **Extractive/팩트성 QA**  
  - **EM(Exact Match)**: 토큰 단위 완전 일치  
  - **Token-level F1**: 정답 토큰과 예측 토큰의 정밀도/재현율 조화 평균  
  $$\mathrm{F1}=\frac{2PR}{P+R}$$
- **생성형 QA/설명형**  
  - **SemSim**(임베딩 유사도):  
  $$\cos(\mathbf{a},\mathbf{g})=\frac{\mathbf{a}\cdot\mathbf{g}}{\|\mathbf{a}\|\,\|\mathbf{g}\|}$$  
  (정답 표면형이 다양할 때 EM/F1 보완)
  - **Key-point Recall**: 골드 정답 원문에서 **핵심 키워드/개념**이 포함되었는지(룰+NLP)
  - **Coverage**: 서브-질문(분해형 질의) 대비 커버율

### 1.2 정확성 평가 코드(정규화 + EM/F1 + 임베딩 유사도)

```python
import re, json, numpy as np, math, unicodedata
from collections import Counter
from typing import List, Dict

def normalize_text(s: str) -> str:
    s = s.lower()
    s = "".join(ch for ch in s if ch.isalnum() or ch.isspace() or ch in "-_/")
    s = unicodedata.normalize("NFKC", s)
    return " ".join(s.split())

def em_score(pred: str, golds: List[str]) -> int:
    p = normalize_text(pred)
    return int(any(p == normalize_text(g) for g in golds))

def f1_score_token(pred: str, golds: List[str]) -> float:
    def to_toks(x): return normalize_text(x).split()
    p = to_toks(pred)
    best = 0.0
    for g in golds:
        g = to_toks(g)
        common = Counter(p) & Counter(g)
        num_same = sum(common.values())
        if len(p)==0 or len(g)==0:
            f1 = 1.0 if len(p)==len(g) else 0.0
        elif num_same==0:
            f1 = 0.0
        else:
            prec = num_same/len(p)
            rec  = num_same/len(g)
            f1 = 2*prec*rec/(prec+rec)
        if f1>best: best=f1
    return best

# (선택) 임베딩 유사도 — Hugging Face 임베더 사용
from transformers import AutoTokenizer, AutoModel
import torch

class SimpleEmbedder:
    def __init__(self, name="intfloat/e5-small-v2"):
        self.tok = AutoTokenizer.from_pretrained(name)
        self.enc = AutoModel.from_pretrained(name)

    @torch.no_grad()
    def embed(self, texts: List[str], device="cuda" if torch.cuda.is_available() else "cpu"):
        self.enc.to(device).eval()
        outs=[]
        for i in range(0,len(texts),32):
            b = texts[i:i+32]
            x = self.tok(b, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
            h = self.enc(**x).last_hidden_state
            m = x["attention_mask"].unsqueeze(-1).float()
            v = (h*m).sum(1)/m.sum(1).clamp(min=1e-6)
            v = torch.nn.functional.normalize(v, p=2, dim=1)
            outs.append(v.cpu())
        return torch.cat(outs,0).numpy()

def semsim(pred: str, golds: List[str], emb: SimpleEmbedder) -> float:
    vs = emb.embed([pred] + golds)
    p = vs[0]
    gs = vs[1:]
    return float(np.max(gs @ p))
```

---

## 2) **리트리벌 품질** 체크리스트

**목표**: 검색 단계에서 정답 근거를 **얼마나 잘 끌어오는가?**

- **Recall@k**: Top-k에 **정답 문서**가 하나라도 포함되는 비율  
- **MRR**(Mean Reciprocal Rank), **nDCG@k**(랭킹 품질)  
- **Coverage@k**: 다중 근거 문제에서 **필요한 증거 수 대비 포함 비율**  
- **Diversity/Overlap**: 중복 청크 비율 ↓

```python
def recall_at_k(gold_docs: List[str], retrieved: List[str], k=5) -> float:
    return float(len(set(gold_docs) & set(retrieved[:k]))>0)

def mrr(gold_docs: List[str], retrieved: List[str]) -> float:
    for r,doc in enumerate(retrieved, start=1):
        if doc in gold_docs: return 1.0/r
    return 0.0

def ndcg_at_k(gains: List[int], k=10) -> float:
    # gains: 랭킹 순서대로 [1/0/graded] 관련성 벡터
    def dcg(gs):
        return sum((g/math.log2(i+2) for i,g in enumerate(gs[:k])))
    ideal = sorted(gains, reverse=True)
    return dcg(gains)/max(dcg(ideal), 1e-9)

def coverage_at_k(gold_sets: List[List[str]], retrieved: List[str], k=5) -> float:
    # gold_sets: 각 서브정답을 지지하는 증거 집합(여러개 중 하나 충족)
    satisfied = 0
    for cand_set in gold_sets:
        if set(cand_set) & set(retrieved[:k]): satisfied += 1
    return satisfied/len(gold_sets) if gold_sets else 0.0
```

**체크 포인트**

- [ ] Top-k에서 **Recall@k ≥ 0.9**(닫힌 코퍼스 기준)  
- [ ] **MRR**과 **nDCG**도 함께 추적(재순위 효과 가시화)  
- [ ] **중복 청크** 비율 < 20%  
- [ ] **Latency**(p50/p95) 로그 및 회귀 추적

---

## 3) **근거성(Groundedness/Faithfulness)** 체크리스트

**목표**: 답변의 모든 주장/사실이 **제공된 컨텍스트에서 정당화**되는가? (환각 방지)

### 3.1 인용 기반 정량화(골드 evidence가 있을 때)

- **Citation Precision/Recall/F1**  
  $$\text{CitPrec}=\frac{|C\cap G|}{|C|},\quad \text{CitRec}=\frac{|C\cap G|}{|G|}$$  
  - \(C\): 모델이 인용한 문서 집합, \(G\): 골드 근거 집합(여러 집합일 경우 합집합/부분집합 기준 선택)

- **Unsupported Claim Rate**: 답변 문장을 분해 → **근거 문서 내 문장**과 정합성 검사 실패 비율

### 3.2 NLI 기반 자동 판정(골드 evidence가 없을 때)

- 각 **답변 문장** \(s\) 과 **컨텍스트** \(c\) 에 대해 **NLI**를 수행:  
  - **Entailed**: 근거 있음  
  - **Contradicted/Neutral**: 근거 없음 또는 모순 → **미근거/환각**  
- 지표: **Entailed ratio**, **Contradiction ratio**, **Unsupported ratio**

```python
import nltk, torch, numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
nltk.download("punkt", quiet=True)

def sent_split(text: str) -> List[str]:
    from nltk.tokenize import sent_tokenize
    return [s.strip() for s in sent_tokenize(text) if s.strip()]

class NLIFaithfulness:
    # roberta-large-mnli 등 — PyTorch
    def __init__(self, name="roberta-large-mnli"):
        self.tok = AutoTokenizer.from_pretrained(name)
        self.m = AutoModelForSequenceClassification.from_pretrained(name).eval().to(
            "cuda" if torch.cuda.is_available() else "cpu"
        )
        self.ent_idx = 2  # MNLI: [contradiction, neutral, entailment]

    @torch.no_grad()
    def entail_prob(self, premise: str, hypothesis: str) -> float:
        x = self.tok(premise, hypothesis, return_tensors="pt", truncation=True, max_length=512).to(self.m.device)
        y = self.m(**x).logits.softmax(-1)[0, self.ent_idx].item()
        return y

    def score_answer(self, answer: str, context: str, th_ent=0.7, th_contra=0.7) -> Dict:
        sents = sent_split(answer)
        entailed, unsupported, contrad = 0,0,0
        for s in sents:
            p_ent = self.entail_prob(context, s)
            p_con = 1.0 - p_ent  # 간단 근사(엄밀히는 3-class 사용)
            if p_ent >= th_ent: entailed += 1
            elif p_con >= th_contra: contrad += 1
            else: unsupported += 1
        n = max(len(sents), 1)
        return {
            "entailed_ratio": entailed/n,
            "contrad_ratio": contrad/n,
            "unsupported_ratio": unsupported/n,
            "n_sents": n
        }
```

> 실무 팁: 컨텍스트가 길면 **상위 하이라이트 문장**만 premise로 사용하거나 문단 단위로 슬라이딩합니다.

---

## 4) **지시 따르기(Instruction-Following)** 체크리스트

**목표**: 사용자의 **형식·스타일·언어·인용 규칙**을 지켰는가?

- **형식**: JSON/마크다운/코드 블록/불릿 등 스펙 충족  
- **스타일**: 톤(정중/간결), 길이 제한(예: “200자 이내”), 언어(ko/en)  
- **인용 규칙**: `[ #doc_id ]` 표기, **허용된 출처만** 인용  
- **안전/금지 규칙**: “근거 없으면 ‘모름’” 등 거짓 확신 금지  
- **콘텐츠 요구**: 단계별 절차/리스트/테이블 등 포함/불포함 항목

### 4.1 자동 검증기(룰 베이스 + 스키마 검증)

```python
import re, json, jsonschema
from typing import Optional

def check_language(text: str, lang="ko") -> bool:
    # 매우 단순한 휴리스틱: 한글 비중
    if lang=="ko":
        cnt_ko = sum(0xAC00 <= ord(ch) <= 0xD7A3 for ch in text)
        return cnt_ko >= max(10, len(text)*0.2)
    return True

def check_citations(text: str, allowed_ids: Optional[set]=None) -> Dict:
    cites = re.findall(r"\[#([A-Za-z0-9_\-:.]+)\]", text)
    cites_set = set(cites)
    ok = True
    if allowed_ids is not None:
        ok = cites_set.issubset(allowed_ids)
    return {"has_cite": len(cites)>0, "cites": list(cites_set), "cites_ok": ok}

def check_style(text: str, style="bullet") -> bool:
    if style=="bullet":
        lines = [l for l in text.splitlines() if l.strip()]
        return sum(1 for l in lines if l.lstrip().startswith(("-", "*", "•"))) >= max(1, len(lines)*0.5)
    return True

def check_length(text: str, max_chars=None) -> bool:
    if max_chars is None: return True
    return len(text) <= max_chars

def validate_json_schema(text: str, schema: dict) -> bool:
    try:
        obj = json.loads(text)
        jsonschema.validate(obj, schema)
        return True
    except Exception:
        return False
```

---

## 5) **엔드-투-엔드 평가 스크립트**

**입력**: `corpus.jsonl`, `queries.jsonl`, `predictions.jsonl`  
**출력**: 집계 리포트(정확성/근거성/지시준수), 쿼리별 상세 테이블, 회귀 비교용 요약

```python
import json, numpy as np, pandas as pd
from collections import defaultdict

def load_jsonl(path): 
    with open(path, encoding="utf-8") as f:
        for line in f: 
            yield json.loads(line)

def build_doc_map(corpus_path):
    mp={}
    for j in load_jsonl(corpus_path):
        mp[j["doc_id"]] = j
    return mp

def evaluate_all(corpus_path, queries_path, preds_path,
                 do_nli=True, semsim_model="intfloat/e5-small-v2"):
    docmap = build_doc_map(corpus_path)
    queries = {j["qid"]: j for j in load_jsonl(queries_path)}
    preds = {j["qid"]: j for j in load_jsonl(preds_path)}

    embedder = SimpleEmbedder(semsim_model) if semsim_model else None
    nli = NLIFaithfulness() if do_nli else None

    rows=[]
    for qid, q in queries.items():
        p = preds.get(qid, None)
        if not p:
            rows.append({"qid":qid, "status":"missing"}); continue

        gold_answers = q.get("gold_answers", [])
        gold_evidence_sets = q.get("gold_evidence", [])
        gold_docs_union = set().union(*gold_evidence_sets) if gold_evidence_sets else set()

        # 정확성
        em = em_score(p["answer"], gold_answers) if gold_answers else None
        f1 = f1_score_token(p["answer"], gold_answers) if gold_answers else None
        sim = semsim(p["answer"], gold_answers, embedder) if (gold_answers and embedder) else None

        # 리트리벌
        r = p.get("retrieved", []) or []
        rec5 = recall_at_k(list(gold_docs_union), r, 5) if gold_docs_union else None
        my_mrr = mrr(list(gold_docs_union), r) if gold_docs_union else None

        # 근거성 — 인용 일치 + (선택) NLI
        cited = set(p.get("cited", []) or [])
        cit_prec = (len(cited & gold_docs_union)/len(cited)) if (cited and gold_docs_union) else None
        cit_rec  = (len(cited & gold_docs_union)/len(gold_docs_union)) if gold_docs_union else None

        nli_stats = {}
        if nli:
            # 컨텍스트 = 인용 문서 텍스트 결합(너무 길면 자르기)
            ctx = "\n".join([docmap[c]["text"] for c in cited if c in docmap])[:4000]
            if not ctx and r:
                ctx = "\n".join([docmap[d]["text"] for d in r[:3] if d in docmap])[:4000]
            if ctx:
                nli_stats = nli.score_answer(p["answer"], ctx)
            else:
                nli_stats = {"entailed_ratio":None, "contrad_ratio":None, "unsupported_ratio":None, "n_sents":0}

        # 지시 따르기
        cons = q.get("constraints", {})
        style_ok = check_style(p["answer"], cons.get("style","bullet"))
        lang_ok  = check_language(p["answer"], cons.get("lang","ko"))
        cites_ok = check_citations(p["answer"], set(docmap.keys()) if cons.get("cite") else None)

        rows.append({
            "qid": qid, "em": em, "f1": f1, "semsim": sim,
            "recall@5": rec5, "mrr": my_mrr,
            "cit_prec": cit_prec, "cit_rec": cit_rec,
            **{f"nli_{k}":v for k,v in nli_stats.items()},
            "style_ok": style_ok, "lang_ok": lang_ok,
            "has_cite": cites_ok["has_cite"], "cites_ok": cites_ok["cites_ok"],
            "latency_ms": p.get("meta",{}).get("latency_ms", None),
            "tokens_ctx": p.get("meta",{}).get("tokens_ctx", None),
            "tokens_out": p.get("meta",{}).get("tokens_out", None),
        })
    df = pd.DataFrame(rows)

    # 종합 점수(가중합) — 가중치는 태스크 특성에 맞게 조정
    def safe_mean(x): 
        x = [v for v in x if v is not None]
        return float(np.mean(x)) if x else None

    report = {
        "n": len(df),
        "accuracy": {"EM": safe_mean(df["em"]), "F1": safe_mean(df["f1"]), "SemSim": safe_mean(df["semsim"])},
        "retrieval": {"R@5": safe_mean(df["recall@5"]), "MRR": safe_mean(df["mrr"])},
        "grounded": {
            "CitPrec": safe_mean(df["cit_prec"]), "CitRec": safe_mean(df["cit_rec"]),
            "Entailed": safe_mean(df["nli_entailed_ratio"]), "Unsupported": safe_mean(df["nli_unsupported_ratio"]),
            "Contrad": safe_mean(df["nli_contrad_ratio"])
        },
        "instruction": {
            "style_ok": float(df["style_ok"].mean()) if len(df) else None,
            "lang_ok": float(df["lang_ok"].mean()) if len(df) else None,
            "has_cite": float(df["has_cite"].mean()) if len(df) else None,
            "cites_ok": float(df["cites_ok"].mean()) if len(df) else None
        }
    }

    # 가중합 예시: 정확성 0.5, 근거성 0.3, 지시 0.2
    def norm(x, lo=0, hi=1): 
        return 0.0 if x is None else max(lo, min(hi, (x - lo)/(hi - lo + 1e-9)))
    acc = np.nanmean([v for v in [report["accuracy"]["F1"], report["accuracy"]["SemSim"]] if v is not None])
    grd = np.nanmean([v for v in [report["grounded"]["CitPrec"], report["grounded"]["Entailed"]] if v is not None])
    ins = np.nanmean([v for v in [report["instruction"]["style_ok"], report["instruction"]["cites_ok"]] if v is not None])
    report["score_overall"] = 0.5*norm(acc) + 0.3*norm(grd) + 0.2*norm(ins)

    return df, report
```

---

## 6) **컨텍스트 길이/패킹 효율** 모니터링

- **Context Utilization**:  
  - `tokens_ctx_used / tokens_ctx_budget`  
  - **중복률**: 유사 문장 해시 중복 비율
- **Overflow Miss**: 정답 근거가 있었지만 **패킹에서 탈락**한 비율

```python
def context_efficiency(row):
    # row: predictions record로부터 집계했다고 가정
    used = row.get("tokens_ctx") or 0
    # budget은 시스템 설정에서 고정값으로 기록하는 것을 권장(예: 4k중 2.5k를 컨텍스트)
    budget = row.get("tokens_ctx_budget", 2048)
    return used/max(budget,1)

# 중복률 계산(간단 해시)
import hashlib
def dedup_rate(chunks: List[str]):
    sigs = [hashlib.sha1(c.encode("utf-8")).hexdigest() for c in chunks]
    return 1.0 - len(set(sigs))/max(len(sigs),1)
```

---

## 7) **오류 원인 분류(진단)**

**실패 유형 태깅 규칙(휴리스틱)**

- **R-MISS**: Recall@k=0 → 리트리벌 미스  
- **PK-DROP**: 리트리브 OK지만 **패킹 누락**(인용 문서에 골드가 없음)  
- **NLI-UNSUP**: NLI unsupported/contradiction 비중↑  
- **INST-VIOL**: 스타일/언어/인용 규칙 위반  
- **HALLU-NO-CITE**: 인용 없이 단정 서술

```python
def tag_failure(row):
    tags=[]
    if row.get("recall@5")==0: tags.append("R-MISS")
    if row.get("cit_rec",0)==0 and row.get("recall@5",0)>0: tags.append("PK-DROP")
    if (row.get("nli_unsupported_ratio") or 0)>0.5 or (row.get("nli_contrad_ratio") or 0)>0.2:
        tags.append("NLI-UNSUP")
    if not row.get("style_ok", True) or not row.get("lang_ok", True) or not row.get("cites_ok", True):
        tags.append("INST-VIOL")
    if not row.get("has_cite", True) and (row.get("f1",0)<0.5):
        tags.append("HALLU-NO-CITE")
    return tags
```

---

## 8) **회귀(Regression) 비교** — 실험 A vs B

```python
def compare_runs(dfA, dfB, key="qid"):
    mA = dfA.set_index(key); mB = dfB.set_index(key)
    common = list(set(mA.index) & set(mB.index))
    rows=[]
    for q in common:
        a, b = mA.loc[q], mB.loc[q]
        rows.append({
            "qid": q,
            "F1Δ": (b["f1"] or 0) - (a["f1"] or 0),
            "R@5Δ": (b["recall@5"] or 0) - (a["recall@5"] or 0),
            "CitPrecΔ": (b["cit_prec"] or 0) - (a["cit_prec"] or 0),
            "EntailedΔ": (b["nli_entailed_ratio"] or 0) - (a["nli_entailed_ratio"] or 0),
            "StyleOKΔ": (int(b["style_ok"]) - int(a["style_ok"])) if (a.get("style_ok") is not None and b.get("style_ok") is not None) else 0
        })
    return pd.DataFrame(rows).sort_values("F1Δ", ascending=True)
```

---

## 9) **라벨링 지침(휴먼 평가) 요약**

- **정확성(0–2점)**: 2=정확/완전, 1=부분적으로 맞음, 0=틀림/비어있음  
- **근거성(0–2점)**: 2=모든 주장에 근거 인용, 1=일부만, 0=무근거/모순  
- **지시(0–1점)**: 포맷/언어/인용 준수(1) 또는 위반(0)  
- **최종(0–5점)**=가중합(2,2,1)  
- **이중 라벨** + **합의** + **모호 사례 회의록 기록** 권장

---

## 10) **실전 예시 시나리오**

### 10.1 미니 코퍼스
```json
// corpus.jsonl
{"doc_id":"d1","title":"FAISS","text":"FAISS는 페이스북 AI가 만든 벡터 검색 라이브러리로 HNSW, IVF 등 인덱스를 지원한다."}
{"doc_id":"d2","title":"HNSW","text":"HNSW는 근사 최근접 탐색을 위한 소형월드 그래프 구조 인덱스다."}
{"doc_id":"d3","title":"BM25","text":"BM25는 고전 토큰 기반 검색 점수로 k1, b 하이퍼파라미터를 사용한다."}
```

```json
// queries.jsonl
{"qid":"q1","query":"FAISS와 HNSW를 간단히 설명해줘",
 "gold_answers":["FAISS는 벡터 검색 라이브러리","HNSW는 근사 최근접 탐색 그래프 인덱스"],
 "gold_evidence":[["d1"],["d2"]],
 "constraints":{"style":"bullet","cite":true,"lang":"ko"}}
```

```json
// predictions.jsonl (모델 출력 가정)
{"qid":"q1",
 "retrieved":["d3","d1","d2"],
 "cited":["d1","d2"],
 "answer":"- FAISS는 벡터 검색 라이브러리입니다 [#d1]\n- HNSW는 근사 최근접 탐색용 그래프 인덱스입니다 [#d2]",
 "meta":{"latency_ms":180,"tokens_ctx":380,"tokens_out":52}}
```

### 10.2 실행 & 결과(요약)

```python
df, report = evaluate_all("corpus.jsonl","queries.jsonl","predictions.jsonl",
                          do_nli=True, semsim_model="intfloat/e5-small-v2")
print(df.head())
print(json.dumps(report, ensure_ascii=False, indent=2))

# 실패 태그
df["tags"] = df.apply(tag_failure, axis=1)
print(df[["qid","em","f1","recall@5","cit_prec","nli_entailed_ratio","style_ok","cites_ok","tags"]])
```

**예상 결과**
- 정확성: **EM~0, F1~1.0**(표현 다르면 EM=0일 수 있음), **SemSim 높은 값**  
- 리트리벌: **R@5=1.0**, **MRR>0**(d1/d2가 상위)  
- 근거성: **CitPrec=1.0**, **NLI entailed ratio 높음**  
- 지시: **불릿/인용/한글** 충족

---

## 11) **운영 체크리스트(현업)**

- [ ] **골드 세트 버전** 고정(쿼리/정답/근거) + 변경 이력  
- [ ] **리트리벌 로그**(쿼리, 필터, 인덱스 버전, Top-k) 저장  
- [ ] **모델/프롬프트 버전**, **토큰 사용량/지연시간** 로깅  
- [ ] **야간 회귀 배치**: EM/F1, R@k, CitPrec, Entailed ratio, 지시 위반율  
- [ ] 상위 실패 케이스 **자동 샘플링** → **휴먼 재라벨/룰 수정/인덱스 개선**

---

## 12) **자주 묻는 질문(FAQ)**

**Q. 골드 근거가 없으면 근거성은 어떻게 보나요?**  
A. **NLI 평가**(답문장 vs 컨텍스트)로 **Entailed/Unsupported** 비율을 추정하세요. 가능하면 **사후 샘플링**으로 휴먼 검증을 섞으세요.

**Q. EM/F1이 낮은데 SemSim은 높습니다.**  
A. **표현 다양성**이 큰 생성형 QA에서 흔합니다. **키포인트 커버리지**/**NLI**를 병행하면 왜곡 없이 품질을 읽을 수 있습니다.

**Q. 지시 위반이 잦습니다.**  
A. 시스템 프롬프트에 **형식 강제 문구**를 강화하고, **출력 검증기**(JSON schema/정규식)로 **사후 필터**를 적용하세요.

**Q. 리트리벌은 좋은데 근거성이 낮아요.**  
A. **패킹 전략**(중복 제거/하이라이트/요약)을 재점검하고, **인용 인식 규칙**을 명확히(예: `[#[A-Z0-9]+]`) 하세요.

---

## 13) **수식 요약**

- **Token-level F1**  
$$
\mathrm{F1}=\frac{2\cdot \mathrm{Prec}\cdot \mathrm{Rec}}{\mathrm{Prec}+\mathrm{Rec}}
$$

- **MRR**  
$$
\mathrm{MRR}=\frac{1}{|Q|}\sum_{q\in Q}\frac{1}{\mathrm{rank}_q}
$$

- **Citation Precision/Recall**  
$$
\text{CitPrec}=\frac{|C\cap G|}{|C|},\quad 
\text{CitRec}=\frac{|C\cap G|}{|G|}
$$

- **nDCG@k**  
$$
\mathrm{nDCG@k}=\frac{\sum_{i=1}^{k}\frac{2^{rel_i}-1}{\log_2(i+1)}}
{\sum_{i=1}^{k}\frac{2^{rel_i^\ast}-1}{\log_2(i+1)}}
$$

---

# 마무리

- **정확성(EM/F1/Se mSim)**·**근거성(인용·NLI)**·**지시 준수(형식/언어/인용 룰)** 3축을 **동시에** 본다면 RAG 품질을 안정적으로 관리할 수 있습니다.  
- 본문 **엔드-투-엔드 코드**로 **자동 리포트/회귀 비교/실패 태깅**까지 바로 돌릴 수 있습니다.