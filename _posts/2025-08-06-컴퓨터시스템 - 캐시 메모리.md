---
layout: post
title: 컴퓨터시스템 - 캐시 메모리
date: 2025-08-06 20:20:23 +0900
category: 컴퓨터시스템
---
# 캐시 메모리(Cache Memory) — 구조, 동작, 성능 최적화

> 캐시 메모리는 **CPU와 메인 메모리(DRAM)** 사이에서 **속도 격차**를 줄이는 핵심 장치다.  
> 데이터 접근 패턴에서 **지역성(locality)** 을 이용해 **자주 쓰는 데이터**를 CPU 가까이에 두어 성능을 극대화한다.

---

## 1) 캐시 메모리의 필요성

- **CPU 속도 vs 메모리 속도 불균형**
  - CPU 클럭: ns 단위
  - DRAM 지연: 수백 ns  
  → 직접 접근 시 **대기 시간(memory stall)** 이 커져 IPC(Instruction Per Cycle)가 떨어짐.

- **해결책: 다단계 캐시**
  - L1: 가장 빠르고 작음, CPU 코어별 존재
  - L2: L1보다 크고 약간 느림
  - L3(LLC): 여러 코어가 공유, 용량 큼, 지연 증가

---

## 2) 캐시 구조

### 2.1 계층(L1/L2/L3)
| 계층 | 용량 | 지연 | 위치 |
|------|------|------|------|
| L1I/L1D | 32KB~64KB | 1~4 cycles | CPU 코어 내부 |
| L2 | 256KB~1MB | 4~12 cycles | CPU 코어 근처 |
| L3 | 4MB~64MB | 20~50 cycles | 소켓 내 공유 |

### 2.2 캐시 라인(Cache Line)
- 보통 64 Byte 단위로 데이터 블록 저장
- **공간 지역성**을 활용: 한 주소 접근 시 인접 데이터도 함께 로드

### 2.3 매핑(Mapping)
- **직접 매핑**: 한 메모리 블록이 정확히 하나의 캐시 슬롯에만 매핑 → 빠르지만 충돌↑
- **집합 연관(Set-associative)**: 여러 슬롯 중 하나 선택 → 충돌↓
- **완전 연관**: 어떤 슬롯에도 저장 가능 → 비용↑

### 2.4 교체 정책
- **LRU(Least Recently Used)**: 가장 오래 안 쓴 라인 제거
- **Random**: 무작위 제거
- **Pseudo-LRU**: 하드웨어 구현 단순화

---

## 3) 캐시 동작 원리

1. **CPU가 메모리 주소를 요청**
2. **태그(Tag) 비교**  
   - 캐시 세트에서 해당 주소의 태그와 비교
3. **히트(hit)** 시 → 즉시 데이터 전달
4. **미스(miss)** 시  
   - 하위 계층(L2, L3, DRAM)에서 블록 가져오기
   - 교체 정책에 따라 기존 라인 제거

---

## 4) 캐시 미스 종류

- **Compulsory Miss**: 처음 접근하는 데이터 → 반드시 발생
- **Capacity Miss**: 캐시 용량 부족으로 교체 필요
- **Conflict Miss**: 매핑 충돌로 인한 교체
- **Coherence Miss**: 멀티코어 환경에서 다른 코어 쓰기 변경으로 무효화

---

## 5) 성능 모델: AMAT

평균 메모리 접근 시간(AMAT)
\[
\text{AMAT} = T_{L1} + m_1\Big(T_{L2} + m_2\big(T_{L3} + m_3 T_\text{Mem}\big)\Big)
\]
- \(m_k\): 미스율, \(T\): 접근 시간
- 목표: 각 미스율(m)을 최소화 → 전체 지연 감소

---

## 6) 성능 최적화 방법

### 6.1 지역성 활용
- **시간 지역성**: 같은 데이터 반복 접근  
  → 루프 내부 변수를 캐시 유지
- **공간 지역성**: 인접 데이터 접근  
  → 행 우선 순회, 스트라이드 1 접근

### 6.2 캐시 친화적 코드
```c
// 행 우선 접근 (좋음)
for (int i=0;i<N;i++)
  for (int j=0;j<N;j++)
    sum += A[i][j];

// 열 우선 접근 (미스↑)
for (int j=0;j<N;j++)
  for (int i=0;i<N;i++)
    sum += A[i][j];
```

### 6.3 타일링(Blocking)
```c
for (int ii=0; ii<N; ii+=B)
  for (int jj=0; jj<N; jj+=B)
    for (int i=ii; i<ii+B; i++)
      for (int j=jj; j<jj+B; j++)
        C[i][j] += A[i][j] * k;
```
- 작은 블록 단위로 연산 → 캐시에 머무르는 데이터 재사용 극대화

### 6.4 멀티코어 환경 최적화
- 폴스 셰어링 방지(변수 패딩)
- NUMA 환경에서 **first-touch** 메모리 배치

---

## 7) 측정과 분석

- **perf (리눅스)**
```bash
perf stat -e cycles,instructions,L1-dcache-load-misses,LLC-load-misses ./app
```
- **Intel VTune**: 캐시 히트율, 대역폭 분석
- **valgrind --tool=cachegrind**: 코드별 캐시 미스 분석

---

## 8) 결론

캐시 메모리는 **속도-용량-비용**의 절충안이며, 성능은 **지역성을 살리는 접근 패턴**에 달려 있다.  
- 알고리즘 설계 시 **데이터 구조 배치**
- 루프 최적화, 타일링, 프리패칭 적용
- 멀티코어 환경에서는 **코히어런시와 폴스 셰어링**까지 고려

실무에서의 최적화는 **측정→분석→코드 변경→재측정**을 반복하며, 워크로드에 맞는 캐시 활용 전략을 찾는 것이 핵심이다.