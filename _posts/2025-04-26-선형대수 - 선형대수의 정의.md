---
layout: post
title: 선형대수 - 선형대수의 정의
date: 2025-04-26 19:20:23 +0900
category: 선형대수
---
# 선형대수란 무엇인가 — 정의, 핵심 개념, 실제 활용, 코드 예제(전면 개정)

## 표기 규약(Notation)

- 스칼라: $$a, b, \alpha, \lambda \in \mathbb{R}$$
- 벡터: 굵은 소문자(예: $$\mathbf{x}, \mathbf{y}, \mathbf{v} \in \mathbb{R}^n$$). 필요 시 화살표 표기 $$\vec{v}$$ 를 병기합니다.
- 행렬: 굵은 대문자(예: $$\mathbf{A}, \mathbf{B}$$)
- 전치/켤레전치: $$\mathbf{A}^\top,\ \mathbf{A}^*$$
- 노름/내적: $$\lVert\cdot\rVert,\ \langle \cdot,\cdot \rangle$$
- 단위행렬/영벡터: $$\mathbf{I},\ \mathbf{0}$$

---

## 선형대수의 정의

**선형대수(Linear Algebra)** 는 벡터와 행렬, 그리고 그 사이의 **선형 변환**을 다루는 수학 분야입니다. 핵심 관심사는 다음과 같습니다.

- 벡터와 벡터공간(부분공간, 기저, 차원)
- 행렬과 선형변환(합성, 역행렬, 결정식, 랭크)
- 선형 연립방정식 $$\mathbf{A}\mathbf{x}=\mathbf{b}$$ 의 해 존재/유일성/계산
- 고유값/고유벡터, SVD, 최소제곱, 투영 등

예시의 연립방정식:
$$
\begin{cases}
2x + y = 5 \\
4x - 3y = 6
\end{cases}
$$
행렬로 쓰면
$$
\begin{bmatrix}
2 & 1 \\
4 & -3
\end{bmatrix}
\begin{bmatrix}
x \\ y
\end{bmatrix}
=
\begin{bmatrix}
5 \\ 6
\end{bmatrix}.
$$

---

## 벡터, 행렬, 선형시스템의 빠른 스냅샷

- 벡터 $$\mathbf{x} \in \mathbb{R}^n$$: 좌표의 열 리스트로 표현
- 행렬 $$\mathbf{A}\in\mathbb{R}^{m\times n}$$: 선형변환의 좌표 표현(입력 차원 $$n$$ → 출력 차원 $$m$$)
- 선형시스템 $$\mathbf{A}\mathbf{x}=\mathbf{b}$$: 열벡터들의 선형결합 $$x_1\mathbf{a}_1+\cdots+x_n\mathbf{a}_n=\mathbf{b}$$ 로 해석

**기본 연산**
$$
\langle \mathbf{u},\mathbf{v}\rangle = \mathbf{u}^\top \mathbf{v},\quad
\lVert \mathbf{v}\rVert = \sqrt{\mathbf{v}^\top \mathbf{v}},\quad
\operatorname{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{v}^\top\mathbf{u}}{\mathbf{u}^\top\mathbf{u}}\mathbf{u}.
$$

---

## 선형변환과 기하: 회전·스케일·쉬어·결정식

**선형성**: $$T(\alpha\mathbf{x}+\beta\mathbf{y})=\alpha T(\mathbf{x})+\beta T(\mathbf{y})$$ 을 만족하는 변환은 항상 어떤 행렬 $$\mathbf{A}$$ 가 존재해 $$T(\mathbf{x})=\mathbf{A}\mathbf{x}$$ 로 쓸 수 있습니다.

- 2D 회전:
$$
\mathbf{R}(\theta)=
\begin{bmatrix}
\cos\theta & -\sin\theta\\
\sin\theta & \cos\theta
\end{bmatrix}
,\quad \mathbf{y}=\mathbf{R}(\theta)\mathbf{x}.
$$
- 스케일: $$\mathbf{S}=\operatorname{diag}(s_x,s_y)$$
- 쉬어: $$\begin{bmatrix}1 & k \\ 0 & 1\end{bmatrix}$$
- 합성: $$\mathbf{A}\mathbf{B}$$ 는 $$\mathbf{x}\mapsto \mathbf{B}\mathbf{x}\mapsto \mathbf{A}(\mathbf{B}\mathbf{x})$$

**결정식** $$\det(\mathbf{A})$$ 는 부피(면적) 스케일링과 방향 보존/반전 정보를 줍니다.
- $$|\det(\mathbf{A})|$$: 부피 스케일
- $$\det(\mathbf{A})=0$$: 차원 붕괴(정보 소실, 역행렬 없음)

---

## 벡터공간, 부분공간, 기저, 차원, 랭크

- 부분공간: 덧셈/스칼라곱에 닫힌 집합
- 기저: 부분공간을 생성하며 서로 선형독립인 벡터 집합
- 차원: 기저 벡터 수
- 열공간 $$\mathcal{C}(\mathbf{A})$$, 영공간 $$\mathcal{N}(\mathbf{A})$$
- 랭크–널리티: $$\operatorname{rank}(\mathbf{A})+\operatorname{nullity}(\mathbf{A})=n$$

---

## 선형시스템 해법과 최소제곱

- 해 존재성: $$\mathbf{b}\in \mathcal{C}(\mathbf{A})$$ 여부
- 유일성: $$\mathcal{N}(\mathbf{A})=\{\mathbf{0}\}$$ 이면 유일해
- 과결정(방정식이 변수보다 많음)에서 최소제곱:
$$
\min_{\mathbf{x}}\ \lVert \mathbf{A}\mathbf{x}-\mathbf{b}\rVert_2,
\quad
\mathbf{A}^\top\mathbf{A}\mathbf{x}=\mathbf{A}^\top\mathbf{b}.
$$
수치적으로는 QR/SVD 권장.

---

## 고유값·고유벡터, 대각화, SVD, PCA

- 고유쌍: $$\mathbf{A}\mathbf{v}=\lambda \mathbf{v}$$
- 대칭행렬: 실수 고유값, 직교 고유벡터 기저 가능(대각화)
- SVD: $$\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top$$ (모든 직사각 행렬에 성립)
- PCA: 데이터 공분산의 고유분해 또는 SVD로 주성분 추출

---

## 실생활/산업 응용과 코드

아래 코드는 모두 PyTorch 로 작성합니다.

### 선형시스템 풀이(수치해석의 기본)

```python
import torch
torch.set_printoptions(precision=4, sci_mode=False)

A = torch.tensor([[2.0, 1.0],
                  [4.0,-3.0]])
b = torch.tensor([5.0, 6.0])

# 권장: 역행렬 대신 solve

x = torch.linalg.solve(A, b)
print("해 x:", x)  # 선형시스템의 정확해

# 검증

print("검증 A x:", A @ x)
```

설명: `torch.linalg.solve` 는 수치적으로 안정적입니다. 역행렬을 직접 구해 곱하는 방식은 피합니다.

---

### 선형회귀(머신러닝의 기본 선형 모델)

모형: $$y = \mathbf{X}\mathbf{w} + b$$

```python
import torch

# 장난감 데이터

X = torch.tensor([[1., 2.],
                  [3., 4.],
                  [5., 6.]], dtype=torch.float32)  # N x d
y = torch.tensor([3.5, 7.5, 11.5], dtype=torch.float32)      # N

# 바이어스는 X에 1열을 추가해 가중치로 흡수

Xbar = torch.cat([torch.ones(X.size(0),1), X], dim=1)  # [1, x1, x2]
# 최소제곱 해

w_star = torch.linalg.lstsq(Xbar, y).solution          # shape: (d+1,)
print("최소제곱 해(바이어스 포함):", w_star)

# 예측

y_hat = Xbar @ w_star
print("예측:", y_hat)
print("잔차노름:", torch.linalg.norm(y_hat - y).item())
```

설명: `lstsq` 는 내부적으로 QR/SVD 를 사용해 안정적으로 최소제곱 해를 구합니다.

---

### 공분산, 고유분해, PCA

$$
\mathbf{X}\in\mathbb{R}^{N\times d}
$$ 에서 각 특성(열)을 평균 0 으로 정규화한 후
$$
\mathbf{C}=\frac{1}{N-1}\mathbf{X}^\top\mathbf{X}
$$ 의 고유분해/`eigh` 로 주성분을 구합니다.

```python
import torch

# 데이터 생성(2D, 상관된 분포)

torch.manual_seed(0)
N = 200
Z = torch.randn(N, 2)
A = torch.tensor([[2.0, 0.8],
                  [0.8, 1.0]])
X = Z @ torch.linalg.cholesky(A).T  # 공분산 ~ A

# 열 평균을 0으로

Xc = X - X.mean(dim=0, keepdim=True)

# 공분산 행렬

C = (Xc.T @ Xc) / (N - 1)

# 대칭이므로 eigh 사용(안정)

eigvals, eigvecs = torch.linalg.eigh(C)  # 오름차순
# 내림차순 정렬

idx = torch.argsort(eigvals, descending=True)
eigvals = eigvals[idx]; eigvecs = eigvecs[:, idx]

print("고유값:", eigvals)
print("주성분(열벡터):\n", eigvecs)

# 주성분 좌표로 사영(2->1 차원 축소 예)

PC1 = eigvecs[:, 0:1]                  # 1st principal direction
scores = Xc @ PC1                      # N x 1
recon  = scores @ PC1.T + X.mean(0)    # 근사 복원

print("PC1 분산 비율:", float(eigvals[0] / eigvals.sum()))
print("복원 오차:", float(torch.mean((recon - X)**2)))
```

설명: `eigh` 는 대칭(실대칭) 행렬의 고유분해에 특화되어 안정적입니다. PCA 는 분산을 최대화하는 축(주성분)을 찾는 과정으로, 차원 축소와 노이즈 제거에 쓰입니다.

---

### 컴퓨터 그래픽스: 변환의 합성

2D 점들을 회전 후 스케일합니다. 합성 순서에 따라 결과가 달라집니다(비가환).

```python
import torch

# 2D 점 집합(사각형의 꼭짓점)

P = torch.tensor([[0.,0.],
                  [1.,0.],
                  [1.,1.],
                  [0.,1.]])

theta = torch.tensor(0.6)  # radians
R = torch.stack([torch.stack([ torch.cos(theta), -torch.sin(theta)]),
                 torch.stack([ torch.sin(theta),  torch.cos(theta)])])

S = torch.diag(torch.tensor([2.0, 0.5]))

# 순서 1: 회전 후 스케일

P1 = (P @ R.T) @ S.T

# 순서 2: 스케일 후 회전

P2 = (P @ S.T) @ R.T

print("회전→스케일 vs 스케일→회전 차이(첫 점):", P1[1]-P2[1])
```

설명: 일반적으로 $$\mathbf{S}\mathbf{R}\neq \mathbf{R}\mathbf{S}$$ 입니다.

---

### 로봇공학/제어: 좌표계 변환(합성)

동차좌표(3×3)로 2D의 회전·이동을 한 번에 표현합니다.

$$
\mathbf{T}=
\begin{bmatrix}
\mathbf{R} & \mathbf{t}\\
\mathbf{0}^\top & 1
\end{bmatrix},
\quad
\mathbf{p}_{\text{hom}}=
\begin{bmatrix}\mathbf{p}\\1\end{bmatrix}.
$$

```python
import torch

def T2D(theta, tx, ty):
    c, s = torch.cos(theta), torch.sin(theta)
    T = torch.tensor([[c, -s, tx],
                      [s,  c, ty],
                      [0., 0., 1. ]])
    return T

p = torch.tensor([2., 1., 1.])  # hom coord

T_a = T2D(theta=torch.tensor(0.5), tx=1.0, ty=0.0)
T_b = T2D(theta=torch.tensor(-0.3), tx=0.0, ty=2.0)

# 좌표계 A에서 B로: 합성 변환

T_ba = T_b @ T_a
p_in_b = T_ba @ p
print("변환된 점:", p_in_b)
```

---

### 물리/공학: 회로/구조 해석의 선형시스템

간단한 노드 전압 해석(키르히호프 법칙)은 보통 $$\mathbf{A}\mathbf{x}=\mathbf{b}$$ 형태로 귀결됩니다.

```python
import torch

# 예: 2 노드 단순 회로의 등가 연립방정식 (임의 예시)

A = torch.tensor([[10., -2.],
                  [-2.,  5.]])
b = torch.tensor([3., 1.])

v = torch.linalg.solve(A, b)
print("노드 전압:", v)
```

---

### NLP: 임베딩과 코사인 유사도

단어/문장 벡터 간 유사도:
$$
\cos(\theta) = \frac{\mathbf{a}^\top\mathbf{b}}{\lVert\mathbf{a}\rVert \lVert\mathbf{b}\rVert}.
$$

```python
import torch

E = torch.tensor([[ 0.2,  0.1,  0.7],
                  [ 0.3,  0.0,  0.6],
                  [-0.1,  0.4,  0.1]], dtype=torch.float32)  # 3 단어, 3 차원

def cos_sim(a, b, eps=1e-9):
    num = (a @ b)
    den = (torch.linalg.norm(a) * torch.linalg.norm(b) + eps)
    return num / den

print("단어0-1 코사인:", float(cos_sim(E[0], E[1])))
print("단어0-2 코사인:", float(cos_sim(E[0], E[2])))
```

---

## 수치계산 팁과 함정

1. 역행렬 남용 금지: $$\mathbf{x}=\mathbf{A}^{-1}\mathbf{b}$$ 대신 `solve`/`lstsq` 사용
2. 정규화: 스케일이 크게 다른 특성은 정규화/표준화 후 계산
3. 조건수: ill-conditioned 문제는 작은 오차가 크게 증폭됩니다(정규화, 정규화항 사용)
4. 직교기저: Gram–Schmidt 대신 수정형/Householder/QR 권장
5. 각도 계산: 수치오차로 $$\cos\theta$$ 가 범위를 살짝 벗어날 수 있어 `clamp` 처리

---

## 추가 예제: 최소제곱 직관 시뮬레이션

점들 $$(1,1),(2,2),(3,2.5)$$ 에 대해 직선 $$y\approx \beta_0+\beta_1 x$$.

```python
import torch

X = torch.tensor([[1., 1.],
                  [1., 2.],
                  [1., 3.]])     # [1, x]
y = torch.tensor([1., 2., 2.5])

beta = torch.linalg.lstsq(X, y).solution
yhat = X @ beta
res  = y - yhat

print("beta:", beta)                  # 절편, 기울기
print("잔차 제곱합:", float((res**2).sum()))
```

---

## 핵심 정리와 연결고리

- 선형변환의 합성 ↔ 행렬곱
- 부피 스케일 ↔ 결정식, 가역성
- 출력 범위/정보 손실 ↔ 열공간/영공간, 랭크
- 최적 예측(직교 투영) ↔ 최소제곱
- 방향 보존 변환 ↔ 직교행렬(회전), 노름/내적 보존
- 데이터 축 방향/분산 ↔ 고유분해, SVD, PCA

---

## 요약 표

| 주제 | 한줄 요약 |
|---|---|
| 벡터/행렬 | 선형변환의 좌표 표현 |
| 선형시스템 | $$\mathbf{A}\mathbf{x}=\mathbf{b}$$ 의 해 존재/유일성/계산 |
| 변환 기하 | 회전/스케일/쉬어, 합성, 결정식(부피) |
| 벡터공간 | 부분공간, 기저, 차원, 랭크 |
| 스펙트럼 | 고유값/벡터, 대각화, SVD |
| 데이터 | 공분산, PCA(차원 축소/노이즈 제거) |
| 실무 팁 | `solve/lstsq`, 정규화, 조건수, 직교화 |

---

## 연습문제(해설 힌트)

1) 행렬
$$
\mathbf{A}=
\begin{bmatrix}
2 & 1\\
4 & -3
\end{bmatrix},\quad
\mathbf{b}=
\begin{bmatrix}
5\\6
\end{bmatrix}
$$
에 대하여 `solve` 로 해 $$\mathbf{x}$$ 를 구하고, 직접 곱으로 검증하시오.
힌트: `torch.linalg.solve(A, b)` 사용.

2) 2D 회전행렬 $$\mathbf{R}(\theta)$$ 가 직교행렬(즉 $$\mathbf{R}^\top\mathbf{R}=\mathbf{I}$$)임을 보이고, 임의의 벡터에 대해 노름을 보존함을 증명하시오.
힌트: 내적 보존.

3) 데이터 행렬 $$\mathbf{X}\in\mathbb{R}^{N\times d}$$ 에서 열 평균을 제거한 후 공분산의 고유분해로 PCA 를 구하시오. 첫 주성분으로 사영했을 때 분산 비율을 계산하시오.
힌트: `eigh` 사용, 분산 비율은 $$\lambda_1/\sum_i \lambda_i$$.

4) 다음 변환 $$\mathbf{S}=\operatorname{diag}(2,0.5), \ \mathbf{R}(\theta)$$ 에 대해 점 집합 $$\{\,(0,0),(1,0),(1,1),(0,1)\,\}$$ 를 변환했을 때, 순서를 바꾸면 결과가 어떻게 달라지는지 수치로 확인하시오.
힌트: 비가환성.

5) 최소제곱 문제 $$\min_{\mathbf{x}}\lVert\mathbf{A}\mathbf{x}-\mathbf{b}\rVert_2$$ 의 정규방정식을 유도하고, `lstsq` 와 결과를 비교하시오.
힌트: 미분=0 조건으로 유도.

---

## 마무리

선형대수는 **현대 기술의 언어**입니다. 벡터/행렬의 기하·대수 구조가 **그래픽스, 로봇, 통신, 데이터과학, 딥러닝, 생명정보학**을 관통합니다.
본 글의 코드/공식을 토대로, 이어지는 장(투영/QR/최소제곱, SVD/PCA, 스펙트럼 해석, 수치 안정성 심화)으로 확장하시기 바랍니다.
