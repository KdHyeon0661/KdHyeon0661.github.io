---
layout: post
title: 미적분 - 편도함수의 연쇄법칙
date: 2025-08-07 20:20:23 +0900
category: 미적분
---
# 방향도함수와 그래디언트

## 정의와 첫 직관

### 방향도함수의 정의 (게이트 미분)

스칼라장 \(f:\mathbb{R}^n\to\mathbb{R}\), 점 \(\mathbf{x}\in\mathbb{R}^n\), 방향 \(\mathbf{v}\in\mathbb{R}^n\)에 대하여 **방향도함수** \(D_{\mathbf{v}}f(\mathbf{x})\)를
$$
D_{\mathbf{v}} f(\mathbf{x})
= \lim_{t\to 0}\frac{f(\mathbf{x}+t\mathbf{v})-f(\mathbf{x})}{t}
$$
로 정의한다(극한이 존재할 때).
단위벡터 \(\hat{\mathbf{v}}=\mathbf{v}/\|\mathbf{v}\|\)를 쓰면
$$
D_{\mathbf{v}} f(\mathbf{x}) = \|\mathbf{v}\|\cdot D_{\hat{\mathbf{v}}} f(\mathbf{x}).
$$

### 그래디언트(그라디언트)의 정의

\(f\)가 \(\mathbf{x}\)에서 미분가능(프레셰 미분가능)할 때, **유일한 벡터** \(\nabla f(\mathbf{x})\in\mathbb{R}^n\)가 존재하여
$$
f(\mathbf{x}+\mathbf{h})=f(\mathbf{x})+\nabla f(\mathbf{x})\cdot \mathbf{h} + o(\|\mathbf{h}\|)\quad(\mathbf{h}\to \mathbf{0})
$$
이 성립한다. 이 \(\nabla f(\mathbf{x})\)를 **그래디언트**라 한다.

### 핵심 연결식

프레셰 미분가능이면 모든 방향 \(\mathbf{v}\)에 대해
$$
D_{\mathbf{v}} f(\mathbf{x})=\nabla f(\mathbf{x})\cdot \mathbf{v}.
$$
즉 **방향도함수는 그래디언트의 내적**으로 주어진다. 이는 “총미분/전미분” 항에서 이미 본 연쇄법칙의 벡터화 버전이다.

---

## 기하학적 의미: 최대 증가 방향·레벨셋의 법선

### 최대 증가 방향 정리

단위벡터 \(\hat{\mathbf{v}}\)에 대한 증가율은
$$
D_{\hat{\mathbf{v}}} f(\mathbf{x})=\nabla f(\mathbf{x})\cdot \hat{\mathbf{v}}
=\|\nabla f(\mathbf{x})\|\ \|\hat{\mathbf{v}}\|\ \cos\theta
=\|\nabla f(\mathbf{x})\|\cos\theta,
$$
여기서 \(\theta\)는 \(\nabla f(\mathbf{x})\)와 \(\hat{\mathbf{v}}\) 사이 각. 따라서
- **최대 증가율**은 \(\|\nabla f(\mathbf{x})\|\)이며,
- 그 방향은 \(\hat{\mathbf{v}}=\frac{\nabla f(\mathbf{x})}{\|\nabla f(\mathbf{x})\|}\) (그래디언트 방향),
- **최대 감소율**은 \(-\|\nabla f(\mathbf{x})\|\)이며 방향은 \(-\nabla f\).

이는 코시-슈바르츠 부등식으로 즉시 나온다.

### 레벨셋의 법선

레벨셋 \(S_c=\{\mathbf{x}\mid f(\mathbf{x})=c\}\)에서 \(\nabla f(\mathbf{x})\neq\mathbf{0}\)이면 \(\nabla f(\mathbf{x})\)는 \(S_c\)의 **법선(정규) 벡터**다.
이유: \(S_c\) tangent 방향 \(\mathbf{w}\)에 대해 첫차 근사 \(f(\mathbf{x}+t\mathbf{w}) \approx c + t\,\nabla f\cdot\mathbf{w}\). 레벨셋 위에 머무르려면 \(\nabla f\cdot\mathbf{w}=0\), 즉 접공간은 \(\nabla f\)에 수직.

---

## 편도·전미분·연쇄법칙과의 연결

### 편도 → 그래디언트

표준 좌표 \((x_1,\dots,x_n)\)에서
$$
\nabla f(\mathbf{x})=\begin{bmatrix}
\frac{\partial f}{\partial x_1}(\mathbf{x})\\
\vdots\\
\frac{\partial f}{\partial x_n}(\mathbf{x})
\end{bmatrix}.
$$
따라서
$$
D_{\mathbf{v}} f(\mathbf{x})=\sum_{i=1}^n \frac{\partial f}{\partial x_i}(\mathbf{x})\, v_i
=\nabla f(\mathbf{x})\cdot \mathbf{v}.
$$

### 연쇄법칙(야코비안 관점)

합성 \(f(\mathbf{x})=g(\mathbf{u}(\mathbf{x}))\), \(\mathbf{u}:\mathbb{R}^n\to\mathbb{R}^m\) 미분가능이면
$$
\nabla_{\mathbf{x}} f = J_{\mathbf{u}}(\mathbf{x})^\top\, \nabla_{\mathbf{u}} g,\qquad
D_{\mathbf{v}} f = \nabla_{\mathbf{u}} g^\top\,(J_{\mathbf{u}} \mathbf{v}).
$$
즉 **먼저 입력 방향 \(\mathbf{v}\)를 중간공간 방향으로 보낸 뒤** 그라디언트와 내적.

---

## 예제: 손으로 계산해 보는 방향도함수

### 다항-삼각 혼합

$$
f(x,y)=x^2\sin y + y^3.
$$
그래디언트
$$
\nabla f=\begin{bmatrix}2x\sin y\\ x^2\cos y + 3y^2\end{bmatrix}.
$$
방향 \(\mathbf{v}=(a,b)\)에서
$$
D_{\mathbf{v}} f=\nabla f\cdot\mathbf{v}
= 2ax\sin y + b(x^2\cos y + 3y^2).
$$

### 지수-로그 합성

$$
f(x,y)=\exp(x\sin y)\,\ln(1+x^2+y^2).
$$
곱규칙과 연쇄법칙으로
$$
\nabla f
= \exp(x\sin y)\begin{bmatrix}\sin y\\ x\cos y\end{bmatrix}\ln(1+r^2)
+ \exp(x\sin y)\frac{1}{1+r^2}\begin{bmatrix}2x\\ 2y\end{bmatrix},
$$
여기서 \(r^2=x^2+y^2\). 주어진 \(\mathbf{v}\)에 대해 \(D_{\mathbf{v}}f=\nabla f\cdot \mathbf{v}\).

### 극좌표에서의 방향도함수

극변환 \(x=r\cos\theta,\ y=r\sin\theta\).
\(f(x,y)=\phi(r,\theta)\)일 때
$$
D_{\mathbf{e}_r} f=\frac{\partial \phi}{\partial r},\qquad
D_{\mathbf{e}_\theta} f=\frac{1}{r}\frac{\partial \phi}{\partial \theta}.
$$
이는 \(\nabla f = f_r\,\mathbf{e}_r + \frac{1}{r}f_\theta\,\mathbf{e}_\theta\)와 정합.

---

## 미분가능성과 방향도함수: 존재만으로 충분한가?

**모든 방향도함수의 존재**가 **프레셰 미분가능**을 보장하지는 않는다.

대표 예:
$$
f(x,y)=\begin{cases}
\dfrac{x^2\,y}{x^2+y^2}, & (x,y)\ne (0,0),\\
0, & (x,y)=(0,0).
\end{cases}
$$
원점에서 모든 방향 \(\mathbf{v}=(a,b)\)에 대해
$$
D_{\mathbf{v}} f(\mathbf{0})
=\lim_{t\to 0}\frac{f(ta,tb)-0}{t}
= \frac{a^2 b}{a^2+b^2}
$$
이 **존재**하나, \(\mathbf{v}\mapsto \frac{a^2 b}{a^2+b^2}\)는 **선형이 아니므로** 프레셰 미분(선형 근사)의 형태가 아니다. 따라서 \(f\)는 원점에서 미분가능하지 않다.

요지:
- 프레셰 미분가능 ⇔ 방향도함수들이 **선형 형태** \(D_{\mathbf{v}}f=\nabla f\cdot \mathbf{v}\)로 주어짐.
- 단순히 “모든 방향에서 극한이 존재”는 충분조건이 아님.

---

## 보조 정리들

### 리프시츠 경계(국소)

만약 \(\nabla f\)가 점근적으로 유계이면 근방에서
$$
|D_{\mathbf{v}} f(\mathbf{x})| \le \|\nabla f(\mathbf{x})\|\,\|\mathbf{v}\|.
$$

### 평균값형 추정(1차 근사)

\(\mathbf{y}=\mathbf{x}+\mathbf{h}\)일 때, 적당한 \(\xi\)가 있어
$$
f(\mathbf{y})-f(\mathbf{x}) = \nabla f(\mathbf{\xi})\cdot \mathbf{h}.
$$
(완전한 고전적 다변수 평균값정리 형태는 세부조건이 있으나, 1차 근사 해석에 널리 쓰이는 관찰.)

---

## 최적화에서의 의미: 스티프한 골짜기와 스텝 선택

### 스티프한 방향과 선형화

1차 테일러 근사:
$$
f(\mathbf{x}+\alpha\mathbf{d}) \approx f(\mathbf{x}) + \alpha\,\nabla f(\mathbf{x})\cdot \mathbf{d}.
$$
따라서 **최대 감소**를 원하면 \(\mathbf{d}=-\nabla f(\mathbf{x})\)를 택한다.
감소량은 \(-\alpha\|\nabla f(\mathbf{x})\|^2\). 선탐색(backtracking/Armijo 등)과 함께 사용.

### 제약된 방향

정규화 제약(예: 스텝은 접공간 \(T_{\mathbf{x}}\mathcal{M}\) 내)에서는 방향도함수를 그 접공간에 투영한 그래디언트(리만 그래디언트)로 해석한다.

---

## 비매끄러운 함수: 방향도함수와 클라크 부분그래디언트 개관

절댓값/노름/최대값 등에서 그래디언트가 존재하지 않을 수 있다.
예: \(f(\mathbf{x})=\|\mathbf{x}\|\). \(\mathbf{x}\ne \mathbf{0}\)이면 \(\nabla f=\frac{\mathbf{x}}{\|\mathbf{x}\|}\).
원점에서는 미분불가능. 이때 **클라크 부분그래디언트**
$$
\partial^{\circ} f(\mathbf{0})=\{\mathbf{g}\in\mathbb{R}^n:\ \|\mathbf{g}\|\le 1\}
$$
로 일반화하고, 방향도함수의 상한/하한과 연결한다.
실전 최적화(SGD with ReLU, hinge 등)에서 이 일반화가 쓰인다.

---

## 대표 패턴별 방향도함수/그래디언트

### 선형/이차형

- \(f(\mathbf{x})=\mathbf{a}^\top\mathbf{x}\Rightarrow \nabla f=\mathbf{a},\ D_{\mathbf{v}}f=\mathbf{a}\cdot\mathbf{v}\).
- \(f(\mathbf{x})=\tfrac12 \mathbf{x}^\top Q \mathbf{x} + \mathbf{b}^\top\mathbf{x}\Rightarrow \nabla f=Q\mathbf{x}+\mathbf{b}\).

### 합성 로그-소프트맥스(정규화)

\[
p_i(\mathbf{z})=\frac{e^{z_i}}{\sum_j e^{z_j}},\quad f(\mathbf{z})=-\sum_i y_i\ln p_i.
\]
연쇄 결과(표준):
\[
\nabla f(\mathbf{z})=\mathbf{p}-\mathbf{y},\quad
D_{\mathbf{v}}f=(\mathbf{p}-\mathbf{y})\cdot \mathbf{v}.
\]

### 거리/노름

- \(f(\mathbf{x})=\|\mathbf{x}\|\): \(\mathbf{x}\ne 0\)에서 \(\nabla f=\frac{\mathbf{x}}{\|\mathbf{x}\|}\).
- \(f(\mathbf{x})=\tfrac12 \|\mathbf{x}-\mathbf{c}\|^2\): \(\nabla f=\mathbf{x}-\mathbf{c}\).

---

## 수치 검증: SymPy/NumPy/PyTorch

### SymPy로 심볼릭 확인

```python
import sympy as sp

x, y, a, b = sp.symbols('x y a b', real=True)
f = x**2*sp.sin(y) + y**3

grad = sp.Matrix([sp.diff(f, x), sp.diff(f, y)])  # ∇f
v = sp.Matrix([a, b])

Dv = sp.diff(f.subs({x:x+a*sp.symbols('t'), y:y+b*sp.symbols('t')}), sp.symbols('t')).subs(sp.symbols('t'), 0)
dot = (grad.dot(v)).simplify()

print(sp.simplify(Dv - dot))  # 0이면 D_v f = ∇f·v 확인
```

### 유한차분으로 방향도함수 수치 근사

```python
import numpy as np

def f(xy):
    x, y = xy
    return x**2*np.sin(y) + y**3

def grad(xy):
    x, y = xy
    return np.array([2*x*np.sin(y), x**2*np.cos(y) + 3*y**2])

def directional_fd(xy, v, h=1e-6):
    v = np.asarray(v, float)
    return (f(xy + h*v) - f(xy - h*v)) / (2*h)

def directional_grad(xy, v):
    return grad(xy).dot(v)

xy0 = np.array([0.7, 1.2])
v = np.array([1.0, -2.0])
print(directional_fd(xy0, v), directional_grad(xy0, v))
```

### PyTorch로 자동미분 확인

```python
import torch

def torch_demo(x0=0.7, y0=1.2, a=1.0, b=-2.0):
    x = torch.tensor([x0], dtype=torch.float64, requires_grad=True)
    y = torch.tensor([y0], dtype=torch.float64, requires_grad=True)
    f = x**2*torch.sin(y) + y**3
    f.backward()
    grad = torch.stack([x.grad, y.grad]).reshape(-1)
    v = torch.tensor([a, b], dtype=torch.float64)
    Dv = (grad * v).sum()
    return float(Dv), [float(g) for g in grad]

print(torch_demo())
```

---

## 좌표계별 그래디언트와 방향도함수 (요약표)

- **직교 좌표 \((x,y,z)\)**:
  $$
  \nabla f = \left\langle f_x, f_y, f_z \right\rangle,\quad
  D_{\mathbf{v}} f = f_x v_x + f_y v_y + f_z v_z.
  $$
- **극 좌표 \((r,\theta)\)**:
  $$
  \nabla f = f_r\,\mathbf{e}_r + \frac{1}{r}f_\theta\,\mathbf{e}_\theta,\quad
  D_{\mathbf{e}_r} f=f_r,\quad D_{\mathbf{e}_\theta} f=\frac{1}{r}f_\theta.
  $$
- **구면 좌표 \((\rho,\phi,\theta)\)**: 표준 정리 사용(선형대수·벡터해석 교과서 참조).

---

## 곡면/다양체 위의 방향도함수(개관)

매끄러운 다양체 \(\mathcal{M}\subset\mathbb{R}^n\) 위 함수 \(f:\mathcal{M}\to\mathbb{R}\), \(T_{\mathbf{x}}\mathcal{M}\)은 접공간.
곡선 \(\gamma:(-\epsilon,\epsilon)\to\mathcal{M}\)에서 \(\gamma(0)=\mathbf{x},\ \gamma'(0)=\mathbf{v}\in T_{\mathbf{x}}\mathcal{M}\)이면
$$
D_{\mathbf{v}} f(\mathbf{x}) = \frac{d}{dt}\Big|_{t=0} f(\gamma(t)).
$$
리만 기하에서는 그래디언트가 **메트릭에 의해** 정의되고, 최급강하 방향은 메트릭에 대한 음의 그래디언트.

---

## 연습문제 (정답 스케치 포함)

1) \(f(x,y)=\exp(xy)+\cos(x-y)\). \(\nabla f\)와 \(D_{\mathbf{v}}f\) (임의 \(\mathbf{v}\))를 구하라.
   **스케치**: \(f_x = y e^{xy}-\sin(x-y)\), \(f_y = x e^{xy}+\sin(x-y)\). 내적으로 방향도함수.

2) \(f(x,y)=\sqrt{x^2+y^2}\). \((x,y)\ne (0,0)\)에서 \(\nabla f\), 원점에서의 방향도함수 존재 여부를 논하라.
   **스케치**: \(\nabla f = \frac{(x,y)}{\sqrt{x^2+y^2}}\). 원점에선 미분불가. 방향도함수는 \(D_{\mathbf{v}} f(0)=\|\mathbf{v}\|\)의 **상한** 개념이 자연스럽고, 클라크 부분그래디언트가 단위볼.

3) 위 반례 \(f(x,y)=\frac{x^2y}{x^2+y^2}\ (0,0)=0\).
   \(D_{\mathbf{v}} f(0,0)=\frac{a^2 b}{a^2+b^2}\)를 유도하고 선형성 실패를 보이라.

4) 극좌표 변환에서 \(f(x,y)=\ln(x^2+y^2)\). \(f_r, f_\theta\)를 구하라.
   **스케치**: \(f_r=2/r,\ f_\theta=0\).

5) 이차형 \(f(\mathbf{x})=\tfrac12\mathbf{x}^\top Q \mathbf{x}\) (대칭 \(Q\)).
   \(\nabla f=Q\mathbf{x}\)와 \(D_{\mathbf{v}} f=\mathbf{v}^\top Q \mathbf{x}\). \(Q\succ 0\)이면 \(\nabla f=\mathbf{0}\Rightarrow\) 전역최소.

---

## 실전 체크리스트

- 방향도함수는 일차 근사의 **선형형식 평가**: \(D_{\mathbf{v}} f=\nabla f\cdot \mathbf{v}\) (미분가능 시).
- “모든 방향도함수 존재”가 “미분가능”을 보장하지 않는다 → 선형성/일치성 점검.
- 최급상승/최급하강은 \(\pm \nabla f\) 방향, 크기는 \(\|\nabla f\|\).
- 좌표변환(극/구면)에서는 기저가 바뀐다: \(f_r, f_\theta, f_\phi\)로 해석.
- 비매끄러운 경우: 부분그래디언트(클라크)로 일반화.
- 수치검증: **중심차분**과 **자동미분**으로 gradient check.

---

## 한 페이지 요약

1) 정의
$$
D_{\mathbf{v}} f(\mathbf{x})
= \lim_{t\to 0}\frac{f(\mathbf{x}+t\mathbf{v})-f(\mathbf{x})}{t}.
$$

2) 미분가능 ⇔ 그래디언트 존재, 그리고
$$
D_{\mathbf{v}} f=\nabla f\cdot \mathbf{v}.
$$

3) 최대 증가/감소
$$
\max_{\|\hat{\mathbf{v}}\|=1} D_{\hat{\mathbf{v}}} f=\|\nabla f\|,\quad
\min_{\|\hat{\mathbf{v}}\|=1} D_{\hat{\mathbf{v}}} f=-\|\nabla f\|.
$$

4) 레벨셋 법선: \(\nabla f\perp T_{\mathbf{x}}\{f=c\}\).

5) 연쇄법칙(야코비안)
$$
\nabla_{\mathbf{x}}(g\circ \mathbf{u})=J_{\mathbf{u}}^\top \nabla_{\mathbf{u}} g,\quad
D_{\mathbf{v}}(g\circ \mathbf{u})=\nabla_{\mathbf{u}} g^\top (J_{\mathbf{u}}\mathbf{v}).
$$

---

## 부록: 간단한 그라디언트 체크 도구 (NumPy)

```python
import numpy as np

def grad_check(f, grad, x, tol=1e-6, trials=20, seed=0):
    """
    f: R^n -> R, callable
    grad: R^n -> R^n, analytic gradient
    x: point to test
    """
    rng = np.random.default_rng(seed)
    ok = True
    for _ in range(trials):
        v = rng.normal(size=x.shape)
        v /= np.linalg.norm(v) + 1e-12
        g = grad(x).dot(v)
        # central difference at multiple scales
        errs = []
        for h in (1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5):
            fd = (f(x + h*v) - f(x - h*v)) / (2*h)
            errs.append(abs(fd - g))
        if min(errs) > tol:
            ok = False
            print("Direction failed:", v, "min error:", min(errs))
    return ok

# = x^2 sin y + y^3

def f_xy(x):
    x1, x2 = x
    return x1**2*np.sin(x2) + x2**3

def g_xy(x):
    x1, x2 = x
    return np.array([2*x1*np.sin(x2), x1**2*np.cos(x2)+3*x2**2])

x0 = np.array([0.7, 1.2])
print("Grad check:", grad_check(f_xy, g_xy, x0))
```

---

## 마무리

- 방향도함수는 “그 방향으로의 **즉시 변화율**”, 그래디언트는 그 변화율을 한 번에 담는 “**벡터**”.
- 미분가능이면 \(D_{\mathbf{v}} f=\nabla f\cdot\mathbf{v}\)가 모든 것을 설명한다: 최급상승·최급하강·법선·선형화·연쇄.
- 존재만으로 충분치 않은 미분가능성의 함정(전형적 반례)을 반드시 기억하자.
- 실전에서는 좌표계·제약·비매끄러움·수치검증(차분/AD)을 함께 본다.

이제 임의의 문제를 만나면:
(1) **그래디언트**를 구하고, (2) **방향도함수**를 내적으로 확인하며, (3) **연쇄법칙**과 **좌표계**를 적절히 적용하고, (4) 필요하면 **수치적 그라드 체크**로 검증하라.
이 네 스텝이면 분석부터 구현까지 한 호흡으로 연결된다.
