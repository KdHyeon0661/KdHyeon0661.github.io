---
layout: post
title: 데이터 통신 - QOS (2)
date: 2024-09-25 23:20:23 +0900
category: DataCommunication
---
# Chapter 30. QoS — Integrated Services(IntServ)와 Differentiated Services(DiffServ)

## 30.3 Integrated Services (IntServ)

IntServ는 IETF에서 제안했던 “실시간 트래픽(음성·영상)이 **정해진 지연·손실 한도 내**에서 전달되도록, 네트워크 노드(라우터)에 **플로우 단위 상태 정보를 저장하고 리소스를 예약**하는 아키텍처”이다.  

대표 요소:

- **Flow Specification(FlowSpec)**: 각 플로우의 트래픽 특징 + 원하는 QoS를 정량적으로 기술
- **Admission Control**: 새 플로우를 받아들일지(accept) 거절할지(reject) 결정
- **Service Classes**: Guaranteed Service, Controlled Load Service 등
- **RSVP**: 라우터에 예약 상태를 설치하는 시그널링 프로토콜  

### 30.3.1 Flow Specification (FlowSpec)

IntServ에서 FlowSpec은 보통 두 부분으로 나뉜다.  

- **TSpec(Traffic Specification)**: 해당 플로우 트래픽이 어떤 패턴으로 흘러오는지
- **RSpec(Reservation Specification)**: 네트워크에서 어떤 QoS를 제공해줘야 하는지

대표적인 TSpec 파라미터(토큰 버킷 모델):

| 파라미터 | 의미 |
|---------|------|
| $$r$$   | 평균 트래픽률 (token rate, bits/s) |
| $$b$$   | 버스트 허용량 (bucket size, bits) |
| $$p$$   | 최대 전송률(peak rate, bits/s) |
| $$m$$   | 최소 패킷 크기(minimum policed unit) |
| $$M$$   | 최대 패킷 크기(maximum packet size) |

**토큰 버킷 제약식**의 전형적인 형태는:

$$
A(t_0, t_1) \le r \cdot (t_1 - t_0) + b
$$

여기서  
- $$A(t_0, t_1)$$: 시간 구간 $$[t_0, t_1]$$ 동안 도착한 트래픽 양  
- 오른쪽은 “평균률 $$r$$로 토큰이 쌓이고, 한 번에 최대 $$b$$ 만큼 버스트가 가능하다”는 의미다.  

**RSpec**은 서비스 클래스에 따라 다르지만, 예를 들어 Guaranteed Service의 경우:  

- $$R$$: 예약 대역폭(bits/s)  
- $$S$$: 버퍼 크기 관련 파라미터(슬랙, 버퍼 마진 등)  

#### 예제 1: VoIP 플로우의 FlowSpec

상황:

- 코덱: G.711, 64 kbps
- 패킷화 간격: 20 ms
- IP/UDP/RTP 헤더 포함 패킷 크기: 약 80바이트(payload) + 헤더 40바이트 ≈ 120바이트
- 지연 요구사항: end-to-end 150 ms 이하 (일반 VoIP 가이드라인 수준)  

이 경우 TSpec은 대략:

- $$r = 64\ \text{kbit/s}$$
- $$M \approx 960\ \text{bit} \;(120\ \text{byte})$$
- 버스트 허용량은 코덱 특성상 크지 않으므로, 예를 들어 $$b = 2M$$ 정도로 설정.

RSpec(Guaranteed Service):

- 예약률 $$R \approx r + \text{오버헤드} \approx 80\ \text{kbit/s}$$ 정도(링크 오버헤드 포함)
- 버퍼 슬랙 $$S$$ 를 고려하여, 경로 상 각 라우터가 delay bound를 계산.

이렇게 FlowSpec을 명시하면, RSVP가 각 라우터에 이 정보를 전달하고, 라우터는 **스케줄러/버퍼 자원을 예약**한다.

#### 간단한 FlowSpec 데이터 구조 예 (파이썬 유사 코드)

```python
from dataclasses import dataclass

@dataclass
class TSpec:
    r: int  # bits per second
    b: int  # burst size in bits
    p: int  # peak rate in bits/s
    m: int  # min policed unit in bytes
    M: int  # max packet size in bytes

@dataclass
class RSpec:
    R: int  # reserved rate in bits/s
    S: int  # slack/buffer parameter (abstract)

@dataclass
class FlowSpec:
    tspec: TSpec
    rspec: RSpec

# G.711 예시
tspec_voip = TSpec(
    r=64000,
    b=2 * 120 * 8,
    p=80000,
    m=40,
    M=120
)

rspec_voip = RSpec(R=80000, S=10)
flowspec_voip = FlowSpec(tspec_voip, rspec_voip)
```

이 정도 구조가 RSVP 메시지 안에, 혹은 라우터의 QoS 상태 테이블 안에서 표현된다고 생각하면 된다.

---

### 30.3.2 Admission (Admission Control)

**Admission Control**은 새 FlowSpec을 가진 플로우가 들어올 때 “네트워크가 이 요구를 만족시킬 수 있는지”를 판단하는 메커니즘이다.  

각 라우터는 링크마다 이미 예약된 대역폭을 기억하고 있다가, 새 플로우가 요청한 예약률 $$R_\text{new}$$ 를 더했을 때, 링크 용량 $$C$$ 를 초과하거나 QoS 제약(지연 bound 등)을 깨지 않는지 검증한다.  

가장 단순한 형태의 admission 조건은:

$$
\sum_i R_i + R_\text{new} \le \alpha \cdot C
$$

- $$R_i$$: 이미 수락된 플로우들의 예약률
- $$C$$: 링크 용량
- $$\alpha \in (0, 1)$$: 통계적 여유(예: 0.8)

#### 예제 2: 10 Mbps 링크에서 VoIP 플로우 Admission

상황:

- 링크 용량: 10 Mbps
- 보수적으로 $$\alpha = 0.8$$ (80%까지만 예약)
- 각 VoIP 플로우: $$R = 80\ \text{kbit/s}$$ (위 예)

수락 가능한 최대 플로우 수 $$N$$는:

$$
N \cdot 80\,000 \le 0.8 \cdot 10\,000\,000
$$

$$
N \le \frac{8\,000\,000}{80\,000} = 100
$$

즉, 이 링크에 **최대 100개의 VoIP 플로우까지만 admission** 한다.

간단한 admission control 의사 코드는 다음과 같이 표현할 수 있다.

```python
LINK_CAPACITY = 10_000_000      # 10 Mbps
ALPHA = 0.8
RESERVED = 0

def can_admit(new_R):
    global RESERVED
    return RESERVED + new_R <= ALPHA * LINK_CAPACITY

def admit_flow(new_R):
    global RESERVED
    if can_admit(new_R):
        RESERVED += new_R
        return True
    return False

for i in range(1, 150):
    if admit_flow(80_000):
        print(f"Flow {i} admitted, reserved={RESERVED}")
    else:
        print(f"Flow {i} rejected at reserved={RESERVED}")
        break
```

실제 라우터는 여기에 **지연·버퍼·큐잉 지연 분석**까지 포함된 복잡한 수식을 사용하지만, 핵심 개념은 위와 같다.

---

### 30.3.3 Service Classes

IETF IntServ 모델은 대표적으로 두 가지 서비스 클래스를 정의했다.  

1. **Guaranteed Service (GS, RFC 2212)**  
   - 손실률과 지연 upper bound를 엄격히 보장하는 서비스
   - 애플리케이션은 “최대 지연 $$D_{\max}$$ 에서 패킷이 도착한다”는 보장을 받는다.
2. **Controlled-Load Service (CLS, RFC 2211)**  
   - “가볍게 로드된(best effort이지만 혼잡이 거의 없는) 네트워크에서 경험하는 것과 비슷한 성능”을 제공
   - 지연·손실 bound는 확률적이지만, 혼잡 상황에서도 QoS가 크게 나빠지지 않도록 자원 예약

#### Guaranteed Service 개념

Guaranteed Service는 다음과 같은 식으로 지연 upper bound를 정량화한다(개념 수준):  

- 토큰 버킷으로 모델링된 입력 트래픽
- 서비스 곡선(서비스율 $$\mu$$)을 가진 큐
- 네트워크 요소(라우터)를 여러 개 지날 때의 지연 상한을 합산

간단하게 표현하면, 각 링크에서:

$$
D_i \le \frac{b_i}{\mu_i - r_i} + \text{propagation}_i
$$

- $$b_i$$: 플로우의 버스트 허용량
- $$r_i$$: 평균 도착률
- $$\mu_i$$: 링크 서비스율
- propagation: 전파 지연

총 지연 bound는 각 홉의 지연을 다 더한 값으로 upper bound를 잡을 수 있다.

#### Controlled-Load Service 개념

Controlled-Load는 “실제 지연은 크게 줄이고, 패킷 손실율도 낮추지만, 수학적 worst-case bound는 느슨하게 두는” 서비스다.  
예:

- 트래픽이 짧은 시간 동안 버스트가 생겨도, 예약된 자원으로 대부분의 패킷은 지연·손실 없이 전달.
- 하지만 extreme worst-case 상황에서의 절대 지연 bound는 **명시적으로 보장하지 않을 수 있음**.

이 두 서비스 모두 RSVP와 FlowSpec을 통해 설정된다.

---

### 30.3.4 Resource Reservation Protocol (RSVP)

RSVP는 IntServ를 위한 **자원 예약 시그널링 프로토콜**이다.  

특징:

- **Soft state**: 라우터의 RSVP 상태는 주기적인 refresh 메시지로 유지, 끊기면 자동 삭제
- **Receiver-based reservation**: 송신자가 아니라 수신자가 필요한 QoS를 요청
- **Path / Resv 메시지**:
  - 송신자가 PATH 메시지를 목적지까지 보냄 (경로 설치 + 트래픽 spec 전달)
  - 수신자가 RESV 메시지로 역방향으로 예약 요청(FloSpec 포함)
- **Multicast 지원**: 한 송신자에서 여러 수신자 그룹에 대한 예약을 자연스럽게 처리

#### 예제 3: RSVP를 이용한 화상회의 예약 시나리오

상황:

- A(송신자) → B(수신자) 화상회의
- 3개의 라우터 R1, R2, R3를 거쳐서 전송
- A는 2 Mbps의 비디오 스트림을 송출

과정:

1. **PATH 단계**  
   - A는 TSpec(2 Mbps 비디오)과 함께 PATH 메시지를 전송
   - R1, R2, R3는 PATH를 forwarding 하면서 "이 플로우는 이 인터페이스로 들어와서 저 인터페이스로 나간다"는 상태 저장

2. **RESV 단계**  
   - B는 필요한 RSpec(예: 2.5 Mbps, 특정 지연 bound)을 포함한 RESV 메시지 전송
   - RESV는 R3 → R2 → R1 → A 방향으로 전달
   - 각 라우터는 **Admission Control 수행**  
     - 여유 자원이 있으면 예약 상태 생성
     - 없으면 에러 반환

3. **데이터 전송**  
   - 예약이 완료되면, A는 해당 플로우에 대해 RSVP 핸들 지정(Flow ID)과 함께 패킷을 보내고, 각 라우터는 예약된 큐/스케줄링을 적용

4. **Soft State 유지**  
   - 주기적으로 PATH/RESV refresh 메시지를 보내 상태 유지
   - 흐름이 끊기면, 타이머 만료로 상태 자동 삭제

---

### 30.3.5 Problems with Integrated Services

IntServ는 이론적으로 강력하지만, 실제 인터넷 백본 수준에서는 잘 쓰이지 않는다. 주요 이유는 다음과 같다.  

1. **스케일 문제(Per-Flow State Scalability)**  
   - 각 라우터가 플로우마다 상태를 저장해야 한다.
   - 백본 라우터 한 대가 수십만~수백만 플로우를 동시에 처리해야 하는 상황에서, 상태 저장/refresh 오버헤드가 매우 큼.

2. **RSVP 시그널링 오버헤드**  
   - 플로우가 동적으로 생성·종료될 때마다 PATH/RESV 교환
   - 모바일·대규모 웹 트래픽 환경에서 이 오버헤드는 상당함.

3. **종단 간 배포 문제**  
   - IntServ/RSVP를 제대로 쓰려면 **경로상의 모든 라우터**가 RSVP/IntServ를 지원해야 한다.
   - ISP, IX, 클라우드 사업자 간 정책/관리 상의 이유로 end-to-end 배포가 거의 불가능에 가깝다.

4. **멀티도메인 환경에서의 정책/과금 문제**  
   - 각 도메인은 자신만의 정책과 과금 모델을 가지고 있음.
   - end-to-end로 “보장된 service level”을 합의하는 것은 정치·비즈니스적으로도 복잡하다.

이러한 한계 때문에, IETF와 운영자들은 **코어에서는 IntServ 대신 DiffServ** 를 주로 사용하고, 필요한 경우 **엣지에서 IntServ ↔ DiffServ 매핑**을 사용하는 하이브리드 구조를 연구했다.  

---

## 30.4 Differentiated Services (DiffServ)

DiffServ는 IntServ의 스케일 문제를 해결하기 위해 제안된 아키텍처로, **코어 라우터에서 개별 플로우가 아니라 “트래픽 클래스(behavior aggregate)”에 대해서만 상태를 유지**한다.  

핵심 아이디어:

- 패킷 헤더의 **DS Field**(Differentiated Services Field)에 **DSCP(Differentiated Services Code Point)** 를 표시
- 패킷이 코어 라우터를 지날 때, **DSCP에 따라 Per-Hop Behavior(PHB)** 를 적용
- 엣지 라우터에서 **Traffic Conditioner**(meter/marker/shaper/policer) 을 통해 각 클래스별 트래픽을 계약에 맞게 조절

### 30.4.1 DS Field

DiffServ는 IPv4의 ToS(8비트)와 IPv6의 Traffic Class 필드를 재정의해서 **DS 필드**로 사용한다.  

- 상위 6비트: **DSCP**  
- 하위 2비트: ECN(Explicit Congestion Notification)에 사용 (RFC 3168)  

DSCP 값은 논리적으로 다음과 같이 나눌 수 있다.  

1. **Default PHB (Best Effort)**: DSCP 000000
2. **Class Selector PHB**: 기존 IP Precedence와 호환되도록 정의된 클래스
3. **Assured Forwarding (AF)**: 여러 우선순위·drop precedence 조합
4. **Expedited Forwarding (EF)**: 지연·지터가 매우 작은 “전용 회선 비슷한” 서비스용 DSCP (보통 101110, 46)

#### DSCP 예시 표

| 서비스 | DSCP (10진) | DSCP (2진) | 설명 |
|--------|------------|-----------|------|
| EF(Expedited Forwarding) | 46 | 101110 | 음성 등 실시간 트래픽 |
| AF11 | 10 | 001010 | 낮은 drop precedence, 클래스 1 |
| AF21 | 18 | 010010 | 클래스 2 |
| AF31 | 26 | 011010 | 클래스 3 |
| AF41 | 34 | 100010 | 클래스 4 |
| Default | 0  | 000000 | Best Effort |

실제 서비스 설계 가이드라인은 IETF RFC 4594에서 여러 서비스 클래스를 정의하고, DSCP를 매핑하는 방식을 제시한다.  

#### 예제 4: 기업망에서 DSCP 사용 시나리오

상황:

- 사내 IP 전화: EF(46)로 마킹
- 화상회의: AF41(고우선)
- 일반 웹 트래픽: Default(0)
- 백업/대용량 파일 전송: AF11(낮은 우선순위)

엣지 라우터(Access 라우터)는 다음과 같이 DSCP를 설정할 수 있다.

```bash
# 의사 Cisco-style config 예시

class-map match-any VOICE
 match protocol rtp audio

class-map match-any VIDEO
 match protocol rtp video

class-map match-any BULK
 match access-group name BULK_TRAFFIC

policy-map MARKING
 class VOICE
  set dscp ef
 class VIDEO
  set dscp af41
 class BULK
  set dscp af11
 class class-default
  set dscp default

# 인터페이스에 적용
interface GigabitEthernet0/0
 service-policy input MARKING
```

코어 라우터는 개별 플로우가 아니라 DSCP 기반으로만 큐잉·스케줄링을 수행하므로, 상태 수가 훨씬 작다.

---

### 30.4.2 Per-Hop Behavior (PHB)

**PHB**는 “DiffServ 노드가 특정 DSCP 값의 패킷에 대해 보여주는 외부 관찰 가능한 forwarding 행동”이다.  

대표 PHB들:

1. **Default PHB**  
   - Best Effort 큐로 전송, 특별한 우선순위 없음

2. **Expedited Forwarding (EF PHB, RFC 3246)**  
   - 매우 낮은 지연·지터, 거의 손실 없는 서비스 제공
   - 구현 예: 우선순위 큐(priority queue)를 사용해 EF 트래픽이 항상 가장 먼저 서비스받도록 함

3. **Assured Forwarding (AF PHB, RFC 2597)**  
   - 네 개의 클래스(AF1x~AF4x)와 각 클래스별 세 개의 drop precedence(x=1,2,3) 제공
   - 예: AF41이 AF43보다 동일 클래스 내에서 drop 우선순위가 낮음

#### 예제 5: PHB 별 큐 동작 개념

가상의 출구 인터페이스에 세 개의 큐가 있다고 하자.

- 큐 1: EF (우선순위 큐, strict priority)
- 큐 2: AF4x, AF3x (Weighted Round Robin)
- 큐 3: 나머지 (Best Effort)

동작:

1. 항상 EF 큐에 패킷이 있으면, 먼저 EF 큐를 비운다.
2. EF 큐가 비어 있으면, AF 큐들을 가중치에 따라 서비스
3. 남는 시간에 Best Effort 큐 서비스

이를 간단한 파이썬 유사 코드로 표현하면:

```python
def scheduler_step(queues):
    # queues: dict {name: [packets]}
    if queues["EF"]:
        return queues["EF"].pop(0)   # strict priority
    elif queues["AF"]:
        # 예: 가중치에 따라 round robin
        return queues["AF"].pop(0)
    else:
        if queues["BE"]:
            return queues["BE"].pop(0)
        else:
            return None
```

실제 구현은 훨씬 복잡하지만, 개념은 “DSCP → PHB → 큐/스케줄링 동작”이다.

---

### 30.4.3 Traffic Conditioners

DiffServ의 **Traffic Conditioner**는 주로 **도메인 엣지**에서 동작하며, 계약된 트래픽 특성(트래픽 프로필)을 보장하기 위해 다음 요소로 구성된다.  

1. **Meter**  
   - 트래픽이 프로필에 맞게 들어오고 있는지 측정
   - 대표적으로 **토큰 버킷(token bucket)** 사용

2. **Marker**  
   - 패킷의 DSCP를 설정/변경
   - 프로필을 만족하면 “green”(낮은 drop), 초과분은 “yellow/red”로 마킹

3. **Shaper**  
   - 패킷을 지연시켜 트래픽을 평탄하게 만드는 역할
   - 출력률이 일정하도록 버퍼에 넣었다가 일정 속도로 내보냄 (leaky bucket, token bucket 기반)

4. **Dropper/Policer**  
   - 프로필을 심하게 벗어난 패킷을 **드랍**하거나 더 낮은 우선순위로 마킹

#### 토큰 버킷 Meter 예제

토큰 버킷의 기본 동작은 다음 수식으로 표현할 수 있다.

- 토큰 생성률 $$r$$ (bits/s), 버킷 크기 $$B$$ (bits)
- 패킷 크기 $$L$$ 의 패킷이 도착했을 때, 버킷 안의 토큰 수를 $$T$$ 라고 하면:

1. 시간 경과에 따라 토큰 수 증가:

$$
T \leftarrow \min(B, T + r \cdot \Delta t)
$$

2. 패킷이 왔을 때:

- $$T \ge L$$ 이면, 패킷은 **in-profile** (green)
  - $$T \leftarrow T - L$$
- $$T < L$$ 이면, 패킷은 **out-of-profile** (yellow/red)

아주 단순한 파이썬 시뮬레이션:

```python
class TokenBucketMeter:
    def __init__(self, rate_bps, bucket_size_bits):
        self.rate = rate_bps
        self.bucket_size = bucket_size_bits
        self.tokens = bucket_size_bits
        self.last_time = 0.0

    def _add_tokens(self, now):
        dt = now - self.last_time
        self.tokens = min(self.bucket_size, self.tokens + self.rate * dt)
        self.last_time = now

    def conform(self, packet_size_bits, now):
        self._add_tokens(now)
        if self.tokens >= packet_size_bits:
            self.tokens -= packet_size_bits
            return True  # in-profile
        else:
            return False # out-of-profile
```

실제 DiffServ 설계에서는 **Two-Rate Three-Color Marker(trTCM)** 같은 고급 토큰 버킷 기반 conditioner가 널리 사용된다.  

#### 예제 6: ISP 엣지에서 트래픽 조건부 허용

상황:

- 고객 계약: “최대 50 Mbps, 버스트 10 MB”
- ISP 엣지 라우터는 토큰 버킷을 `rate=50 Mbps`, `bucket=10 MB`로 설정
- in-profile 패킷은 DSCP AF41, out-of-profile 패킷은 AF43으로 마킹

의사 설정 예:

```bash
policy-map CUSTOMER1-POLICY
 class class-default
  police cir 50M bc 10M
   conform-action set-dscp-transmit af41
   exceed-action set-dscp-transmit af43
   violate-action drop
```

코어에서는 AF41 > AF43 > default 순으로 drop 우선순위를 두어, 혼잡 시 계약 범위를 크게 벗어난 트래픽이 먼저 버려진다.

---

### 30.4.x IntServ와 DiffServ의 연동 (보충)

실제 네트워크는 **엣지에서 IntServ/RSVP를 사용하고, 코어에서는 DiffServ로 aggregate QoS를 제공**하는 하이브리드 모델을 사용할 수 있다.  

- 엣지 라우터:
  - 플로우 단위로 RSVP/IntServ를 처리해 FlowSpec을 수신
  - 플로우를 특정 DiffServ 클래스(DSCP)에 매핑
- 코어 라우터:
  - DSCP 기반 DiffServ PHB만 적용
- 다시 엣지에서:
  - IntServ/RSVP 상태와 DiffServ aggregate QoS를 연동해 end-to-end QoS를 추정

이렇게 하면, **코어의 스케일 문제를 피하면서도 엣지에서는 애플리케이션에게 좀 더 세밀한 QoS 인터페이스**를 제공할 수 있다.

---

## 정리

- IntServ는 플로우별 FlowSpec과 RSVP를 이용해 **엄격한 QoS 보장**을 제공하지만, **per-flow 상태와 시그널링 오버헤드** 때문에 인터넷 규모에서는 잘 쓰이지 않는다.
- DiffServ는 IPv4/IPv6 헤더의 **DS 필드와 DSCP**를 이용해, 패킷을 **few traffic classes**로 나누고, 각 클래스에 대해 PHB와 Traffic Conditioner를 적용하는 방식으로 **스케일 문제를 해결**했다.
- 오늘날 인터넷 백본/대형 ISP는 주로 DiffServ 기반 QoS, MPLS-TE, 그리고 일부 도메인 내 RSVP-TE 등을 조합해서 사용하며, IntServ는 주로 이론·연구 또는 제한된 도메인에서만 부분적으로 사용된다.
