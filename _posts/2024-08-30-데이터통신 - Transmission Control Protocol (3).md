---
layout: post
title: 데이터 통신 - Transmission Control Protocol (3)
date: 2024-08-30 20:20:23 +0900
category: DataCommunication
---
# Transmission Control Protocol — Windows, Flow Control, Error Control (상세)

이 글은 앞에서 정리한 **TCP 서비스/세그먼트/상태 머신** 위에,
이번에는 특히 **윈도우(window)**, **플로우 제어(flow control)**, **에러 제어(error control)** 를 깊게 파고드는 장이다.

- “윈도우”는 **얼마나 많이 미리 보낼 수 있는지**를 정하는 핵심 규칙이고,
- “플로우 제어”는 **수신 측이 감당 가능한 만큼만** 데이터를 보내도록 하는 메커니즘이며,
- “에러 제어”는 **손실·손상된 세그먼트를 어떻게 감지하고 복구하는지**에 대한 규칙이다.

---

## Windows in TCP

### 윈도우 개념 – “한 번에 얼마나 앞서 나갈 수 있나?”

TCP는 **슬라이딩 윈도우(sliding window)** 모델을 사용한다.

- 송신자는 “ACK를 아직 받지 않았지만 **보낼 수 있는 바이트 범위**”를 **송신 윈도우**라고 부른다.
- 이 윈도우 안에 들어오는 바이트들은 **전송 가능(in-flight)**,
  윈도우 밖(앞)은 **아직 보내면 안 되는 영역**이다.
- ACK가 도착하면, **확인된 부분만큼 왼쪽 경계가 앞으로 밀려(slide)** 윈도우가 오른쪽으로 이동한다.

아주 단순화된 그림:

```text
시퀀스 번호:  1000             2000             3000
               |-----------------|-----------------|
현재 윈도우:       [======보낼 수 있는 범위======]

보낸 바이트 (미확인): 1000~1499
보낼 수 있는 바이트: 1500~2499
아직 금지된 영역:    2500~
```

ACK가 1500까지 왔다면:

```text
새 윈도우:                   [======보낼 수 있는 범위======]
시퀀스 번호:  1000             2000             3000
                             ^
                        윈도우 왼쪽 경계가 1500으로 슬라이드
```

이 개념은 **Go-Back-N, Selective Repeat ARQ**에서 쓰던 슬라이딩 윈도우와 동일한 아이디어를 TCP 바이트 스트림에 적용한 것이다.

---

### 세 가지 윈도우: rwnd, cwnd, send window

현대 TCP에서는 논리적으로 세 가지 윈도우를 구분하는 것이 일반적이다.

1. **수신 윈도우(receive window, rwnd)**
   - 수신 측이 “내 버퍼에 이 정도 여유 있으니, 그만큼만 더 보내도 돼”라고 알려주는 값.
   - TCP 헤더의 **Window 필드**(16비트) + Window Scale 옵션을 통해 광고(advertise)한다.
2. **혼잡 윈도우(congestion window, cwnd)**
   - 송신 측이 “네트워크(중간 라우터들)가 감당 가능한 수준”이라고 추정하는 값.
   - 혼잡 제어 알고리즘(Tahoe/Reno/NewReno/CUBIC/BBR 등)에 의해 RTT·손실률을 보며 동적으로 변경된다.
3. **실제 송신 윈도우(send window)**
   - 실제로 송신자가 사용할 수 있는 윈도우 크기:

     $$W_{\text{send}} = \min(W_{\text{rwnd}}, W_{\text{cwnd}})$$

   - 즉, **수신자 버퍼가 허용하는 양과, 네트워크가 허용하는 양 중 더 작은 것**으로 제한.

직관적으로:

- rwnd는 “**수신자 보호**”,
- cwnd는 “**네트워크 보호(혼잡 방지)**”,
- send window는 “**실제 송신 가능량**” 이다.

#### 예시 1: rwnd가 작은 경우 (수신자 병목)

- rwnd = 20 KB
- cwnd = 200 KB

이면

- 송신 가능량: `min(20KB, 200KB) = 20KB`
- 네트워크는 더 많이 처리할 여유가 있어도, 수신자가 20KB 버퍼만 열어 두었기 때문에 그 이상 보내면 안 된다.

#### 예시 2: cwnd가 작은 경우 (네트워크 병목)

- rwnd = 1 MB
- cwnd = 64 KB

이면

- 송신 가능량: `min(1MB, 64KB) = 64KB`
- 수신자는 충분한 버퍼를 제공하지만, 네트워크가 아직 포화를 못 버텨서 cwnd가 작게 유지되는 상황이다.

---

### TCP Window 필드와 Window Scaling

TCP 헤더의 **Window 필드**는 16비트이다.
따라서 **광고 가능한 윈도우의 최대값은 65,535 바이트**(약 64KB)이다.

그러나 오늘날 인터넷은

- 수백 Mbps~수 Gbps 링크,
- 수십~수백 ms 왕복 지연(RTT)

같은 환경이 흔해서, **대역폭-지연 곱(Bandwidth-Delay Product, BDP)**이 매우 크다.

BDP는

$$
\text{BDP} = \text{Bandwidth} \times \text{RTT}
$$

로 정의되며, “링크 위에 동시에 떠 있어야 하는 데이터 양”을 의미한다.

#### 예시 3: 1 Gbps, RTT = 100ms 링크에서 필요한 윈도우

- 대역폭 = 1 Gbps = 1,000,000,000 bit/s ≈ 125 MB/s
- RTT = 0.1 s

$$
\text{BDP} = 125\ \text{MB/s} \times 0.1\ \text{s} = 12.5\ \text{MB}
$$

즉, 약 **12.5MB** 정도의 데이터를 “한 번에 날리고 ACK를 기다리는 사이”에 네트워크에 떠 있게 해야 풀링크를 활용할 수 있다.

- 64KB 윈도우로는 턱없이 부족하다.
- 그래서 **Window Scale 옵션(RFC 7323)** 이 도입되었다.

#### Window Scaling 아이디어

- 16비트 Window 필드를 **shift factor(0~14)** 만큼 왼쪽으로 시프트하여 **실제 윈도우 크기를 확장**한다.
- 협상 방식(핵심 아이디어만 단순화):

  1. 3-way handshake 과정에서 양쪽이 “나 window scaling 지원해”라고 알림.
  2. 각자 **scale factor N (0~14)** 를 광고.
  3. 실제 윈도우 크기는

     $$
     W_{\text{effective}} = W_{\text{header}} \times 2^N
     $$

     로 계산.

- 이론적으로 최대 **2^30 바이트(1GB)** 정도까지 윈도우를 키울 수 있다.

#### 예시 4: 1 Gbps/100ms 예제에서 필요한 scale factor

- 필요한 윈도우: 약 12.5MB
- Window 필드 최대: 65,535 ≈ 64KB
- 필요한 배수: 12.5MB / 64KB ≈ 195

2의 거듭제곱으로 가장 가까운 값은:

- \(2^7 = 128\) → 64KB × 128 ≈ 8MB (부족)
- \(2^8 = 256\) → 64KB × 256 ≈ 16MB (충분)

따라서 scale factor = 8이면, 최대 16MB 윈도우를 광고할 수 있고, BDP 12.5MB도 충분히 커버 가능하다.

이런 계산은 “왜 window scaling이 없으면 고속·고지연 링크에서 TCP 성능이 비참해지는지”를 잘 보여준다.

---

### 관계

이론적으로 TCP 연결의 최대 처리율은

$$
\text{Throughput}_{\max} \approx \frac{W_{\text{send}}}{\text{RTT}}
$$

로 거칠게 근사할 수 있다.

- 예: send window = 64KB, RTT = 100ms일 때

$$
\text{Throughput} \approx \frac{64\ \text{KB}}{0.1\ \text{s}} = 640\ \text{KB/s} \approx 5.12\ \text{Mbps}
$$

- 1Gbps 링크 위라도, 윈도우가 64KB면 **대략 5Mbps 정도밖에 못 쓰는 셈**이다.

#### 간단한 BDP 계산 코드 예시 (Python)

```python
def bdp_bytes(bandwidth_mbps, rtt_ms):
    # bandwidth: Mbps, rtt_ms: milliseconds
    bandwidth_bps = bandwidth_mbps * 1_000_000
    rtt_s = rtt_ms / 1000.0
    return int(bandwidth_bps * rtt_s / 8)

for bw in [100, 1000]:
    for rtt in [10, 50, 100]:
        bdp = bdp_bytes(bw, rtt)
        print(f"{bw}Mbps, RTT {rtt}ms -> BDP ≈ {bdp / (1024*1024):.2f}MB")
```

이 코드를 실행해 보면, 고속/고지연 조합에서 몇 MB ~ 수십 MB의 윈도우가 필요하다는 것을 금방 체감할 수 있다.

---

### Silly Window Syndrome과 윈도우 관리

윈도우 관리가 잘못되면 **Silly Window Syndrome(SWS)** 를 겪게 된다.

- 수신 측이 아주 작은 단위(몇 바이트)만 버퍼를 비울 때마다 **조금씩 rwnd를 열어 광고**하면,
- 송신 측이 그때마다 작은 세그먼트를 만들어 보내서 **네트워크에 자잘한 패킷이 쏟아짐**.
- 헤더 오버헤드, 처리 비용, RTT가 모두 낭비된다.

이에 대한 해결책:

1. **Clark’s solution(수신 측)**
   - 수신자는 버퍼가 어느 정도 충분히 비워질 때까지 광고 윈도우를 0 또는 아주 작게 유지.
   - 일정 임계치(예: MSS 또는 윈도우의 절반) 이상 버퍼가 비워지면 그때 한 번에 큰 윈도우를 광고.
2. **Nagle’s algorithm(송신 측)**
   - 아직 이전 세그먼트에 대한 ACK를 받지 못했는데, 자잘한 데이터가 들어오면 **버퍼에 모았다가** 한 번에 전송.
   - 단, 지연 상호작용(작은 요청/응답이 빈번한 Telnet/SSH 계열)에서는 문제가 될 수 있어 비활성화하기도 한다.

실제 운영에서는

- 수신 측의 **윈도우 광고 정책**,
- 송신 측의 **Nagle’s algorithm + Delayed ACK** 조합

이 서로 영향을 미쳐서 지연·처리율에 큰 차이를 만든다.

---

## Flow Control in TCP

### 목표: 수신자 보호

플로우 제어(flow control)의 목적은 **“수신자를 과부하시키지 않는 것”**이다.

- 수신 애플리케이션이 느리게 데이터를 읽을 수도 있고,
- OS의 수신 버퍼 크기도 제한되어 있다.
- 따라서 송신자가 너무 빠르게 보내면 **수신 버퍼 오버플로우**가 발생할 수 있다.

이를 방지하기 위해,

- TCP는 수신 측에서 유지하는 **수신 버퍼**의 여유 공간을 기준으로,
- “지금부터 이만큼까지는 더 보내도 된다”는 크기를 **rwnd(Receive Window)**로 광고한다.

### Receive Window 광고 방식

수신 측 TCP는 다음과 같은 정보를 가진다.

- `RcvBuffer` : 수신 버퍼 전체 크기
- `RcvNext` : 다음에 기대하는 시퀀스 번호
- `RcvWindow` : 광고할 윈도우 크기

수신자가 수신 버퍼에 데이터를 쌓아가다가, 애플리케이션이 `read()`를 통해 데이터를 소비하면 **버퍼가 비워지고**→ 그만큼의 크기를 다시 window로 광고한다.

그 로직을 매우 단순하게 적어 보면:

```pseudo
RcvWindow = RcvBuffer - (데이터_쌓인_양)

ACK를 보낼 때마다:
    TCP 헤더 Window 필드에 RcvWindow (또는 scaled 값)를 넣어 광고
```

송신자는

- 지금까지 받은 ACK 중 **가장 최신의 Window 값**을 기억하고,
- 해당 값만큼만 in-flight 데이터를 유지한다.

즉,

$$
\text{in-flight bytes} \leq W_{\text{rwnd}}
$$

을 항상 만족시키도록 전송량을 조절한다.

---

### 예제: 느린 수신 애플리케이션과 윈도우 축소

시나리오:

- 수신 버퍼: 32KB
- 애플리케이션은 5초에 한 번씩만 8KB씩 읽는다.
- 송신자는 대용량 파일을 계속 보내고 싶어한다.

시간 흐름:

1. 처음 연결 직후
   - 버퍼 비어 있음 → RcvWindow = 32KB 광고.
   - 송신자는 최대 32KB까지 보낼 수 있다.
2. 송신자가 32KB를 다 보내면,
   - 수신 TCP는 이 데이터들을 버퍼에 쌓고 ACK를 보낼 때마다 남은 여유를 광고.
   - 버퍼가 꽉 차면 RcvWindow ≈ 0.
3. 애플리케이션이 8KB를 읽으면
   - 버퍼에 여유 8KB 생김 → 다음 ACK에서 RcvWindow = 8KB.
4. 송신자는 다시 8KB만큼만 전송 가능.
5. 이런 식으로, **수신 애플리케이션이 데이터를 소비하는 속도에 맞춰 송신 속도가 조절**된다.

이게 바로 플로우 제어가 제대로 작동하는 예이다.

---

### Zero Window와 Window Probe

수신자가 버퍼를 꽉 채우면,
TCP 헤더의 Window 필드를 0으로 설정하여 **0 윈도우(Zero Window)**를 광고할 수 있다.

- 이것은 “지금은 더 이상 보낼 수 있어선 안 돼, 잠깐 기다려”라는 의미이다.
- 송신자는 이 시점부터 **새로운 데이터 전송을 중단**한다.

그러나, 만약 수신 애플리케이션이 다음과 같은 상태에 빠지면?

- 애플리케이션이 더 이상 데이터를 읽지 않거나, 중간에 블록되어 멈춤.
- 수신 TCP는 Window를 0으로 유지.
- 송신자는 영원히 기다릴 수도 있다.

이를 방지하기 위해 송신자는 **Window Probe(Zero Window Probe)** 라는 작은 세그먼트를 주기적으로 보낸다.

- 예를 들어, 1바이트짜리 세그먼트를 보내서 수신자에게 “혹시 이제 버퍼 비워졌니?”라고 묻는다.
- 수신자는 이 세그먼트를 ACK하면서 최신의 RcvWindow를 다시 광고한다.
  - 만약 이제 버퍼에 여유가 있다면 0이 아닌 값을 보내고,
  - 없으면 계속 0을 유지한다.

이 메커니즘 덕분에

- “윈도우 0 → 영원히 멈춤” 상황을 피할 수 있고,
- 수신 애플리케이션이 다시 동작하기 시작하면 송신도 자동으로 재개된다.

---

### 플로우 제어와 애플리케이션 설계

플로우 제어는 **TCP 내부에서 자동으로 동작**하지만, 애플리케이션 설계에도 영향을 준다.

예를 들어 로그 수집 시스템:

- Agent 프로세스가 TCP로 로그를 보내고,
- Collector 프로세스는 디스크에 쓰면서 수신.

Collector가 I/O 병목으로 느려지면:

- 수신 애플리케이션이 `read()` 호출을 늦게 하게 되고,
- 수신 버퍼가 가득 차면 RcvWindow가 0에 가까워진다.
- 결국 Agent 쪽 송신 TCP는 전송을 조절하게 되고,
- 상위 애플리케이션은 “왜 전송 속도가 느려지지?” 하고 의아해할 수 있다.

결론:

- TCP의 플로우 제어는 **수신 애플리케이션의 처리 속도까지 포함해 전체 시스템의 병목을 반영**한다.
- 고성능 서버에서는 `SO_RCVBUF`(receive buffer 크기) 튜닝, 애플리케이션 read 패턴, 백프레셔(backpressure) 설계가 모두 중요하다.

---

## Error Control in TCP

이제 TCP의 **에러 제어(error control)** 를 보자.
에러 제어는 크게 두 가지 구성 요소를 가진다.

1. **에러 검출(error detection)**
   - 세그먼트가 손상되었는지 확인하는 것.
2. **에러 복구(error recovery)**
   - 손실되거나 손상된 세그먼트를 다시 받도록 하는 것.

---

### 에러 검출: TCP 체크섬

TCP는 **16비트 1의 보수(ones’ complement) 합**을 이용한 체크섬을 사용한다.

- 체크섬 계산 대상:
  1. TCP 헤더
  2. TCP 데이터
  3. IP pseudo-header (출발지/목적지 IP 등 일부 필드)

계산 과정(개념):

1. 16비트 단위로 쪼개서 모두 더한다(1의 보수 덧셈).
2. 나온 결과를 다시 1의 보수(비트 반전)한다.
3. 이 값을 TCP 헤더의 `Checksum` 필드에 넣는다.
4. 수신 측도 같은 방식으로 계산한 후,
   - 결과가 `0xFFFF` (1의 보수 합이 모두 1) 가 아니면
     “어딘가 손상된 것”으로 보고 세그먼트를 폐기한다.

TCP 체크섬은 **CRC만큼 강력하지는 않다**는 것이 잘 알려져 있다.
그러나 IP/TCP/애플리케이션 레벨에서 **중복 체크**가 가능한 환경에서는 여전히 널리 사용되고 있고,
NIC 하드웨어 오프로드 기능 덕분에 성능도 크게 문제되지 않는다.

---

### 에러 복구: ARQ 기반 재전송

TCP는 본질적으로 **ARQ(Automatic Repeat reQuest)** 프로토콜의 일종이다.

- 모든 데이터는 **시퀀스 번호**를 갖고 전송된다.
- 수신자는 **누적 ACK(cumulative ACK)** 를 보낸다.
- 손실이 의심되면 송신자는 **재전송(retransmission)** 을 수행한다.

에러 복구의 기본 시나리오:

1. 송신자가 세그먼트를 전송하고 **재전송 타이머(RTO)** 를 설정한다.
2. 수신자가 정상 수신하면 ACK를 반환하고, 송신자는 해당 구간을 “정상 수신”으로 표시한다.
3. RTO가 만료될 때까지 해당 세그먼트에 대한 ACK가 도착하지 않으면,
   - 세그먼트가 손실되었거나 ACK가 손실되었다고 보고 **재전송**.
4. 이 과정을 반복하면서 결국 해당 자료를 정상적으로 전달하려고 한다.

---

### 재전송 타임아웃(RTO)와 RTT 추정

RTO를 정하는 것은 에러 제어에서 매우 중요하다.

- 너무 짧으면 → 잠깐 지연만 된 세그먼트도 “손실”로 오인 → 불필요한 재전송, 네트워크 부하 증가.
- 너무 길면 → 진짜 손실 발생 시 복구가 느려져 처리율 급락.

그래서 TCP는 **RTT(왕복 지연 시간)** 를 측정해 RTO를 동적으로 조정한다.

기본 아이디어(지수 평균 + 분산):

1. 새 RTT 샘플 \(RTT_{\text{sample}}\) 을 얻을 때마다
2. 지수 이동 평균으로 SRTT(Smoothed RTT)를 업데이트:

   $$
   SRTT \leftarrow (1 - \alpha) \cdot SRTT + \alpha \cdot RTT_{\text{sample}}
   $$

   일반적으로 \(\alpha \approx 1/8\) 정도 값이 사용된다.

3. 동일하게 RTT 변동성(편차)도 추정해서,
4. RTO를 대략 **SRTT + K · DevRTT** 정도로 설정한다(자세한 공식은 RFC 6298에 정의).

이렇게 하면 네트워크 지연 변화에 어느 정도 적응하면서도 안정적인 RTO를 유지할 수 있다.

---

### Fast Retransmit & Fast Recovery

단순히 타임아웃만 기다리는 방식은 너무 느리다.
그래서 TCP에는 **Fast Retransmit / Fast Recovery** 메커니즘이 있다.

핵심 아이디어:

1. 수신자가 패킷을 순서대로 받다가 “중간 하나”가 빠지면,
   - 그 이후 도착한 세그먼트들은 **중복 ACK(duplicate ACK)** 를 유발한다.
2. 예를 들어 송신자가

   - 세그먼트 1, 2, 3, 4를 보냈는데,
   - 2가 손실, 3과 4는 도착했다고 하자.

3. 수신자는 3, 4를 받았지만 여전히 **다음에 기대하는** 시퀀스는 2이므로,
   - ACK 번호를 계속 2에 맞춰 보낸다(중복 ACK).
4. 송신자는 “연속된 중복 ACK가 3개 이상” 발생하면,
   - 타임아웃을 기다리지 않고 **해당 세그먼트를 즉시 재전송(Fast Retransmit)** 한다.
5. 그러고 나서 윈도우를 일시적으로 줄이고, 다시 점진적으로 늘리는 과정을 **Fast Recovery**라고 부른다.

이 메커니즘 덕분에, 누적 ACK만 사용하는 상황에서도 **하나의 패킷 손실**은 매우 빠르게 복구할 수 있다.

---

### – 다중 손실에서의 향상된 에러 제어

누적 ACK 기반의 TCP는 **한 RTT에 사실상 “하나의 손실”만 직접 감지**할 수 있다.

- 여러 세그먼트가 한꺼번에 손실되면,
  - 송신자는 한 번에 하나씩만 빠르게 재전송하고,
  - 나머지는 결국 타임아웃까지 기다리게 되는 경우가 많다.

이를 개선하기 위해 도입된 것이 **SACK(Selective Acknowledgment, RFC 2018)** 이다.

아이디어:

- 수신자는 “어떤 구간의 데이터는 받았고, 어떤 구간은 못 받았는지”를 **SACK 블록**으로 상세하게 알려준다.
- 예:

  - 전달된 데이터 범위:
    - [1000, 1499], [2000, 2499], [3000, 3499]
  - 손실된 범위:
    - [1500, 1999]

- 수신자는 ACK 번호 외에,
  “지금까지 받은 비연속 구간들”을 `SACK=[(1000,1500), (2000,2500), (3000,3500)]` 등의 형태로 보낸다.
- 송신자는 이 정보를 보고 **정확히 손실된 범위만 골라서 재전송**할 수 있다.
  - 사실상 Selective Repeat ARQ에 가까운 동작을 하게 된다.

#### 예시 5: SACK의 효과

상황:

- 대용량 데이터 전송 중,
- 하나의 윈도우 안에서 3개 세그먼트가 손실되었다.

1. **SACK 없는 경우**

   - 송신자는 3중복 ACK로 첫 번째 손실만 빠르게 재전송.
   - 나머지 2개 손실은 타임아웃이 날 때까지 잘 모름.
   - 결국 **여러 RTT 동안 비효율적인 복구**를 하게 된다.

2. **SACK 있는 경우**

   - 수신자는 “이 구간, 이 구간은 받았고, 이 구간만 비었다”고 상세히 알려준다.
   - 송신자는 곧바로 3개의 손실 구간을 모두 재전송 가능.
   - RTT 1~2번 안에 전체 복구가 끝날 수 있다.

고손실·고지연 환경(예: 일부 위성 링크, 무선 링크)에서는 SACK의 효과가 매우 크다.

---

### 에러 제어와 혼잡 제어의 관계

에러 제어는 “손실/손상된 데이터를 다시 보내는 것”이지만,
TCP는 **손실을 “혼잡의 신호”로도 해석**한다.

- 패킷 손실 → 라우터 큐가 넘쳐서 버려졌을 가능성이 큼 → 네트워크 포화 상태.
- 따라서 loss 이벤트가 발생하면,
  혼잡 윈도우(cwnd)를 줄여서 네트워크를 보호해야 한다.

전통적인 TCP Reno 모델:

1. Fast Retransmit 발생 시:
   - cwnd를 절반으로 줄이고,
   - 혼잡 회피 모드로 들어간다.
2. RTO 타임아웃 발생 시:
   - 훨씬 더 강하게 cwnd를 줄이고 (예: 1 MSS),
   - 슬로우 스타트로 다시 시작.

즉, **에러 제어(재전송)** 와 **혼잡 제어(윈도우 조절)** 는
서로 강하게 연결되어 있어 함께 이해해야 한다.

---

### 간단한 에러 제어 시뮬레이션 코드 예시

아래는 매우 단순화된 파이썬 코드로,
손실률을 적용해 “몇 번 재전송이 필요한지” 감을 잡기 위한 장난감 예시이다.

```python
import random

def send_with_loss(num_segments, loss_prob=0.1):
    attempts = 0
    for seg in range(num_segments):
        delivered = False
        while not delivered:
            attempts += 1
            if random.random() >= loss_prob:
                delivered = True
    return attempts

for loss in [0.01, 0.05, 0.1]:
    attempts = send_with_loss(100, loss_prob=loss)
    print(f"Loss {loss*100:.0f}%: 100 segments -> {attempts} transmissions")
```

- 실제 TCP는 RTT, 윈도우, SACK, Fast Retransmit 등 훨씬 복잡한 메커니즘을 사용하지만,
- 손실률이 커질수록 **재전송 횟수가 기하급수적으로 늘어나고 처리율이 떨어진다**는 점은 이 정도 장난감 모델로도 직관을 잡을 수 있다.

---

## 전체 정리

이번 글에서 다룬 핵심을 정리하면 다음과 같다.

1. **윈도우**
   - TCP는 슬라이딩 윈도우 기반으로 동작하며,
     수신 윈도우(rwnd), 혼잡 윈도우(cwnd)를 통해 실제 송신 윈도우를 결정한다.
   - Window Scale 옵션으로 64KB 한계를 넘어 최대 1GB까지 윈도우를 키워
     고속·고지연 환경에서도 효율적으로 동작할 수 있다.
   - 윈도우 크기와 RTT의 곱인 BDP는 **최대 처리율을 결정하는 중요한 지표**다.

2. **플로우 제어**
   - 목적은 **수신자 보호**이다.
   - 수신자는 수신 버퍼 여유에 따라 RcvWindow를 광고하고,
     송신자는 그 범위 내에서만 in-flight 데이터를 유지한다.
   - 버퍼가 꽉 차면 Zero Window를 광고하고, 송신자는 Window Probe로 상태를 주기적으로 확인한다.
   - 애플리케이션의 `read()` 패턴과 버퍼 크기 튜닝은 플로우 제어 성능에 직접적인 영향을 준다.

3. **에러 제어**
   - 에러 검출은 TCP 체크섬으로, 손상된 세그먼트를 폐기한다.
   - 에러 복구는 ARQ 기반 재전송, 타임아웃, Fast Retransmit/Fast Recovery, SACK 등으로 이루어진다.
   - 단순 누적 ACK만으로는 여러 세그먼트 손실에 취약하며,
     SACK은 이를 크게 보완하여 Selective Repeat에 가까운 효율을 제공한다.
   - 손실은 혼잡의 신호로 해석되어 cwnd 조정을 유도하므로,
     에러 제어와 혼잡 제어는 강하게 결합된 하나의 메커니즘으로 봐야 한다.

이 내용을 앞서 정리한 **TCP 서비스/세그먼트/상태 머신**과 함께 묶어보면,
TCP는 “**윈도우·타이머·ACK·재전송**”이라는 비교적 단순한 원리를 조합해
오늘날 인터넷의 대부분 트래픽을 “적당히 빠르고, 꽤 안정적으로” 전달하고 있음을 이해할 수 있다.
