---
layout: post
title: 기계학습 - 로지스틱 회귀
date: 2025-08-18 20:25:23 +0900
category: 기계학습
---
# 로지스틱 회귀(Logistic Regression)

**로지스틱 회귀(Logistic Regression)**는 이름에 "회귀"가 들어가지만, 실제로는 **분류(Classification)** 문제에 사용되는 지도학습 알고리즘입니다.  
특히 **이진 분류(Binary Classification)**에서 널리 쓰이며, 확률을 예측한 후 이를 기준으로 클래스를 결정합니다.

---

## 1. 개념

### (1) 정의
- 입력 특징(Feature)과 출력(Label) 사이의 관계를 모델링하여 **클래스에 속할 확률**을 예측
- 선형 회귀와 달리 예측 결과를 **0~1 사이의 확률값**으로 변환
- 예측 확률이 **임계값(Threshold)**보다 크면 한 클래스, 작으면 다른 클래스로 분류

---

### (2) 기본 아이디어
1. 선형 조합 계산:
$$
z = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
$$
2. 시그모이드(Sigmoid) 함수를 사용하여 확률로 변환:
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$
3. 확률 값 \(\hat{y} = \sigma(z)\)를 기준으로 분류:
- \(\hat{y} \ge 0.5 \) → 클래스 1
- \(\hat{y} < 0.5 \) → 클래스 0

---

## 2. 수학적 표현

### (1) 시그모이드 함수
- 출력 범위: (0, 1)
- S자(S-shape) 곡선
- 수식:
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$
- 특징: 입력이 매우 크거나 작으면 포화(Saturation)되어 기울기가 0에 가까워짐 (Vanishing Gradient 현상)

---

### (2) 로지스틱 회귀 모델
- 확률 예측:
$$
P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b)
$$
- 로그 오즈(Log Odds):
$$
\log \frac{P(y=1|\mathbf{x})}{1 - P(y=1|\mathbf{x})} = \mathbf{w}^T \mathbf{x} + b
$$
  → 확률을 선형 함수로 연결해 해석 가능

---

## 3. 손실 함수 (Log Loss)

로지스틱 회귀는 **우도 최대화(MLE)** 기반으로 학습하며, 손실 함수로 **로그 손실(Log Loss)** 또는 **이진 크로스 엔트로피(Binary Cross-Entropy)**를 사용합니다.

$$
L(\mathbf{w}, b) = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

여기서:
- \(y_i\): 실제 값 (0 또는 1)
- \(\hat{y}_i\): 예측 확률

---

## 4. 모델 학습

### (1) 경사하강법(Gradient Descent)
- 손실 함수를 최소화하도록 가중치 업데이트
$$
w_j := w_j - \alpha \frac{\partial L}{\partial w_j}
$$

### (2) 확률적 경사하강법(SGD)
- 한 개 혹은 소규모 배치 데이터로 기울기 계산
- 대규모 데이터에서 효율적

---

## 5. 다중 클래스 확장

로지스틱 회귀는 **소프트맥스 회귀(Softmax Regression)**로 확장하여 **다중 클래스 분류(Multi-class Classification)** 가능:
$$
P(y=c|\mathbf{x}) = \frac{e^{\mathbf{w}_c^T \mathbf{x}}}{\sum_{k=1}^K e^{\mathbf{w}_k^T \mathbf{x}}}
$$

---

## 6. 장단점

### 장점
- 구현 간단, 해석 용이 (가중치 → 변수 영향도 해석 가능)
- 계산 효율적, 고차원 데이터에도 적용 가능
- 확률 예측 가능 → 의사결정 활용 가능

### 단점
- 비선형 결정 경계(Decision Boundary) 학습 어려움
- 특성 간 다중공선성에 민감
- 이상치에 영향 받음

---

## 7. 파이썬 예제
```python
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# 데이터 생성
X, y = make_classification(n_samples=1000, n_features=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 모델 학습
model = LogisticRegression()
model.fit(X_train, y_train)

# 예측
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# 평가
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

---

## 8. 활용 사례
- **이메일 스팸 필터** (스팸 / 정상)
- **의료 진단** (양성 / 음성)
- **금융 사기 탐지**
- **마케팅** (구매 / 비구매 고객 예측)
- **소셜 미디어 분석** (긍정 / 부정 감성 분석)

---

## 📌 정리
- 로지스틱 회귀는 **분류 문제를 확률적으로 풀어내는 선형 모델**
- 시그모이드 함수로 출력 확률 변환
- 손실 함수로 **로그 손실(Log Loss)** 사용
- 다중 클래스 분류는 소프트맥스 회귀로 확장 가능
- 단순하지만 해석이 용이하고, 많은 산업 분야에서 기본 분류 모델로 활용됨