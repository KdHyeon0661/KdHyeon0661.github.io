---
layout: post
title: 기계학습 - 로지스틱 회귀
date: 2025-08-18 20:25:23 +0900
category: 기계학습
---
# 로지스틱 회귀(Logistic Regression)

## 로지스틱 회귀의 개념 재정의

### (1) 무엇을 풀고 싶은가?

이진 분류에서 정답 \(y\in\{0,1\}\) 에 대해
$$
P(y=1\mid \mathbf{x}) \equiv \hat{p}(\mathbf{x}) \in (0,1)
$$
을 직접 예측하고, 임계값(기본 0.5)으로 클래스를 정한다.

### (2) 선형 점수 → 확률로

선형 점수 \(z = \mathbf{w}^\top \mathbf{x} + b\) 를 **시그모이드**에 통과:
$$
\sigma(z) = \frac{1}{1+e^{-z}},\qquad
\hat{p} = \sigma(z).
$$
결정 경계는 \(\hat{p}=0.5\ \Rightarrow\ z=0\) 이므로 **선형(초평면)**이다.

### (3) 로짓(로그오즈) 해석

$$
\log\frac{\hat{p}}{1-\hat{p}} = \mathbf{w}^\top\mathbf{x}+b.
$$
\(\,w_j\) 는 \(x_j\) 가 1 단위 증가할 때 **odds가 \(\exp(w_j)\) 배**가 됨을 의미(해석 가능성의 핵심).

---

## 손실: 이항 로지스틱의 음의 로그가능도 = BCE

표본 \(\{(\mathbf{x}_i,y_i)\}_{i=1}^n\) 이 독립이고 \(y_i\sim \mathrm{Bernoulli}(p_i)\), \(p_i=\sigma(\mathbf{w}^\top \mathbf{x}_i+b)\) 라면, (평균) **로그 손실**은
$$
\mathcal{L}(\mathbf{w},b) = -\frac{1}{n}\sum_{i=1}^n \Big[\, y_i\log p_i + (1-y_i)\log(1-p_i)\,\Big].
$$

### (1) 그래디언트(벡터화)

편의상 바이어스는 확장특징 \(x_0=1\) 로 흡수. \(X\in\mathbb{R}^{n\times d}\), \(\mathbf{p}=\sigma(X\mathbf{w})\), \(\mathbf{y}\in\{0,1\}^n\) 이면
$$
\nabla \mathcal{L}(\mathbf{w}) = \frac{1}{n} X^\top(\mathbf{p}-\mathbf{y}).
$$

### (2) 헤시안(양의 준정)과 **볼록성**

$$
\nabla^2 \mathcal{L}(\mathbf{w}) = \frac{1}{n} X^\top W X,\quad
W=\mathrm{diag}(p_i(1-p_i))\succeq 0.
$$
따라서 **엄밀하게 볼록(convex)** → 전역최적해 보장(분리 문제가 심할 때 예외적 현상은 §7 참고).

---

## 최적화: GD/SGD, L-BFGS, Newton/IRLS, SAGA

| 방법 | 핵심 | 장점 | 주의 |
|---|---|---|---|
| 배치 GD | \(\mathbf{w}\leftarrow \mathbf{w}-\alpha\nabla\mathcal{L}\) | 단순 | 느린 수렴 |
| SGD / Mini-batch | 무작위 표본/배치로 갱신 | 대규모 데이터 효율 | 하이퍼파라미터 민감 |
| L-BFGS | 준뉴턴, 이력으로 헤시안 근사 | 빠른 수렴, 안정 | 메모리/스케일링 필요 |
| Newton/IRLS | \(\Delta=\big(X^\top W X\big)^{-1}X^\top(\mathbf{y}-\mathbf{p})\) | 2차 수렴 | 대규모시 비쌈 |
| SAG/SAGA | 확률적 평균 그라디언트(가속) | 큰 데이터 효율 | L1에는 SAGA 권장 |

> **scikit-learn** 기본: 소형/중형엔 `lbfgs`, **L1/ElasticNet**엔 `saga`, 희소·고차원엔 `liblinear` 또는 `saga`.

---

## 정규화(Regularization): 과적합 제어와 해석

### (1) L2 (Ridge)

$$
\min_{\mathbf{w}} \ \mathcal{L}(\mathbf{w}) + \lambda \|\mathbf{w}\|_2^2
\quad(\text{sklearn은 }C=1/\lambda).
$$
- 가우시안 사전(베이지안)와 등가.
- 다중공선성 완화, 안정적.

### (2) L1 (Lasso)

$$
\min_{\mathbf{w}} \ \mathcal{L}(\mathbf{w}) + \lambda \|\mathbf{w}\|_1.
$$
- 라플라스 사전.
- **가중치 희소화** → **특징 선택** 효과.

### (3) Elastic Net

$$
\mathcal{L} + \lambda\big(\alpha\|\mathbf{w}\|_1 + (1-\alpha)\|\mathbf{w}\|_2^2\big),
$$
- 상관 피처가 많은 상황에서 **그룹 유지 + 희소화** 절충.

> 실무 팁: **표준화 필수**, 하이퍼 \(C\)·\(\alpha\)는 **로그스케일 CV**로 탐색.

---

## 수치 안정성과 분리 문제

- **수치 안정화**:
  \(\log(1+e^{z})\) 는 큰 \(z\)에서 overflow → **log-sum-exp** 트릭 사용.
  예: \(\log(1+e^{-z}) = \max(0,-z) + \log(1+e^{-|z|})\).

- **완전 분리(complete separation)**:
  선형 경계로 훈련데이터 완벽 분리 가능 시 MLE이 발산(계수 \(\to\infty\)).
  **대응**: L2 정규화(일반적 해결책), 소표본이면 Firth 보정(편향 감소) 참고.

---

## 해석(Explainability): 계수, 오즈비, 신뢰구간

### (1) 오즈비(odds ratio)

\(w_j\) 해석: \(x_j\) 1 증가 시
$$
\frac{\hat{p}/(1-\hat{p})\ \text{(after)}}{\hat{p}/(1-\hat{p})\ \text{(before)}} = e^{w_j}.
$$

### (2) 표준오차/신뢰구간(대략)

헤시안 \(H = \frac{1}{n}X^\top W X\). 공분산 \(\Sigma \approx H^{-1}\).
Wald 95% CI for \(w_j\): \(w_j \pm 1.96\sqrt{\Sigma_{jj}}\).
오즈비 CI: \(\exp(\text{CI for }w_j)\). (소표본/강한 정규화 시 해석 주의)

---

## 클래스 불균형·임계값·확률 보정

### (1) 불균형 대응

- `class_weight='balanced'` 또는 **가중 손실**.
- 임계값 조정: **Youden’s J**(TPR−FPR 최대), **비용 민감** 기준 적용.
- **PR AUC**(희귀 양성) 지표로 모니터링.

### (2) 확률 보정(calibration)

- 로지스틱은 비교적 잘 보정되지만, 정규화·고차원·강한 불균형 시 왜곡.
- **Platt scaling**(sigmoid) / **Isotonic Regression** → `CalibratedClassifierCV`.

---

## 다중 클래스: OVR vs Multinomial(Softmax)

### (1) OVR(One-vs-Rest)

클래스 \(K\) 개면 **K개 이진 로지스틱**. 단순, 불균형 대응 용이.

### (2) **Multinomial(Softmax)** (권장)

$$
P(y=c\mid \mathbf{x}) = \frac{\exp(\mathbf{w}_c^\top\mathbf{x})}{\sum_{k=1}^K \exp(\mathbf{w}_k^\top\mathbf{x})}.
$$
- 손실: **다중크로스엔트로피(음의 로그가능도)**,
- 솔버: `lbfgs` or `saga`(L1/EN).

---

## 다른 모델과의 관계

- **SVM**: 힌지손실 vs 로지스틱손실(둘 다 마진기반, 로지스틱은 확률출력/매끄러움).
- **LDA**: 클래스별 **공분산 동일 가우시안** 가정 ⇒ **선형 경계**, 로지스틱과 유사한 결정함수.
- **나이브 베이즈**: 독립가정하의 생성모형; 충분통계로 보면 로지스틱의 선형판별식으로 귀결되기도 한다(특정 가정하).

---

## 평가 지표·분석

| 지표 | 정의/의미 | 사용 포인트 |
|---|---|---|
| 정확도 | \((TP+TN)/N\) | 불균형에 취약 |
| PR AUC | 정밀도-재현율 곡선 면적 | 희귀 양성 중요 |
| ROC AUC | TPR 대 FPR | 임계값 독립 |
| F1 | 조화평균 \(2PR/(P+R)\) | 불균형·균형 trade-off |
| Brier score | 확률 정확도 | 보정(칼리브레이션) 체크 |

---

## 파이썬 실전 — 파이프라인·튜닝·해석

### (A) 이진 분류: 스케일링 + L2 + ROC/PR + 임계값 조정

```python
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    roc_auc_score, average_precision_score, precision_recall_curve,
    roc_curve, classification_report
)

# 데이터 (불균형 예)

X, y = make_classification(
    n_samples=4000, n_features=20, n_informative=8, n_redundant=4,
    weights=[0.9, 0.1], flip_y=0.01, random_state=42
)

Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)

pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(
        penalty="l2", solver="lbfgs", max_iter=200, class_weight="balanced"
    ))
])

param_grid = {"lr__C": np.logspace(-3, 2, 10)}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
gs = GridSearchCV(pipe, param_grid, scoring="average_precision", cv=cv, n_jobs=-1)
gs.fit(Xtr, ytr)

best = gs.best_estimator_
proba = best.predict_proba(Xte)[:,1]
roc = roc_auc_score(yte, proba)
ap = average_precision_score(yte, proba)
print("Best C:", gs.best_params_["lr__C"])
print("ROC AUC:", roc, "PR AUC:", ap)

# 임계값 튜닝 (Youden's J)

fpr, tpr, thr = roc_curve(yte, proba)
j_scores = tpr - fpr
t_opt = thr[np.argmax(j_scores)]
yp = (proba >= t_opt).astype(int)
print("Optimal threshold:", t_opt)
print(classification_report(yte, yp, digits=3))
```

### (B) **해석**: 계수 → 오즈비(odds ratio)

```python
import pandas as pd
lr = best.named_steps["lr"]
coef = lr.coef_.ravel()  # 이진의 경우 (1, d)
odds = np.exp(coef)
df = pd.DataFrame({"coef": coef, "odds_ratio": odds})
print(df.sort_values("odds_ratio", ascending=False).head(10))
```

### (C) **L1(희소화)** & ElasticNet (`saga`)

```python
from sklearn.linear_model import LogisticRegression

l1_model = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(
        penalty="l1", solver="saga", max_iter=500, C=0.5, class_weight="balanced"
    ))
]).fit(Xtr, ytr)

en_model = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(
        penalty="elasticnet", l1_ratio=0.5, solver="saga",
        max_iter=500, C=1.0, class_weight="balanced"
    ))
]).fit(Xtr, ytr)
```

### (D) **확률 보정**(CalibratedClassifierCV)

```python
from sklearn.calibration import CalibratedClassifierCV, calibration_curve

base = Pipeline([
  ("scaler", StandardScaler()),
  ("lr", LogisticRegression(penalty="l2", solver="lbfgs", max_iter=200))
]).fit(Xtr, ytr)

cal = CalibratedClassifierCV(base, method="isotonic", cv=5)
cal.fit(Xtr, ytr)
proba_cal = cal.predict_proba(Xte)[:,1]
print("Brier (uncalibrated):", np.mean((base.predict_proba(Xte)[:,1]-yte)**2))
print("Brier (calibrated):  ", np.mean((proba_cal - yte)**2))
```

### (E) **다중 클래스**: Multinomial(Softmax)

```python
from sklearn.datasets import make_classification
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, log_loss

X, y = make_classification(n_samples=3000, n_features=30, n_informative=10,
                           n_classes=5, n_clusters_per_class=2, random_state=0)

Xtr, Xte, ytr, yte = train_test_split(X, y, stratify=y, test_size=0.25, random_state=1)

softmax = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(
        multi_class="multinomial", solver="lbfgs", penalty="l2",
        C=1.0, max_iter=300
    ))
]).fit(Xtr, ytr)

yp = softmax.predict(Xte)
proba = softmax.predict_proba(Xte)
print("Accuracy:", accuracy_score(yte, yp), "LogLoss:", log_loss(yte, proba))
```

### (F) **직접 구현**: 벡터화된 배치 GD(시그모이드/안정화)

```python
import numpy as np

def sigmoid(z):
    # 수치 안정화: z>0/<=0 분할
    pos = z >= 0
    neg = ~pos
    out = np.empty_like(z, dtype=float)
    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))
    ez = np.exp(z[neg])
    out[neg] = ez / (1.0 + ez)
    return out

def logistic_loss_grad(X, y, w, lam=0.0):
    z = X @ w
    p = sigmoid(z)
    n = X.shape[0]
    loss = -np.mean(y*np.log(p+1e-12) + (1-y)*np.log(1-p+1e-12)) + lam*np.sum(w[1:]**2)
    grad = (X.T @ (p - y)) / n
    grad[1:] += 2*lam*w[1:]  # 바이어스 제외 L2
    return loss, grad

rng = np.random.default_rng(42)
n, d = 2000, 15
X = rng.normal(size=(n, d))
true_w = rng.normal(size=(d+1,))
Xb = np.c_[np.ones(n), X]
p = 1/(1+np.exp(-(Xb@true_w)))
y = rng.binomial(1, p)

w = np.zeros(d+1)
lr = 0.5
for it in range(200):
    loss, grad = logistic_loss_grad(Xb, y, w, lam=1e-3)
    w -= lr * grad
    if it % 20 == 0:
        print(f"iter {it:3d} loss {loss:.4f}")
```

---

## 특성 엔지니어링·전처리

- **스케일링 필수**(표준화): 정규화 없는 계수 해석·수렴 저하.
- **범주형**: 원-핫 인코딩(고차원 시 L1 유리).
- **상호작용/다항특징**: **결정경계 비선형화**(다만 해석/과적합 주의).
- **다중공선성**: VIF 확인, L2로 완화.
- **누수(leakage)**: 파이프라인으로 `fit` 시 훈련셋만 사용(스케일러/인코더 포함).

---

## 튜닝 체크리스트

1. **solver**: `lbfgs`(일반), `saga`(L1/EN/희소), `liblinear`(작은 데이터, L1/L2).
2. **penalty & C**: `C`는 규제의 역수 — 로그스케일 탐색.
3. **class_weight**: 심한 불균형이면 `'balanced'` 또는 가중치 지정.
4. **평가 지표**: ROC AUC + PR AUC + F1(업무 비용 반영).
5. **임계값**: 고정 0.5 대신, **업무 손실** 기준 최적화.
6. **칼리브레이션**: 불균형/강정규화 시 `CalibratedClassifierCV`.
7. **설명가능성**: 계수→오즈비, 표준오차로 CI(참고).
8. **분리/폭주**: 정규화 또는 데이터 증강/피처 재설계.

---

## 표 — 손실/그라디언트/헤시안 요약

| 항목 | 식 |
|---|---|
| 시그모이드 | $$\sigma(z)=\frac{1}{1+e^{-z}},\quad \sigma'(z)=\sigma(z)\big(1-\sigma(z)\big)$$ |
| BCE 손실 | $$\mathcal{L}=-\frac{1}{n}\sum_i\big[y_i\log p_i+(1-y_i)\log(1-p_i)\big]$$ |
| 그래디언트 | $$\nabla\mathcal{L}=\frac{1}{n}X^\top(\mathbf{p}-\mathbf{y})$$ |
| 헤시안 | $$\nabla^2\mathcal{L}=\frac{1}{n}X^\top W X,\ W_{ii}=p_i(1-p_i)$$ |
| L2 추가 | $$\nabla\mathcal{L}_{L2}=\nabla\mathcal{L}+2\lambda w_{\neg b}$$ |

---

## 요약

- 로지스틱 회귀는 **선형 점수 + 시그모이드**로 **확률**을 직접 예측하는 **볼록** 최적화 모델.
- 손실은 **이항 로그손실(BCE)**, 그래디언트/헤시안이 간단해 **빠르고 안정**.
- **정규화(L1/L2/EN)** 로 과적합/공선성/특징선택을 제어.
- **불균형·임계값·보정**을 실무 손실에 맞게 조정.
- 다중클래스는 **Multinomial(Softmax)** 권장, L1/EN엔 `saga`.
- 계수는 **오즈비**로 해석 가능(표준오차로 CI).
- 스케일링/파이프라인/교차검증/업무비용 반영이 **실전 성패**를 가른다.
