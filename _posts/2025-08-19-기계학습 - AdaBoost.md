---
layout: post
title: 기계학습 - AdaBoost
date: 2025-08-19 20:25:23 +0900
category: 기계학습
---
# ⚡ AdaBoost (Adaptive Boosting)

## 1. 개요
**AdaBoost (Adaptive Boosting)**는 가장 오래되고 대표적인 **부스팅(boosting)** 기법 중 하나입니다.  
"Adaptive"라는 이름처럼, 이전 단계에서 잘못 분류된 샘플에 **가중치를 더 크게 부여**하여 다음 학습기가 이를 더 집중적으로 학습하도록 하는 방식입니다.  

- 기본 아이디어:  
  "이전 학습기가 놓친 부분을 다음 학습기가 보완하도록 만들자!"

- 약한 학습기(weak learner): 보통 깊이가 1인 **결정 stump(Decision Stump)**를 사용  

---

## 2. 알고리즘 직관
1. 모든 데이터에 **동일한 가중치** 부여 → 균등 학습 시작  
2. 약한 학습기 학습 → 잘못 분류된 샘플의 가중치를 **증가**  
3. 다음 학습기가 **더 어려운 데이터에 집중**  
4. 여러 개의 약한 학습기를 **가중합(weighted sum)**으로 결합하여 강한 학습기(strong learner) 완성  

---

## 3. 알고리즘 수학적 절차

### 3.1 초기화
- 데이터셋: \((x_1, y_1), \dots, (x_n, y_n)\), 레이블 \(y_i \in \{-1, +1\}\)  
- 가중치 초기화:
  \[
  w_i^{(1)} = \frac{1}{n}, \quad i=1, \dots, n
  \]

---

### 3.2 각 반복 단계 (\(m=1, \dots, M\))
1. **약한 학습기 \(h_m(x)\) 학습**  
   - 데이터 가중치 \(w_i^{(m)}\)를 반영하여 학습  

2. **가중 오차율(error) 계산**  
   \[
   \varepsilon_m = \frac{\sum_{i=1}^n w_i^{(m)} \cdot \mathbf{1}(h_m(x_i) \neq y_i)}{\sum_{i=1}^n w_i^{(m)}}
   \]

3. **학습기 가중치 \(\alpha_m\) 계산**  
   \[
   \alpha_m = \frac{1}{2} \ln \left( \frac{1 - \varepsilon_m}{\varepsilon_m} \right)
   \]
   → 성능이 좋은 학습기일수록 \(\alpha_m\) 값이 커짐  

4. **샘플 가중치 업데이트**  
   \[
   w_i^{(m+1)} = w_i^{(m)} \cdot \exp\left(-\alpha_m y_i h_m(x_i)\right)
   \]
   - 올바른 예측 → 가중치 감소  
   - 잘못된 예측 → 가중치 증가  

5. 정규화:
   \[
   w_i^{(m+1)} = \frac{w_i^{(m+1)}}{\sum_{j=1}^n w_j^{(m+1)}}
   \]

---

### 3.3 최종 모델
최종 예측은 약한 학습기의 **가중 합**으로 결정됩니다:

\[
H(x) = \text{sign} \left( \sum_{m=1}^M \alpha_m h_m(x) \right)
\]

---

## 4. 특징
- 약한 학습기의 성능이 조금이라도 무작위보다 좋으면(\(\varepsilon < 0.5\)) 점차적으로 성능을 강화 가능  
- 이상치(outlier)에 민감 (가중치가 계속 커져 영향력 ↑)  
- 모델이 직관적이고 수학적으로 깔끔  

---

## 5. 파이썬 구현 예제

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 준비
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# AdaBoost 분류기
clf = AdaBoostClassifier(
    n_estimators=100,    # 약한 학습기 개수
    learning_rate=0.5,   # 학습률
    random_state=42
)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

## 6. 장단점
✅ 장점
- 간단하고 구현이 용이
- 이론적으로 잘 정립됨 (boosting의 시초적 역할)
- 과적합에 비교적 강함 (적당한 트리 개수일 때)

❌ 단점
- 이상치에 민감 → 잘못된 데이터가 있으면 오히려 성능 저하
- 깊은 트리에는 잘 맞지 않음 (보통 Decision Stump 사용)

---

## 7. AdaBoost vs Gradient Boosting 비교

| 구분 | AdaBoost | Gradient Boosting |
|------|-----------|-------------------|
| 학습 방식 | 오분류 샘플에 가중치 증가 | 손실 함수의 **그래디언트** 기반 오차 보정 |
| 가중치 업데이트 | 데이터 샘플에 가중치 | 예측 함수에 직접 보정항 추가 |
| 약한 학습기 | 보통 Decision Stump(1-depth 트리) | 얕은 결정트리 (max_depth=3~5) |
| 이상치 영향 | 매우 민감 | 상대적으로 덜 민감 |
| 손실 함수 | 기본적으로 지수 손실(Exponential Loss) | 다양한 손실 함수 지원 (MSE, 로지스틱, 사용자 정의) |
| 직관성 | 단순, 이해하기 쉬움 | 수학적 최적화 기반, 유연성 높음 |
| 계산량 | 가벼움 | 더 무겁고 느림 |

---

## 📌 요약
- **AdaBoost**는 오분류 데이터에 가중치를 늘려, 다음 학습기가 이를 집중 학습하도록 하는 방식  
- **Gradient Boosting**은 손실 함수의 기울기를 활용하여 모델 전체를 점진적으로 최적화  
- AdaBoost는 단순하고 직관적이지만 이상치에 민감, Gradient Boosting은 더 유연하고 강력  

즉, AdaBoost는 **부스팅의 기본형**이고, Gradient Boosting은 **최적화 확장판**이라 볼 수 있습니다.