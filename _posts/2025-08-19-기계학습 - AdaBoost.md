---
layout: post
title: 기계학습 - AdaBoost
date: 2025-08-19 20:25:23 +0900
category: 기계학습
---
# AdaBoost (Adaptive Boosting)

## TL;DR

- AdaBoost는 **이전 단계가 틀린 샘플의 가중치를 키워** 다음 약한 학습기가 그 부분을 보완하게 하는 **가법 모형(additive model)** 이다.
- 바이너리(M1)의 핵심 갱신:
  $$
  \varepsilon_m=\frac{\sum_i w_i^{(m)}\mathbf{1}[h_m(x_i)\neq y_i]}{\sum_i w_i^{(m)}},\quad
  \alpha_m=\tfrac12\ln\frac{1-\varepsilon_m}{\varepsilon_m},\quad
  w_i^{(m+1)}\propto w_i^{(m)}e^{-\alpha_m y_i h_m(x_i)}.
  $$
- 최종 예측:
  $$
  F_M(x)=\sum_{m=1}^M\alpha_m h_m(x),\quad H(x)=\mathrm{sign}\big(F_M(x)\big).
  $$
- **지수 손실**을 단계적으로 최소화 ⇒ **마진**을 키우며 학습오류에 대한 상계가 급격히 줄어든다.
- 장점: 단순/강력/설명력(마진). 단점: **이상치/라벨노이즈에 민감**.

---

## 직관

- 초기에 모든 샘플 가중치 동일.
- 약한 분류기 \(h_1\) 학습 → 틀린 샘플 가중치 ↑ → \(h_2\)가 어려운 샘플에 집중 …
- 이렇게 얻은 여러 약한 학습기를 **가중합**(weight sum)하여 **강한 학습기**를 만든다.

---

## 알고리즘(AdaBoost.M1; 이진, \(y\in\{-1,+1\}\))

### 초기화

$$
w_i^{(1)}=\frac{1}{n},\qquad i=1,\dots,n.
$$

### 반복 \(m=1,\dots,M\)

1) **약한 학습기** \(h_m:\mathcal{X}\to\{-1,+1\}\) 를 **가중치 \(w^{(m)}\)** 로 학습.
2) **가중 오분류율**
   $$
   \varepsilon_m=\frac{\sum_{i=1}^n w_i^{(m)}\mathbf{1}\big(h_m(x_i)\neq y_i\big)}{\sum_{i=1}^n w_i^{(m)}}.
   $$
   - 조건: \( \varepsilon_m<0.5 \) (무작위보다 좋아야 함).
3) **학습기 가중치**
   $$
   \alpha_m=\tfrac12\ln\frac{1-\varepsilon_m}{\varepsilon_m}.
   $$
4) **샘플 가중치 갱신**
   $$
   w_i^{(m+1)}=w_i^{(m)}\exp\!\big(-\alpha_m y_i h_m(x_i)\big),\quad
   \text{정규화로 } \sum_i w_i^{(m+1)}=1.
   $$
   - 맞추면 \(e^{-\alpha_m}\) 배로 **감소**, 틀리면 \(e^{+\alpha_m}\) 배로 **증가**.

### 최종 예측

$$
F_M(x)=\sum_{m=1}^M\alpha_m h_m(x),\qquad
H(x)=\mathrm{sign}\big(F_M(x)\big).
$$

---

## 수학적 정당성: 지수손실·단계적 가법모형·마진

### 지수 손실 최소화 관점

가법 모형 \(F(x)=\sum_{m=1}^M \alpha_m h_m(x)\)에 대해 **지수 손실**:
$$
\mathcal{L}(F)=\sum_{i=1}^n \exp\!\big(-y_i F(x_i)\big).
$$
**전진 단계적 최적화**(forward stagewise):
\(F_{m-1}\)가 주어졌을 때 \(\alpha_m,h_m\)를 선택하여
$$
(\alpha_m,h_m)=\arg\min_{\alpha,h}\sum_i \exp\!\big(-y_i(F_{m-1}(x_i)+\alpha h(x_i))\big).
$$
가중치 \(w_i^{(m)}\propto \exp(-y_iF_{m-1}(x_i))\)로 두면,
위 식은 사실상 **가중 분류오차**를 최소화하고 \(\alpha_m=\frac12\ln\frac{1-\varepsilon_m}{\varepsilon_m}\)가 됨을 보일 수 있다.

### 정규화 상수와 학습오류 상계

정규화 상수(에너지 감소율):
$$
Z_m=\sum_i w_i^{(m)}\exp\big(-\alpha_m y_i h_m(x_i)\big)
= 2\sqrt{\varepsilon_m(1-\varepsilon_m)}.
$$
학습 종료 후 **훈련 0–1 오차**는
$$
\frac{1}{n}\sum_i \mathbf{1}\big(y_i\neq H(x_i)\big)
\le \frac{1}{n}\sum_i \exp\big(-y_i F_M(x_i)\big)
= \prod_{m=1}^M Z_m.
$$
즉, 각 단계에서 \(\varepsilon_m<0.5\)만 유지되면 \(Z_m<1\)이고 **오류 상계가 기하급수적으로 감소**.

### 마진(margin)

마진 정의:
$$
\gamma_i = y_i F_M(x_i).
$$
마진이 클수록 예측 신뢰가 높다. AdaBoost는 지수손실을 줄이면서 **마진 분포를 오른쪽으로 이동**시키며 일반화 성능 개선과 연결된다(마진 기반 일반화 이론).

---

## 변형과 다중클래스

### Real AdaBoost (confidence-rated)

약한 학습기가 확률 \(p(y|x)\)을 내면,
$$
h_m(x)=\tfrac12\ln\frac{p(y=+1|x)}{p(y=-1|x)},\qquad
F\leftarrow F+h_m.
$$
출력의 **크기**(로짓)가 불확실성을 반영.

### 다중클래스 AdaBoost — **SAMME / SAMME.R**

클래스 수 \(K\ge3\).

- **SAMME**(Discrete):
  약한 학습기 \(h_m:\mathcal{X}\to\{1,\dots,K\}\),
  $$\alpha_m=\ln\frac{1-\varepsilon_m}{\varepsilon_m}+\ln(K-1).$$
  최종 점수 \(F_k(x)=\sum_m \alpha_m \mathbf{1}[h_m(x)=k]\), 예측은 \(\arg\max_k F_k\).

- **SAMME.R**(Real):
  약한 학습기 확률 \(p_{m,k}(x)\) 사용,
  $$
  F_k(x)\leftarrow F_k(x)+\bigg(\frac{K-1}{K}\bigg)\left(\ln p_{m,k}(x) - \frac{1}{K}\sum_{j=1}^K\ln p_{m,j}(x)\right).
  $$
  연속 출력을 활용해 보통 **더 우수/안정**.

---

## 회귀형: **AdaBoost.R2**

- 손실(예: 절대오차)을 기반으로 샘플 가중치 갱신.
- 예측은 약한 회귀기의 **가중 중앙값/평균**.
- 이상치 민감 → **Huber/Quantile** 회귀약한기와 함께 쓰면 안정적.

---

## 하이퍼파라미터 & 실전 팁

### 핵심 하이퍼파라미터

| 파라미터 | 의미/권장 |
|---|---|
| `n_estimators` | 약한 학습기 개수. 50~400 시작, **validation**으로 조기중단. |
| `learning_rate` | \(\alpha_m\)에 곱하는 **수축(shrinkage)**. 0.05~0.5 탐색. 작을수록 더 많은 트리 필요. |
| 약한 학습기 | 보통 `DecisionTreeClassifier(max_depth=1)`(stump). 다소 깊게(2~3) 하면 표현력↑, 과적합 주의. |
| `algorithm` | `"SAMME"` vs `"SAMME.R"` (multiclass). 확률출력 가능하면 **SAMME.R** 권장. |
| `random_state` | 재현성 확보. |

### 클래스 불균형

- `class_weight` 또는 **초기 샘플 가중치**로 보정.
- `SAMME.R` + 보정된 확률 보정(Calibration) 병행.

### 이상치·노이즈 대응

- **라벨 검증/정제**, outlier clipping.
- **학습률↓ + 조기중단**, **가중치 상한(cap)**(이론적 AdaBoost와는 달리 실무적 트릭).
- 대안: **GentleBoost/LogitBoost**(보다 완만한 손실).

### 복잡도

- 대략 \(O(M\cdot \mathrm{cost}(h_m))\). Stump는 \(O(nd\log n)\) 근처(정렬 재사용 시).
- 메모리: 샘플 가중치 \(O(n)\) 유지.

---

## 파이썬: 실습 코드 모음

### 바이너리 분류 — 기본 사용

```python
import numpy as np
from sklearn.datasets import make_classification
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# 데이터

X, y = make_classification(n_samples=5000, n_features=20, n_informative=8,
                           n_redundant=2, weights=[0.7, 0.3], random_state=42)
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

# 약한 학습기: stump

stump = DecisionTreeClassifier(max_depth=1, random_state=42)

clf = AdaBoostClassifier(
    estimator=stump,      # scikit-learn >=1.2: estimator (이전 버전은 base_estimator)
    n_estimators=200,
    learning_rate=0.2,
    algorithm="SAMME.R",
    random_state=42
)
clf.fit(X_tr, y_tr)
pred = clf.predict(X_te)
proba = clf.predict_proba(X_te)[:, 1]

print("Accuracy:", accuracy_score(y_te, pred))
print("ROC-AUC :", roc_auc_score(y_te, proba))
print(classification_report(y_te, pred, digits=3))
```

### 학습률/트리개수 트레이드오프 관측

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

for lr in [1.0, 0.5, 0.2, 0.1]:
    model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),
                               n_estimators=400, learning_rate=lr,
                               algorithm="SAMME.R", random_state=42)
    scores = cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0),
                             scoring="roc_auc")
    print(f"lr={lr:<4} AUC: {scores.mean():.4f} ± {scores.std():.4f}")
```

### 다중 클래스 — SAMME vs SAMME.R

```python
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score

X, y = make_classification(n_samples=3000, n_features=15, n_classes=4,
                           n_informative=7, n_redundant=0, random_state=0)
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)

for algo in ["SAMME", "SAMME.R"]:
    clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),
                             n_estimators=300, learning_rate=0.5,
                             algorithm=algo, random_state=0)
    clf.fit(X_tr, y_tr)
    print(algo, "Accuracy:", accuracy_score(y_te, clf.predict(X_te)))
```

### 회귀 — AdaBoost.R2

```python
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_regression
from sklearn.metrics import mean_absolute_error, r2_score

X, y = make_regression(n_samples=5000, n_features=20, noise=15.0, random_state=42)
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, random_state=42)

reg = AdaBoostRegressor(
    estimator=DecisionTreeRegressor(max_depth=3, random_state=42),
    n_estimators=300, learning_rate=0.1, random_state=42
)
reg.fit(X_tr, y_tr)
pred = reg.predict(X_te)
print("MAE:", mean_absolute_error(y_te, pred))
print("R2 :", r2_score(y_te, pred))
```

### **from-scratch** (결정 stump + AdaBoost.M1; 교육용 미니멀)

```python
import numpy as np

def best_stump(X, y, w):
    # X: (n,d), y: {-1,+1}, w: weights (sum to 1)
    n, d = X.shape
    best = {"dim": None, "thr": None, "pol": 1, "err": 1.0, "pred": None}
    for j in range(d):
        idx = np.argsort(X[:, j])
        xj, yj, wj = X[idx, j], y[idx], w[idx]
        # 후보 threshold는 인접 값의 중간
        thr_cand = (xj[:-1] + xj[1:]) / 2.0
        for thr in thr_cand:
            for pol in (+1, -1):  # pol=+1: pred=+1 if x>=thr else -1 (반대도 함께 검사)
                pred = np.where(xj >= thr, pol, -pol)
                err = (wj * (pred != yj)).sum()
                if err < best["err"]:
                    best = {"dim": j, "thr": thr, "pol": pol, "err": err, "pred": pred.copy()}
    return best

def adaboost_train(X, y, M=100):
    n, d = X.shape
    w = np.ones(n) / n
    models = []
    for m in range(M):
        stump = best_stump(X, y, w)
        eps = stump["err"] / w.sum()
        if eps <= 1e-12: eps = 1e-12
        if eps >= 0.5:   break  # 실패 약한기 → 중단
        alpha = 0.5 * np.log((1 - eps) / eps)
        # 원 좌표 idx 복원 위해 다시 예측
        pred_all = stump_predict(X, stump)
        w *= np.exp(-alpha * y * pred_all)
        w /= w.sum()
        models.append((stump, alpha))
    return models

def stump_predict(X, stump):
    j, thr, pol = stump["dim"], stump["thr"], stump["pol"]
    return np.where(X[:, j] >= thr, pol, -pol)

def adaboost_predict(X, models):
    F = np.zeros(X.shape[0])
    for stump, alpha in models:
        F += alpha * stump_predict(X, stump)
    return np.sign(F), F

# 예시 데이터 (2D, 선형 분리 어려운 난형)

rng = np.random.default_rng(0)
n = 600
X = rng.normal(size=(n, 2))
y = np.where(X[:,0]**2 + 0.5*X[:,1] > 0.3, 1, -1)
# 레이블 약간 뒤섞어 outlier 생성

flip = rng.choice(n, size=30, replace=False)
y[flip] *= -1

models = adaboost_train(X, y, M=120)
pred, score = adaboost_predict(X, models)
acc = (pred == y).mean()
print(f"#weak={len(models)}, acc={acc:.3f}, margin mean={np.mean(y*score):.3f}")
```

---

## 실험 메모(그림 설명)

- **마진 분포**: 학습 단계가 진행될수록 \(y_i F(x_i)\) 히스토그램이 오른쪽으로 이동(양의 마진 증가).
- **학습률↓**: 더 많은 약한기가 필요하지만 **일반화**가 종종 개선(수축 효과).
- **이상치/노이즈**: 특정 점들의 가중치가 폭증 → 결정경계 뒤틀림. 조기중단/가중치 상한/라벨검증이 유효.
- **Stump vs depth=2**: depth=2는 표현력↑, 과적합 위험↑. 교차검증으로 균형.

---

## 자주 하는 질문(FAQ)

**Q1. 스케일링이 필요한가?**
트리 기반 약한학습기는 분할이 순서 기반이라 **필수는 아님**. 다만 수치적 안정/속도 관점에서 표준화가 해가 되지 않는다.

**Q2. 왜 이상치에 민감?**
틀린 샘플 가중치가 **지수적으로 증가**. 노이즈가 큰 점이 과도하게 영향.

**Q3. 랜덤 포레스트/그래디언트부스팅과 비교?**
- RF: 배깅(병렬)으로 분산↓, 튼튼.
- GBDT: 임의 손실의 음의 그래디언트를 근사 → 더 **유연**.
- AdaBoost: **지수손실**·마진 관점에서 깔끔, 설정이 단순.

---

## 체크리스트(실전)

1. **약한기**: `max_depth=1`부터 시작.
2. **n_estimators vs learning_rate**: 서로 보상 관계. grid로 동시 탐색.
3. **조기중단**: validation 지수손실/ROC-AUC 기준.
4. **클래스 불균형**: sample_weight / class_weight, cutoff 조정, PR-AUC 확인.
5. **노이즈 관리**: 라벨 점검, 가중치 cap, Gentle/LogitBoost 고려.
6. **다중클래스**: `"SAMME.R"` + 확률 잘 내는 약한기.
7. **해석**: 마진/오류 상계 로그, 샘플 가중치 분포, stump 분할빈도(간이 중요도).

---

## 도출 스케치

가중치 \(w_i^{(m)}\) 하에서
$$
\tilde{\mathcal{L}}(\alpha,h)=\sum_i w_i^{(m)}\exp(-\alpha y_i h(x_i))
= e^{-\alpha}\sum_{i: y_i=h(x_i)}w_i^{(m)} + e^{\alpha}\sum_{i: y_i\neq h(x_i)}w_i^{(m)}.
$$
이를 \(\alpha\)로 미분하여 0으로 두면
$$
e^{-\alpha}(1-\varepsilon_m) = e^{\alpha}\varepsilon_m
\Rightarrow \alpha_m = \tfrac12\ln\frac{1-\varepsilon_m}{\varepsilon_m}.
$$
또한 \(Z_m=\min_{\alpha}\tilde{\mathcal{L}}(\alpha,h)=2\sqrt{\varepsilon_m(1-\varepsilon_m)}\).

---

## 요약

- AdaBoost는 **지수손실 최소화**를 통해 **마진을 키우며** 강한 분류기를 만든다.
- 핵심 갱신식과 학습오류 상계(\(\prod Z_m\))가 단순·강력.
- 이상치 민감성과 노이즈 대응만 주의하면, 가벼운 전처리로 **강력한 베이스라인**을 제공한다.
