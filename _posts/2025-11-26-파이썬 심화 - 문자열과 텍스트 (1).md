---
layout: post
title: 파이썬 심화 - 문자열과 텍스트 (1)
date: 2025-11-26 17:25:23 +0900
category: 파이썬 심화
---
# 문자열과 텍스트 (1)

## 서론: 파이썬 문자열 처리의 중요성

현대 소프트웨어 개발에서 텍스트 데이터 처리는 필수적인 기술입니다. 로그 분석, 데이터 전처리, 웹 스크래핑, 자연어 처리 등 다양한 분야에서 문자열 조작 능력은 개발자의 생산성을 결정짓는 핵심 요소입니다. 파이썬은 이러한 요구에 부응하기 위해 풍부한 문자열 처리 도구들을 제공하며, 이를 효과적으로 활용하는 방법을 깊이 있게 알아보겠습니다.

## 여러 구분자로 문자열 나누기: 정규표현식의 힘

단순한 하나의 구분자가 아닌 복잡한 조건으로 문자열을 분할해야 할 때가 있습니다. 예를 들어, 사용자 입력, 로그 데이터, 다양한 형식의 텍스트 파일 등을 처리할 때 `str.split()`만으로는 부족한 경우가 많습니다.

### 기본적인 다중 구분자 분할

```python
import re

# 실전 예제 1: 다양한 구분자가 혼합된 텍스트 처리
log_entry = "2024-01-15 14:30:25,ERROR;database|connection failed,retry=3"
print("원본 로그:", log_entry)

# 쉼표, 세미콜론, 파이프, 등호로 분할
parts = re.split(r'[,;|=]+', log_entry)
print("기본 분할 결과:", parts)
# 결과: ['2024-01-15 14:30:25', 'ERROR', 'database', 'connection failed', 'retry', '3']

# 실전 예제 2: 자연어 텍스트 처리
text = "안녕하세요! 오늘은 날씨가 좋네요. 기분이 어떠신가요?"
korean_parts = re.split(r'[!.?]+', text)
print("문장 분할 결과:", korean_parts)
# 결과: ['안녕하세요', ' 오늘은 날씨가 좋네요', ' 기분이 어떠신가요', '']

# 실전 예제 3: CSV와 TSV 혼합 데이터
data_line = "apple,banana\tcherry;date|fig orange"
mixed_delimiters = re.split(r'[,\t;| ]+', data_line)
print("혼합 구분자 분할:", mixed_delimiters)
# 결과: ['apple', 'banana', 'cherry', 'date', 'fig', 'orange']
```

### 고급: 구분자 캡처와 보존

때로는 구분자 자체도 데이터의 일부로 보존해야 하는 경우가 있습니다. 특히 텍스트 분석이나 자연어 처리에서 문장 부호는 중요한 의미를 가질 수 있습니다.

```python
# 구분자를 함께 캡처하는 고급 분할
def tokenize_with_punctuation(text):
    """텍스트를 단어와 문장 부호로 토큰화"""
    # 단어 경계와 문장 부호를 기준으로 분할
    tokens = re.split(r'(\W+)', text)
    # 빈 문자열 제거
    return [token for token in tokens if token.strip()]

# 예제 실행
sample_text = "Hello, world! How are you today? I'm fine, thank you."
tokens = tokenize_with_punctuation(sample_text)
print("토큰화 결과:")
for i, token in enumerate(tokens, 1):
    print(f"  토큰 {i:2d}: '{token}'")

# 출력:
# 토큰  1: 'Hello'
# 토큰  2: ','
# 토큰  3: 'world'
# 토큰  4: '!'
# 토큰  5: 'How'
# 토큰  6: 'are'
# 토큰  7: 'you'
# 토큰  8: 'today'
# 토큰  9: '?'
# 토큰 10: 'I'
# 토큰 11: "'m"
# 토큰 12: 'fine'
# 토큰 13: ','
# 토큰 14: 'thank'
# 토큰 15: 'you'
# 토큰 16: '.'

# 실전 응용: 간단한 파서 구현
def parse_simple_query(query):
    """간단한 검색 쿼리 파서"""
    # 논리 연산자와 괄호를 기준으로 분할 (구분자 보존)
    pattern = r'\s*(AND|OR|NOT|\(|\))\s*'
    components = re.split(pattern, query)
    # 빈 문자열과 None 값 제거
    return [comp.strip() for comp in components if comp and comp.strip()]

query = "python AND (data OR analysis) NOT java"
parsed = parse_simple_query(query)
print(f"\n쿼리 파싱 결과: {parsed}")
# 결과: ['python', 'AND', '(', 'data', 'OR', 'analysis', ')', 'NOT', 'java']
```

### 성능 고려사항과 대안

정규표현식은 강력하지만 때로는 성능 문제를 일으킬 수 있습니다. 간단한 경우에는 문자열 메서드 조합이 더 효율적일 수 있습니다.

```python
import timeit

# 성능 비교: 다양한 분할 방법
test_string = "a,b;c|d\te f" * 1000  # 긴 문자열 생성

def split_with_regex():
    return re.split(r'[,;|\t ]+', test_string)

def split_with_replace():
    # 구분자를 통일한 후 분할
    temp = test_string.replace(';', ',').replace('|', ',').replace('\t', ',').replace(' ', ',')
    return temp.split(',')

# 성능 측정
regex_time = timeit.timeit(split_with_regex, number=100)
replace_time = timeit.timeit(split_with_replace, number=100)

print(f"정규표현식 분할 시간: {regex_time:.4f}초")
print(f"치환 후 분할 시간: {replace_time:.4f}초")
print(f"차이: {abs(regex_time - replace_time):.4f}초")

# 결론: 간단한 패턴은 문자열 메서드가 빠를 수 있지만,
# 복잡한 패턴에서는 정규표현식이 더 명확하고 유지보수하기 좋습니다.
```

## 문자열 시작 또는 끝 부분 패턴 매칭의 실전 활용

`str.startswith()`와 `str.endswith()` 메서드는 단순해 보이지만 다양한 상황에서 강력한 도구로 사용될 수 있습니다.

### 파일 시스템 작업에서의 활용

```python
import os

# 디렉토리 내 파일 필터링
def categorize_files(directory):
    """디렉토리 내 파일을 확장자별로 분류"""
    file_categories = {
        'python': [],
        'data': [],
        'document': [],
        'image': [],
        'executable': [],
        'other': []
    }
    
    for filename in os.listdir(directory):
        if os.path.isfile(os.path.join(directory, filename)):
            # 여러 패턴을 튜플로 한 번에 확인
            if filename.endswith(('.py', '.pyw')):
                file_categories['python'].append(filename)
            elif filename.endswith(('.csv', '.json', '.xml', '.txt')):
                file_categories['data'].append(filename)
            elif filename.endswith(('.md', '.doc', '.docx', '.pdf')):
                file_categories['document'].append(filename)
            elif filename.endswith(('.jpg', '.png', '.gif', '.bmp')):
                file_categories['image'].append(filename)
            elif filename.startswith(('run', 'start', 'setup')) or filename.endswith(('.exe', '.sh', '.bat')):
                file_categories['executable'].append(filename)
            else:
                file_categories['other'].append(filename)
    
    return file_categories

# URL 패턴 검증
def validate_urls(urls):
    """URL 리스트의 유효성 검증"""
    valid_urls = []
    invalid_urls = []
    
    for url in urls:
        # 여러 프로토콜 지원
        if url.startswith(('http://', 'https://', 'ftp://', 'sftp://')):
            valid_urls.append(url)
        else:
            invalid_urls.append(url)
    
    return valid_urls, invalid_urls

# 테스트
url_list = [
    'https://example.com',
    'http://test.com',
    'ftp://fileserver.com',
    'file://local/path',
    'invalid_url',
    'https://secure.site'
]

valid, invalid = validate_urls(url_list)
print(f"유효한 URL ({len(valid)}개): {valid}")
print(f"무효한 URL ({len(invalid)}개): {invalid}")
```

### 데이터 유효성 검사

```python
def validate_input_data(inputs):
    """다양한 형식의 입력 데이터 유효성 검사"""
    validation_results = []
    
    for data in inputs:
        result = {
            'data': data,
            'is_email': False,
            'is_phone': False,
            'is_date': False,
            'is_numeric': False
        }
        
        # 이메일 주소 검증 (간단한 형태)
        if data.endswith(('.com', '.net', '.org', '.co.kr', '.kr')):
            result['is_email'] = True
        
        # 한국 전화번호 패턴 (간단한 검증)
        if data.startswith(('010-', '02-', '031-', '032-', '033-', '041-', '042-', '043-', '044-',
                           '051-', '052-', '053-', '054-', '055-', '061-', '062-', '063-', '064-')):
            result['is_phone'] = True
        
        # 날짜 형식 (YYYY-MM-DD 또는 YY/MM/DD)
        import re
        if re.match(r'^\d{4}-\d{2}-\d{2}$', data) or re.match(r'^\d{2}/\d{2}/\d{2}$', data):
            result['is_date'] = True
        
        # 숫자 여부 (정수 또는 실수)
        try:
            float(data)
            result['is_numeric'] = True
        except ValueError:
            pass
        
        validation_results.append(result)
    
    return validation_results

# 테스트 데이터
test_inputs = [
    'user@example.com',
    '010-1234-5678',
    '2024-01-15',
    '123.45',
    'hello world',
    '02-9876-5432',
    'invalid_email',
    '24/01/15'
]

results = validate_input_data(test_inputs)
print("\n입력 데이터 유효성 검사 결과:")
for res in results:
    print(f"{res['data']:20} -> 이메일: {res['is_email']}, 전화번호: {res['is_phone']}, "
          f"날짜: {res['is_date']}, 숫자: {res['is_numeric']}")
```

### 성능 최적화 팁

```python
# 성능 비교: startswith/endswith vs 정규표현식 vs 슬라이싱
import timeit

test_string = "https://example.com/path/to/resource"
test_cases = 100000

# 방법 1: startswith()
def method_startswith():
    return test_string.startswith(('http://', 'https://', 'ftp://'))

# 방법 2: 정규표현식
import re
pattern = re.compile(r'^(https?|ftp)://')
def method_regex():
    return bool(pattern.match(test_string))

# 방법 3: 슬라이싱
def method_slicing():
    return (test_string[:7] == 'http://' or 
            test_string[:8] == 'https://' or 
            test_string[:6] == 'ftp://')

# 성능 측정
print("성능 비교 (작을수록 빠름):")
print(f"startswith(): {timeit.timeit(method_startswith, number=test_cases):.4f}초")
print(f"정규표현식:    {timeit.timeit(method_regex, number=test_cases):.4f}초")
print(f"슬라이싱:      {timeit.timeit(method_slicing, number=test_cases):.4f}초")

# 결론: startswith()가 가독성과 성능 모두에서 우수합니다.
```

## 쉘 와일드카드 패턴으로 문자열 매칭: fnmatch 모듈 깊이 이해하기

`fnmatch` 모듈은 유닉스 쉘 스타일의 패턴 매칭을 제공하여 파일 시스템 작업이나 간단한 패턴 매칭에 유용합니다.

### 와일드카드 패턴 문법 상세

```python
from fnmatch import fnmatch, fnmatchcase, filter

# 기본 와일드카드 패턴
patterns_examples = [
    # 패턴, 설명, 예제 매칭 문자열
    ('*.py', '모든 파이썬 파일', 'script.py'),
    ('test_*.py', 'test로 시작하는 테스트 파일', 'test_example.py'),
    ('data[0-9].csv', 'data0.csv부터 data9.csv', 'data5.csv'),
    ('[A-Z]*.txt', '대문자로 시작하는 텍스트 파일', 'Report.txt'),
    ('file?.txt', 'file 다음에 한 문자 오는 텍스트 파일', 'file1.txt'),
    ('*.{jpg,png,gif}', '여러 이미지 형식', 'photo.jpg'),
    ('*[!0-9].txt', '숫자로 끝나지 않는 텍스트 파일', 'document.txt'),
]

print("와일드카드 패턴 예제:")
for pattern, description, example in patterns_examples:
    matches = fnmatch(example, pattern)
    print(f"  패턴: {pattern:20} 설명: {description:40} 예제: {example:15} → 매칭: {matches}")

# 실제 파일 시스템 예제
import os

def find_files_by_pattern(directory, pattern):
    """디렉토리에서 패턴에 맞는 파일 찾기"""
    matches = []
    for root, dirs, files in os.walk(directory):
        for filename in files:
            if fnmatch(filename, pattern):
                full_path = os.path.join(root, filename)
                matches.append(full_path)
    return matches

# filter 함수를 이용한 간결한 필터링
filenames = ['data1.csv', 'data2.csv', 'config.ini', 'test.py', 'main.py']
python_files = filter(filenames, '*.py')
print(f"\n*.py 패턴 필터링: {list(python_files)}")
```

### 대소문자 구분: fnmatch vs fnmatchcase

```python
# 대소문자 처리의 중요성 (다른 운영체제의 차이)
test_files = [
    'README.md',
    'readme.md',
    'ReadMe.MD',
    'config.ini',
    'Config.INI',
    'data.TXT',
    'data.txt',
]

print("\n대소문자 구분 비교:")
print("fnmatch (OS에 따라 다름 - 대소문자 무시 가능):")
for filename in test_files:
    matches_md = fnmatch(filename, '*.md')
    matches_ini = fnmatch(filename, '*.ini')
    matches_txt = fnmatch(filename, '*.txt')
    print(f"  {filename:15} → *.md: {matches_md}, *.ini: {matches_ini}, *.txt: {matches_txt}")

print("\nfnmatchcase (정확한 대소문자 구분):")
for filename in test_files:
    matches_md = fnmatchcase(filename, '*.md')
    matches_MD = fnmatchcase(filename, '*.MD')
    matches_ini = fnmatchcase(filename, '*.ini')
    matches_txt = fnmatchcase(filename, '*.txt')
    print(f"  {filename:15} → *.md: {matches_md}, *.MD: {matches_MD}, "
          f"*.ini: {matches_ini}, *.txt: {matches_txt}")
```

### 고급 패턴 매칭 예제

```python
# 복잡한 비즈니스 규칙 구현
def validate_filenames(filenames, business_rules):
    """비즈니스 규칙에 따른 파일명 검증"""
    violations = []
    
    for filename in filenames:
        rule_violations = []
        
        # 규칙 1: 특정 접두사 필수
        if business_rules.get('required_prefix') and \
           not fnmatch(filename, business_rules['required_prefix'] + '*'):
            rule_violations.append(f"접두사 '{business_rules['required_prefix']}'가 필요합니다")
        
        # 규칙 2: 금지된 접미사 확인
        if business_rules.get('forbidden_suffixes'):
            for suffix in business_rules['forbidden_suffixes']:
                if fnmatch(filename, '*' + suffix):
                    rule_violations.append(f"접미사 '{suffix}'는 허용되지 않습니다")
        
        # 규칙 3: 특정 패턴 필수
        if business_rules.get('required_pattern') and \
           not fnmatch(filename, business_rules['required_pattern']):
            rule_violations.append(f"패턴 '{business_rules['required_pattern']}'을 따라야 합니다")
        
        # 규칙 4: 날짜 형식 포함 확인
        if business_rules.get('require_date') and \
           not any(fnmatch(filename, f'*{pattern}*') for pattern in ['????-??-??', '??-??-????']):
            rule_violations.append("날짜 형식(YYYY-MM-DD 또는 MM-DD-YYYY)이 필요합니다")
        
        if rule_violations:
            violations.append({
                'filename': filename,
                'violations': rule_violations
            })
    
    return violations

# 비즈니스 규칙 정의
business_rules = {
    'required_prefix': 'REPORT',
    'forbidden_suffixes': ['.tmp', '.bak', '.old'],
    'required_pattern': 'REPORT_*_*.xlsx',
    'require_date': True
}

# 테스트 파일 목록
test_filenames = [
    'REPORT_SALES_2024-01-15.xlsx',
    'report_sales_2024-01-15.xlsx',  # 접두사 규칙 위반 (대소문자)
    'REPORT_SALES_20240115.xlsx',    # 날짜 형식 규칙 위반
    'REPORT_SALES_2024-01-15.tmp',   # 금지된 접미사
    'DATA_SALES_2024-01-15.xlsx',    # 접두사 규칙 위반
    'REPORT_SALES_01-15-2024.xlsx',  # 정상 (다른 날짜 형식)
]

print("\n비즈니스 규칙 검증 결과:")
violations = validate_filenames(test_filenames, business_rules)
for v in violations:
    print(f"\n파일: {v['filename']}")
    for violation in v['violations']:
        print(f"  - {violation}")
```

### fnmatch를 넘어서: glob 모듈과의 통합

```python
import glob
import os

# glob 모듈은 fnmatch 패턴을 사용하여 실제 파일 시스템 검색
def find_files_advanced(pattern, root_dir='.'):
    """고급 파일 검색 기능"""
    results = {
        'exact_matches': [],
        'recursive_matches': [],
        'multiple_patterns': []
    }
    
    # 기본 검색
    results['exact_matches'] = glob.glob(pattern)
    
    # 재귀적 검색
    recursive_pattern = os.path.join('**', pattern)
    results['recursive_matches'] = glob.glob(recursive_pattern, recursive=True)
    
    # 여러 패턴 동시 검색
    if isinstance(pattern, (list, tuple)):
        all_matches = []
        for p in pattern:
            all_matches.extend(glob.glob(p))
        results['multiple_patterns'] = list(set(all_matches))  # 중복 제거
    
    return results

# 사용 예시
print("\n고급 파일 검색 예제:")
patterns = ['*.py', '*.txt', '*.md']
search_results = find_files_advanced(patterns)

print(f"단일 패턴 검색: {len(search_results['exact_matches'])}개 파일")
print(f"재귀적 검색: {len(search_results['recursive_matches'])}개 파일")
print(f"다중 패턴 검색: {len(search_results['multiple_patterns'])}개 파일")

# 실제 파일 내용 기반 필터링
def find_files_by_content(pattern, content_pattern, root_dir='.'):
    """파일명 패턴과 내용 패턴으로 파일 검색"""
    import re
    
    matches = []
    for filename in glob.glob(os.path.join(root_dir, '**', pattern), recursive=True):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                content = f.read()
                if re.search(content_pattern, content, re.IGNORECASE):
                    matches.append(filename)
        except (UnicodeDecodeError, IOError):
            continue  # 읽을 수 없는 파일은 건너뛰기
    
    return matches
```

## 정규표현식을 활용한 텍스트 패턴 매칭과 검색: 완벽 가이드

정규표현식(Regex)은 텍스트 패턴 매칭의 궁극적인 도구입니다. 복잡한 패턴을 간결하게 표현할 수 있지만, 학습 곡선이 가파른 편입니다.

### 기본 정규표현식 문법 마스터하기

```python
import re

# 기본 메타문자 학습
def demonstrate_regex_patterns():
    """다양한 정규표현식 패턴 시연"""
    
    text = """
    Contact Information:
    - Email: john.doe@example.com
    - Phone: 010-1234-5678
    - Address: 123 Main St, Seoul
    - Date: 2024-01-15
    - URL: https://www.example.com
    - IP: 192.168.1.1
    """
    
    patterns = [
        # 이메일 주소
        (r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '이메일 주소'),
        # 한국 전화번호
        (r'\b\d{2,3}-\d{3,4}-\d{4}\b', '전화번호'),
        # 날짜 (YYYY-MM-DD)
        (r'\b\d{4}-\d{2}-\d{2}\b', '날짜'),
        # URL
        (r'https?://(?:www\.)?[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}', 'URL'),
        # IP 주소
        (r'\b(?:\d{1,3}\.){3}\d{1,3}\b', 'IP 주소'),
        # 단어 경계
        (r'\b\w+\b', '모든 단어'),
        # 숫자
        (r'\b\d+\b', '숫자'),
    ]
    
    print("정규표현식 패턴 매칭 결과:")
    for pattern, description in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            print(f"\n{description}:")
            for match in matches:
                print(f"  - {match}")

demonstrate_regex_patterns()
```

### re 모듈의 주요 함수 심층 분석

```python
# findall() - 모든 매칭 찾기
def analyze_findall():
    """findall() 함수의 다양한 사용법"""
    
    text = "The prices are $10.99, $20.50, and $5.75 for items A, B, and C."
    
    # 기본 사용
    prices = re.findall(r'\$\d+\.\d{2}', text)
    print(f"모든 가격: {prices}")
    
    # 그룹 캡처
    items_and_prices = re.findall(r'(\$\d+\.\d{2}) for (\w+)', text)
    print(f"아이템과 가격: {items_and_prices}")
    
    # 비캡처링 그룹 (?:...)
    # 그룹화는 하지만 캡처하지 않음
    non_capturing = re.findall(r'(?:\$\d+\.\d{2})(?: for (\w+))?', text)
    print(f"비캡처링 그룹 테스트: {non_capturing}")

# search() - 첫 번째 매칭 찾기
def analyze_search():
    """search()와 match 객체 활용"""
    
    text = "Order #12345 was placed on 2024-01-15 by customer John Doe."
    
    pattern = r'Order #(\d+) was placed on (\d{4}-\d{2}-\d{2}) by customer (\w+ \w+)'
    match = re.search(pattern, text)
    
    if match:
        print(f"\n전체 매칭: {match.group(0)}")
        print(f"주문번호: {match.group(1)}")
        print(f"주문일자: {match.group(2)}")
        print(f"고객이름: {match.group(3)}")
        print(f"매칭 위치: {match.start()}~{match.end()}")
        print(f"매칭 범위 텍스트: '{text[match.start():match.end()]}'")

# finditer() - 매칭 객체 이터레이터
def analyze_finditer():
    """대용량 텍스트에서 효율적인 검색"""
    
    large_text = "Error 404 at 10:30, Error 500 at 11:45, Error 403 at 14:20"
    
    # finditer는 제너레이터를 반환하여 메모리 효율적
    error_pattern = r'Error (\d{3}) at (\d{2}:\d{2})'
    
    print("\n에러 로그 분석:")
    for match in re.finditer(error_pattern, large_text):
        error_code = match.group(1)
        time = match.group(2)
        position = match.start()
        print(f"  위치 {position}: 에러 {error_code} at {time}")

analyze_findall()
analyze_search()
analyze_finditer()
```

### 컴파일된 정규표현식과 성능 최적화

```python
# 정규표현식 컴파일의 중요성
def benchmark_regex_performance():
    """컴파일된 정규표현식 vs 즉석 정규표현식 성능 비교"""
    
    import time
    
    # 테스트 텍스트 생성
    test_text = "test" * 10000 + "target" + "test" * 10000
    
    # 컴파일된 패턴
    compiled_pattern = re.compile(r'target')
    
    # 즉석 패턴 사용 함수
    def inline_pattern():
        return re.search(r'target', test_text)
    
    # 컴파일된 패턴 사용 함수
    def compiled_pattern_func():
        return compiled_pattern.search(test_text)
    
    # 성능 측정
    iterations = 1000
    
    start = time.time()
    for _ in range(iterations):
        inline_pattern()
    inline_time = time.time() - start
    
    start = time.time()
    for _ in range(iterations):
        compiled_pattern_func()
    compiled_time = time.time() - start
    
    print(f"\n성능 비교 ({iterations}회 반복):")
    print(f"즉석 정규표현식: {inline_time:.4f}초")
    print(f"컴파일된 정규표현식: {compiled_time:.4f}초")
    print(f"성능 향상: {(inline_time/compiled_time):.1f}배 빠름")

# 플래그 활용
def demonstrate_flags():
    """정규표현식 플래그의 다양한 활용"""
    
    text = "Python is great.\nPYTHON is powerful.\npython is easy."
    
    flags_demo = [
        (re.IGNORECASE, '대소문자 무시'),
        (re.MULTILINE, '멀티라인 모드'),
        (re.DOTALL, '점(.)이 줄바꿈 포함'),
        (re.VERBOSE, '공백과 주석 허용'),
    ]
    
    for flag, description in flags_demo:
        matches = re.findall(r'^python', text, flag)
        print(f"{description}: {matches}")

benchmark_regex_performance()
demonstrate_flags()
```

### 실전 정규표현식 패턴 라이브러리

```python
class RegexPatternLibrary:
    """자주 사용하는 정규표현식 패턴 모음"""
    
    @staticmethod
    def get_pattern(pattern_name):
        """패턴 이름으로 정규표현식 반환"""
        
        patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'korean_phone': r'\b(?:0\d{1,2}-\d{3,4}-\d{4}|01[016789]-\d{3,4}-\d{4})\b',
            'url': r'https?://(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b(?:[-a-zA-Z0-9()@:%_\+.~#?&//=]*)',
            'ipv4': r'\b(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b',
            'date_yyyy_mm_dd': r'\b\d{4}[-/]\d{2}[-/]\d{2}\b',
            'time_24h': r'\b(?:[01]?[0-9]|2[0-3]):[0-5][0-9](?::[0-5][0-9])?\b',
            'korean_text': r'[가-힣]+',
            'credit_card': r'\b(?:\d[ -]*?){13,16}\b',
            'html_tag': r'<[^>]+>',
            'hashtag': r'#\w+',
            'mention': r'@\w+',
        }
        
        return patterns.get(pattern_name, None)
    
    @staticmethod
    def validate_with_pattern(text, pattern_name):
        """패턴으로 텍스트 검증"""
        pattern = RegexPatternLibrary.get_pattern(pattern_name)
        if not pattern:
            return False
        
        compiled = re.compile(pattern)
        return bool(compiled.search(text))

# 사용 예시
print("\n정규표현식 패턴 라이브러리 사용 예:")
test_cases = [
    ('user@example.com', 'email'),
    ('010-1234-5678', 'korean_phone'),
    ('https://www.google.com', 'url'),
    ('안녕하세요', 'korean_text'),
    ('#파이썬', 'hashtag'),
]

for text, pattern_name in test_cases:
    is_valid = RegexPatternLibrary.validate_with_pattern(text, pattern_name)
    print(f"'{text}' → 패턴 '{pattern_name}': {'유효' if is_valid else '무효'}")
```

## 텍스트 검색과 치환: 단순한 작업부터 복잡한 변환까지

텍스트 치환은 데이터 정제, 형식 변환, 마스킹 등 다양한 용도로 사용됩니다.

### 기본 치환 기법

```python
# 단순 문자열 치환
def demonstrate_basic_replacements():
    """기본적인 문자열 치환 방법"""
    
    text = "The quick brown fox jumps over the lazy dog."
    
    # 1. 기본 replace() 메서드
    replaced1 = text.replace('fox', 'cat')
    print(f"기본 치환: {replaced1}")
    
    # 2. 여러 문자열 한 번에 치환
    replacements = {
        'quick': 'fast',
        'brown': 'red',
        'lazy': 'sleepy',
        'dog': 'puppy'
    }
    
    result = text
    for old, new in replacements.items():
        result = result.replace(old, new)
    print(f"여러 치환: {result}")
    
    # 3. translate() 메서드 (단일 문자 치환에 효율적)
    translation_table = str.maketrans({
        'a': '@',
        'e': '3',
        'i': '1',
        'o': '0',
        's': '$'
    })
    leet_speak = text.translate(translation_table)
    print(f"Leet speak: {leet_speak}")

demonstrate_basic_replacements()
```

### 정규표현식을 이용한 고급 치환

```python
# 복잡한 패턴 기반 치환
def advanced_pattern_replacement():
    """정규표현식을 이용한 고급 텍스트 치환"""
    
    # 1. 날짜 형식 변환
    text_with_dates = "Events on 01/15/2024, 12/25/2023, and 07/04/2022."
    
    # MM/DD/YYYY → YYYY-MM-DD
    date_converted = re.sub(
        r'(\d{2})/(\d{2})/(\d{4})',
        r'\3-\1-\2',
        text_with_dates
    )
    print(f"날짜 형식 변환: {date_converted}")
    
    # 2. 전화번호 형식 통일
    text_with_phones = """
    Contact: 010-1234-5678, 02.987.6543, 031 123 4567
    Emergency: 119, International: +82-10-1234-5678
    """
    
    # 다양한 형식을 표준 형식으로 통일
    phone_standardized = re.sub(
        r'(\+82[-.\s]?|0)?(\d{1,2})[-.\s]?(\d{3,4})[-.\s]?(\d{4})',
        r'0\2-\3-\4',
        text_with_phones
    )
    print(f"전화번호 표준화:\n{phone_standardized}")
    
    # 3. 마스킹 처리 (개인정보 보호)
    text_with_private_info = """
    이름: 홍길동, 주민등록번호: 901212-1234567
    계좌번호: 123-456-789012, 카드번호: 1234-5678-9012-3456
    """
    
    # 주민등록번호 마스킹
    ssn_masked = re.sub(
        r'(\d{6})-(\d{1})\d{6}',
        r'\1-*******',
        text_with_private_info
    )
    
    # 계좌번호 마스킹
    account_masked = re.sub(
        r'(\d{3}-\d{3}-)\d{6}',
        r'\1******',
        ssn_masked
    )
    
    # 카드번호 마스킹 (첫 4자리와 마지막 4자리만 표시)
    card_masked = re.sub(
        r'(\d{4}-)\d{4}-\d{4}-(\d{4})',
        r'\1****-****-\2',
        account_masked
    )
    
    print(f"개인정보 마스킹:\n{card_masked}")

advanced_pattern_replacement()
```

### 콜백 함수를 이용한 동적 치환

```python
# 콜백 함수를 이용한 지능형 치환
def callback_based_replacement():
    """콜백 함수를 이용한 동적 텍스트 치환"""
    
    # 1. 단어 길이에 따른 변환
    text = "The artificial intelligence system processes data efficiently."
    
    def emphasize_long_words(match):
        word = match.group()
        if len(word) > 8:
            return f"**{word.upper()}**"
        elif len(word) > 5:
            return f"*{word}*"
        else:
            return word
    
    emphasized = re.sub(r'\b\w+\b', emphasize_long_words, text)
    print(f"길이별 강조: {emphasized}")
    
    # 2. 수치 데이터 변환
    data_text = "The temperature is 25C, humidity is 65%, and pressure is 1013hPa."
    
    def convert_units(match):
        value = float(match.group(1))
        unit = match.group(2)
        
        if unit == 'C':
            fahrenheit = value * 9/5 + 32
            return f"{value}C ({fahrenheit:.1f}F)"
        elif unit == '%':
            if value > 80:
                return f"{value}% (High)"
            elif value < 30:
                return f"{value}% (Low)"
            else:
                return f"{value}% (Normal)"
        else:
            return match.group()
    
    converted = re.sub(r'(\d+(?:\.\d+)?)(C|%|hPa)', convert_units, data_text)
    print(f"단위 변환: {converted}")
    
    # 3. 링크 자동 생성
    text_with_refs = """
    Check out Python at https://python.org and 
    Django at https://djangoproject.com for more info.
    """
    
    def make_clickable_links(match):
        url = match.group()
        return f'<a href="{url}">{url}</a>'
    
    linked_text = re.sub(
        r'https?://[^\s]+',
        make_clickable_links,
        text_with_refs
    )
    print(f"링크 자동 생성:\n{linked_text}")

callback_based_replacement()
```

### 성능을 고려한 대용량 텍스트 치환

```python
# 대용량 텍스트 처리 최적화
def optimize_large_text_replacement():
    """대용량 텍스트 치환 성능 최적화"""
    
    import time
    
    # 대용량 텍스트 생성
    large_text = "Lorem ipsum dolor sit amet, " * 10000
    
    # 다양한 치환 방법 성능 비교
    patterns = [
        ('ipsum', 'IPSUM'),
        ('dolor', 'DOLOR'),
        ('amet', 'AMET'),
        ('sit', 'SIT')
    ]
    
    # 방법 1: 순차적 replace
    def method_sequential(text, patterns):
        result = text
        for old, new in patterns:
            result = result.replace(old, new)
        return result
    
    # 방법 2: 정규표현식 단일 패턴
    def method_regex_single(text, patterns):
        # 모든 패턴을 하나의 정규표현식으로 결합
        pattern = '|'.join(re.escape(old) for old, _ in patterns)
        replacement_dict = {old: new for old, new in patterns}
        
        def replacer(match):
            return replacement_dict[match.group()]
        
        return re.sub(pattern, replacer, text)
    
    # 방법 3: 정규표현식 컴파일된 패턴
    def method_regex_compiled(text, patterns):
        pattern = '|'.join(re.escape(old) for old, _ in patterns)
        compiled = re.compile(pattern)
        replacement_dict = {old: new for old, new in patterns}
        
        def replacer(match):
            return replacement_dict[match.group()]
        
        return compiled.sub(replacer, text)
    
    # 성능 측정
    iterations = 10
    
    print("대용량 텍스트 치환 성능 비교:")
    
    start = time.time()
    for _ in range(iterations):
        method_sequential(large_text, patterns)
    sequential_time = time.time() - start
    
    start = time.time()
    for _ in range(iterations):
        method_regex_single(large_text, patterns)
    regex_single_time = time.time() - start
    
    start = time.time()
    for _ in range(iterations):
        method_regex_compiled(large_text, patterns)
    regex_compiled_time = time.time() - start
    
    print(f"순차적 replace: {sequential_time:.4f}초")
    print(f"정규표현식 단일: {regex_single_time:.4f}초")
    print(f"정규표현식 컴파일: {regex_compiled_time:.4f}초")
    
    # 결론: 패턴 수와 텍스트 크기에 따라 최적의 방법이 달라집니다.
    # - 소량의 패턴: 순차적 replace가 빠를 수 있음
    # - 대량의 패턴: 정규표현식이 더 효율적
    # - 반복 사용: 컴파일된 정규표현식이 가장 빠름

optimize_large_text_replacement()
```

## 대소문자 구분 없이 텍스트 검색과 치환: 완벽한 해법

대소문자 무시 처리는 국제화(i18n)와 사용자 입력 처리에서 매우 중요합니다.

### 다양한 대소문자 무시 처리 방법

```python
# 대소문자 무시 처리의 여러 방법
def case_insensitive_techniques():
    """대소문자 무시 처리의 다양한 기법"""
    
    text = "Python Programming: Learn PYTHON, python, and PyThOn!"
    
    # 방법 1: lower()/upper() 사용
    target = "python"
    if target.lower() in text.lower():
        print(f"lower() 사용: '{target}' 찾음")
    
    # 방법 2: casefold() 사용 (더 강력한 대소문자 무시)
    # casefold()는 더 많은 유니코드 케이스를 처리
    german_text = "straße"
    search_word = "STRASSE"
    if search_word.casefold() in german_text.casefold():
        print(f"casefold() 사용: '{search_word}' 찾음 (독일어)")
    
    # 방법 3: 정규표현식 IGNORECASE 플래그
    matches = re.findall(r'python', text, re.IGNORECASE)
    print(f"정규표현식 IGNORECASE: {matches}")
    
    # 방법 4: str.casefold()와 정규표현식 조합
    import unicodedata
    
    def normalize_for_comparison(text):
        """텍스트 비교를 위한 정규화"""
        # NFD 분해 후 소문자 변환
        normalized = unicodedata.normalize('NFD', text)
        # 결합 문자 제거 (선택적)
        stripped = ''.join(c for c in normalized if not unicodedata.combining(c))
        return stripped.casefold()
    
    text1 = "Café"
    text2 = "cafe\u0301"  # 결합 악센트 사용
    print(f"정규화 비교: '{text1}' == '{text2}': {normalize_for_comparison(text1) == normalize_for_comparison(text2)}")

case_insensitive_techniques()
```

### 대소문자 패턴 유지하며 치환하기

```python
# 원본 대소문자 패턴 유지하며 치환
def preserve_case_replacement():
    """원본 텍스트의 대소문자 패턴을 유지하며 치환"""
    
    def smart_replace(text, old_word, new_word):
        """지능형 치환: 대소문자 패턴 유지"""
        
        def replacer(match):
            original = match.group()
            replacement = new_word
            
            # 원본의 대소문자 패턴 분석
            if original.isupper():
                return replacement.upper()
            elif original.islower():
                return replacement.lower()
            elif original.istitle():
                return replacement.title()
            else:
                # 첫 문자만 대문자인 경우
                if original[0].isupper() and original[1:].islower():
                    return replacement.capitalize()
                # 그 외 혼합 패턴
                else:
                    return replacement
        
        # 대소문자 무시 패턴 생성
        pattern = re.compile(re.escape(old_word), re.IGNORECASE)
        return pattern.sub(replacer, text)
    
    # 테스트
    text = """
    Python is great. I love PYTHON programming. 
    The Python community is amazing. python rules!
    Learning python is fun. PYTHON developers are in demand.
    """
    
    result = smart_replace(text, 'python', 'JavaScript')
    print("대소문자 패턴 유지 치환 결과:")
    print(result)

preserve_case_replacement()
```

### 다국어 텍스트 처리

```python
# 다국어 대소문자 처리
def multilingual_case_handling():
    """다국어 환경에서의 대소문자 처리"""
    
    # 다양한 언어의 텍스트
    multilingual_text = """
    English: Hello World
    Spanish: Hola Mundo
    German: Hallo Welt
    French: Bonjour le Monde
    Turkish: Merhaba Dünya
    Greek: Γεια σου Κόσμε
    Russian: Привет мир
    """
    
    # 언어별 대소문자 변환 특성
    def demonstrate_case_operations():
        test_cases = [
            ("english", "HELLO"),
            ("turkish", "İSTANBUL"),  # Türkçe İ har비
            ("greek", "ΓΕΙΑ"),
            ("german", "straße"),
        ]
        
        for language, text in test_cases:
            print(f"\n{language.capitalize()}:")
            print(f"  원본: {text}")
            print(f"  lower(): {text.lower()}")
            print(f"  casefold(): {text.casefold()}")
            print(f"  upper(): {text.upper()}")
    
    demonstrate_case_operations()
    
    # 다국어 대소문자 무시 검색
    def multilingual_search(text, pattern):
        """다국어 텍스트에서 대소문자 무시 검색"""
        import unicodedata
        
        # 텍스트 정규화
        def normalize_text(t):
            t = unicodedata.normalize('NFKD', t)
            t = ''.join(c for c in t if not unicodedata.combining(c))
            return t.casefold()
        
        normalized_text = normalize_text(text)
        normalized_pattern = normalize_text(pattern)
        
        return normalized_pattern in normalized_text
    
    # 테스트
    search_term = "ΓΕΙΑ"  # 그리스어 대문자
    found = multilingual_search(multilingual_text, search_term)
    print(f"\n다국어 검색: '{search_term}' 찾음? {found}")

multilingual_case_handling()
```

### 실전 애플리케이션: 검색 시스템 구현

```python
# 대소문자 무시 검색 시스템 구현
class CaseInsensitiveSearchEngine:
    """대소문자 무시 검색 엔진"""
    
    def __init__(self):
        self.index = {}  # 단어 → 문서 ID 목록
        self.documents = {}  # 문서 ID → 문서 내용
        self.next_doc_id = 1
    
    def add_document(self, text, doc_id=None):
        """문서 추가 및 인덱싱"""
        if doc_id is None:
            doc_id = self.next_doc_id
            self.next_doc_id += 1
        
        self.documents[doc_id] = text
        
        # 텍스트 인덱싱 (대소문자 무시)
        words = re.findall(r'\b\w+\b', text.lower())
        for word in set(words):  # 중복 제거
            if word not in self.index:
                self.index[word] = []
            if doc_id not in self.index[word]:
                self.index[word].append(doc_id)
        
        return doc_id
    
    def search(self, query, fuzzy=False):
        """대소문자 무시 검색"""
        query_words = re.findall(r'\b\w+\b', query.lower())
        
        if not query_words:
            return []
        
        # 관련 문서 찾기
        relevant_docs = {}
        for word in query_words:
            if word in self.index:
                for doc_id in self.index[word]:
                    relevant_docs[doc_id] = relevant_docs.get(doc_id, 0) + 1
        
        # 유사도 순 정렬
        sorted_docs = sorted(
            relevant_docs.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [(doc_id, self.documents[doc_id], score) 
                for doc_id, score in sorted_docs]
    
    def highlight_matches(self, text, query, tag='strong'):
        """검색어 하이라이팅"""
        pattern = '|'.join(re.escape(word) for word in re.findall(r'\b\w+\b', query))
        compiled = re.compile(pattern, re.IGNORECASE)
        
        def highlight(match):
            return f'<{tag}>{match.group()}</{tag}>'
        
        return compiled.sub(highlight, text)

# 사용 예시
print("\n대소문자 무시 검색 시스템:")
search_engine = CaseInsensitiveSearchEngine()

# 문서 추가
doc1 = search_engine.add_document(
    "Python is an interpreted, high-level programming language."
)
doc2 = search_engine.add_document(
    "PYTHON programming is great for data analysis and machine learning."
)
doc3 = search_engine.add_document(
    "Many developers love python for its simplicity and readability."
)

# 검색
results = search_engine.search("Python programming")
print(f"'{'Python programming'}' 검색 결과 ({len(results)}개):")
for doc_id, content, score in results:
    highlighted = search_engine.highlight_matches(content, "Python programming")
    print(f"  문서 {doc_id} (점수: {score}): {highlighted}")
```

## 결론: 문자열 처리의 미래와 모범 사례

파이썬의 문자열 처리 기능은 지속적으로 발전하고 있으며, 현대적인 텍스트 처리 요구사항을 충족시키기 위한 다양한 도구와 기법을 제공합니다. 효과적인 문자열 처리를 위한 핵심 원칙을 정리하면 다음과 같습니다:

### 1. **적절한 도구 선택의 중요성**
- 간단한 작업에는 기본 문자열 메서드(`replace()`, `startswith()`, `endswith()`) 사용
- 파일 시스템 패턴 매칭에는 `fnmatch` 또는 `glob` 모듈 사용
- 복잡한 패턴 매칭에는 정규표현식(`re` 모듈) 사용
- 성능이 중요한 경우 정규표현식 컴파일과 적절한 알고리즘 선택

### 2. **대소문자 처리의 세심함**
- 영어 텍스트: `lower()`/`upper()`로 충분
- 국제화(i18n)가 필요한 경우: `casefold()` 사용
- 발음 구별 기호가 있는 언어: 유니코드 정규화(`unicodedata.normalize`) 고려

### 3. **성능과 가독성의 균형**
- 작은 텍스트: 가독성을 우선
- 대용량 텍스트: 성능 최적화 필요
- 반복 작업: 정규표현식 컴파일, 결과 캐싱 고려

### 4. **에러 처리와 견고성**
- 사용자 입력 처리: 검증과 이스케이프 필수
- 파일 처리: 인코딩 문제 대비
- 정규표현식: 잘못된 패턴으로 인한 성능 저하 방지

### 5. **미래를 대비한 코드 작성**
- 파이썬 3의 문자열 모델 이해(유니코드 기본 지원)
- f-문자열과 최신 문자열 메서드 활용
- 타입 힌트를 통한 코드 명확성 향상

문자열 처리는 단순한 기술이 아니라 데이터와 사용자 간의 교량 역할을 하는 중요한 기술입니다. 파이썬의 풍부한 문자열 처리 도구들을 효과적으로 활용하면 더 견고하고 효율적인 애플리케이션을 개발할 수 있으며, 이는 결국 더 나은 사용자 경험으로 이어집니다. 각 상황에 맞는 최적의 도구를 선택하고, 성능과 가독성, 유지보수성을 고려한 균형 잡힌 접근 방식이 성공적인 문자열 처리의 핵심입니다.