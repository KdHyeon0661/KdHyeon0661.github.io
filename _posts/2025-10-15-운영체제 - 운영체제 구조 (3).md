---
layout: post
title: 운영체제 - 운영체제 구조 (3)
date: 2025-10-15 19:25:23 +0900
category: 운영체제
---
# Operating-System Structure • Build/Boot • Debugging (실행 예제 포함)

본 장은 운영체제의 **구조(모놀리식·레이어드·마이크로커널·모듈·하이브리드)**를 비교하고,  
**빌드/부팅(Generation·Boot)** 과정을 실제 명령과 최소 예제로 재현한 뒤,  
**디버깅(장애분석·성능튜닝·트레이싱·BCC/eBPF)** 까지 한 번에 정리한다.

---

## 2.8 Operating-System Structure

운영체제 구조는 **성능–안정성–보안–유지보수성** 사이의 트레이드오프를 다르게 최적화한다.

### 2.8.1 Monolithic Structure (모놀리식)

- **특성**: 대부분의 OS 기능(스케줄러, VM, VFS, 네트워크, 드라이버)이 **커널 공간**에서 동작.
- **장점**: 시스템 호출 경로 짧음, **컨텍스트 스위치/IPC 비용↓** → **성능↑**.
- **단점**: 코드베이스 크고 신뢰 경계 넓음(버그·취약점 영향 범위↑), 모듈성·격리 약함.

**사례**: Linux(모놀리식+모듈), 전통적 UNIX.

#### [관찰 실습] 시스템콜 경로 체감
```bash
# 문자열 1줄 출력의 커널 경로 관찰(리눅스)
strace -o /tmp/w.trace -e write echo "hello"
sed -n '1,10p' /tmp/w.trace
```

---

### 2.8.2 Layered Approach (레이어드)

- **개념**: 하드웨어 ↔ 커널 코어 ↔ 서비스/서브시스템 ↔ API ↔ 응용, 계층 간 **명확한 인터페이스**.
- **장점**: **캡슐화**로 테스트/검증 더 쉬움, 유지보수 용이.
- **단점**: 계층 경유 오버헤드, 유연성 제약(경로가 길어질 수 있음).

#### [유저 공간 레이어링 미니 모델]
```python
# layered.py — "디스크" 레이어 ↔ "파일" 레이어 ↔ "API" 레이어
class BlockDev:
    def __init__(self): self.blocks={}
    def read(self,bid): return self.blocks.get(bid, b"\x00"*4096)
    def write(self,bid,data): self.blocks[bid]=data

class FS:  # 매우 단순화
    def __init__(self, dev): self.dev, self.map={}, dev
    def create(self, name, data):
        bid=len(self.map); self.dev.write(bid, data); self.map[name]=bid
    def read(self, name): return self.dev.read(self.map[name])

class API:
    def __init__(self, fs): self.fs=fs
    def put(self, path, data:bytes): self.fs.create(path, data)
    def get(self, path)->bytes: return self.fs.read(path)

d=BlockDev(); fs=FS(d); api=API(fs)
api.put("hello.txt", b"HELLO")
print(api.get("hello.txt"))
```

---

### 2.8.3 Microkernels (마이크로커널)

- **개념**: 커널에는 **최소 기능**(스케줄·IPC·기본 메모리 관리)만 두고, 파일시스템/드라이버/네트워킹은 **사용자 공간 서버**로 분리.
- **장점**: **격리/신뢰 경계↑**, 장애 범위 축소, 컴포넌트 교체 용이.
- **단점**: **IPC·컨텍스트 스위치** 증가 → 성능 비용 가능.

**사례**: Mach, L4 계열, Minix3, seL4.

#### [개념 시뮬] 사용자 공간 파일서버를 통한 read
```python
# micro_ipc.py — "커널"은 메시지 큐만 제공, 파일서버가 read 서비스
from queue import Queue
kernel_ipc = {"fs": Queue(), "proc": Queue()}

# 파일서버(유저 공간)
def fs_server():
    store = {"a.txt": b"alpha"}
    while True:
        dst, req = kernel_ipc["fs"].get()
        if req["op"]=="read":
            data = store.get(req["path"], b"")
            kernel_ipc[dst].put({"ok":True,"data":data})

# 프로세스(클라이언트)
def proc_read(path):
    kernel_ipc["fs"].put(("proc", {"op":"read","path":path}))
    resp = kernel_ipc["proc"].get()
    return resp["data"]

# 실행
import threading
threading.Thread(target=fs_server, daemon=True).start()
print(proc_read("a.txt"))
```

---

### 2.8.4 Modules (커널 모듈)

- **개념**: 모놀리식 커널 + **동적 로딩 가능한 모듈**(드라이버/파일시스템)을 지원.
- **장점**: 런타임 확장, 불필요한 기능 미적재로 **공간/공격면↓**.
- **예**: Linux `insmod/rmmod/modprobe`.

#### [실습] 모듈 개념 맛보기(유저 공간 흉내)
```bash
# "모듈"을 동적 라이브러리로 간주하고 런타임 로딩
cat > mod_add.c <<'C'
int add(int a,int b){ return a+b; }
C
gcc -shared -fPIC mod_add.c -o libmodadd.so

cat > host.c <<'C'
#include <dlfcn.h>
#include <stdio.h>
int main(){
    void* h=dlopen("./libmodadd.so", 1);
    int (*add)(int,int)=dlsym(h,"add");
    printf("add=%d\n", add(3,5));
    return 0;
}
C
gcc host.c -ldl -o host && ./host
```

---

### 2.8.5 Hybrid Systems (하이브리드)

- **개념**: 마이크로커널 아이디어(모듈성·IPC)와 모놀리식 성능을 **절충**.  
- **사례**: Windows NT(하이브리드), XNU(마하+BSD), 현대 모바일 OS.

#### 2.8.5.1 macOS and iOS (XNU)

- **구성**: Mach 마이크로커널(+ IPC/스케줄/메모리) + **BSD 계층**(POSIX, VFS, 네트워크) + **IOKit**(드라이버, C++ 객체 모델).
- **보안/샌드박스**: 코드 서명, entitlements, SIP(System Integrity Protection).

#### 2.8.5.2 Android

- **커널**: Linux + Android 전용 패치(e.g., Binder IPC, wakelock, ashmem/ion 등).
- **프레임워크**: Zygote(앱 프로세스 fork), ART 런타임, SELinux(Enforcing), 권한 모델.
- **IPC**: **Binder**(고성능 RPC), 시스템 서비스들과의 상호작용 기반.

---

## 2.9 Building and Booting an Operating System

OS는 **부트스트랩**으로 메모리로 적재되고, 하드웨어 초기화 후 사용자 프로세스를 실행한다.

### 2.9.1 Operating-System Generation (커널 빌드/구성)

- **설정**: 아키텍처/드라이버/파일시스템 선택  
- **빌드**: 커널 이미지, 모듈, initramfs  
- **패키징**: 부트로더 설정 업데이트

#### [리눅스 간단 빌드 흐름(개념)]
```bash
# 소스 트리에서(도구 설치 필요):
make defconfig
make -j$(nproc) bzImage modules
# 모듈 설치(루트 필요): make modules_install
# QEMU 테스트:
qemu-system-x86_64 -kernel arch/x86/boot/bzImage -m 1024 -nographic \
  -append "console=ttyS0" -initrd rootfs.cpio.gz
```

### 2.9.2 System Boot (부트 과정)

**UEFI/BIOS → Bootloader → Kernel → init** 의 단계.

1) **UEFI/BIOS**: 하드웨어 초기 셋업, 부팅 장치 선택  
2) **Bootloader**(GRUB/Syslinux): 커널 이미지(+initrd) 로드, 커맨드라인 전달  
3) **Kernel 초기화**: MMU/페이지테이블, 장치 초기화, 루트파일시스템 마운트  
4) **init**(`systemd` 등): 서비스 그래프 시작, 사용자 세션 준비

#### [개념 미니부트: 2단계 부트로더 → 커널 C 진입]
```nasm
; boot1.asm — MBR 1단계(개념: 512바이트, 실사용은 주의)
BITS 16
org 0x7c00
; ... 디스크에서 2단계 로더 읽기 ...
; 여기서는 생략하고 메시지만
mov si, msg
call print
jmp $

print:
lodsb
or al, al
jz .done
mov ah, 0x0e
int 0x10
jmp print
.done: ret
msg db "Stage1",0

times 510-($-$$) db 0
dw 0xaa55
```

```c
/* kmain.c — C 레벨 진입(개념) */
extern void puts(const char*);
void kmain(void){
  puts("Hello, kernel!");
  for(;;) { __asm__ __volatile__("hlt"); }
}
```

> 실제 부트는 보호모드/롱모드 전환, GDT/IDT 설정, 페이징, 드라이버 초기화 등 많은 단계가 필요하다.

---

## 2.10 Operating-System Debugging

운영체제 디버깅은 **장애분석(Failure Analysis)**, **성능 모니터링/튜닝**, **트레이싱**으로 나뉜다.  
현대 리눅스는 **perf/ftrace/eBPF/BPFTRACE/BCC** 로 강력한 관측을 제공한다.

### 2.10.1 Failure Analysis (장애 분석)

- **커널 패닉/오옵스**: 스택트레이스/레지스터 덤프, kdump로 메모리 덤프 저장  
- **soft lockup/hard lockup**: NMI 워처, RCU stall  
- **I/O 오류**: S.M.A.R.T., dmesg, 장치 재시도/오류 코드

#### [실습] kdump 설정(개요)
```bash
# 패키지 설치 후
sudo systemctl enable --now kdump.service
# /etc/kdump.conf 조정 후 패닉 재현 실험(위험!)은 VM/테스트 환경에서만
```

#### [실습] dmesg/오류 패턴 찾기
```bash
dmesg -T | egrep -i 'error|fail|warn|panic' | tail -100
```

---

### 2.10.2 Performance Monitoring and Tuning (성능 모니터링/튜닝)

- **CPU**: `top`, `pidstat`, `perf stat`/`perf record`  
- **메모리**: `vmstat`, `free`, `sar -r`, `pmap/smaps`  
- **디스크**: `iostat -xz`, `blktrace`, `fio`  
- **네트워크**: `ss -tpi`, `sar -n DEV,TCP`, `nstat`  
- **스케줄러/컨텍스트 스위치**: `pidstat -w`, `perf sched`  
- **튜닝**: CPU 주파수/거버너, I/O 스케줄러, sysctl, cgroups

#### [실습] perf로 시스템콜 비용 보기
```bash
sudo perf stat -e cycles,instructions,cache-misses,syscalls:sys_enter_write \
  -- bash -c 'for i in $(seq 1 10000); do echo x > /dev/null; done'
```

#### [실습] iostat로 디스크 병목 탐지
```bash
iostat -xz 1 | awk 'NR<4 || $1=="Device:" || $NF>10 {print}'
# %util, await, svctm, r/s, w/s 확인
```

---

### 2.10.3 Tracing (커널/유저 경로 트레이싱)

- **ftrace**: 커널 함수 진입/리턴, 스케줄, IRQ, 이벤트 트레이스
- **perf**: 샘플 기반 프로파일링, `perf script`로 스택 분석
- **bpftrace/eBPF/BCC**: 동적 probe(kprobe/tracepoint/USDT), 안전한 BPF bytecode

#### [실습] ftrace로 함수 호출 추적
```bash
sudo su -c 'echo function > /sys/kernel/debug/tracing/current_tracer'
sudo su -c 'echo sched_switch > /sys/kernel/debug/tracing/set_ftrace_filter || true'
sudo su -c 'echo 1 > /sys/kernel/debug/tracing/tracing_on'
sleep 1
sudo su -c 'echo 0 > /sys/kernel/debug/tracing/tracing_on; cat /sys/kernel/debug/tracing/trace | head -50'
```

#### [실습] bpftrace — 특정 시스템콜 카운트
```bash
sudo bpftrace -e 'tracepoint:syscalls:sys_enter_openat { @[comm] = count(); }'
# Ctrl-C 후 프로세스별 openat 호출 빈도 출력
```

---

### 2.10.4 BCC (BPF Compiler Collection)

**BCC**는 eBPF 프로그램을 **Python/C++에서 작성/로딩**하게 해주는 프레임워크.  
리눅스 트레이싱에 광범위하게 쓰이는 도구(execsnoop, opensnoop, biolatency, tcplife …)가 포함되어 있다.

> 요구: 커널 eBPF 지원 + bcc 패키지 설치. **루트/권한** 필요.

#### [예제 1] opensnoop.py — 파일 open 이벤트 추적(준비된 예제 실행)
```bash
sudo /usr/share/bcc/tools/opensnoop
# PID COMM FD ERR PATH 형태로 실시간 표시
```

#### [예제 2] execsnoop.py — 실행 추적
```bash
sudo /usr/share/bcc/tools/execsnoop -l 1  # 실행 경로/인자
```

#### [예제 3] biolatency.py — 블록 I/O 레이턴시 히스토그램
```bash
sudo /usr/share/bcc/tools/biolatency 1 3
# 1초 주기로 3회 수집, ms 단위 히스토그램
```

#### [예제 4] “나만의” BCC 스크립트(간단 kprobe: do_sys_openat2)

> 커널 심볼/버전에 따라 함수명이 다를 수 있다. tracepoint 기반이 더 호환성 좋다.

```python
# my_open_trace.py — openat 호출자의 경로 출력(요약)
from bcc import BPF
prog = r"""
#include <uapi/linux/ptrace.h>
#include <linux/sched.h>

int kprobe__do_sys_openat2(struct pt_regs *ctx) {
    u32 pid = bpf_get_current_pid_tgid() >> 32;
    bpf_trace_printk("openat2 pid=%d\\n", pid);
    return 0;
}
"""
b = BPF(text=prog)
b.trace_print()
```

```bash
sudo python3 my_open_trace.py
# 다른 터미널에서 파일 열기 동작을 발생시키면 커널 trace_pipe에 로그가 보임
```

#### [예제 5] tracepoint 기반(권장) — `sys_enter_openat`
```python
# my_open_tp.py
from bcc import BPF
prog = r"""
TRACEPOINT_PROBE(syscalls, sys_enter_openat) {
    bpf_trace_printk("openat: pid=%d\\n", bpf_get_current_pid_tgid()>>32);
    return 0;
}
"""
b = BPF(text=prog)
b.trace_print()
```

---

## 종합 시나리오 — “정적 파일 서버”를 구조/부팅/관측으로 엮어보기

1) **구조 선택**: 모놀리식(단일 프로세스 + 스레드), 모듈(플러그인 핸들러 dlopen), 마이크로(파일서버 프로세스 분리)로 **3변형** 구현.
2) **서비스화**: systemd 소켓 활성화(유휴 시 프로세스 0).
3) **부팅 경로**: QEMU 가상머신에서 커널 커맨드라인로 `init=`를 달리하여 각 변형 자동 실행 비교.
4) **장애 주입**: 파일 핸들러 플러그인에 예외 유발 → 모놀리식 vs 모듈 vs 마이크로의 **격리/복구** 차이 관측.
5) **관측/튜닝**: BCC `biolatency`, `tcplife`, bpftrace로 **I/O·TCP 지연**을 수집. `perf`로 sys_write 경로 프로파일.

**지표 직관 수식**  
동시 접속 $$k$$, 요청당 서버 처리시간 $$T_s$$, 디스크 대기 $$D$$, 네트워크 왕복 $$R$$, IPC 추가 비용(마이크로) $$I$$라 하면
$$
T_{\text{mono}} \approx T_s + D + R,\qquad
T_{\text{micro}} \approx T_s + D + R + I.
$$
IPC 비용 $$I$$는 구조/구현에 따라 상수 항으로 붙지만, **장애 격리 확률**을 높여 **MTTR↓**로 이어진다.  
튜닝 목표는 $$D$$(캐시/리드어헤드/스케줄러)와 $$T_s$$(복잡도) 최소화, 필요 시 $$I$$도 **공유 메모리·zero-copy**로 줄인다.

---

## 체크리스트 요약

- **구조**:  
  - 모놀리식—성능↑, 신뢰 경계 큼  
  - 레이어드—캡슐화·검증 용이  
  - 마이크로—IPC 비용↔격리·신뢰성  
  - 모듈—런타임 확장/공격면 축소  
  - 하이브리드—절충(XNU/NT/Android)
- **빌드/부트**: 커널 설정→이미지→부트로더→커널 init→`systemd`  
- **디버깅**:  
  - 장애분석—kdump/dmesg  
  - 성능—perf/iostat/sar/fio  
  - 트레이싱—ftrace/perf/eBPF  
  - **BCC**—execsnoop/opensnoop/biolatency + 맞춤 스크립트

---

## 더 해보기(실습 과제)

1) **모듈형 핸들러**를 두 개(`libplain.so`, `libgzip.so`) 만들어 길이 1MB 이상 요청만 gzip 압축 후 응답. `LD_PRELOAD`로 write 후킹하여 성능 영향 관찰.  
2) QEMU에서 **두 개 커널**(CONFIG 옵션 상이)을 번갈아 부팅, `perf stat`으로 syscall/branch-misses 비교.  
3) BCC `tcpretrans`로 **TCP 재전송** 이벤트 추적 후, `tc qdisc netem loss 1%`에서 재현.  
4) 마이크로 구조에서 **공유 메모리 + ring buffer**로 IPC 비용 $$I$$를 줄여, 처리량 변화를 그래프로 그려보기.

---

## 마무리

운영체제 구조의 선택은 **성능/격리/유지보수**라는 삼각형의 어느 꼭짓점을 더 강조할지 결정하는 일이다.  
빌드–부팅 파이프라인을 이해하면 **하부 구현의 제약**이 보이고, BCC/eBPF 같은 현대적 관측 도구는  
실 시스템에서 **원인→현상**을 잇는 **증거**를 제공한다. 이제 여러분의 실험 환경(QEMU/VM)에서 본문 예제를 조립해,  
**구조별 지연·처리량·장애 복원력**을 수치로 비교해 보자.