---
layout: post
title: 선형대수 - 그래프 라플라시안과 고유분해
date: 2025-06-27 22:20:23 +0900
category: 선형대수
---
# 그래프 라플라시안과 고유분해

> **요점 한 줄**
> 그래프 라플라시안 $$L=D-A$$(또는 정규화 $$L_{\mathrm{sym}},L_{\mathrm{rw}}$$)의 **고유값·고유벡터**는 **연결성, 군집 구조, 부드러움(스무딩), 확산(heat)** 등 그래프의 핵심 특성을 드러낸다. **Fiedler 값/벡터**는 2-분할의 기준이며, **작은 고유벡터들을 모아 임베딩**한 뒤 **k-means**를 하면 **스펙트럴 클러스터링**이 된다.

---

## 기본 정의: 인접행렬, 차수행렬, 라플라시안

### 인접행렬(Adjacency)과 차수행렬(Degree)

- 무향 가중 그래프 \(G=(V,E,w)\), \(|V|=n\).
- 인접행렬 \(A\in\mathbb{R}^{n\times n}\),
  $$A_{ij}=\begin{cases}w_{ij}>0,&(i,j)\in E\\ 0,&\text{else}\end{cases}$$
- 차수행렬 \(D=\operatorname{diag}(d_1,\dots,d_n)\),
  $$d_i=\sum_j A_{ij}.$$

> **참고**: 자기루프( \(A_{ii}>0\) )는 \(D\)에 더해져 정규화에 영향을 준다. 무향 그래프는 \(A=A^{\mathsf T}\).

### 라플라시안(Laplacian)과 정규화 라플라시안

- **비정규화 라플라시안**:
  $$
  \boxed{L=D-A}
  $$
- **정규화 라플라시안(대칭형)**:
  $$
  \boxed{L_{\mathrm{sym}}=I-D^{-1/2}AD^{-1/2}=D^{-1/2}LD^{-1/2}}
  $$
- **정규화 라플라시안(랜덤워크형)**:
  $$
  \boxed{L_{\mathrm{rw}}=I-D^{-1}A}
  $$

> 해석: \(L\)은 **이산 그래디언트의 발산**에 해당. \(L_{\mathrm{rw}}\)는 한 스텝 랜덤워크의 잔차.

---

## 핵심 성질(증명 스케치 포함)

### 대칭성·양의 반정정부호(PSD)

모든 \(x\in\mathbb{R}^n\)에 대해
$$
\boxed{x^{\mathsf T}Lx=\tfrac12\sum_{i,j}A_{ij}(x_i-x_j)^2\ge 0}
$$
따라서 \(L\)은 **대칭 PSD**. \(L_{\mathrm{sym}}\)도 대칭 PSD, \(L_{\mathrm{rw}}\)는 보통 비대칭이지만 유사 대칭화 가능.

### 고유값과 연결 성분

- \(L\mathbf{1}=\mathbf{0}\) → **\(\lambda_0=0\)** 고유값 존재.
- 0 고유값의 **중복도 = 연결 성분의 개수**.
- 각 성분의 지시벡터가 영공간을 이룬다.

### 정규화 라플라시안의 스펙트럼 범위

- \(L_{\mathrm{sym}}\)의 고유값은 \([0,2]\)에 놓인다.
- \(\lambda=2\)는 이분그래프의 지표가 되기도 한다.

### 레일리 몫과 변분 특성

- 레일리 몫
  $$
  R_L(x)=\frac{x^{\mathsf T}Lx}{x^{\mathsf T}x}.
  $$
- 가장 작은 고유값 \(\lambda_0=\min_{x\neq0}R_L(x)=0\) (최소점은 상수벡터).
- **Fiedler 값** \(\lambda_1=\min_{x\perp\mathbf{1}}R_L(x)\): 두 번째 고유값. 작을수록 “약하게 연결됨”.

### (개념만)

그래프 절단값(노멀라이즈드 컷)과 \(\lambda_1\) 사이에 상·하한이 존재.
→ **Fiedler 벡터**를 임계값으로 자르면 좋은 분할을 준다(근사 보장).

---

## 스펙트럴 클러스터링: 알고리즘 설계

### 2-분할(언노멀라이즈드)

1. \(L=D-A\) 구성.
2. \(\lambda_1\)에 대응하는 **Fiedler 벡터** \(u_1\) 계산.
3. \(u_1\)의 부호(또는 중간값 임계)로 노드를 두 집합으로 분할.

### k-분할(정규화 버전, 실무 표준)

1. **유사도/그래프** 구성: 가우시안 커널 등으로 \(A\) 만들고 \(D\) 계산.
2. \(L_{\mathrm{sym}}=I-D^{-1/2}AD^{-1/2}\).
3. **가장 작은 k개의 고유벡터** \(U\in\mathbb{R}^{n\times k}\) 추출.
4. 각 행을 정규화(단위 길이).
5. 행벡터들에 **k-means** → 클러스터 라벨.

> **k 선택**: **eigen-gap**( \(\lambda_k\)와 \(\lambda_{k+1}\)의 급격한 차이 )을 이용.

---

## 예시로 직관 얻기

### 경로 그래프 \(0-1-2\)

$$
A=\begin{bmatrix}0&1&0\\1&0&1\\0&1&0\end{bmatrix},\quad
D=\begin{bmatrix}1&0&0\\0&2&0\\0&0&1\end{bmatrix},\quad
L=\begin{bmatrix}1&-1&0\\-1&2&-1\\0&-1&1\end{bmatrix}.
$$

고유값은
$$
\lambda_0=0,\quad \lambda_1=1,\quad \lambda_2=3.
$$
\(\lambda_1\)의 고유벡터는 중간 노드(1)를 경계로 “좌–우”를 나누는 방향.

### 두 클러스터가 약하게 연결된 그래프

두 완전그래프 \(K_{m},K_{n}\)를 얇은 간선 하나로 연결.
\(\lambda_1\)이 **작아지고**, 해당 고유벡터는 두 덩어리에 **서로 반대 부호**를 갖는다 → 분할 신뢰도↑.

---

## 그래프 신호처리(GSP): 부드러움, 푸리에, 확산

### 라플라시안 이차형식과 부드러움

신호 \(x\in\mathbb{R}^n\)의 **변동(부드러움 비용)**:
$$
\boxed{x^{\mathsf T}Lx=\tfrac12\sum_{(i,j)}A_{ij}(x_i-x_j)^2}
$$
작을수록 인접노드 간 값이 비슷(부드러움↑).

### 그래프 푸리에 변환(GFT)

- \(L=Q\Lambda Q^{\mathsf T}\) (정규직교 \(Q=[q_0,\dots,q_{n-1}]\)).
- **주파수 성분**: \(\hat x=Q^{\mathsf T}x\).
- 작은 \(\lambda\) = 저주파(느린 변화), 큰 \(\lambda\) = 고주파.

### 열확산(Heat diffusion)·필터링

- 열방정식: \(\dot f(t)=-Lf(t)\Rightarrow f(t)=e^{-tL}f(0)\).
- 필터 \(g(\Lambda)\)로 스펙트럼 필터링: \(x_{\mathrm{filt}}=Qg(\Lambda)Q^{\mathsf T}x\).
  (예: **heat kernel** \(g(\lambda)=e^{-t\lambda}\), **저역통과** \(g(\lambda)=\mathbf{1}_{\lambda\le\lambda_c}\))

### 라플라시안 정규화(Tikhonov)

- 회귀·보간에서
  $$
  \min_x \|y-Mx\|_2^2+\alpha\,x^{\mathsf T}Lx
  $$
  는 **이웃-평활화**를 강제.

---

## 세미·자기지도 학습: 라벨 전파(조화 함수)

라벨 있는 집합 \(L\), 없는 집합 \(U\), 순서대로 노드 정렬:
$$
L=\begin{bmatrix}L_{LL}&L_{LU}\\L_{UL}&L_{UU}\end{bmatrix},\quad
\text{라벨 }y_L\ \text{고정}.
$$
**조화(harmonic) 해**는
$$
\boxed{L_{UU}x_U=-L_{UL}y_L}
$$
을 풀어 \(x_U\) 를 얻는다(라벨 전파).

---

## 구현 실전: SciPy/NetworkX/PyTorch

### 작은 그래프의 라플라시안과 Fiedler 벡터

```python
import numpy as np
from scipy.linalg import eigh

# 경로 그래프 0-1-2

A = np.array([[0,1,0],
              [1,0,1],
              [0,1,0]], dtype=float)
D = np.diag(A.sum(axis=1))
L = D - A

# 고유분해: 오름차순

w, V = eigh(L)
print("고유값:", w)           # [0., 1., 3.]
print("Fiedler 벡터:", V[:,1]) # 두 번째 고유벡터
```

### 정규화 라플라시안과 k-분할 스펙트럴 클러스터링

```python
import numpy as np
from numpy.linalg import norm
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import eigsh
from sklearn.cluster import KMeans

def normalized_spectral_clustering(A, k=2, use_sparse=True):
    """
    입력: 대칭 인접행렬 A (ndarray 또는 csr_matrix), 클러스터 수 k
    절차: L_sym의 가장 작은 k개 고유벡터 -> 행 정규화 -> k-means
    """
    # 희소/밀집 처리
    if use_sparse:
        if not isinstance(A, csr_matrix):
            A = csr_matrix(A)
        d = np.array(A.sum(axis=1)).ravel()
        d_sqrt_inv = np.power(d, -0.5, where=d>0)
        Dsi = csr_matrix(np.diag(d_sqrt_inv))
        # L_sym = I - D^{-1/2} A D^{-1/2}
        I = csr_matrix(np.eye(A.shape[0]))
        Lsym = I - Dsi @ A @ Dsi
        # 가장 작은 k개 고유쌍 (symmetric positive semi-definite → 'SM')
        vals, vecs = eigsh(Lsym, k=k, which='SM')  # (n,k)
        U = vecs
    else:
        d = A.sum(axis=1)
        Dsi = np.diag(1.0/np.sqrt(np.where(d>0,d,1.0)))
        Lsym = np.eye(A.shape[0]) - Dsi @ A @ Dsi
        vals, U = np.linalg.eigh(Lsym)
        U = U[:, :k]

    # 행 정규화
    rownorm = np.linalg.norm(U, axis=1, keepdims=True)
    rownorm[rownorm==0] = 1.0
    Y = U / rownorm

    # k-means
    labels = KMeans(n_clusters=k, n_init=10, random_state=0).fit_predict(Y)
    return labels, (vals, U)

# 데모: 두 개의 조밀 클러스터 + 희박 연결

def two_cluster_graph(n1=15, n2=15, p_in=0.8, p_out=0.02, seed=0):
    rng = np.random.default_rng(seed)
    n = n1 + n2
    A = np.zeros((n,n))
    # 내부 연결
    for i in range(n1):
        for j in range(i+1, n1):
            if rng.random()<p_in:
                A[i,j]=A[j,i]=1
    for i in range(n1, n):
        for j in range(i+1, n):
            if rng.random()<p_in:
                A[i,j]=A[j,i]=1
    # 사이 연결
    for i in range(n1):
        for j in range(n1, n):
            if rng.random()<p_out:
                A[i,j]=A[j,i]=1
    return A

A = two_cluster_graph()
labels, (vals, U) = normalized_spectral_clustering(A, k=2)
print("고유값(작은 순):", vals[:2])
print("라벨 분포:", np.bincount(labels))
```

### 그래프 푸리에/열필터(저역통과)

```python
import numpy as np
from scipy.sparse import csgraph
from scipy.linalg import eigh

# 인접행렬 A → 라플라시안 L

def laplacian_dense(A):
    D = np.diag(A.sum(axis=1))
    return D - A

# GFT: L = Q Λ Q^T

def gft(L, x):
    w, Q = eigh(L)           # 작은 그래프 가정(밀집)
    xhat = Q.T @ x           # 스펙트럼 계수
    return w, Q, xhat

# Heat filter g(λ)=exp(-tλ)

def heat_filter(L, x, t=1.0):
    w, Q = eigh(L)
    g = np.exp(-t*w)
    return Q @ (g * (Q.T @ x))

# 데모

A = two_cluster_graph()
L = laplacian_dense(A)
x = np.random.randn(A.shape[0])
x_smooth = heat_filter(L, x, t=1.0)
print("원신호 변동 x^T L x:", x.T@L@x)
print("스무딩 후 변동:", x_smooth.T@L@x_smooth)
```

### — 작은 예시

```python
import numpy as np
from numpy.linalg import solve

# 간단한 그래프와 일부 라벨

A = np.array([
    [0,1,1,0,0],
    [1,0,1,1,0],
    [1,1,0,0,0],
    [0,1,0,0,1],
    [0,0,0,1,0]], dtype=float)

n = A.shape[0]
L = np.diag(A.sum(axis=1)) - A

labeled = np.array([0,3])   # 노드 0,3 에 라벨 존재(예: 클래스 +1,-1)
yL = np.array([1.0, -1.0])
unlabeled = np.array([i for i in range(n) if i not in labeled])

# 블록 분해 L=[LL LU; UL UU]

LL = L[np.ix_(labeled, labeled)]
LU = L[np.ix_(labeled, unlabeled)]
UL = L[np.ix_(unlabeled, labeled)]
UU = L[np.ix_(unlabeled, unlabeled)]

# 조화 조건: L_UU x_U = - L_UL y_L

xU = solve(UU, -UL @ yL)
x = np.zeros(n)
x[labeled] = yL
x[unlabeled] = xU
print("전파된 점수:", np.round(x,3))
print("부호로 예측 라벨:", np.sign(x))
```

### PyTorch로 작은 고유문제(연구/실험용)

```python
import torch

def laplacian_torch(A: torch.Tensor):
    d = A.sum(dim=1)
    return torch.diag(d) - A

A = torch.tensor([[0.,1.,0.],
                  [1.,0.,1.],
                  [0.,1.,0.]])
L = laplacian_torch(A)
# torch.linalg.eigh 는 대칭행렬 고유분해

w, Q = torch.linalg.eigh(L)
print("고유값:", w)
print("Fiedler:", Q[:,1])
```

> 대규모 그래프는 **희소(Sparse)** 로 저장하고 `eigsh`(Lanczos), LOBPCG, Randomized eigensolvers 사용.

---

## 방향 그래프(Directed)와 변형 라플라시안

- **방향 그래프**: 출차수 \(D_{\text{out}}=\operatorname{diag}(A\mathbf{1})\), 전이행렬 \(P=D_{\text{out}}^{-1}A\).
  **랜덤워크 라플라시안**:
  $$
  L_{\mathrm{rw}}^{\rightarrow}=I-P.
  $$
- 정상분포 \(\pi^{\mathsf T}P=\pi^{\mathsf T}\) 를 이용해 대칭화:
  $$
  L_{\text{sym}}^{\rightarrow}=\tfrac12\left(\Pi^{1/2}(I-P)\Pi^{-1/2}+\Pi^{-1/2}(I-P^{\mathsf T})\Pi^{1/2}\right),
  $$
  (\(\Pi=\operatorname{diag}(\pi)\))
  또는 **마그네틱 라플라시안** 등 위상(방향성) 보존 방식도 존재.

> 방향성 데이터는 **PageRank/Random Walk** 해석과 함께 쓰는 것이 실무적으로 안정적.

---

## 계산·스케일: 실무 주의사항

- **희소성 활용**: \(A\)는 보통 희소. CSR/CSC로 저장, `eigsh(k, which='SM')`로 작은 고유쌍만.
- **정규화 선택**: **정규화 라플라시안**이 **차수 편차가 큰 그래프**에 더 안정적.
- **고립 노드/성분**: 분리 성분이 있으면 0 고유값이 여러 개 → 성분별 처리 또는 소규모 연결 추가.
- **k 선택**: **eigen-gap** + 안정적이면 실루엣 점수/모듈러리티로 보조.
- **수치 안정성**: 매우 큰 그래프는 **근사**(Randomized/Lanczos), **샘플링**(Nystrom) 사용.
- **가중치 스케일**: 유사도 커널(가우시안)에서 \(\sigma\) 선택이 중요(언더/오버-스무딩 방지).

---

## 응용 스냅샷

- **스펙트럴 클러스터링**: 문서/이미지/소셜그래프 분할.
- **라플라시안 임베딩**: Laplacian Eigenmaps, 노드 임베딩(저차원화).
- **그래프 신호 스무딩**: 센서 네트워크 노이즈 제거, 반칙치 보정.
- **이미지 분할**: Normalized Cuts.
- **물리/전기망**: **효과저항** \(R_{ij}=(e_i-e_j)^{\mathsf T}L^{+}(e_i-e_j)\), 통행시간/커뮤트 타임과 연결.
- **GCN 관점**: \(\tilde{A}=A+I\), \(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\) 로 메시지 패싱(스무딩) 구현.

---

## 요약 표

| 주제 | 핵심 수식/개념 | 실전 포인트 |
|---|---|---|
| 라플라시안 | $$L=D-A,\ L_{\mathrm{sym}}=I-D^{-1/2}AD^{-1/2},\ L_{\mathrm{rw}}=I-D^{-1}A$$ | PSD, 0 고유값 ↔ 성분 수 |
| 부드러움 | $$x^{\mathsf T}Lx=\tfrac12\sum A_{ij}(x_i-x_j)^2$$ | 정규화항으로 많이 사용 |
| Fiedler | 두 번째 고유값/벡터 $$\lambda_1,u_1$$ | 2-분할 기준, 체거 부등식 |
| k-클러스터링 | 작은 k개 고유벡터 → 행 정규화 → k-means | eigen-gap으로 k 택 |
| GFT/필터 | \(x=Q\hat x,\ g(L)=Qg(\Lambda)Q^{\mathsf T}\) | heat/저역통과 필터 |
| 라벨 전파 | $$L_{UU}x_U=-L_{UL}y_L$$ | 반지도(supervised prior) |
| 대규모 | 희소 + eigsh/LOBPCG/Randomized | 정규화/전처리 필수 |

---

## 체크리스트 (현업)

- [ ] 그래프가 **무향/가중/희소**인지 확인, 필요시 대칭화.
- [ ] **정규화 라플라시안** 기본값으로 고려(차수 편차 완화).
- [ ] **eigen-gap**으로 \(k\) 후보 설정, 라벨 없으면 안정성 지표 병행.
- [ ] 고립 노드 처리(제거/작은 self-loop).
- [ ] 대규모는 **희소 고유해석기** + **근사**.
- [ ] 결과 해석 시 **Fiedler 벡터 부호/크기** 시각화로 검증.

---

### 스펙트럴 클러스터링 — 간단 2-way 함수 (Thresholding)

```python
import numpy as np
from scipy.sparse.linalg import eigsh

def spectral_bipartition(A):
    """ L = D - A 의 Fiedler 벡터 부호로 2-분할 """
    d = np.array(A.sum(axis=1)).ravel() if hasattr(A, 'sum') else A.sum(axis=1)
    L = np.diag(d) - A
    # 가장 작은 두 개 고유쌍 (0과 Fiedler)
    vals, vecs = np.linalg.eigh(L) if isinstance(L, np.ndarray) else eigsh(L, k=2, which='SM')
    u1 = vecs[:, 1]
    thr = np.median(u1)
    labels = (u1 > thr).astype(int)
    return labels, (vals, vecs)
```

### `networkx`로 그래프 생성 + Laplacian 사용

```python
import networkx as nx
import numpy as np
from scipy.sparse.linalg import eigsh

G = nx.barbell_graph(10, 1)  # 두 덩어리를 가는 다리로 연결
A = nx.to_scipy_sparse_array(G, format='csr', dtype=float)
d = np.array(A.sum(axis=1)).ravel()
Dsi = 1.0 / np.sqrt(np.maximum(d, 1e-12))
Lsym = nx.laplacian_matrix(G, weight=None).astype(float)  # L = D - A
# 대칭 정규화

from scipy.sparse import diags, identity
Lsym = identity(A.shape[0]) - diags(Dsi) @ A @ diags(Dsi)
vals, vecs = eigsh(Lsym, k=2, which='SM')
fiedler = vecs[:,1]
print("정규화 라플라시안 작은 고유값:", vals)
```

---

## 마무리

그래프 라플라시안은 **선형대수적 도구(고유분해)** 로 **네트워크의 구조**를 수량화한다.
**Fiedler 값/벡터**는 연결성을 정량화하고, **작은 고유공간**은 강력한 **저차원 임베딩**을 제공한다.
**스펙트럴 클러스터링**, **그래프 신호 필터링**, **라벨 전파**, **효과저항/통행시간** 등
다양한 문제를 **하나의 공통 언어**(라플라시안 스펙트럼)로 풀어낼 수 있다는 점이 가장 큰 힘이다.
