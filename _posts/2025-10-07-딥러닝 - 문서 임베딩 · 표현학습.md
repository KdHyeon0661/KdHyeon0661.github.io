---
layout: post
title: 딥러닝 - 문서 임베딩 · 표현학습
date: 2025-10-07 16:25:23 +0900
category: 딥러닝
---
# 문서 임베딩 · 표현학습 총정리

## 왜 임베딩인가?

- **고차원 희소 BOW/TF-IDF**는 표현력이 높지만,
  **형태 변이(교착/활용), 동의어, 의미적 거리**를 직접 반영하긴 어렵다.
- **분산 표현(Distributed Representation)**은
  $$\text{유사한 맥락} \Rightarrow \text{가까운 벡터}$$
  라는 가정으로 학습되어, **유사어/유관어**가 **코사인 공간**에서 가깝게 놓인다.
- 문서 임베딩은 (1) **단어 임베딩의 조합** 또는 (2) **문서 자체를 파라미터화**하거나
  (3) **문장/문서 인코더(트랜스포머)** 로 직접 얻을 수 있다.

---

## Word2Vec 이론 압축 정리

### 표기

- 코퍼스 토큰 열을 \(w_1,\dots,w_T\), 단어 집합을 \(\mathcal{V}\) 라 하자.
- 각 단어 \(w\in\mathcal{V}\) 에 대해 **입력 임베딩** \(\mathbf{v}_w \in \mathbb{R}^d\),
  **출력 임베딩** \(\mathbf{u}_w \in \mathbb{R}^d\) 를 갖는다(두 행렬).

### Skip-gram(단어 → 주변 예측)

중심 단어 \(c\) 가 주어질 때 윈도우 내 컨텍스트 \(o\) 를 예측:
$$
\max \sum_{t=1}^{T} \sum_{o \in \mathcal{N}(t)} \log p(o \mid c = w_t)
$$
소프트맥스:
$$
p(o \mid c) = \frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{\sum_{w \in \mathcal{V}} \exp(\mathbf{u}_w^\top \mathbf{v}_c)}
$$
→ \(|\mathcal{V}|\) 크기 합이 비싸므로 **대체 손실**을 쓴다.

### CBOW(주변 → 중심 예측)

컨텍스트 평균 \(\bar{\mathbf{v}} = \frac{1}{|\mathcal{N}|}\sum_{o\in\mathcal{N}} \mathbf{v}_o\) 로 중심 단어 \(c\) 를 예측:
$$
\max \sum_{t=1}^{T} \log p(c=w_t \mid \{o\})
,\quad
p(c \mid \{o\}) = \frac{\exp(\mathbf{u}_c^\top \bar{\mathbf{v}})}{\sum_{w\in\mathcal{V}}\exp(\mathbf{u}_w^\top \bar{\mathbf{v}})}
$$

### Negative Sampling(NS)

Skip-gram의 (c,o) 양성 쌍에 대해 **K개의 음성 단어** \(n_1,\dots,n_K\sim q(w)\) 를 뽑아
$$
\mathcal{L}_{\text{NS}}
= -\log \sigma(\mathbf{u}_o^\top \mathbf{v}_c)
 - \sum_{k=1}^{K} \log \sigma\!\big(-\mathbf{u}_{n_k}^\top \mathbf{v}_c\big),
\quad \sigma(x)=\frac{1}{1+e^{-x}}
$$
\(q(w)\) 는 보통 **빈도 분포의 \(0.75\) 제곱**:
$$
q(w) \propto f(w)^{0.75}
$$

### 서브샘플링(Subsampling, 빈도 높은 단어 버림)

매우 빈번한 단어의 학습 비중을 낮춰 의미적 신호를 강화:
$$
P_{\text{discard}}(w) = 1 - \sqrt{\frac{t}{f(w)}}
\quad (\text{보통 } t \approx 10^{-5})
$$

### 학습된 벡터 사용

- 단어 유사도: \(\cos(\mathbf{v}_a,\mathbf{v}_b)\)
- 단어 산술(고전 예):
  $$\mathbf{v}_{\text{king}} - \mathbf{v}_{\text{man}} + \mathbf{v}_{\text{woman}} \approx \mathbf{v}_{\text{queen}}$$

---

## PyTorch로 Skip-gram + Negative Sampling 구현

> **상황**: 한국어 위키 일부/회사 문서 로그 등으로 **도메인 단어 임베딩**을 만든다.
> **토크나이저**: 간단 예시는 공백 기준, 실제는 **BPE/SentencePiece**를 권장(이모지/외래어 안정).

### 데이터 전처리(어휘/빈도/서브샘플링/컨텍스트 쌍)

```python
# -*- coding: utf-8 -*-
# pip install tqdm

from collections import Counter, defaultdict
import math, random, re
from tqdm import tqdm

def simple_tokenize(s):
    # 데모용: 실제는 SentencePiece/BPE 권장
    s = re.sub(r"\s+", " ", s.strip())
    return s.split()

def build_vocab(lines, min_count=5, max_vocab=400_000):
    cnt = Counter()
    for ln in lines:
        cnt.update(simple_tokenize(ln))
    # 빈도 필터
    words = [w for w,c in cnt.most_common() if c >= min_count]
    if len(words) > max_vocab:
        words = words[:max_vocab]
    stoi = {w:i for i,w in enumerate(words, start=4)}
    # 특수 토큰
    PAD, UNK, BOS, EOS = 0,1,2,3
    itos = {i:w for w,i in stoi.items()}
    itos[PAD], itos[UNK], itos[BOS], itos[EOS] = "[PAD]","[UNK]","<s>","</s>"
    stoi["[PAD]"], stoi["[UNK]"], stoi["<s>"], stoi["</s>"] = PAD,UNK,BOS,EOS
    freqs = [0]*len(stoi)
    for w,c in cnt.items():
        freqs[stoi.get(w, UNK)] += c
    total = sum(freqs)
    probs = [f/total for f in freqs]
    return stoi, itos, freqs, probs

def subsample(tokens, stoi, probs, t=1e-5):
    # 확률적으로 자주 나오는 단어를 버림
    kept = []
    for w in tokens:
        idx = stoi.get(w, stoi["[UNK]"])
        f = probs[idx]
        p_discard = 1 - math.sqrt(t / max(f, 1e-12))
        if random.random() > max(0.0, p_discard):
            kept.append(w)
    return kept

def make_skipgram_pairs(lines, stoi, probs, window=5, dyn_window=True):
    pairs = []
    for ln in tqdm(lines, desc="pairs"):
        toks = simple_tokenize(ln)
        toks = subsample(toks, stoi, probs)
        idxs = [stoi.get(w, stoi["[UNK]"]) for w in toks]
        for i,c in enumerate(idxs):
            if dyn_window:
                w = random.randint(1, window)
            else:
                w = window
            left = max(0, i-w); right = min(len(idxs), i+w+1)
            for j in range(left, right):
                if j==i: continue
                pairs.append((c, idxs[j]))  # (center, context)
    return pairs
```

### 네거티브 샘플러(0.75 지수 분포)

```python
import numpy as np
import torch

class UnigramTable:
    def __init__(self, freqs, power=0.75):
        f = np.array(freqs, dtype=np.float64)
        f = np.power(f, power)
        self.prob = torch.tensor(f / f.sum(), dtype=torch.float)

    def sample(self, num_samples, device=None):
        # torch.multinomial 로 바로 샘플
        return torch.multinomial(self.prob.to(device or "cpu"),
                                 num_samples, replacement=True)
```

### + 손실(log-sigmoid)

```python
import torch.nn as nn
import torch.nn.functional as F

class SkipGramNS(nn.Module):
    def __init__(self, vocab_size, dim):
        super().__init__()
        self.in_emb  = nn.Embedding(vocab_size, dim)
        self.out_emb = nn.Embedding(vocab_size, dim)
        nn.init.uniform_(self.in_emb.weight,  -0.5/dim, 0.5/dim)
        nn.init.zeros_(self.out_emb.weight)  # 흔한 초기화 선택지

    def forward(self, center, pos, neg):
        """
        center: [B]
        pos   : [B]      (양성 컨텍스트)
        neg   : [B, K]   (음성)
        """
        v_c = self.in_emb(center)            # [B,d]
        u_o = self.out_emb(pos)              # [B,d]
        u_n = self.out_emb(neg)              # [B,K,d]

        pos_score = torch.sum(v_c * u_o, dim=1)                 # [B]
        neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) # [B,K]

        loss = -F.logsigmoid(pos_score).mean() \
               -F.logsigmoid(-neg_score).mean()
        return loss
```

### 학습 루프

```python
from torch.utils.data import Dataset, DataLoader
from tqdm import trange

class PairDataset(Dataset):
    def __init__(self, pairs): self.pairs = pairs
    def __len__(self): return len(self.pairs)
    def __getitem__(self, i): return self.pairs[i]

def train_skipgram_ns(lines, dim=200, window=5, K=5, min_count=5, epochs=2, bs=4096, lr=2e-3, device="cuda"):
    stoi, itos, freqs, probs = build_vocab(lines, min_count=min_count)
    pairs = make_skipgram_pairs(lines, stoi, probs, window=window, dyn_window=True)
    ds = PairDataset(pairs)
    dl = DataLoader(ds, batch_size=bs, shuffle=True, drop_last=True, num_workers=2)

    table = UnigramTable(freqs, power=0.75)
    model = SkipGramNS(vocab_size=len(stoi), dim=dim).to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr)

    for ep in range(1, epochs+1):
        running = 0.0
        for center, pos in tqdm(dl, desc=f"epoch {ep}"):
            center = center.to(device); pos = pos.to(device)
            neg = table.sample(center.size(0)*K, device=device).view(center.size(0), K)
            opt.zero_grad(set_to_none=True)
            loss = model(center, pos, neg)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            opt.step()
            running += loss.item()
        print(f"[ep {ep}] loss={running/len(dl):.4f}")
    # 임베딩 추출(일반적으로 in/out 평균이 안정적)
    with torch.no_grad():
        emb = (model.in_emb.weight + model.out_emb.weight) / 2.0
    return emb, stoi, itos

# 사용 예
# lines = open("corpus_ko.txt", encoding="utf-8").read().splitlines()
# emb, stoi, itos = train_skipgram_ns(lines, dim=300, window=5, K=10, epochs=3, min_count=10)

```

### 이웃/아날로지 빠른 테스트

```python
import torch
import numpy as np

def normed(e): return e / (e.norm(dim=1, keepdim=True)+1e-9)

def topk_sim(word, emb, stoi, itos, k=10):
    if word not in stoi: return []
    E = normed(emb)
    v = E[stoi[word]].unsqueeze(0)
    sim = torch.matmul(v, E.T).squeeze(0).cpu().numpy()
    idx = np.argpartition(-sim, range(k+1))[:k+1]
    idx = idx[np.argsort(-sim[idx])]
    return [(itos[i], float(sim[i])) for i in idx if itos[i]!=word][:k]

def analogy(a,b,c, emb, stoi, itos, k=10):
    for w in (a,b,c):
        if w not in stoi: return []
    E = normed(emb)
    v = E[stoi[b]] - E[stoi[a]] + E[stoi[c]]
    sim = torch.matmul(v, E.T).cpu().numpy()
    idx = np.argpartition(-sim, range(k))[:k]
    idx = idx[np.argsort(-sim[idx])]
    return [(itos[i], float(sim[i])) for i in idx if itos[i] not in (a,b,c)][:k]

# print(topk_sim("서울", emb, stoi, itos))
# print(analogy("남자","왕","여자", emb, stoi, itos))

```

> **팁**
> - 실제 서비스용은 **학습률 스케줄(워밍업/디케이)**, **더 큰 배치**(마이크로배치+그라디언트 누적)로 품질을 끌어올린다.
> - 토크나이저는 **서브워드**를 쓰고, 도메인 특수기호(URL/숫자/이모지) 규칙을 조정하면 훨씬 안정적이다.

---

## 간단 구현

```python
class CBOWNS(nn.Module):
    def __init__(self, vocab_size, dim):
        super().__init__()
        self.in_emb  = nn.Embedding(vocab_size, dim)
        self.out_emb = nn.Embedding(vocab_size, dim)
        nn.init.uniform_(self.in_emb.weight,  -0.5/dim, 0.5/dim)
        nn.init.zeros_(self.out_emb.weight)

    def forward(self, context, target, neg):
        """
        context: [B, M] (왼+오른쪽 M개 컨텍스트, PAD=0은 무시)
        target : [B]
        neg    : [B, K]
        """
        mask = (context!=0).float().unsqueeze(-1)           # [B,M,1]
        v_ctx = self.in_emb(context) * mask                 # [B,M,d]
        v_bar = v_ctx.sum(dim=1) / (mask.sum(dim=1)+1e-9)   # [B,d]
        u_t = self.out_emb(target)                           # [B,d]
        u_n = self.out_emb(neg)                              # [B,K,d]

        pos = torch.sum(v_bar*u_t, dim=1)
        negs= torch.bmm(u_n, v_bar.unsqueeze(2)).squeeze(2)
        loss= -F.logsigmoid(pos).mean() - F.logsigmoid(-negs).mean()
        return loss
```

- **경험칙**: **희귀 단어** 표현엔 Skip-gram이 유리,
  **빠른 학습/짧은 문장**엔 CBOW가 유리한 경우가 많다.

---

## 문서 임베딩 아이디어

문서 임베딩은 크게 4가지 축으로 생각하면 깔끔하다.

1) **단어 임베딩의 집계**: 평균/가중 평균/주의풀링/문장 토큰 집계
2) **SIF(Smooth Inverse Frequency)**: 빈도에 따른 가중 + 주성분 제거
3) **Doc2Vec(PV-DM/PV-DBOW)**: 문서를 **직접 파라미터화**
4) **트랜스포머 인코더**: [CLS] 또는 Mean-Pooling, 대조학습(SimCSE/SBERT 류)

### 단어 임베딩 평균(평균/TF-IDF/학습 가중)

- **평균**:
  $$
  \mathbf{d} = \frac{1}{|W_d|}\sum_{w\in W_d} \mathbf{v}_w
  $$
- **TF-IDF 가중 평균**:
  $$
  \mathbf{d} = \sum_{w\in W_d} \underbrace{\text{tf}(w,d)\cdot \text{idf}(w)}_{s_w}\,\mathbf{v}_w
  $$
- 장점: 간단/빠름/메모리 작음.
  단점: 어순/구문 정보 손실, 길이·주제 섞임에 취약.

**코드: TF-IDF 가중 평균**

```python
from collections import Counter
import math, numpy as np
import torch

def tfidf_doc_embeddings(docs, emb, stoi, smooth_idf=True):
    # docs: 토큰 리스트들의 리스트
    N = len(docs)
    df = Counter()
    for toks in docs:
        df.update(set(t for t in toks if t in stoi))
    idf = {}
    for w in df:
        idf[w] = math.log((N + int(smooth_idf)) / (df[w] + int(smooth_idf))) + 1.0
    D = emb.size(1)
    vecs = []
    for toks in docs:
        weights, mats = [], []
        cnt = Counter(t for t in toks if t in stoi)
        for w,c in cnt.items():
            tf = c / len(toks)
            weights.append(tf * idf.get(w, 0.0))
            mats.append(emb[stoi[w]].cpu().numpy())
        if not mats:
            vecs.append(np.zeros(D, dtype=np.float32)); continue
        mats = np.stack(mats, 0); weights = np.array(weights, dtype=np.float32)
        dvec = (weights[:,None]*mats).sum(0) / (weights.sum()+1e-9)
        vecs.append(dvec.astype(np.float32))
    return np.stack(vecs, 0)  # [num_docs, D]
```

### SIF(Arora et al.) — **빈도 기반 스무딩 + 주성분 제거**

- 단어 가중:
  $$
  a \in (0,1)\quad\text{(작은 상수)},\quad
  s_w = \frac{a}{a + p(w)}
  $$
  여기서 \(p(w)\) 는 말뭉치 단어 확률.
- 문서 벡터:
  $$
  \mathbf{d} = \frac{1}{|W_d|}\sum_{w\in W_d} s_w\,\mathbf{v}_w
  $$
- 모든 문서 벡터를 모아 **첫 번째 주성분 \(\mathbf{u}_1\)** 을 제거:
  $$
  \mathbf{d}' = \mathbf{d} - (\mathbf{u}_1^\top \mathbf{d})\,\mathbf{u}_1
  $$

**코드: SIF + 1PC 제거**

```python
import numpy as np

def sif_embeddings(docs, emb, stoi, word_prob, a=1e-3):
    D = emb.size(1); vecs=[]
    for toks in docs:
        xs=[]
        for w in toks:
            i = stoi.get(w)
            if i is None: continue
            s = a/(a + word_prob[i])
            xs.append(s * emb[i].cpu().numpy())
        if xs:
            vecs.append(np.mean(np.stack(xs,0), 0))
        else:
            vecs.append(np.zeros(D, dtype=np.float32))
    X = np.stack(vecs, 0)  # [N,D]
    # 첫 주성분 제거
    mu = X.mean(0, keepdims=True)
    Xc = X - mu
    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)
    u1 = Vt[0]  # [D]
    Xp = X - (X @ u1[:,None]) * u1[None,:]
    return Xp.astype(np.float32)
```

> **SIF 효과**: 빈번 단어 영향 감소 + 코퍼스 공통 방향 제거 → **주제성/의미**가 비교적 잘 남는다.

### Doc2Vec — **문서 ID를 임베딩으로 학습**

- **PV-DM**(Distributed Memory): 컨텍스트 단어 + **문서 벡터** → 중앙 단어 예측(=CBOW에 문서 벡터 추가).
- **PV-DBOW**(Distributed Bag of Words): **문서 벡터로 랜덤 단어**를 예측(=Skip-gram에서 중심이 문서 ID).

여기선 구현이 간단한 **PV-DBOW** 스케치:

```python
class Doc2VecDBOW(nn.Module):
    def __init__(self, num_docs, vocab_size, dim):
        super().__init__()
        self.doc_emb = nn.Embedding(num_docs, dim)     # 문서 벡터
        self.out_emb = nn.Embedding(vocab_size, dim)   # 단어 출력 벡터
        nn.init.uniform_(self.doc_emb.weight, -0.5/dim, 0.5/dim)
        nn.init.zeros_(self.out_emb.weight)

    def forward(self, doc_ids, pos_words, neg_words):
        d = self.doc_emb(doc_ids)             # [B,d]
        u_p = self.out_emb(pos_words)         # [B,d]
        u_n = self.out_emb(neg_words)         # [B,K,d]
        pos = (d*u_p).sum(1)
        neg = torch.bmm(u_n, d.unsqueeze(2)).squeeze(2)
        loss = -F.logsigmoid(pos).mean() - F.logsigmoid(-neg).mean()
        return loss

# 학습 아이디어:
#  - 각 문서에서 임의 단어를 샘플해 pos_words로 두고,
#  - 동일 분포에서 negative K개 뽑기 → 문서 임베딩이 "해당 문서의 단어들"을 잘 예측하도록.
# 추론:
#  - 새 문서에 대해 doc_emb 벡터를 "고정 단어 파라미터를 잠그고" 역으로 최적화(몇 step)하여 획득(infer).

```

장점: 문서마다 **독립 벡터**를 갖게 되어, 긴 문서/주제성 표현에 유리.
단점: **새 문서 추론**에 **추가 최적화** 단계가 필요(실시간성 이슈).

### 트랜스포머 기반 문서 임베딩

- **[CLS] pooling**: 마지막 레이어의 CLS 토큰 벡터 사용.
- **Mean pooling**: 토큰 임베딩 평균(마스크로 패딩 제외).
- **대조학습(SimCSE/SBERT류)**:
  $$\mathcal{L} = -\log \frac{\exp(\cos(\mathbf{z}_i,\mathbf{z}_i^+)/\tau)}{\sum_j \exp(\cos(\mathbf{z}_i,\mathbf{z}_j)/\tau)}$$
  같은 의미(긍정) 쌍을 가깝게, 다른 문장을 멀게.

**예시: HuggingFace + PyTorch, Mean Pooling**

```python
# pip install transformers

import torch
from transformers import AutoTokenizer, AutoModel

def encode_docs_transformer(texts, model_name="klue/roberta-base", max_len=256, device="cuda"):
    tok = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name).to(device).eval()
    outs = []
    with torch.no_grad():
        for i in range(0, len(texts), 32):
            batch = texts[i:i+32]
            enc = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors="pt").to(device)
            h = model(**enc).last_hidden_state     # [B,T,H]
            attn = enc["attention_mask"].unsqueeze(-1)  # [B,T,1]
            vec = (h*attn).sum(1) / attn.sum(1)    # mean pooling
            outs.append(vec.cpu())
    return torch.cat(outs, 0)  # [N, H]

# vecs = encode_docs_transformer(["문서 A 내용...", "문서 B 내용..."])

```

장점: 문맥/어순/상호작용을 반영, **문장·문서 의미**를 잘 포착.
단점: 비용↑, 긴 문서는 **슬라이딩 윈도/계층 풀링**이 필요.

---

## 검색·분류 파이프라인

### 코사인 검색(문서 벡터 인덱스)

```python
import numpy as np
from numpy.linalg import norm

def topk_cosine(query_vec, doc_vecs, k=5):
    q = query_vec / (norm(query_vec)+1e-9)
    D = doc_vecs / (norm(doc_vecs, axis=1, keepdims=True)+1e-9)
    sim = (D @ q)
    idx = sim.argsort()[-k:][::-1]
    return list(zip(idx.tolist(), sim[idx].tolist()))
```

- 대규모에서는 **faiss**(GPU)나 **Annoy/HNSW** 로 근사최근접 검색(ANN) 사용.

### 문서 분류(로지스틱/선형 SVM)

```python
import torch.nn as nn

class DocClassifier(nn.Module):
    def __init__(self, in_dim, num_cls):
        super().__init__()
        self.fc = nn.Linear(in_dim, num_cls)
    def forward(self, x): return self.fc(x)

# X: [N,d] (SIF/TF-IDF 평균/트랜스포머 임베딩 등), y: [N]
# 간단히 BCEWithLogits/CrossEntropy로 미세튜닝하면 강력

```

---

## 한국어/도메인 실전 팁

1) **토크나이저**: 한국어는 형태 변화가 크다. **SentencePiece Unigram** 또는 **byte-fallback BPE** 권장.
2) **전처리**: 숫자/URL/이메일은 **패턴 보존**(정규식 프리토크나이즈) 후 서브워드 적용.
3) **Stopwords**: 무차별 제거는 위험. **SIF**/서브샘플링으로 대체하거나, 도메인에 맞춰 최소한만 제거.
4) **윈도우**: 뉴스/기술문서 등은 **동적 윈도우**(1..5/10) 가 일반적으로 품질↑.
5) **빈도 컷**: min_count를 너무 높이면 희귀 전문어가 사라진다(도메인 목적에 맞게 조정).
6) **평가**:
   - 단어 유사도(수작업 seed), 동의어/태깅 사전으로 **Top-k hit rate**
   - 문서 검색: 질의→정답 문서 **리콜@k/MRR**
   - 분류: **F1/macro-F1**

---

## 수식 메모 — NS의 그래디언트(요청 많아 자주 틀리는 부분)

Skip-gram + NS에서 한 샘플의 손실:
$$
\ell = -\log \sigma(\mathbf{u}_o^\top \mathbf{v}_c)
 - \sum_{k=1}^{K} \log \sigma(-\mathbf{u}_{n_k}^\top \mathbf{v}_c)
$$
경사:
$$
\frac{\partial \ell}{\partial \mathbf{v}_c}
= (\sigma(\mathbf{u}_o^\top \mathbf{v}_c)-1)\mathbf{u}_o
 + \sum_{k=1}^{K} \sigma(\mathbf{u}_{n_k}^\top \mathbf{v}_c)\,\mathbf{u}_{n_k}
$$
$$
\frac{\partial \ell}{\partial \mathbf{u}_o}
= (\sigma(\mathbf{u}_o^\top \mathbf{v}_c)-1)\mathbf{v}_c
,\quad
\frac{\partial \ell}{\partial \mathbf{u}_{n_k}}
= \sigma(\mathbf{u}_{n_k}^\top \mathbf{v}_c)\,\mathbf{v}_c
$$

---

## 위키 단락 검색기

> **상황**: 사내 Q&A 검색. 긴 문서를 **단락/문장**으로 쪼개고 **SIF 임베딩**으로 인덱싱 후 질문과 유사도 검색.

```python
# 단락 나누기 & 토큰화

paras = [p for p in open("wiki_ko_paras.txt", encoding="utf-8").read().split("\n\n") if p.strip()]
tok_paras = [simple_tokenize(p) for p in paras]

# Word2Vec 학습(또는 사전 학습 임베딩 로드)

lines = [" ".join(t) for t in tok_paras]
emb, stoi, itos = train_skipgram_ns(lines, dim=300, window=5, K=10, epochs=2, min_count=5)

# 단락 임베딩(SIF)
# 를 vocab probs로 대용

_, _, _, probs = build_vocab(lines, min_count=5)
doc_vecs = sif_embeddings(tok_paras, emb, stoi, word_prob=np.array(probs), a=1e-3)

# 검색 함수

def search(query, topk=5):
    q_toks = simple_tokenize(query)
    q_vec  = sif_embeddings([q_toks], emb, stoi, word_prob=np.array(probs), a=1e-3)[0]
    return [(i, s) for i,s in topk_cosine(q_vec, doc_vecs, k=topk)]

# 사용
# print(search("세종대왕 집현전 업적"))
# 인덱스 i로 paras[i]를 화면에 보여주면 끝.

```

**비고**
- 정확도를 더 끌어올리고 싶다면 **트랜스포머 임베딩(SBERT/Ko-SimCSE)** 을 쓰고,
  빠른 검색은 **Faiss** 로 ANN 인덱싱을 얹는다.
- 반대로 초저비용/온프레미스 상황이면 **Word2Vec+SIF** 만으로도 의외로 견고하다.

---

## 자주 묻는 질문(FAQ)

- **Q. Skip-gram vs CBOW 어느 걸?**
  **희귀 단어/전문어**가 중요하면 **Skip-gram**. **속도/짧은 문장**은 **CBOW**가 편하다.
- **Q. 윈도우 크기?**
  뉴스/블로그: 5~10, 기술문서/코드: 5 전후, 법률/논문: 10~15도 시도.
- **Q. 사전 크기/차원?**
  도메인 일반: vocab 50k~300k, dim 200~400. 리소스 제한이면 100~200도 충분.
- **Q. 네거티브 K?**
  5~20 사이 튜닝. 코퍼스 크기/빈도 분포에 의존.
- **Q. 문서 임베딩은 무엇이 기본값?**
  **SIF**(a=1e-3) + **1PC 제거** → 빠르고 강건. 시간이 더 있으면 **트랜스포머 Mean-Pooling**.

---

## 체크리스트

- [ ] 토크나이저: **SentencePiece/byte-fallback**, 숫자·URL 규칙
- [ ] Word2Vec: **NS(0.75)**, **서브샘플링**(t=1e-5), **동적 윈도우**
- [ ] 모니터링: loss/코사인 유사어 샘플/아날로지
- [ ] 문서 벡터: **SIF** → 1PC 제거 / TF-IDF 평균 / **Doc2Vec** / **트랜스포머**
- [ ] 검색: 코사인 + **Faiss ANN** (대규모)
- [ ] 리그레션: 토크나이저/하이퍼 변경 시 **유사도·리콜@k** 회귀 점검

---

### 마무리

- **Word2Vec**(Skip-gram/CBOW)은 **저비용·설명가능**한 강력한 베이스라인이다.
- **문서 임베딩**은 **SIF/가중 평균**만으로도 실무에서 종종 놀랄 만큼 잘 통한다.
- **Doc2Vec**은 문서 고유 인식이 필요한 태스크에서 여전히 쓸모가 있고,
- **트랜스포머 임베딩**은 최종 성능을 책임지는 **상위 옵션**이다.
- 핵심은 **도메인 데이터/토크나이저/하이퍼**를 묶어 **일관된 파이프라인**으로 운영하며
  **지표 회귀**를 자동화하는 것 — 그러면 어떤 말뭉치가 와도 당황하지 않는다.
