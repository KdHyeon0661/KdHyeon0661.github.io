---
layout: post
title: ê¸°ê³„í•™ìŠµ - XGBoost, LightGBM, CatBoost
date: 2025-08-19 21:25:23 +0900
category: ê¸°ê³„í•™ìŠµ
---
# ğŸŒ³ XGBoost / LightGBM / CatBoost ë¹„êµ ë° ìƒì„¸ ì •ë¦¬

ë¶€ìŠ¤íŒ… ê³„ì—´ ì•Œê³ ë¦¬ì¦˜(Gradient Boosting)ì˜ ë°œì „ëœ í˜•íƒœë¡œ, **XGBoost**, **LightGBM**, **CatBoost**ëŠ” ì˜¤ëŠ˜ë‚  ë¨¸ì‹ ëŸ¬ë‹ ì‹¤ë¬´ì—ì„œ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  
ì„¸ ê°€ì§€ ëª¨ë‘ **Gradient Boosting Decision Tree(GBDT)** ê¸°ë°˜ì´ì§€ë§Œ, êµ¬í˜„ ë°©ì‹ê³¼ ìµœì í™” ê¸°ë²•, ë°ì´í„° ì²˜ë¦¬ ë°©ì‹ì—ì„œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.

---

## 1. XGBoost (Extreme Gradient Boosting)

### ê°œìš”
- **2016ë…„ ë°œí‘œ**, ê°€ì¥ ë¨¼ì € ëŒ€ì¤‘í™”ëœ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜
- Kaggleê³¼ ê°™ì€ ë¨¸ì‹ ëŸ¬ë‹ ëŒ€íšŒì—ì„œ "êµ­ë¯¼ ì•Œê³ ë¦¬ì¦˜"ìœ¼ë¡œ ë¶ˆë¦´ ì •ë„ë¡œ ë§ì´ ì‚¬ìš©ë¨
- "ì •í™•ì„±"ê³¼ "ì•ˆì •ì„±"ì— ê°•ì 

### ì£¼ìš” íŠ¹ì§•
1. **ì •ê·œí™”(Regularization) ì¶”ê°€**
   - ì „í†µì ì¸ GBDTì— L1, L2 ê·œì œë¥¼ ì ìš©í•˜ì—¬ ê³¼ì í•© ì–µì œ
2. **ë‘ ë²ˆì§¸ ë¯¸ë¶„ ì‚¬ìš©**
   - Gradient(1ì°¨ ë¯¸ë¶„)ë¿ë§Œ ì•„ë‹ˆë¼ Hessian(2ì°¨ ë¯¸ë¶„)ì„ ì´ìš© â†’ ë” ì •êµí•œ ì†ì‹¤ ìµœì í™”
3. **Shrinkage (Learning Rate)**
   - ê° íŠ¸ë¦¬ì˜ ê¸°ì—¬ë„ë¥¼ ì¤„ì—¬ ì ì§„ì  ìµœì í™”
4. **Column Subsampling**
   - ëœë¤ í¬ë ˆìŠ¤íŠ¸ì²˜ëŸ¼ íŠ¹ì§•ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê³¼ì í•© ë°©ì§€
5. **ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›**
   - ë‹¤ì¤‘ ìŠ¤ë ˆë“œ ê¸°ë°˜ í•™ìŠµ ì†ë„ ê°œì„ 

### ëª©ì  í•¨ìˆ˜ (ìˆ˜í•™ì  í˜•íƒœ)
\[
Obj(\theta) = \sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)
\]

- \(l(y_i, \hat{y}_i)\): ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆ: MSE, ë¡œì§€ìŠ¤í‹± ì†ì‹¤)  
- \(\Omega(f_k) = \gamma T + \frac{1}{2}\lambda \|w\|^2\): íŠ¸ë¦¬ ë³µì¡ë„ ê·œì œ í•­  

---

## 2. LightGBM (Light Gradient Boosting Machine)

### ê°œìš”
- **ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ì—ì„œ ê°œë°œ**
- "ì†ë„ì™€ ë©”ëª¨ë¦¬ íš¨ìœ¨"ì— ì´ˆì ì„ ë§ì¶˜ ë¶€ìŠ¤íŒ… í”„ë ˆì„ì›Œí¬
- ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ íŠ¹íˆ ê°•ë ¥

### ì£¼ìš” íŠ¹ì§•
1. **Histogram ê¸°ë°˜ í•™ìŠµ**
   - ì—°ì†í˜• íŠ¹ì§•ê°’ì„ êµ¬ê°„(bin)ìœ¼ë¡œ ë‚˜ëˆ  ì €ì¥
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ, ì—°ì‚° ì†ë„ ê°œì„ 
2. **GOSS (Gradient-based One-Side Sampling)**
   - ê·¸ë˜ë””ì–¸íŠ¸ê°€ í° ë°ì´í„°(ì–´ë ¤ìš´ ìƒ˜í”Œ)ëŠ” ìœ ì§€í•˜ê³ , ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê°€ì§„ ë°ì´í„° ì¼ë¶€ëŠ” ìƒ˜í”Œë§
   - ì—°ì‚°ëŸ‰ ê°ì†Œ, ì†ë„ í–¥ìƒ
3. **EFB (Exclusive Feature Bundling)**
   - ì„œë¡œ ìƒí˜¸ë°°íƒ€ì ì¸ í¬ì†Œ(sparse) í”¼ì²˜ë¥¼ ë¬¶ì–´ ì°¨ì› ì¶•ì†Œ
4. **Leaf-wise Tree Growth**
   - XGBoostëŠ” Level-wise í™•ì¥(ê· í˜•ì ì¸ ì„±ì¥)  
   - LightGBMì€ Leaf-wise í™•ì¥(ì†ì‹¤ ê°ì†Œê°€ í° ë¦¬í”„ë¥¼ ìš°ì„  ë¶„í• ) â†’ ë” ë¹ ë¥´ê²Œ ì†ì‹¤ ê°ì†Œ  
   - ë‹¨, ê³¼ì í•© ìœ„í—˜ â†‘

---

## 3. CatBoost (Categorical Boosting)

### ê°œìš”
- **Yandex(ëŸ¬ì‹œì•„ ê²€ìƒ‰ì—”ì§„ íšŒì‚¬)ì—ì„œ ê°œë°œ**
- ë²”ì£¼í˜• ë³€ìˆ˜(categorical feature) ì²˜ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ì§€ì›í•˜ëŠ” ì ì´ í° ê°•ì 
- ë°ì´í„° ì „ì²˜ë¦¬ ë¶€ë‹´ì„ í¬ê²Œ ì¤„ì—¬ì¤Œ

### ì£¼ìš” íŠ¹ì§•
1. **ë²”ì£¼í˜• ë³€ìˆ˜ ìë™ ì²˜ë¦¬**
   - One-hot ì¸ì½”ë”©ì´ë‚˜ Label Encoding ë¶ˆí•„ìš”
   - ìˆœì°¨ì  í†µê³„(Smoothed Target Encoding) ì‚¬ìš©
2. **Ordered Boosting**
   - ì „í†µì  ë¶€ìŠ¤íŒ…ì€ target leakage(ë°ì´í„° ëˆ„ìˆ˜) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ  
   - CatBoostëŠ” ë°ì´í„°ë¥¼ ìˆœì„œëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ëˆ„ìˆ˜ ë°©ì§€
3. **ëŒ€ì¹­ íŠ¸ë¦¬(Symmetric Tree) êµ¬ì¡°**
   - ê° ê¹Šì´ì—ì„œ ë™ì¼í•œ ë¶„í•  ê·œì¹™ ì ìš© â†’ ì˜ˆì¸¡ ì‹œ ë¹ ë¦„, GPU ìµœì í™” ìš©ì´
4. **GPU í•™ìŠµ ê°•ë ¥ ì§€ì›**
   - ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ + GPU = ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œë„ íš¨ìœ¨ì 

---

## 4. ë¹„êµ ìš”ì•½

| í•­ëª© | XGBoost | LightGBM | CatBoost |
|------|----------|-----------|-----------|
| ê°œë°œì‚¬ | ì˜¤í”ˆì†ŒìŠ¤ (ì»¤ë®¤ë‹ˆí‹° ì£¼ë„) | Microsoft | Yandex |
| ì£¼ìš” ì¥ì  | ì •í™•ì„±, ì•ˆì •ì„±, íŠœë‹ ë‹¤ì–‘ | ë¹ ë¥¸ í•™ìŠµ, ëŒ€ê·œëª¨ ë°ì´í„° ìµœì í™” | ë²”ì£¼í˜• ë³€ìˆ˜ ìë™ ì²˜ë¦¬, ë°ì´í„° ì „ì²˜ë¦¬ ë‹¨ìˆœ |
| í•™ìŠµ ì†ë„ | ì¤‘ê°„ | ë§¤ìš° ë¹ ë¦„ | ë¹ ë¦„ (íŠ¹íˆ ì¹´í…Œê³ ë¦¬ ë°ì´í„°ì—ì„œ) |
| ë©”ëª¨ë¦¬ íš¨ìœ¨ | ì¤‘ê°„ | ë†’ìŒ (Histogram, GOSS, EFB) | ì¤‘ê°„ |
| íŠ¸ë¦¬ ì„±ì¥ ë°©ì‹ | Level-wise | Leaf-wise | Symmetric |
| ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ | ìˆ˜ë™ ì¸ì½”ë”© í•„ìš” | ìˆ˜ë™ ì¸ì½”ë”© í•„ìš” | ìë™ ì§€ì› |
| ê³¼ì í•© ìœ„í—˜ | ì¤‘ê°„ | ë†’ìŒ (Leaf-wise) | ë‚®ìŒ (Ordered Boosting) |

---

## 5. íŒŒì´ì¬ êµ¬í˜„ ì˜ˆì œ

### XGBoost
```python
import xgboost as xgb
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = xgb.XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=4)
model.fit(X_train, y_train)
print("XGBoost Accuracy:", accuracy_score(y_test, model.predict(X_test)))
```

### LightGBM
```python
import lightgbm as lgb

train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

params = {
    'objective': 'binary',
    'metric': 'accuracy',
    'boosting_type': 'gbdt',
    'learning_rate': 0.1,
    'num_leaves': 31
}

lgb_model = lgb.train(params, train_data, valid_sets=[test_data], num_boost_round=200, early_stopping_rounds=20)
y_pred = (lgb_model.predict(X_test) > 0.5).astype(int)
print("LightGBM Accuracy:", accuracy_score(y_test, y_pred))
```

### CatBoost
```python
from catboost import CatBoostClassifier

cat_model = CatBoostClassifier(
    iterations=200,
    learning_rate=0.1,
    depth=6,
    verbose=0
)

cat_model.fit(X_train, y_train)
print("CatBoost Accuracy:", accuracy_score(y_test, cat_model.predict(X_test)))
```

---

## 6. ì„ íƒ ê°€ì´ë“œ

- **XGBoost**  
  â†’ ê¸°ë³¸ê¸° íƒ„íƒ„, ë‹¤ì–‘í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš”í•  ë•Œ  
- **LightGBM**  
  â†’ ë°ì´í„°ê°€ í¬ê³ , í•™ìŠµ ì†ë„ê°€ ì¤‘ìš”í•  ë•Œ  
- **CatBoost**  
  â†’ ë²”ì£¼í˜• ë°ì´í„°ê°€ ë§ê³ , ì „ì²˜ë¦¬ë¥¼ ìµœì†Œí™”í•˜ê³  ì‹¶ì„ ë•Œ  

---

## ğŸ“Œ ìš”ì•½
- XGBoost: ì •í™•í•˜ê³  ì•ˆì •ì , í‘œì¤€ì ì¸ ì„ íƒ  
- LightGBM: ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ëŒ€ê·œëª¨ ë°ì´í„° ì í•©  
- CatBoost: ë²”ì£¼í˜• ë³€ìˆ˜ì— ìµœì , ìë™í™”ëœ ì „ì²˜ë¦¬  

â†’ ê²°êµ­ **ë°ì´í„° íŠ¹ì„±ê³¼ ë¬¸ì œ ì„±ê²©ì— ë”°ë¼ ì„ íƒ**í•˜ëŠ” ê²ƒì´ ê°€ì¥ í˜„ëª…í•©ë‹ˆë‹¤.