---
layout: post
title: 딥러닝 - 세그멘테이션
date: 2025-10-02 20:25:23 +0900
category: 딥러닝
---
# 세그멘테이션 총정리  
**U-Net / ResUNet 개요 · 손실 선택(Dice, Focal-Dice, Tversky) · 데이터/학습/평가/배포까지**

## 0) 문제 정의 & 메트릭

### 0.1 픽셀 단위 분류
입력 이미지 \(\mathbf{I}\in\mathbb{R}^{H\times W\times 3}\)에 대해 각 픽셀 \(p\)에 클래스 \(y_p\in\{1,\dots,K\}\)를 예측.

### 0.2 IoU · Dice
픽셀 집합 \(A\)(예측), \(B\)(정답)에 대해
$$
\mathrm{IoU}(A,B)=\frac{|A\cap B|}{|A\cup B|},\qquad
\mathrm{Dice}(A,B)=\frac{2|A\cap B|}{|A|+|B|}.
$$
소프트 형태(미분 가능)로는 확률 \(p\in[0,1]\), 바이너리 타깃 \(g\in\{0,1\}\)에 대해
$$
\mathrm{Dice}_{\text{soft}}=\frac{2\sum p g + \epsilon}{\sum p + \sum g + \epsilon}.
$$

- **희소 마스크**(양성 적음)에서는 **Dice**가 훨씬 안정.  
- **멀티클래스**: 클래스별 Dice를 평균(평균 방식: macro/빈도 가중).

---

## 1) U-Net 개요

### 1.1 핵심 아이디어
- **인코더(Contracting)**: Conv(3×3)×2 + 다운샘플(Stride=2 또는 MaxPool)로 **문맥**(receptive field) 확대  
- **디코더(Expanding)**: 업샘플(Deconv 또는 Bilinear+Conv) + **스킵 연결**(encoder의 대응 feature concat)로 **정밀 위치** 복원

### 1.2 설계 선택
- 업샘플: `ConvTranspose2d`(학습가능) vs `Upsample(bilinear)+1×1/3×3 Conv`(가벼움)  
- 정규화: `BatchNorm2d`(일반), 작은 배치면 `GroupNorm/InstanceNorm` 안정  
- 활성화: ReLU/LeakyReLU/GELU(선택)  
- 마지막: **클래스 수 K**에 맞춰 `out_channels=K`;  
  - **이진**: `K=1`, 출력 로짓 \(\rightarrow\) `sigmoid`  
  - **멀티**: `K=C`, 출력 로짓 \(\rightarrow\) `softmax`

### 1.3 기본 U-Net(PyTorch)
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

def conv_block(cin, cout, norm='bn'):
    layers = [nn.Conv2d(cin, cout, 3, padding=1, bias=False)]
    if norm=='bn': layers.append(nn.BatchNorm2d(cout))
    elif norm=='gn': layers.append(nn.GroupNorm(8, cout))
    layers += [nn.ReLU(inplace=True),
               nn.Conv2d(cout, cout, 3, padding=1, bias=False)]
    if norm=='bn': layers.append(nn.BatchNorm2d(cout))
    elif norm=='gn': layers.append(nn.GroupNorm(8, cout))
    layers += [nn.ReLU(inplace=True)]
    return nn.Sequential(*layers)

class UNet(nn.Module):
    def __init__(self, in_ch=3, num_classes=1, base=64, norm='bn', up='deconv'):
        super().__init__()
        self.enc1 = conv_block(in_ch, base, norm)
        self.enc2 = conv_block(base, base*2, norm)
        self.enc3 = conv_block(base*2, base*4, norm)
        self.enc4 = conv_block(base*4, base*8, norm)
        self.center = conv_block(base*8, base*16, norm)
        if up=='deconv':
            self.up4 = nn.ConvTranspose2d(base*16, base*8, 2, stride=2)
            self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)
            self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)
            self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)
        else:
            self.up = lambda c: nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                                              nn.Conv2d(c, c//2, 1))
            self.up4 = self.up(base*16); self.up3 = self.up(base*8)
            self.up2 = self.up(base*4);  self.up1 = self.up(base*2)
        self.dec4 = conv_block(base*16, base*8, norm)
        self.dec3 = conv_block(base*8,  base*4, norm)
        self.dec2 = conv_block(base*4,  base*2, norm)
        self.dec1 = conv_block(base*2,  base,   norm)
        self.outc = nn.Conv2d(base, num_classes, 1)
        self.pool = nn.MaxPool2d(2)

    def forward(self, x):
        e1 = self.enc1(x)            # H
        e2 = self.enc2(self.pool(e1))# H/2
        e3 = self.enc3(self.pool(e2))# H/4
        e4 = self.enc4(self.pool(e3))# H/8
        c  = self.center(self.pool(e4))# H/16
        d4 = self.up4(c)
        d4 = self.dec4(torch.cat([d4, e4], dim=1))
        d3 = self.up3(d4)
        d3 = self.dec3(torch.cat([d3, e3], dim=1))
        d2 = self.up2(d3)
        d2 = self.dec2(torch.cat([d2, e2], dim=1))
        d1 = self.up1(d2)
        d1 = self.dec1(torch.cat([d1, e1], dim=1))
        return self.outc(d1)         # logits
```

---

## 2) ResUNet 개요

### 2.1 왜 Residual?
- 다운/업 경로 모두 **Residual block(입력 + F(x))** 사용 →  
  **깊어져도 학습 안정**, **정보 흐름** 강화, **성능↑**.

### 2.2 ResBlock & ResUNet
- `Conv3×3 → Norm → ReLU → Conv3×3 → Norm` + **skip**  
- 채널이 바뀌면 `1×1 Conv`로 스킵 정렬.

```python
class ResBlock(nn.Module):
    def __init__(self, c, c_out=None, norm='bn'):
        super().__init__()
        c_out = c if c_out is None else c_out
        self.conv1 = nn.Conv2d(c, c_out, 3, padding=1, bias=False)
        self.conv2 = nn.Conv2d(c_out, c_out, 3, padding=1, bias=False)
        self.relu = nn.ReLU(inplace=True)
        if norm=='bn':
            self.n1 = nn.BatchNorm2d(c_out); self.n2 = nn.BatchNorm2d(c_out)
        else:
            self.n1 = nn.GroupNorm(8, c_out); self.n2 = nn.GroupNorm(8, c_out)
        self.proj = None
        if c != c_out:
            self.proj = nn.Conv2d(c, c_out, 1, bias=False)

    def forward(self, x):
        identity = x
        out = self.relu(self.n1(self.conv1(x)))
        out = self.n2(self.conv2(out))
        if self.proj is not None:
            identity = self.proj(identity)
        out = self.relu(out + identity)
        return out

class ResUNet(nn.Module):
    def __init__(self, in_ch=3, num_classes=1, base=64, norm='bn'):
        super().__init__()
        self.enc1 = nn.Sequential(ResBlock(in_ch, base, norm), ResBlock(base, base, norm))
        self.enc2 = nn.Sequential(ResBlock(base, base*2, norm), ResBlock(base*2, base*2, norm))
        self.enc3 = nn.Sequential(ResBlock(base*2, base*4, norm), ResBlock(base*4, base*4, norm))
        self.enc4 = nn.Sequential(ResBlock(base*4, base*8, norm), ResBlock(base*8, base*8, norm))
        self.center = nn.Sequential(ResBlock(base*8, base*16, norm))
        self.up4 = nn.ConvTranspose2d(base*16, base*8, 2, stride=2)
        self.dec4 = nn.Sequential(ResBlock(base*16, base*8, norm))
        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)
        self.dec3 = nn.Sequential(ResBlock(base*8, base*4, norm))
        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)
        self.dec2 = nn.Sequential(ResBlock(base*4, base*2, norm))
        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)
        self.dec1 = nn.Sequential(ResBlock(base*2, base, norm))
        self.outc = nn.Conv2d(base, num_classes, 1)
        self.pool = nn.MaxPool2d(2)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))
        c  = self.center(self.pool(e4))
        d4 = self.dec4(torch.cat([self.up4(c), e4], dim=1))
        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))
        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))
        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))
        return self.outc(d1)
```

**언제 ResUNet?**  
- **경계·미세 구조**가 중요(의료·위성) + **깊게** 쌓고 싶을 때, 작은 배치에서도 **GN/IN**과 결합해 안정적으로 학습.

---

## 3) 손실 선택: Dice / Focal-Dice 중심

### 3.1 Binary Dice Loss (soft)
$$
\mathrm{Dice}(p,g)=\frac{2\sum p g + \epsilon}{\sum p + \sum g + \epsilon},\quad
\mathcal{L}_{\text{Dice}}=1-\mathrm{Dice}(p,g).
$$
- \(p=\sigma(z)\) (`sigmoid(logits)`), \(g\in\{0,1\}\).  
- **장점**: 클래스 비율 치우친 데이터에서도 안정(양성 희소).  
- **주의**: 배치가 매우 작으면 변동↑ → **batch-level** 또는 **image-level** 평균 방식 선택.

### 3.2 Multi-class Dice (mean of per-class)
원-핫 타깃 \(G\in\{0,1\}^{N\times C\times H\times W}\), 확률 \(P=\mathrm{softmax}(Z)\).
$$
\mathrm{Dice}_c=\frac{2\sum P_c G_c + \epsilon}{\sum P_c + \sum G_c + \epsilon},\quad
\mathcal{L}=\frac{1}{C}\sum_{c=1}^C (1-\mathrm{Dice}_c).
$$
클래스 불균형이면 **가중 평균**.

### 3.3 Focal + Dice (Focal-Dice)
- 픽셀 단위 **Focal**로 어려운 픽셀에 집중:
$$
\mathcal{L}_{\text{Focal}} = -\alpha\, (1-p_t)^\gamma \log(p_t),
$$
여기서 \(p_t = p\) (정답 1), \(p_t = 1-p\) (정답 0).
- **조합**:  
$$
\mathcal{L}=\lambda\,\mathcal{L}_{\text{Dice}} + (1-\lambda)\,\mathcal{L}_{\text{Focal}}.
$$
- **언제?**: 양성 **희소** + **경계/작은 객체** 중요, 오탐/미탐 비용 비대칭.  
- 권장 초기값: \(\gamma=2.0\), \(\alpha=0.25\), \(\lambda\in[0.3,0.7]\).

### 3.4 Tversky / Focal-Tversky (경계·불균형 강화)
- Dice의 일반화(거짓양성/거짓음성 가중):
$$
\mathrm{Tversky}(p,g)=\frac{\sum pg + \epsilon}
{\sum pg + \alpha\sum p(1-g) + \beta \sum (1-p)g + \epsilon}.
$$
- Focal-Tversky:
$$
\mathcal{L}=(1-\mathrm{Tversky})^\gamma.
$$
- **언제?**: **FN(미검출)** 비용이 특히 클 때(\(\beta>\alpha\)), 얇은 경계.

---

## 4) 손실 · 메트릭 구현(Python)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

def dice_loss_binary(logits, targets, eps=1e-6, reduction='mean'):
    # logits: (N,1,H,W), targets: (N,1,H,W) in {0,1}
    probs = torch.sigmoid(logits)
    num = 2*(probs*targets).sum(dim=(2,3)) + eps
    den = probs.sum(dim=(2,3)) + targets.sum(dim=(2,3)) + eps
    loss = 1 - (num/den)
    return loss.mean() if reduction=='mean' else loss

def dice_loss_multi(logits, targets, eps=1e-6, weight=None):
    # logits: (N,C,H,W), targets: (N,C,H,W) one-hot
    probs = logits.softmax(dim=1)
    num = 2*(probs*targets).sum(dim=(2,3))
    den = probs.sum(dim=(2,3)) + targets.sum(dim=(2,3)) + eps
    dice = num/den  # (N,C)
    loss = 1 - dice
    if weight is not None:
        loss = loss * weight[None,:]
        return loss.mean()
    return loss.mean()

class FocalLossBinary(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__(); self.alpha=alpha; self.gamma=gamma; self.reduction=reduction
    def forward(self, logits, targets):
        # logits: (N,1,H,W), targets: {0,1}
        p = torch.sigmoid(logits)
        ce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        p_t = p*targets + (1-p)*(1-targets)
        loss = ce * ((1-p_t)**self.gamma)
        if self.alpha is not None:
            alpha_t = self.alpha*targets + (1-self.alpha)*(1-targets)
            loss = alpha_t*loss
        return loss.mean() if self.reduction=='mean' else loss

class FocalDiceBinary(nn.Module):
    def __init__(self, lam=0.5, alpha=0.25, gamma=2.0):
        super().__init__(); self.lam=lam; self.fl=FocalLossBinary(alpha,gamma)
    def forward(self, logits, targets):
        return self.lam*dice_loss_binary(logits, targets) + (1-self.lam)*self.fl(logits, targets)

def tversky_loss_binary(logits, targets, alpha=0.3, beta=0.7, eps=1e-6):
    p = torch.sigmoid(logits)
    tp = (p*targets).sum(dim=(2,3))
    fp = (p*(1-targets)).sum(dim=(2,3))
    fn = ((1-p)*targets).sum(dim=(2,3))
    t = (tp + eps) / (tp + alpha*fp + beta*fn + eps)
    return (1 - t).mean()

def focal_tversky_loss(logits, targets, alpha=0.3, beta=0.7, gamma=1.33, eps=1e-6):
    p = torch.sigmoid(logits)
    tp = (p*targets).sum(dim=(2,3))
    fp = (p*(1-targets)).sum(dim=(2,3))
    fn = ((1-p)*targets).sum(dim=(2,3))
    t = (tp + eps) / (tp + alpha*fp + beta*fn + eps)
    return ((1 - t)**gamma).mean()
```

**빈 마스크(전부 0) 주의**: \( \sum g=0\)일 때 Dice 분모가 작아 폭주 가능 → `eps` 넣고, 배치 내 **모든 0 샘플**은 별도 처리(예: BCE만 적용).

---

## 5) 데이터셋/증강/로더(마스크 동시 변환)

### 5.1 폴더 구조 예
```
data/
  images/ 0001.png 0002.png ...
  masks/  0001.png 0002.png ...
```
- 마스크: 바이너리(0/255) 또는 멀티클래스(Palette → 인덱스).  
- **같은 변환**을 이미지·마스크에 동일 적용 필요(기하 변환).

### 5.2 간단 변환(Flip/RandomCrop/Normalize)
```python
from PIL import Image
import numpy as np
import random
import torch
from torch.utils.data import Dataset

class SegDataset(Dataset):
    def __init__(self, img_paths, mask_paths, size=256, mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225), n_classes=1):
        self.imgs = img_paths; self.masks = mask_paths
        self.size = size; self.mean = mean; self.std = std; self.n_classes = n_classes

    def __len__(self): return len(self.imgs)

    def _to_tensor(self, im):
        arr = np.array(im).astype(np.float32)/255.0
        arr = (arr - np.array(self.mean)) / np.array(self.std)
        return torch.from_numpy(arr.transpose(2,0,1)).float()

    def _mask_to_tensor(self, m):
        arr = np.array(m)
        if self.n_classes==1:
            # binary: map {0,255} -> {0,1}
            if arr.max() > 1: arr = (arr>127).astype(np.uint8)
            return torch.from_numpy(arr[None,...]).float()
        else:
            # palette to index if needed
            if arr.ndim==3: raise ValueError("Provide indexed mask for multi-class")
            return torch.from_numpy(arr).long()

    def __getitem__(self, i):
        img = Image.open(self.imgs[i]).convert('RGB')
        mask = Image.open(self.masks[i])
        # random crop
        w,h = img.size
        s = self.size
        if (w>=s and h>=s):
            x = random.randint(0, w-s); y = random.randint(0, h-s)
            img = img.crop((x,y,x+s,y+s)); mask = mask.crop((x,y,x+s,y+s))
        # flip
        if random.random()<0.5:
            img = img.transpose(Image.FLIP_LEFT_RIGHT); mask = mask.transpose(Image.FLIP_LEFT_RIGHT)
        if random.random()<0.2:
            img = img.transpose(Image.FLIP_TOP_BOTTOM);  mask = mask.transpose(Image.FLIP_TOP_BOTTOM)
        x = self._to_tensor(img)
        y = self._mask_to_tensor(mask)
        return x, y
```

---

## 6) 학습 루프(이진 & 멀티클래스)

### 6.1 이진 세그멘테이션 + Focal-Dice
```python
import torch, torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader

device = "cuda" if torch.cuda.is_available() else "cpu"
model = ResUNet(in_ch=3, num_classes=1, base=32).to(device)
criterion = FocalDiceBinary(lam=0.5, alpha=0.25, gamma=2.0)
opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
scaler = GradScaler()

def iou_binary(pred, target, thresh=0.5, eps=1e-6):
    pred = (pred>thresh).float()
    inter = (pred*target).sum(dim=(2,3))
    union = (pred+target - pred*target).sum(dim=(2,3)) + eps
    return (inter/union).mean().item()

for epoch in range(20):
    model.train(); tot=0
    for xb, yb in DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, pin_memory=True):
        xb, yb = xb.to(device), yb.to(device)
        opt.zero_grad(set_to_none=True)
        with autocast(dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32):
            logits = model(xb)
            loss = criterion(logits, yb)
        scaler.scale(loss).backward()
        scaler.unscale_(opt)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(opt); scaler.update()
        tot += loss.item()*xb.size(0)
    # val
    model.eval(); miou=0; n=0
    with torch.no_grad():
        for xb, yb in DataLoader(val_ds, batch_size=8):
            logits = model(xb.to(device))
            prob = torch.sigmoid(logits).cpu()
            miou += iou_binary(prob, yb, 0.5); n+=1
    print(f"[{epoch+1:02d}] loss={tot/len(train_ds):.4f} mIoU={miou/n:.4f}")
```

### 6.2 멀티클래스 + Dice(가중)
```python
def one_hot(target, C, device):
    # target: (N,H,W) long → (N,C,H,W)
    return torch.zeros(target.size(0), C, target.size(1), target.size(2), device=device)\
          .scatter_(1, target.unsqueeze(1), 1.0)

num_classes = 4
model = UNet(in_ch=3, num_classes=num_classes, base=32).to(device)
opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)

# 클래스 빈도 기반 가중(희귀 클래스↑)
cls_counts = torch.tensor([12000, 3000, 1500, 800], dtype=torch.float32, device=device)
w = (cls_counts.sum() / (cls_counts+1e-6)); w = w/w.mean()

for epoch in range(30):
    model.train(); tot=0
    for xb, yb in DataLoader(train_ds_mc, batch_size=6, shuffle=True, num_workers=4):
        xb, yb = xb.to(device), yb.to(device)   # yb: (N,H,W)
        opt.zero_grad(set_to_none=True)
        logits = model(xb)                      # (N,C,H,W)
        y_oh = one_hot(yb, num_classes, device)
        loss = dice_loss_multi(logits, y_oh, weight=w)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        opt.step()
        tot += loss.item()*xb.size(0)
    print(f"[{epoch+1:02d}] dice_loss={tot/len(train_ds_mc):.4f}")
```

---

## 7) 추론 & 후처리

### 7.1 이진
- 로짓 → `sigmoid` → 임계값 \(t\) (기본 0.5, **PR 곡선**으로 튜닝)  
- 작은 잡음 제거: **형태학적 연산**(열림/닫힘), **Connected Components**로 작은 면적 제거

```python
import numpy as np
import cv2

def post_binary(prob, thresh=0.5, min_area=50):
    m = (prob[0].numpy() > thresh).astype(np.uint8)  # (H,W)
    # morphology
    kernel = np.ones((3,3), np.uint8)
    m = cv2.morphologyEx(m, cv2.MORPH_OPEN, kernel, iterations=1)
    # small area removal
    num, labels, stats, _ = cv2.connectedComponentsWithStats(m, connectivity=8)
    keep = np.zeros_like(m)
    for i in range(1, num):
        if stats[i, cv2.CC_STAT_AREA] >= min_area:
            keep[labels==i] = 1
    return keep
```

### 7.2 멀티
- 로짓 → `softmax` → `argmax`로 클래스 인덱스  
- 경계 부드럽게 하려면 CRF/Guided filter(선택)

---

## 8) 메모리·대해상도 처리(타일링)

**큰 이미지(5k×5k)** 는 그대로 학습/추론 불가 → **슬라이딩 윈도우(Overlap)**:
```python
def tile_infer(model, img, tile=512, overlap=64, device='cuda'):
    # img: (1,3,H,W) tensor normalized
    _,_,H,W = img.size()
    out = torch.zeros(1, model.outc.out_channels, H, W, device=device)
    weight = torch.zeros(1,1,H,W, device=device)
    for y in range(0, H, tile-overlap):
        for x in range(0, W, tile-overlap):
            ys = min(y+tile, H); xs = min(x+tile, W)
            y0 = ys - tile; x0 = xs - tile
            crop = img[:,:,y0:ys, x0:xs]
            with torch.inference_mode():
                logits = model(crop.to(device))
                out[:,:,y0:ys, x0:xs] += logits
                weight[:,:,y0:ys, x0:xs] += 1.0
    out = out / weight
    return out
```
- 오버랩 영역 평균/가중합(블렌딩)으로 타일 경계 artifact 완화.

---

## 9) 시나리오별 레시피

### 9.1 의료(병변/장기; 희소·경계 중요)
- 모델: **ResUNet**(GN/IN), **base 채널↑**  
- 손실: **Focal-Tversky** 또는 **Focal-Dice**(FN 비용↑)  
- 증강: 강한 **Intensity/Blur/Noise** + 약한 기하(해부학 유지), 작은 회전±10°  
- 평가: **Dice / mIoU / Hausdorff Distance(경계)**  
- 팁: **Deep Supervision**(중간 디코더 출력에 보조 Dice)으로 수렴 안정

### 9.2 항공/원격탐사(도로/건물; 대해상도)
- 모델: U-Net/ResUNet + **대형 입력(>1024)** 은 **타일링**  
- 손실: **Dice + BCE**(클래스별 가중) 또는 **Focal-Dice**  
- 증강: 강한 **기하**(스케일/회전/플립), 컬러(밝기/대비), JPEG 압축  
- 후처리: 작은 영역 제거, **스켈레톤/두께 보정**(도로)

---

## 10) 학습 안정 & 속도

- **AMP(autocast)** + **Gradient Clipping**(1.0)  
- 작은 배치: **GroupNorm/InstanceNorm** or **SyncBN**  
- 초기화: Kaiming(He), ResBlock은 **pre-act** 변형도 고려  
- 스케줄: `CosineAnnealingLR` + **Warmup**(5–10% step)  
- 데이터 불균형: **양성 패치 오버샘플링**(샘플러)  
- 로더 성능: `num_workers`, `pin_memory`, `persistent_workers`

---

## 11) 통합 훈련 스크립트(바이너리; ResUNet + Focal-Dice + 타일 추론 옵션)

```python
import os, glob, random
import torch, numpy as np
from torch.utils.data import DataLoader, random_split

# 1) 경로 수집
imgs = sorted(glob.glob("data/images/*.png"))
masks = sorted(glob.glob("data/masks/*.png"))
pairs = list(zip(imgs, masks))
random.seed(42); random.shuffle(pairs)
split = int(0.85*len(pairs))
train_pairs, val_pairs = pairs[:split], pairs[split:]

train_ds = SegDataset([p[0] for p in train_pairs], [p[1] for p in train_pairs], size=384, n_classes=1)
val_ds   = SegDataset([p[0] for p in val_pairs],   [p[1] for p in val_pairs],   size=512, n_classes=1)  # 큰 크기 평가

train_loader = DataLoader(train_ds, batch_size=6, shuffle=True, num_workers=4, pin_memory=True)
val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=2)

# 2) 모델/손실/옵티마
device = "cuda" if torch.cuda.is_available() else "cpu"
model = ResUNet(in_ch=3, num_classes=1, base=32, norm='gn').to(device)
criterion = FocalDiceBinary(lam=0.5, alpha=0.25, gamma=2.0)
opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=30)
scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

def evaluate(loader):
    model.eval(); dices=[]; ious=[]
    with torch.no_grad():
        for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=torch.cuda.is_available()):
                logits = model(xb)
                probs = torch.sigmoid(logits)
            inter = (probs.round()*yb).sum(dim=(2,3))
            union = (probs.round()+yb - probs.round()*yb).sum(dim=(2,3))+1e-6
            dice = (2*inter/(probs.round().sum(dim=(2,3))+yb.sum(dim=(2,3))+1e-6)).mean().item()
            iou  = (inter/union).mean().item()
            dices.append(dice); ious.append(iou)
    return float(np.mean(dices)), float(np.mean(ious))

best=0.0
for epoch in range(30):
    model.train(); running=0
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        opt.zero_grad(set_to_none=True)
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=torch.cuda.is_available()):
            logits = model(xb)
            loss = criterion(logits, yb)
        scaler.scale(loss).backward()
        scaler.unscale_(opt)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(opt); scaler.update()
        running += loss.item()*xb.size(0)
    sched.step()
    md, mi = evaluate(val_loader)
    print(f"[{epoch+1:02d}] loss={running/len(train_ds):.4f}  Dice={md:.4f}  IoU={mi:.4f}")
    if md>best:
        best=md; torch.save(model.state_dict(), "best_resunet.pt")
```

---

## 12) 디버깅 체크리스트

- [ ] **마스크 dtype/스케일**: 바이너리는 `{0,1}`(또는 `{0,255}` → 변환), 멀티는 `long` 인덱스/원-핫 일관  
- [ ] **출력 채널**과 손실 매칭: 이진→`1`+`sigmoid`; 멀티→`C`+`softmax`  
- [ ] **패딩/크기**: 업샘플/concat 시 텐서 크기 mismatch → `padding='same'` 또는 crop/center align  
- [ ] **빈 마스크 배치**: Dice 폭주 → eps, BCE 보조, 또는 배치 레벨 가중  
- [ ] **학습은 좋아지나 검증 정체**: 증강 강도↓, 라벨 품질 점검, 손실 가중(λ/α/γ) 조정  
- [ ] **경계 거칠음**: Tversky/Boundary loss 도입, 업샘플 방식 변경(Deconv↔Bilinear)  
- [ ] **대해상도**: 타일링/오버랩, 메모리 고정 버퍼, AMP

---

## 13) 빠른 결정표

- **희소 양성(의료/도로틈)** → **Focal-Dice**(γ=2.0, α=0.25, λ≈0.5)  
- **FN 비용↑(놓치면 큰 손해)** → **Focal-Tversky**(β>α, 예: α=0.3, β=0.7)  
- **클래스 다수** → Multi-Dice(클래스 가중) ± CE 보조  
- **속도/메모리** 이슈 → base 채널↓, 경량 업샘플, GroupNorm, AMP

---

## 14) 수식 모음(요약)

- **Soft Dice (binary)**  
$$
\mathrm{Dice}=\frac{2\sum p g + \epsilon}{\sum p + \sum g + \epsilon},\quad \mathcal{L}=1-\mathrm{Dice}
$$
- **Focal (binary)**  
$$
\mathcal{L}_{\text{Focal}}=-\alpha (1-p_t)^\gamma \log(p_t)
$$
- **Tversky**  
$$
\mathrm{Tversky}=\frac{\sum pg + \epsilon}{\sum pg + \alpha\sum p(1-g)+\beta\sum (1-p)g + \epsilon},\quad
\mathcal{L}=1-\mathrm{Tversky}
$$

---

## 15) 마무리

- **U-Net**은 “**문맥을 넓히되 위치 정보를 스킵으로 보존**”하는 구조,  
  **ResUNet**은 여기에 **Residual**을 더해 **깊이/안정성**을 확보합니다.  
- **손실 선택**은 데이터 분포·업무 비용에 좌우: **Dice**(불균형 기본), **Focal-Dice/Tversky**(희소/경계/비대칭 비용).  
- 제공한 **손실/모델/타일링/후처리** 스니펫을 조합하면, 의료·원격탐사 등 다양한 세그멘테이션을 **바로 시작**할 수 있습니다.  
- 끝으로: 항상 **라벨 품질**을 의심하고, **PR/ROC/임계값**을 함께 리포팅하세요. 숫자가 진실을 말해줍니다.