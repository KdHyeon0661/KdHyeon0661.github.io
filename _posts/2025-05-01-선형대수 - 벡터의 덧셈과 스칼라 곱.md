---
layout: post
title: 선형대수 - 벡터의 덧셈과 스칼라 곱
date: 2025-05-01 19:20:23 +0900
category: 선형대수
---
# 벡터의 덧셈과 스칼라 곱 — 정의, 기하, 성질, 실전 코드, 연습문제

본 문서는 벡터의 덧셈과 스칼라 곱을 **정의 → 기하학적 해석 → 대수적 성질(벡터공간 공리) → 선형결합/볼록결합 → 선형변환과의 연결 → 수치적 주의점 → PyTorch 예제 → 연습문제** 순서로 정리합니다.  

---

## 0. 표기 규약(Notation)

- 스칼라(실수): $$k,m,\alpha,\beta \in \mathbb{R}.$$  
- 벡터(열벡터로 이해): $$\mathbf{a},\mathbf{b},\mathbf{v},\mathbf{x}\in \mathbb{R}^n.$$  
- 영벡터: $$\mathbf{0}$$, 전치: $$\mathbf{v}^\top$$, 노름: $$\lVert \cdot \rVert \equiv \lVert \cdot \rVert_2.$$

---

## 1. 벡터의 덧셈(Vector Addition)

### 1.1 정의(성분별 가산)
같은 차원의 두 벡터 $$\mathbf{a},\mathbf{b}\in\mathbb{R}^n$$ 에 대해,
$$
\mathbf{a}+\mathbf{b}
=
\begin{bmatrix}
a_1\\ a_2\\ \vdots\\ a_n
\end{bmatrix}
+
\begin{bmatrix}
b_1\\ b_2\\ \vdots\\ b_n
\end{bmatrix}
=
\begin{bmatrix}
a_1+b_1\\ a_2+b_2\\ \vdots\\ a_n+b_n
\end{bmatrix}.
$$

#### 예제(수치)
$$
\mathbf{a}=
\begin{bmatrix}1\\3\end{bmatrix},\quad
\mathbf{b}=
\begin{bmatrix}4\\2\end{bmatrix}
\ \Rightarrow\
\mathbf{a}+\mathbf{b}=
\begin{bmatrix}5\\5\end{bmatrix}.
$$

### 1.2 기하학적 해석
- 꼬리–머리 방식(tip-to-tail): $$\mathbf{b}$$ 를 평행이동하여 $$\mathbf{a}$$ 의 끝점에서 시작시키면, 원점에서 $$\mathbf{b}$$ 끝점까지의 화살표가 $$\mathbf{a}+\mathbf{b}$$ 입니다.  
- 평행사변형 법칙: 두 벡터로 만든 평행사변형의 대각선이 합벡터입니다.  
- 거리 관계(내적 공간에서):  
  $$
  \lVert \mathbf{a}+\mathbf{b} \rVert^2
  =
  \lVert \mathbf{a} \rVert^2 + \lVert \mathbf{b} \rVert^2 + 2\,\mathbf{a}^\top\mathbf{b}.
  $$

---

## 2. 스칼라 곱(Scalar Multiplication)

### 2.1 정의(스칼라 배)
스칼라 $$k\in\mathbb{R}$$ 와 벡터 $$\mathbf{v}\in\mathbb{R}^n$$ 에 대해,
$$
k\,\mathbf{v}
=
k\,
\begin{bmatrix}
v_1\\ v_2\\ \vdots\\ v_n
\end{bmatrix}
=
\begin{bmatrix}
k v_1\\ k v_2\\ \vdots\\ k v_n
\end{bmatrix}.
$$

#### 예제(수치)
$$
\mathbf{v}=\begin{bmatrix}2\\-1\end{bmatrix},\ k=3
\ \Rightarrow\
k\mathbf{v}=
\begin{bmatrix}6\\-3\end{bmatrix}.
$$

### 2.2 기하학적 해석
- $$k>1$$: 길이가 커집니다.  
- $$0<k<1$$: 길이가 줄어듭니다.  
- $$k<0$$: 길이는 $$\lvert k\rvert$$ 배가 되고 **방향이 반대**가 됩니다.  
- $$k=0$$: 모든 성분이 0이 되어 영벡터가 됩니다.

---

## 3. 벡터 덧셈과 스칼라 곱의 기본 성질(벡터공간 공리)

모든 $$\mathbf{a},\mathbf{b},\mathbf{c}\in\mathbb{R}^n,\ k,m\in\mathbb{R}$$ 에 대하여:

1) 교환법칙  
$$
\mathbf{a}+\mathbf{b}=\mathbf{b}+\mathbf{a}.
$$

2) 결합법칙  
$$
(\mathbf{a}+\mathbf{b})+\mathbf{c}=\mathbf{a}+(\mathbf{b}+\mathbf{c}).
$$

3) 덧셈 항등원 존재  
$$
\exists \ \mathbf{0}\ \text{s.t.}\ \ \mathbf{a}+\mathbf{0}=\mathbf{a}.
$$

4) 덧셈 역원 존재  
$$
\forall \mathbf{a}\ \exists (-\mathbf{a})\ \text{s.t.}\ \ \mathbf{a}+(-\mathbf{a})=\mathbf{0}.
$$

5) 스칼라배의 분배법칙(벡터에 대한 분배)  
$$
k(\mathbf{a}+\mathbf{b})=k\mathbf{a}+k\mathbf{b}.
$$

6) 스칼라배의 분배법칙(스칼라에 대한 분배)  
$$
(k+m)\mathbf{a}=k\mathbf{a}+m\mathbf{a}.
$$

7) 스칼라배의 결합법칙  
$$
k(m\mathbf{a})=(km)\mathbf{a}.
$$

8) 항등 스칼라(1)  
$$
1\cdot \mathbf{a}=\mathbf{a}.
$$

이 성질들이 만족되면 $$\mathbb{R}^n$$ 은 실수체 위의 **벡터공간**이 됩니다.

---

## 4. 선형결합·생성(span)·볼록결합

### 4.1 선형결합과 직선·평면
벡터 $$\mathbf{v}_1,\dots,\mathbf{v}_k$$ 와 스칼라 $$\alpha_1,\dots,\alpha_k$$ 에 대해
$$
\alpha_1\mathbf{v}_1+\cdots+\alpha_k\mathbf{v}_k
$$
을 **선형결합**이라 합니다.  
- 한 벡터 $$\mathbf{v}$$ 로 만드는 집합 $$\{\alpha \mathbf{v}:\alpha\in\mathbb{R}\}$$ 은 원점을 지나는 **직선**입니다.  
- 두 벡터 $$\mathbf{u},\mathbf{v}$$ 가 선형독립이면 $$\{\alpha\mathbf{u}+\beta\mathbf{v}\}$$ 는 원점을 지나는 **평면**을 이룹니다.

### 4.2 생성(span)
$$
\operatorname{span}\{\mathbf{v}_1,\dots,\mathbf{v}_k\}=\left\{\sum_{i=1}^k \alpha_i\mathbf{v}_i\ :\ \alpha_i\in\mathbb{R}\right\}.
$$
이는 해당 벡터들이 만들 수 있는 모든 선형결합의 집합(부분공간)입니다.

### 4.3 볼록결합과 선형보간
계수들이 $$\alpha,\beta\ge 0,\ \alpha+\beta=1$$ 을 만족하면
$$
\alpha \mathbf{a}+\beta \mathbf{b}
$$
을 **볼록결합**이라 하며, 이는 두 점을 잇는 선분의 점을 줍니다.  
특히 $$t\in[0,1]$$ 에 대해
$$
(1-t)\mathbf{a}+t\mathbf{b}
$$
는 $$\mathbf{a}\to\mathbf{b}$$ 의 **선형보간(linear interpolation)** 입니다.

---

## 5. 선형변환과의 연결(행렬-벡터 곱의 선형성)

행렬 $$\mathbf{A}\in\mathbb{R}^{m\times n}$$ 에 대해 변환 $$T(\mathbf{x})=\mathbf{A}\mathbf{x}$$ 는 선형입니다.
$$
T(\alpha\mathbf{x}+\beta\mathbf{y})
=
\mathbf{A}(\alpha\mathbf{x}+\beta\mathbf{y})
=
\alpha \mathbf{A}\mathbf{x}+\beta \mathbf{A}\mathbf{y}
=
\alpha T(\mathbf{x})+\beta T(\mathbf{y}).
$$
즉, **덧셈과 스칼라배가 보존**됩니다. 이는 회전·스케일·전단 같은 선형 변환의 근거입니다(병진은 선형이 아닌 아핀 성분).

---

## 6. 수치 계산에서의 주의점

1) 자료형과 스케일: 매우 큰/작은 값이 섞이면 오차가 커질 수 있어 float64 사용을 고려합니다.  
2) 텐서 모양: PyTorch 에서 $$\text{shape}=(n,)$$ 과 $$\text{shape}=(n,1)$$ 의 차이로 브로드캐스팅 오류가 날 수 있습니다.  
3) 배치 연산: 많은 벡터를 다룰 때는 행렬로 쌓아 **행 단위/열 단위** 연산을 사용합니다.  
4) 정규화와 해석: 길이 비교가 중요한 문제(거리, 각도)는 정규화 후 비교하면 안정적입니다.  
5) 선형보간과 외삽: $$t\notin[0,1]$$ 은 외삽으로, 길게 벗어날 수 있음을 인지합니다.

---

## 7. PyTorch 예제 모음

다음 예제는 독립 실행 가능합니다.

### 7.1 기본 덧셈과 스칼라배
```python
import torch
torch.set_printoptions(precision=4, sci_mode=False)

a = torch.tensor([1., 3.])
b = torch.tensor([4., 2.])
v = torch.tensor([2., -1.])
k = 3.0

print("a + b =", a + b)        # [5., 5.]
print("k * v =", k * v)        # [6., -3.]
```

### 7.2 성질 검증(교환·결합·분배)
```python
import torch

a = torch.randn(5)
b = torch.randn(5)
c = torch.randn(5)
k = torch.randn(1).item()
m = torch.randn(1).item()

lhs1 = a + b
rhs1 = b + a
print("교환법칙:", torch.allclose(lhs1, rhs1))

lhs2 = (a + b) + c
rhs2 = a + (b + c)
print("결합법칙:", torch.allclose(lhs2, rhs2))

lhs3 = k * (a + b)
rhs3 = k * a + k * b
print("분배법칙(k·(a+b)):", torch.allclose(lhs3, rhs3))

lhs4 = (k + m) * a
rhs4 = k * a + m * a
print("분배법칙((k+m)·a):", torch.allclose(lhs4, rhs4))
```

### 7.3 선형보간과 경로 생성
```python
import torch

a = torch.tensor([0., 0.])
b = torch.tensor([4., 2.])

def lerp(a, b, t):
    return (1 - t) * a + t * b

ts = torch.linspace(0, 1, steps=6)
path = torch.stack([lerp(a, b, float(t)) for t in ts])
print("선형보간 경로:\n", path)
```

### 7.4 직선/평면 생성(선형결합)
```python
import torch

u = torch.tensor([1., 0., 1.])
v = torch.tensor([0., 2., 1.])

# 원점을 지나는 평면의 샘플 그리드
alphas = torch.linspace(-1, 1, steps=3)
betas  = torch.linspace(-1, 1, steps=3)

points = []
for a in alphas:
    for b in betas:
        p = a * u + b * v
        points.append(p)
P = torch.stack(points)
print("평면 상의 점 9개:\n", P)
```

### 7.5 행렬-벡터 곱의 선형성 확인
```python
import torch

A = torch.tensor([[2., 1.],
                  [0., 3.]], dtype=torch.float32)
x = torch.tensor([1., 2.])
y = torch.tensor([3., -1.])
alpha, beta = 1.5, -0.7

lhs = A @ (alpha * x + beta * y)
rhs = alpha * (A @ x) + beta * (A @ y)

print("선형성 일치:", torch.allclose(lhs, rhs))
```

### 7.6 배치 벡터 덧셈과 스칼라배(브로드캐스팅)
```python
import torch

X = torch.tensor([[1., 2., 3.],
                  [4., 5., 6.]])       # 2 x 3 (행=표본, 열=특성)
b = torch.tensor([10., 20., 30.])      # 1 x 3 (특성별 상수 더하기)
k = 0.5                                 # 스칼라배

Y = k * (X + b)                         # 각 행에 b를 더한 후 0.5배
print("배치 덧셈+스칼라배:\n", Y)
```

### 7.7 선형보간 기반의 평행사변형 대각선 확인
$$
\mathbf{a}+\mathbf{b}
=
\mathbf{a}+(1)\mathbf{b}
=
(1)\mathbf{a}+(1)\mathbf{b}.
$$
아래는 수치 확인입니다.
```python
import torch

a = torch.tensor([2., 1.])
b = torch.tensor([1., 3.])

diag = a + b
par_mid = (a + (a + b)) / 2.0  # 평행사변형 대각선의 중점
other_mid = ((a + b) + b) / 2.0

print("대각선 끝점 a+b =", diag)
print("대각선 중점(방법1) =", par_mid, "  (방법2) =", other_mid)
```

---

## 8. 실제 시나리오

1) 물리: 힘 벡터의 합력은 성분별 덧셈으로 계산됩니다. 질점에 작용하는 여러 힘 $$\mathbf{F}_i$$ 의 합 $$\sum_i \mathbf{F}_i$$ 가 총합력입니다.  
2) 컴퓨터 그래픽스: 위치/법선/색상 벡터의 보간은 볼록결합(선형보간)으로 처리합니다.  
3) 로봇/경로 계획: 키 프레임 사이를 $$t$$ 로 보간하면 매끄러운 직선 경로를 얻습니다.  
4) 데이터 전처리: 특성별 상수 이동(센터링)은 모든 표본 벡터에 동일 벡터를 더하는 연산입니다.

---

## 9. 흔한 실수와 점검표

- 차원 불일치: 덧셈은 같은 차원에서만 정의됩니다.  
- 텐서 모양: $$\text{shape}=(n,)$$ 과 $$(n,1)$$ 혼동은 브로드캐스팅 오류의 원인입니다.  
- 스칼라와 길이: $$k<0$$ 는 방향 반전을 의미합니다.  
- 선형보간 범위: $$t\in[0,1]$$ 를 벗어나면 외삽이므로 안전성 검토가 필요합니다.  
- 수치 안정성: 매우 큰 값과 매우 작은 값을 함께 더하면 상대 오차가 커질 수 있습니다(스케일 조정 또는 dtype 상향).

---

## 10. 연습문제(해설 힌트 포함)

1) $$\mathbf{a}=[-1,\,2,\,0]^\top,\ \mathbf{b}=[3,\,-1,\,4]^\top$$ 에 대해  
   (i) $$\mathbf{a}+\mathbf{b}$$, (ii) $$2\mathbf{a}-3\mathbf{b}$$ 를 구하시오.  
   힌트: 성분별 덧셈/스칼라배.

2) $$\mathbf{u}=[1,\,0]^\top,\ \mathbf{v}=[2,\,1]^\top$$ 가 만들 수 있는 모든 벡터 $$\alpha\mathbf{u}+\beta\mathbf{v}$$ 의 집합은 어떤 도형인가?  
   힌트: 선형독립인 두 벡터의 선형결합.

3) 두 점 $$\mathbf{p}=(1,\,2),\ \mathbf{q}=(5,\,3)$$ 를 잇는 선분 위의 점을 $$t\in[0,1]$$ 에 대해 표현하고, $$t=0.25,0.5,0.75$$ 에서의 좌표를 구하시오.  
   힌트: $$(1-t)\mathbf{p}+t\mathbf{q}.$$

4) 행렬 $$\mathbf{A}=\begin{bmatrix}2&1\\0&3\end{bmatrix}$$ 에 대해 선형성 $$\mathbf{A}(\alpha\mathbf{x}+\beta\mathbf{y})=\alpha\mathbf{A}\mathbf{x}+\beta\mathbf{A}\mathbf{y}$$ 를 임의의 $$\mathbf{x},\mathbf{y}$$ 로 수치 검증하시오.  
   힌트: 코드로 난수 텐서를 생성해 확인.

5) 볼록결합 $$\alpha\mathbf{a}+(1-\alpha)\mathbf{b}$$ 가 선분을 준다는 사실을 기하적으로 설명하고, $$\alpha=-0.2,1.3$$ 같은 외삽 값에서의 위치를 해석하시오.  
   힌트: 직선 위 점의 파라미터 해석.

---

## 11. 요약

- 벡터의 덧셈은 **성분별 가산**이며, 스칼라 곱은 **방향 보존·크기 조절** 연산입니다.  
- 이 두 연산은 벡터공간 공리(교환·결합·분배·항등·역원)를 만족하며 **선형결합/생성/보간**의 기초가 됩니다.  
- 선형변환(행렬-벡터 곱)은 덧셈과 스칼라배를 보존하므로, 그래픽스·로봇·데이터과학 전반의 모델링에 필수적입니다.  
- 실무에서는 **차원·모양·정규화·스케일**을 주의하며, 배치 연산으로 효율을 높입니다.