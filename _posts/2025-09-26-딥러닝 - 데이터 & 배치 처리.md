---
layout: post
title: 딥러닝 - 데이터 & 배치 처리
date: 2025-09-26 23:25:23 +0900
category: 딥러닝
---
# 1.2 데이터 & 배치 처리
**배치/미니배치/온라인**, **에폭·스텝**, **셔플·시드**, **패딩/마스킹**, **정규화/표준화**, **PyTorch `Dataset`/`DataLoader` 설계(전처리·캐싱·병렬 로딩)**

> 본 장은 **딥러닝 실전의 70%** 를 차지하는 “데이터 → 배치 → 로더” 파이프라인을 **개념+코드**로 끝까지 정리한다.  
> 예제 코드는 **PyTorch** 기준. 수식은 **MathJax**(항상 $$...$$).  
> 상황 가정: 이미지/텍스트/시계열 등 **가변 길이** 데이터까지 모두 다룸.

---

## A. 배치 스킴: 배치/미니배치/온라인

### A-1. 정의와 직관
- **풀 배치(full batch)**: 전체 $$N$$개 샘플로 한 번에 그라디언트 계산.  
  - 장점: 그라디언트 추정 분산 낮음.  
  - 단점: 메모리·연산량 과대, 업데이트 빈도 낮음.
- **미니배치(mini-batch)**: $$B\ll N$$개로 그라디언트 추정 후 업데이트(가장 흔한 방식).  
  - 장점: 분산과 비용의 **균형**, 하드웨어 병렬화 최적.  
- **온라인/확률적(SGD)**: $$B=1$$, 샘플 1개마다 업데이트.  
  - 장점: 빠른 적응, 스트림 데이터에 적합.  
  - 단점: 분산 큼 → 불안정/수렴 지연.

### A-2. 수식으로 비교
- **경험적 위험 최소화**의 그라디언트 근사:
  $$
  \nabla_\theta \mathcal{R}_\text{emp}(\theta)
  = \frac{1}{N}\sum_{i=1}^{N}\nabla_\theta \ell\big(f_\theta(\mathbf{x}_i), y_i\big)
  \approx \frac{1}{B}\sum_{i\in\mathcal{B}}\nabla_\theta \ell\big(f_\theta(\mathbf{x}_i), y_i\big)
  $$
  **미니배치 크기 $$B$$**가 커질수록 분산이 대체로 $$\propto 1/B$$ 로 감소(경험적 경향).

### A-3. 배치 크기 선택 가이드(직관)
- **너무 작음**: 그라디언트 잡음↑, 수렴 불안정, BN 통계 왜곡.  
- **너무 큼**: 일반화 약화 가능성(평탄 최소화 논쟁), LR warmup·스케줄 필수.  
- 초기: **(GPU 메모리/모델 크기/해상도)** 를 고려해 **가장 큰 안정 배치**를 택하고,  
  필요 시 **그라디언트 누적**(뒤에서 코드 제공)로 “유효 배치”를 키운다.

---

## B. 에폭(epoch), 스텝(step), 스케줄(step/epoch-based)

### B-1. 용어
- **에폭(epoch)**: **학습셋을 1회** 모두 소모.  
- **스텝(step)**: **미니배치 1회** 업데이트.  
- **에폭당 스텝 수**:
  $$
  \text{steps\_per\_epoch} = \left\lceil\frac{N}{B}\right\rceil
  $$
- 스케줄러 유형:
  - **스텝 기반**: 특정 step 마다 LR 감소.
  - **에폭 기반**: 특정 epoch 마다 LR 조정.
  - **연속형**: Cosine/Polynomial 같이 **스텝 수**를 직접 입력.

### B-2. 드롭 라스트(drop_last)와 BN/길이 이슈
- 마지막 미니배치가 **작은 크기**가 되면 BN 통계 흔들림 가능.  
- `DataLoader(..., drop_last=True)` 는 일관된 배치 크기를 보장(특히 분산/BN 환경에서 권장).

---

## C. 셔플(shuffle) & 시드(seed)

### C-1. 왜 셔플이 필요한가
- 데이터가 **클래스/시계열/디렉토리**별로 정렬되어 있으면, 초기에 **편향된 배치**가 생김 → 수렴 지연·최적화 불안정.  
- 훈련은 **셔플=True**, 검증/테스트는 **셔플=False** 가 원칙.

### C-2. 시드 고정: 재현성
- 난수 소스: Python `random`, NumPy, PyTorch(CPU/GPU) 등 **다중 소스**.  
- **모두** 고정 + DataLoader의 `generator`/`worker_init_fn`로 **워커별 시드** 관리.

```python
# 재현성 유틸 (CPU/CUDA + cuDNN)
import os, random, numpy as np, torch

def set_seed(seed=42, deterministic=True):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
```

---

## D. 패딩(padding) & 마스킹(masking)

### D-1. 왜 필요한가 (가변 길이 시퀀스/배열)
- 텍스트 토큰, 음성 프레임, 비디오 클립 등 **길이가 다름** → **배치 텐서**로 묶으려면 **최대 길이**에 맞춰 **패딩** 필요.
- **마스크**는 패딩 위치를 가리켜 **손실/주의(attention)** 계산에서 **무시**하게 해준다.

### D-2. 수식적 관점
- 길이 $$L_i$$ 시퀀스 $$\mathbf{x}^{(i)}\in\mathbb{R}^{L_i\times d}$$ 를 **최대 길이** $$L_{\max}$$ 로 패딩하여  
  $$\tilde{\mathbf{x}}^{(i)}\in\mathbb{R}^{L_{\max}\times d}$$,  
  **마스크** $$\mathbf{m}^{(i)}\in\{0,1\}^{L_{\max}}$$ 를 만들어  
  $$
  \mathbf{m}^{(i)}_t =
  \begin{cases}
  1 & \text{유효 토큰}\\
  0 & \text{패딩}
  \end{cases}
  $$
  **Attention** 에서는 보통 **패딩 위치**에 **큰 음수**(e.g., \(-10^9\)) 를 가산하여 softmax에서 무력화.

### D-3. PyTorch 패딩/마스킹 예제 (`pad_sequence`)

```python
from torch.nn.utils.rnn import pad_sequence
import torch, random

# 가변 길이 토큰 시퀀스(정수 ID) 예시
def toy_sequences(n=6, vocab=100, min_len=3, max_len=10, seed=0):
    random.seed(seed)
    seqs = []
    for _ in range(n):
        L = random.randint(min_len, max_len)
        seqs.append(torch.randint(1, vocab, (L,), dtype=torch.long))
    return seqs

seqs = toy_sequences()
# 왼쪽 패딩 대신 보통 '오른쪽' 패딩을 사용
padded = pad_sequence(seqs, batch_first=True, padding_value=0)  # (B, Lmax)
lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
attn_mask = (padded != 0).to(torch.bool)  # True=유효, False=패딩
print(padded.shape, lengths, attn_mask.shape)
```

### D-4. RNN 계열: `pack_padded_sequence` (효율적 계산)
```python
import torch.nn.utils.rnn as rnn_utils

# lengths는 내림차순 정렬되어야 함
lengths_sorted, perm_idx = lengths.sort(0, descending=True)
padded_sorted = padded[perm_idx]

packed = rnn_utils.pack_padded_sequence(
    padded_sorted, lengths_sorted.cpu(), batch_first=True, enforce_sorted=True
)
# RNN/GRU/LSTM에 넣으면 패딩 구간을 스킵하여 효율↑
# out_packed = lstm(packed)
```

### D-5. Transformer 계열: 어텐션 마스크
```python
# 어텐션 마스크: True=keep, False=mask  (PyTorch nn.Transformer 계열은 반대 논리일 수 있음)
# MultiheadAttention의 attn_mask / key_padding_mask 사용 주의
key_padding_mask = ~attn_mask  # True=패딩
```

---

## E. 데이터 정규화/표준화

### E-1. 표준화(z-score)
- **정의**: 채널/피처별 평균·표준편차로 스케일링
  $$
  \hat{x} = \frac{x - \mu}{\sigma+\varepsilon}
  $$
- **주의**: **학습셋**으로만 $$\mu,\sigma$$ 를 추정하고, **검증/테스트**에는 **transform만** 적용(데이터 누수 금지).

### E-2. 이미지 per-channel 정규화(예: RGB)
```python
# 학습셋에서만 mean/std 계산 (스트리밍 방식 권장)
import torch
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import numpy as np

class RandomImageSet(Dataset):
    def __init__(self, n=1000, h=64, w=64):
        self.n, self.h, self.w = n, h, w
        self.data = torch.rand(n, 3, h, w)  # [0,1] 랜덤 예시
    def __len__(self): return self.n
    def __getitem__(self, idx): return self.data[idx], 0

train_ds = RandomImageSet()
train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)

# Welford 방식으로 안정적 평균/분산 추정
cnt = 0
mean = torch.zeros(3)
M2   = torch.zeros(3)
for xb, _ in train_loader:
    B, C, H, W = xb.shape
    xb = xb.view(B, C, -1).mean(dim=2)  # B x C
    for i in range(B):
        cnt += 1
        delta = xb[i] - mean
        mean += delta / cnt
        M2   += delta * (xb[i] - mean)
std = torch.sqrt(M2 / (cnt - 1))

print("mean:", mean, "std:", std)

normalize = transforms.Normalize(mean.tolist(), std.tolist())
# 이후 Dataset에서 transform으로 normalize 적용
```

### E-3. 표/시계열(탭울러) 피처 스케일링
- 피처별 $$\mu,\sigma$$ 로 표준화, 또는 **RobustScaler**(사분위수 기반)로 이상치 영향 완화.
- 범주형은 **원-핫/임베딩**; 순서형은 **정수 스케일** + **임베딩** 고려.

---

## F. PyTorch 데이터 파이프라인 설계

### F-1. `Dataset` 구성 원칙
- `__len__`: 샘플 수
- `__getitem__(idx)`: **순수 함수**처럼 동작(동일 idx는 동일 데이터, 랜덤 증강은 워커 시드로 제어)
- **반환 타입**: 텐서 또는 **dict**(추천: 향후 마스크/길이/메타데이터 확장 용이)

```python
from torch.utils.data import Dataset
import torch

class ToyTextDataset(Dataset):
    def __init__(self, texts, tokenizer, cache_tokens=True):
        self.texts = texts
        self.tok = tokenizer
        self.cache_tokens = cache_tokens
        self._cache = {}  # 간단 캐시

    def __len__(self): return len(self.texts)

    def __getitem__(self, idx):
        if self.cache_tokens and idx in self._cache:
            return self._cache[idx]
        ids = torch.tensor(self.tok(self.texts[idx]), dtype=torch.long)
        sample = {"input_ids": ids, "len": len(ids)}
        if self.cache_tokens:
            self._cache[idx] = sample
        return sample
```

> **캐싱 주의**: 워커가 여러 개인 경우 **각 워커 프로세스마다 독립 메모리 공간**. 큰 캐시는 **디스크 캐시**(memmap/LMDB/WebDataset) 고려.

### F-2. `collate_fn` — 배치 만들기(패딩·마스크 포함)
```python
from torch.nn.utils.rnn import pad_sequence

def pad_collate(batch, pad_id=0):
    ids = [b["input_ids"] for b in batch]
    lens = torch.tensor([len(x) for x in ids], dtype=torch.long)
    padded = pad_sequence(ids, batch_first=True, padding_value=pad_id)
    attn_mask = (padded != pad_id)
    return {
        "input_ids": padded,
        "attention_mask": attn_mask,
        "lengths": lens
    }
```

### F-3. `DataLoader` 성능 옵션
- `batch_size`: GPU 메모리에 맞게
- `num_workers`: CPU 코어/스토리지 IO에 맞춰 탐색(보통 4~8로 시작)
- `pin_memory=True`: CPU→GPU 복사 빠르게(페이지 잠금)
- `persistent_workers=True`: 에폭 간 워커 유지(워커 재스폰 비용 감소)
- `prefetch_factor`: 워커당 프리페치 배치 수(기본 2)
- `worker_init_fn`: 워커 시드 세팅
- `generator`: 셔플 시드 제어

```python
import torch
from torch.utils.data import DataLoader
import numpy as np, random

def worker_init_fn(worker_id):
    # 워커별 시드 고정 (재현성)
    seed = torch.initial_seed() % 2**32
    random.seed(seed); np.random.seed(seed)

g = torch.Generator()
g.manual_seed(123)  # 셔플 결정

loader = DataLoader(
    dataset=..., batch_size=32, shuffle=True,
    num_workers=4, pin_memory=True, persistent_workers=True,
    worker_init_fn=worker_init_fn, generator=g,
    collate_fn=pad_collate, prefetch_factor=4, drop_last=True
)
```

> **실무 팁**  
> - **I/O 바운드**(JPEG 디코드/압축 해제)면 `num_workers`↑, **CPU 바운드**(무거운 증강)면 **증강을 GPU**(kornia 등)로 옮기는 것도 고려.  
> - NVMe/SSD, 이미지 압축 포맷, **tar 시퀀셜 읽기**(WebDataset) 등 **스토리지 패턴**이 병목이면 구조 자체를 바꾸자.

---

## G. 샘플러(sampler) & 불균형 대응

### G-1. 기본 샘플러
- `RandomSampler` / `SequentialSampler` 는 `shuffle=True/False` 와 대응.
- `BatchSampler`: 샘플러 결과를 **배치 단위**로 묶음.

### G-2. 클래스 불균형: `WeightedRandomSampler`
```python
from torch.utils.data import WeightedRandomSampler
import torch

# 클래스 0/1 비율이 90/10이라 가정, 소수 클래스에 높은 가중치
labels = torch.tensor([0,0,0,1, ...])  # 예시
class_counts = torch.bincount(labels)
class_weights = 1.0 / class_counts.float()
sample_weights = class_weights[labels]

sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(sample_weights),
    replacement=True
)

loader_balanced = DataLoader(
    dataset=..., batch_size=64, sampler=sampler,
    collate_fn=..., num_workers=4, pin_memory=True
)
```

> **주의**: `sampler` 사용 시 `shuffle=False` 여야 한다(동시 지정 불가).

---

## H. 전처리 파이프라인: 오프라인 vs 온라인, 캐싱 전략

### H-1. 오프라인(사전) 전처리
- **장점**: 학습 중 부담↓, 재현성↑, I/O 패턴 예측 가능.  
- **예**: 텍스트 토크나이즈 → `.npy/.pt` 저장, 이미지 **타일링/리사이즈** 후 저장.

### H-2. 온라인(런타임) 전처리
- **장점**: 증강 다양성↑, 저장 공간 절약.  
- **단점**: CPU 부하↑, 워커 설정 민감.

### H-3. 캐싱 레이어
- **메모리 캐시**: 빈번 샘플/작은 피처에 효과. (워커별 중복 주의)  
- **디스크 캐시**: **memmap**, **LMDB**, **WebDataset(.tar)** 로 랜덤 I/O를 시퀀셜에 가깝게.  
- **체크섬/버전**: 데이터 변조 탐지 & 일관성.

```python
# 간단 memmap 예시 (float32 벡터 피처 가정)
import numpy as np, os

def build_memmap(feats, path, dtype='float32'):
    N, D = feats.shape
    fp = np.memmap(path, dtype=dtype, mode='w+', shape=(N, D))
    fp[:] = feats[:]
    del fp  # flush

def load_memmap(path, N, D, dtype='float32'):
    return np.memmap(path, dtype=dtype, mode='r', shape=(N, D))

# Dataset에서 __getitem__에서 memmap[index] 반환
```

---

## I. GPU 전송 최적화 & 마이크로배치(그라디언트 누적)

### I-1. 고정 메모리(pin_memory) & 비동기 전송
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for batch in loader:  # loader는 pin_memory=True
    # CPU 텐서 → GPU 텐서 (non_blocking=True)
    x = batch["input_ids"].to(device, non_blocking=True)
    mask = batch["attention_mask"].to(device, non_blocking=True)
    # ...
```

### I-2. 그라디언트 누적 — 유효 배치 확장
- 유효 배치 $$B_\text{eff} = B_\text{per\_gpu} \times \text{accumulate\_steps} \times \text{num\_gpus}$$

```python
model.train()
opt.zero_grad(set_to_none=True)
accum_steps = 4
for step, batch in enumerate(loader, start=1):
    x = batch["input_ids"].to(device, non_blocking=True)
    y = torch.randint(0, 2, (x.size(0),), device=device)  # 예시 라벨
    logits = model(x)
    loss = criterion(logits, y) / accum_steps
    loss.backward()
    if step % accum_steps == 0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        opt.step(); opt.zero_grad(set_to_none=True)
```

> **BN 주의**: 매우 작은 per-GPU 배치에서 BN 통계가 불안정. **SyncBN/GN/LN**, **Ghost BN**(가짜 큰 배치) 또는 **큰 배치** 확보가 해법.

---

## J. 분산 학습(DDP)과 DataLoader

### J-1. `DistributedSampler` + `set_epoch`
- 각 프로세스가 **데이터 부분 집합**을 균등 분배하여 중복 학습 방지.
- 매 에폭마다 `sampler.set_epoch(epoch)` 로 **셔플 시드**를 바꿔준다.

```python
from torch.utils.data.distributed import DistributedSampler

sampler = DistributedSampler(train_dataset, shuffle=True, drop_last=True)
train_loader = DataLoader(
    train_dataset, batch_size=..., sampler=sampler,
    num_workers=..., pin_memory=True, persistent_workers=True, collate_fn=...
)

for epoch in range(num_epochs):
    sampler.set_epoch(epoch)
    for batch in train_loader:
        ...
```

### J-2. 워커 시드 & 증강 일관성
- `worker_init_fn`에서 시드 설정.  
- **시간 의존** 증강(예: 랜덤 크롭)은 프로세스/워커 간 결정론적이거나 **충분히 섞이는지** 확인.

---

## K. 검증/테스트 로더 정책

- **shuffle=False**, **drop_last=False**(전체 평가)  
- **증강 비활성화**(center crop/resize만)  
- **batch_size** 는 GPU 메모리 허용 한도에서 크게(평가 속도↑)  
- 정규화/토크나이즈 등은 **학습셋 통계**로 transform만 수행.

---

## L. 통합 예제 1 — 텍스트(가변 길이) 분류 파이프라인

### L-1. 토크나이저(예시)
```python
# 매우 단순한 공백 기반 토크나이저 (예시)
def tiny_tokenizer(s, vocab=None, unk=1):
    if vocab is None:
        raise ValueError("need vocab dict")
    return [vocab.get(tok, unk) for tok in s.strip().split()]

# vocab 예시
vocab = {"<pad>":0, "<unk>":1, "hello":2, "world":3, "cat":4, "dog":5}
def tok_fn(s): return tiny_tokenizer(s, vocab=vocab)
```

### L-2. Dataset + collate + DataLoader
```python
texts = [
    "hello world",
    "cat cat dog",
    "hello cat",
    "dog world world world",
    "hello",
    "cat"
]

class TextClsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.texts = texts
        self.labels = labels
        self.tok = tokenizer
    def __len__(self): return len(self.texts)
    def __getitem__(self, idx):
        ids = torch.tensor(self.tok(self.texts[idx]), dtype=torch.long)
        y = torch.tensor(self.labels[idx], dtype=torch.long)
        return {"input_ids": ids, "label": y}

from torch.nn.utils.rnn import pad_sequence

def collate_text(batch, pad_id=0):
    ids   = [b["input_ids"] for b in batch]
    ys    = torch.stack([b["label"] for b in batch])
    lens  = torch.tensor([len(x) for x in ids], dtype=torch.long)
    pad   = pad_sequence(ids, batch_first=True, padding_value=pad_id)
    mask  = (pad != pad_id)
    return {"input_ids": pad, "attention_mask": mask, "lengths": lens, "labels": ys}

labels = [0,1,0,1,0,1]
ds = TextClsDataset(texts, labels, tok_fn)

loader = DataLoader(
    ds, batch_size=2, shuffle=True, collate_fn=collate_text,
    num_workers=2, pin_memory=True, persistent_workers=True
)

for b in loader:
    print({k: (v.shape if torch.is_tensor(v) else v) for k,v in b.items()})
    break
```

### L-3. 간단 모델 & 학습 루프(길이 마스크 사용 예)
```python
import torch.nn as nn
import torch.nn.functional as F

class MeanPoolClassifier(nn.Module):
    def __init__(self, vocab_size, emb=32, num_cls=2, pad_id=0):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb, padding_idx=pad_id)
        self.fc  = nn.Linear(emb, num_cls)
    def forward(self, ids, mask):
        x = self.emb(ids)              # (B, L, E)
        mask_f = mask.float().unsqueeze(-1)  # (B, L, 1)
        pooled = (x * mask_f).sum(dim=1) / mask_f.sum(dim=1).clamp_min(1e-6)
        return self.fc(pooled)

model = MeanPoolClassifier(vocab_size=len(vocab))
opt = torch.optim.AdamW(model.parameters(), lr=1e-2)
ce  = nn.CrossEntropyLoss()

for epoch in range(5):
    for batch in loader:
        ids  = batch["input_ids"]
        mask = batch["attention_mask"]
        y    = batch["labels"]
        logits = model(ids, mask)
        loss = ce(logits, y)
        opt.zero_grad(); loss.backward(); opt.step()
    print(f"epoch {epoch+1} done")
```

---

## M. 통합 예제 2 — 이미지(정규화/증강/병렬 로딩)

### M-1. Transform 파이프라인(학습/평가 분리)
```python
from torchvision import transforms

# 학습용: 랜덤 증강 + 정규화
train_tf = transforms.Compose([
    transforms.RandomResizedCrop(64, scale=(0.8,1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])  # 예시
])

# 검증/테스트: 결정적 변환
test_tf = transforms.Compose([
    transforms.Resize(72),
    transforms.CenterCrop(64),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
])
```

### M-2. Dataset 설계(경로/라벨)
```python
from PIL import Image
import os, glob

class ImageFolderTiny(Dataset):
    def __init__(self, root, transform=None):
        self.samples = []  # (path, label)
        classes = sorted(os.listdir(root))
        self.class_to_idx = {c:i for i,c in enumerate(classes)}
        for c in classes:
            for p in glob.glob(os.path.join(root, c, "*.jpg")):
                self.samples.append((p, self.class_to_idx[c]))
        self.transform = transform

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx):
        path, y = self.samples[idx]
        img = Image.open(path).convert("RGB")  # I/O 병목 지점
        if self.transform: img = self.transform(img)
        return img, y
```

### M-3. DataLoader 설정(성능 튜닝)
```python
train_loader = DataLoader(
    ImageFolderTiny("/data/train", transform=train_tf),
    batch_size=128, shuffle=True, num_workers=8,
    pin_memory=True, prefetch_factor=4, persistent_workers=True, drop_last=True
)

val_loader = DataLoader(
    ImageFolderTiny("/data/val", transform=test_tf),
    batch_size=256, shuffle=False, num_workers=8,
    pin_memory=True, prefetch_factor=4, persistent_workers=True
)
```

> **성능 체크**:  
> - CPU 사용률 100%? 증강이 과함 → GPU 증강(kornia) 고려.  
> - GPU `util`이 40% 미만으로 흔들림? `num_workers/prefetch_factor`↑, I/O 형식 점검.  
> - `persistent_workers=True` 로 에폭 경계 워커 재스폰 비용 ↓

---

## N. 트러블슈팅 체크리스트

- **배치가 비어 있음**: `drop_last=True` + 작은 데이터셋에서 `batch_size` 과대.  
- **DataLoader 멈춤/데드락**: 워커 내 예외 발생(메시지 지연), 큰 객체 전달 피하기, `timeout` 설정.  
- **재현성 불일치**: `worker_init_fn`, `generator` 누락, 증강/라이브러리 내부 난수.  
- **OOM**: 배치 축소, mixed-precision(AMP), 해상도↓, 그라디언트 체크포인팅, 누적 사용.  
- **BN 불안정**: 작은 per-GPU 배치 → SyncBN/GN/LN 고려.  
- **데이터 누수**: 정규화 통계/피처 선택을 train만으로 fit.

---

## O. 벤치마킹 스니펫(로더 속도·GPU 활용도 감 보기)

```python
import time, torch

def bench_loader(loader, iters=100):
    t0 = time.time()
    nb = 0
    for i, batch in enumerate(loader):
        if i >= iters: break
        # CPU→GPU 전송 비용 측정해보려면 아래 두 줄 추가
        # if torch.cuda.is_available():
        #     batch = {k: (v.cuda(non_blocking=True) if torch.is_tensor(v) else v) for k,v in batch.items()}
        nb += 1
    dt = time.time() - t0
    print(f"{nb} iters in {dt:.2f}s -> {nb/dt:.2f} it/s")
```

---

## P. 데이터 정책(검증/테스트의 절대 원칙)

1. **Train/Val/Test** 분리 철저.  
2. 전처리 **fit**(평균/표준편차, 토크나이저 vocab) 은 **Train만**.  
3. Val/Test에서 **증강 금지**, 결정적 변환만.  
4. 최종 수치는 **Test 1회** 보고.

---

## Q. 요약 — 실무 세팅 추천(초기값)

- 공통
  - `batch_size`: GPU가 허용하는 최대치(AMP 사용 가정)
  - `num_workers`: 4~8 (코어/스토리지에 맞게 탐색)
  - `pin_memory=True`, `persistent_workers=True`, `prefetch_factor=4`
  - `drop_last=True`(train), `False`(val/test)
  - `shuffle=True`(train), `False`(val/test)
  - `worker_init_fn` + `generator` 로 시드 관리
  - **collate_fn**: 가변 길이면 커스텀(패딩/마스크)
- 이미지  
  - 학습: RandomResizedCrop + Flip + Normalize  
  - 평가: Resize + CenterCrop + Normalize
- 텍스트  
  - 사전 토크나이즈/사전 구축(오프라인) + collate에서 패딩/마스크  
  - 긴 시퀀스는 **슬라이딩 윈도우** 또는 **동적 패딩**
- 시계열/오디오  
  - **윈도잉/스펙트로그램** 오프라인 캐싱 고려(memmap/LMDB)

---

## R. 연습 과제

1. **배치 크기 스윕**: $$B\in\{16,32,64,128\}$$ 로 학습 시간, val loss/acc 비교.  
2. **워커 수 스윕**: `num_workers` ∈ {0, 2, 4, 8} → it/s 측정.  
3. **패딩 전략 비교**: max-length vs bucketing(유사 길이끼리 배치) 로 GPU 효율 비교.  
4. **정규화 통계 실험**: Train-only vs All-data로 계산 시(누수) 성능 비교(차이를 기록).  
5. **캐싱 레이어**: JPEG 원본 vs `pt` 텐서 캐시 vs WebDataset로 **epoch time** 비교.

---

# 부록: 수식 & 개념 큐시트

- **미니배치 그라디언트 추정 분산(경향)**  
  $$
  \mathrm{Var}\!\left[\frac{1}{B}\!\sum_{i\in\mathcal{B}} g_i\right]
  \approx \frac{1}{B}\mathrm{Var}[g]
  $$
- **에폭당 스텝**  
  $$
  \text{steps/epoch} = \left\lceil\frac{N}{B}\right\rceil
  $$
- **표준화**  
  $$
  \hat{x}=\frac{x-\mu}{\sigma+\varepsilon}
  $$
- **마스크 정의**  
  $$
  m_t=\begin{cases}
  1 & \text{유효 위치}\\
  0 & \text{패딩}
  \end{cases}
  $$
- **어텐션에서 패딩 무력화(개념)**  
  $$
  a_{ij} = \frac{\exp(e_{ij} + \alpha (m_j-1))}
  {\sum_k \exp(e_{ik} + \alpha (m_k-1))}, \quad \alpha \ll 0
  $$