---
layout: post
title: 파이썬 심화 - 파일과 입출력 (2)
date: 2025-11-28 18:25:23 +0900
category: 파이썬 심화
---
# 파일과 입출력 (2)

## 압축된 데이터 파일 읽고 쓰기

### 다양한 압축 형식 지원
```python
import gzip
import bz2
import lzma
import zipfile
import tarfile
import os

# Gzip 압축 파일 쓰기
def create_gzip_file():
    """Gzip 형식으로 텍스트 파일 압축"""
    text_content = """이것은 테스트 파일입니다.
여러 줄에 걸친 텍스트 데이터를 압축합니다.
압축을 통해 디스크 공간을 절약할 수 있습니다.
Gzip은 빠른 압축과 해제가 가능한 포맷입니다."""
    
    with gzip.open('example.txt.gz', 'wt', encoding='utf-8') as f:
        f.write(text_content)
    print(f"Gzip 파일 생성 완료: example.txt.gz ({os.path.getsize('example.txt.gz')}바이트)")

# Gzip 파일 읽기
def read_gzip_file():
    """Gzip 파일 읽기"""
    with gzip.open('example.txt.gz', 'rt', encoding='utf-8') as f:
        content = f.read()
        print("Gzip 파일 내용:")
        print(content)

create_gzip_file()
read_gzip_file()

# Bzip2 압축 (더 높은 압축률)
def create_bz2_file():
    """Bzip2 형식으로 데이터 압축"""
    data = "Bzip2는 gzip보다 높은 압축률을 제공합니다.\n" * 100
    
    with bz2.open('data.bz2', 'wt', encoding='utf-8', compresslevel=9) as f:
        f.write(data)
    
    original_size = len(data.encode('utf-8'))
    compressed_size = os.path.getsize('data.bz2')
    ratio = (1 - compressed_size/original_size) * 100
    
    print(f"\nBzip2 압축 결과:")
    print(f"  원본 크기: {original_size}바이트")
    print(f"  압축 크기: {compressed_size}바이트")
    print(f"  압축률: {ratio:.1f}%")

create_bz2_file()

# LZMA 압축 (매우 높은 압축률)
def create_lzma_file():
    """LZMA/XZ 형식으로 데이터 압축"""
    # 대용량 샘플 데이터 생성
    sample_data = []
    for i in range(1000):
        sample_data.append(f"레코드 {i:04d}: " + "x" * 50 + "\n")
    
    data = ''.join(sample_data)
    
    with lzma.open('data.xz', 'wt', encoding='utf-8', preset=9) as f:
        f.write(data)
    
    print(f"\nLZMA 파일 생성 완료: data.xz")

create_lzma_file()

# ZIP 파일 생성 및 처리
def create_zip_archive():
    """여러 파일을 포함하는 ZIP 아카이브 생성"""
    
    # 먼저 압축할 파일들 생성
    files_to_compress = {
        'documents/report.txt': '월간 보고서 내용입니다.\n프로젝트 진행 상황...',
        'documents/budget.csv': '항목,금액\n인건비,5000000\n장비구입,2000000',
        'images/readme.md': '# 이미지 설명\n이 폴더에는 이미지 파일이 저장됩니다.',
        'data/sample.json': '{"name": "테스트", "value": 123}'
    }
    
    # 실제 파일 생성
    for path, content in files_to_compress.items():
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    # ZIP 아카이브 생성
    with zipfile.ZipFile('archive.zip', 'w', compression=zipfile.ZIP_DEFLATED) as zipf:
        for path in files_to_compress.keys():
            zipf.write(path)
            print(f"  추가: {path}")
    
    # 임시 파일 정리
    for path in files_to_compress.keys():
        os.remove(path)
        os.removedirs(os.path.dirname(path))
    
    print(f"\nZIP 아카이브 생성 완료: archive.zip")

create_zip_archive()

# ZIP 파일 내용 읽기
def read_zip_archive():
    """ZIP 아카이브 내용 확인 및 추출"""
    
    with zipfile.ZipFile('archive.zip', 'r') as zipf:
        print("\nZIP 파일 정보:")
        print(f"  파일 목록:")
        for info in zipf.infolist():
            print(f"    - {info.filename} ({info.file_size}바이트)")
        
        # 특정 파일 내용 읽기
        with zipf.open('documents/report.txt') as f:
            content = f.read().decode('utf-8')
            print(f"\n  report.txt 내용:\n{content}")
        
        # 전체 아카이브 압축 해제
        zipf.extractall('extracted')
        print(f"\n  압축 해제 완료: extracted/ 폴더")

read_zip_archive()

# TAR 파일 처리 (유닉스 스타일 아카이브)
def create_tar_archive():
    """TAR 아카이브 생성 (압축 옵션 포함)"""
    
    # 샘플 데이터 파일 생성
    data_files = {
        'app.py': 'print("Hello World")\n# Python 코드',
        'config.yaml': 'app:\n  name: "MyApp"\n  version: 1.0',
        'README.md': '# My Application\n이것은 테스트 앱입니다.'
    }
    
    for filename, content in data_files.items():
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
    
    # TAR.GZ 생성 (gzip 압축)
    with tarfile.open('app.tar.gz', 'w:gz') as tar:
        for filename in data_files.keys():
            tar.add(filename)
            print(f"  추가: {filename}")
    
    # TAR.XZ 생성 (xz 압축)
    with tarfile.open('app.tar.xz', 'w:xz') as tar:
        for filename in data_files.keys():
            tar.add(filename)
    
    # 임시 파일 정리
    for filename in data_files.keys():
        os.remove(filename)
    
    print(f"\nTAR 아카이브 생성 완료:")
    print(f"  - app.tar.gz (gzip 압축)")
    print(f"  - app.tar.xz (xz 압축)")

create_tar_archive()

# 스트리밍 압축 처리 (대용량 데이터)
def stream_compress_large_data(input_file, output_gz):
    """대용량 파일을 스트리밍 방식으로 압축"""
    
    # 샘플 대용량 파일 생성
    with open(input_file, 'w', encoding='utf-8') as f:
        for i in range(10000):  # 10,000줄 생성
            f.write(f"데이터 레코드 {i:06d}: " + "x" * 100 + "\n")
    
    # 청크 단위로 읽고 압축
    chunk_size = 8192  # 8KB 청크
    
    with open(input_file, 'rb') as f_in:
        with gzip.open(output_gz, 'wb') as f_out:
            while True:
                chunk = f_in.read(chunk_size)
                if not chunk:
                    break
                f_out.write(chunk)
    
    original_size = os.path.getsize(input_file)
    compressed_size = os.path.getsize(output_gz)
    
    print(f"\n스트리밍 압축 결과:")
    print(f"  원본: {original_size:,}바이트")
    print(f"  압축: {compressed_size:,}바이트")
    print(f"  압축률: {(1 - compressed_size/original_size)*100:.1f}%")
    
    # 임시 파일 정리
    os.remove(input_file)

stream_compress_large_data('large_data.txt', 'large_data.txt.gz')

# 다양한 압축 포맷 자동 감지
def smart_decompress(filename):
    """파일 확장자에 따라 적절한 압축 해제"""
    
    decompressors = {
        '.gz': gzip.open,
        '.bz2': bz2.open,
        '.xz': lzma.open,
        '.zip': zipfile.ZipFile,
        '.tar.gz': tarfile.open,
        '.tar.xz': tarfile.open
    }
    
    for ext, opener in decompressors.items():
        if filename.endswith(ext):
            try:
                if ext in ['.zip', '.tar.gz', '.tar.xz']:
                    # 아카이브 파일 처리
                    with opener(filename, 'r') as archive:
                        if ext == '.zip':
                            archive.extractall('extracted_smart')
                            return 'zip 아카이브 해제 완료'
                        else:
                            archive.extractall('extracted_smart')
                            return 'tar 아카이브 해제 완료'
                else:
                    # 단일 압축 파일 처리
                    with opener(filename, 'rt', encoding='utf-8') as f:
                        content = f.read(500)  # 첫 500자만 읽기
                        return f"압축 해제된 내용 (일부):\n{content[:200]}..."
            except Exception as e:
                return f"오류 발생: {e}"
    
    return "압축 포맷을 인식할 수 없습니다."

# 다양한 압축 파일 테스트
print("\n압축 파일 자동 감지 테스트:")
test_files = ['example.txt.gz', 'archive.zip', 'app.tar.gz']
for test_file in test_files:
    if os.path.exists(test_file):
        result = smart_decompress(test_file)
        print(f"\n{test_file}: {result}")
```

## 고정 크기 레코드 순환

### 고정 길이 레코드 처리 패턴
```python
import struct

# 고정 길이 레코드 정의
class FixedLengthRecord:
    """고정 길이 레코드 처리 클래스"""
    
    FORMAT = '20s i f 10s'  # 형식: 이름(20바이트), 나이(4바이트), 점수(4바이트), 등급(10바이트)
    RECORD_SIZE = struct.calcsize(FORMAT)  # 레코드 크기 자동 계산
    
    def __init__(self, name, age, score, grade):
        self.name = name
        self.age = age
        self.score = score
        self.grade = grade
    
    def pack(self):
        """레코드를 바이너리로 패킹"""
        # 문자열을 고정 길이로 채우기
        name_bytes = self.name.encode('utf-8')[:20].ljust(20, b'\x00')
        grade_bytes = self.grade.encode('utf-8')[:10].ljust(10, b'\x00')
        
        return struct.pack(self.FORMAT, name_bytes, self.age, self.score, grade_bytes)
    
    @classmethod
    def unpack(cls, binary_data):
        """바이너리 데이터에서 레코드 언패킹"""
        name_bytes, age, score, grade_bytes = struct.unpack(cls.FORMAT, binary_data)
        
        # 널 바이트 제거 및 디코딩
        name = name_bytes.decode('utf-8').rstrip('\x00')
        grade = grade_bytes.decode('utf-8').rstrip('\x00')
        
        return cls(name, age, score, grade)
    
    def __str__(self):
        return f"{self.name:<20} {self.age:>4} {self.score:>6.1f} {self.grade:>10}"

# 고정 길이 레코드 파일 생성
def create_fixed_length_file(filename, num_records=10):
    """고정 길이 레코드 파일 생성"""
    
    records = []
    for i in range(num_records):
        name = f"사용자{i+1:03d}"
        age = 20 + (i % 30)
        score = 50.0 + (i * 3.5)
        grade = chr(65 + (i % 4))  # A, B, C, D
        
        record = FixedLengthRecord(name, age, score, grade)
        records.append(record)
    
    with open(filename, 'wb') as f:
        for record in records:
            f.write(record.pack())
    
    print(f"\n고정 길이 레코드 파일 생성 완료: {filename}")
    print(f"  레코드 크기: {FixedLengthRecord.RECORD_SIZE}바이트")
    print(f"  레코드 수: {num_records}")
    print(f"  파일 크기: {num_records * FixedLengthRecord.RECORD_SIZE}바이트")
    
    return records

# 레코드 파일 읽기
def read_fixed_length_file(filename):
    """고정 길이 레코드 파일 순차적으로 읽기"""
    
    print(f"\n{f' 파일: {filename} ':=^50}")
    print(f"{'이름':<20} {'나이':>4} {'점수':>6} {'등급':>10}")
    print("-" * 50)
    
    records = []
    with open(filename, 'rb') as f:
        record_count = 0
        
        while True:
            # 고정 크기만큼 읽기
            data = f.read(FixedLengthRecord.RECORD_SIZE)
            if not data:
                break
            
            # 레코드 언패킹
            record = FixedLengthRecord.unpack(data)
            records.append(record)
            
            print(record)
            record_count += 1
    
    print(f"\n총 {record_count}개의 레코드를 읽었습니다.")
    return records

# 파일 생성 및 읽기 테스트
records = create_fixed_length_file('fixed_records.dat', 15)
read_fixed_length_file('fixed_records.dat')

# 랜덤 액세스 (특정 레코드 직접 접근)
def random_access_record(filename, record_index):
    """특정 인덱스의 레코드 직접 읽기"""
    
    with open(filename, 'rb') as f:
        # 레코드 위치 계산
        position = record_index * FixedLengthRecord.RECORD_SIZE
        
        # 파일 끝 확인
        f.seek(0, 2)  # 파일 끝으로 이동
        file_size = f.tell()
        
        if position >= file_size:
            print(f"오류: 레코드 인덱스 {record_index}가 파일 범위를 벗어납니다.")
            return None
        
        # 해당 위치로 이동하여 레코드 읽기
        f.seek(position)
        data = f.read(FixedLengthRecord.RECORD_SIZE)
        
        if len(data) < FixedLengthRecord.RECORD_SIZE:
            print(f"오류: 불완전한 레코드 데이터")
            return None
        
        record = FixedLengthRecord.unpack(data)
        print(f"\n레코드 {record_index} 직접 접근:")
        print(f"  위치: {position}바이트")
        print(f"  내용: {record}")
        
        return record

# 특정 레코드 접근 테스트
random_access_record('fixed_records.dat', 5)
random_access_record('fixed_records.dat', 10)
random_access_record('fixed_records.dat', 20)  # 범위 밖

# 레코드 수정 및 업데이트
def update_record(filename, record_index, new_record):
    """특정 레코드 수정"""
    
    with open(filename, 'r+b') as f:  # 읽기/쓰기 모드, 바이너리
        position = record_index * FixedLengthRecord.RECORD_SIZE
        
        # 파일 범위 확인
        f.seek(0, 2)
        file_size = f.tell()
        
        if position >= file_size:
            print(f"오류: 수정할 레코드가 존재하지 않습니다.")
            return False
        
        # 레코드 수정
        f.seek(position)
        f.write(new_record.pack())
        
        print(f"\n레코드 {record_index} 수정 완료:")
        print(f"  변경 전: {FixedLengthRecord.unpack(f.read(FixedLengthRecord.RECORD_SIZE))}")
        
        return True

# 레코드 수정 테스트
new_record = FixedLengthRecord("수정된사용자", 99, 100.0, "A+")
update_record('fixed_records.dat', 7, new_record)

# 수정 결과 확인
print("\n수정 후 파일 내용:")
read_fixed_length_file('fixed_records.dat')

# 인덱스 파일을 이용한 빠른 검색
class IndexedFile:
    """인덱스를 사용한 고속 레코드 접근"""
    
    def __init__(self, data_filename, index_filename=None):
        self.data_filename = data_filename
        self.index_filename = index_filename or data_filename + '.idx'
        self.index = {}  # {키: 레코드_위치}
        
    def build_index(self, key_extractor):
        """레코드 키를 기준으로 인덱스 구축"""
        self.index.clear()
        
        with open(self.data_filename, 'rb') as f:
            record_number = 0
            
            while True:
                position = f.tell()
                data = f.read(FixedLengthRecord.RECORD_SIZE)
                
                if not data:
                    break
                
                # 레코드 파싱 및 키 추출
                record = FixedLengthRecord.unpack(data)
                key = key_extractor(record)
                
                # 인덱스에 저장
                self.index[key] = (record_number, position)
                record_number += 1
        
        # 인덱스 파일 저장
        with open(self.index_filename, 'w', encoding='utf-8') as f:
            for key, (rec_num, pos) in self.index.items():
                f.write(f"{key},{rec_num},{pos}\n")
        
        print(f"인덱스 구축 완료: {len(self.index)}개 항목")
    
    def load_index(self):
        """인덱스 파일 로드"""
        self.index.clear()
        
        if not os.path.exists(self.index_filename):
            return False
        
        with open(self.index_filename, 'r', encoding='utf-8') as f:
            for line in f:
                key, rec_num, pos = line.strip().split(',')
                self.index[key] = (int(rec_num), int(pos))
        
        return True
    
    def find_by_key(self, key):
        """키로 레코드 검색"""
        if not self.index:
            self.load_index()
        
        if key not in self.index:
            return None
        
        record_number, position = self.index[key]
        
        with open(self.data_filename, 'rb') as f:
            f.seek(position)
            data = f.read(FixedLengthRecord.RECORD_SIZE)
            
            if data:
                return FixedLengthRecord.unpack(data)
        
        return None

# 인덱스 파일 테스트
print("\n인덱스 파일 테스트:")
indexed_file = IndexedFile('fixed_records.dat')

# 이름을 키로 인덱스 구축
indexed_file.build_index(lambda record: record.name.strip())

# 빠른 검색
found = indexed_file.find_by_key("사용자005")
if found:
    print(f"검색 결과: {found}")

found = indexed_file.find_by_key("수정된사용자")
if found:
    print(f"검색 결과: {found}")
```

## 바이너리 데이터를 수정 가능한 버퍼에 넣기

### bytearray와 memoryview 활용
```python
# bytearray를 이용한 가변 바이너리 버퍼
def mutable_buffer_example():
    """bytearray를 사용한 가변 바이너리 데이터 처리"""
    
    # bytearray 생성 (초기 데이터 포함)
    buffer = bytearray(b'Initial binary data')
    print(f"초기 buffer: {buffer}")
    print(f"타입: {type(buffer)}, 길이: {len(buffer)}")
    
    # 내용 수정
    buffer[0] = 65  # 'A'의 ASCII 코드
    buffer[1] = 66  # 'B'의 ASCII 코드
    print(f"\n수정 후 buffer: {buffer}")
    print(f"문자열로: {buffer.decode('utf-8')}")
    
    # 데이터 추가
    buffer.extend(b' appended')
    print(f"\n추가 후 buffer: {buffer.decode('utf-8')}")
    
    # 슬라이싱을 통한 부분 수정
    buffer[8:13] = b'MODIFIED'
    print(f"슬라이스 수정 후: {buffer.decode('utf-8')}")
    
    # 검색 및 교체
    old = b'MODIFIED'
    new = b'CHANGED'
    
    index = buffer.find(old)
    if index != -1:
        buffer[index:index+len(old)] = new
    print(f"교체 후: {buffer.decode('utf-8')}")
    
    # memoryview를 통한 효율적인 접근
    print(f"\nmemoryview 사용:")
    mv = memoryview(buffer)
    
    # 슬라이싱은 새로운 memoryview 생성 (데이터 복사 없음)
    slice_view = mv[5:15]
    print(f"부분 뷰: {slice_view.tobytes().decode('utf-8')}")
    
    # memoryview를 통한 수정
    slice_view[0:3] = b'XYZ'
    print(f"뷰 수정 후 원본: {buffer.decode('utf-8')}")  # 원본도 변경됨
    
    return buffer

buffer = mutable_buffer_example()

# 구조화된 바이너리 데이터 수정
class BinaryDataEditor:
    """구조화된 바이너리 데이터 편집기"""
    
    def __init__(self, data_format):
        """
        data_format: 구조체 포맷 문자열
        예: 'I f 10s' -> (unsigned int, float, 10바이트 문자열)
        """
        self.format = data_format
        self.size = struct.calcsize(data_format)
        self.buffer = bytearray(self.size)
        self.view = memoryview(self.buffer)
    
    def pack(self, *values):
        """값들을 버퍼에 패킹"""
        struct.pack_into(self.format, self.buffer, 0, *values)
        return self
    
    def unpack(self):
        """버퍼에서 값 언패킹"""
        return struct.unpack_from(self.format, self.buffer, 0)
    
    def update_field(self, field_index, new_value):
        """특정 필드만 수정"""
        # 현재 값들 가져오기
        values = list(self.unpack())
        
        # 특정 필드 업데이트
        values[field_index] = new_value
        
        # 다시 패킹
        self.pack(*values)
    
    def get_slice(self, field_spec):
        """특정 필드의 memoryview 반환"""
        # 필드 위치 계산 (단순화된 버전)
        # 실제 구현에서는 format 문자열을 파싱해야 함
        if field_spec == 0:  # 첫 번째 필드
            return self.view[0:4]
        elif field_spec == 1:  # 두 번째 필드
            return self.view[4:8]
        # ... 나머지 필드들
    
    def __str__(self):
        values = self.unpack()
        return f"BinaryDataEditor[{self.format}]: {values}"

# BinaryDataEditor 사용 예제
print("\nBinaryDataEditor 예제:")

# 3개의 필드를 가진 데이터 편집기 생성
editor = BinaryDataEditor('I f 10s')  # unsigned int, float, 10바이트 문자열

# 초기 데이터 설정
editor.pack(100, 3.14, b'Hello')
print(f"초기 상태: {editor}")

# 특정 필드 수정
editor.update_field(0, 200)  # 첫 번째 필드 (정수) 수정
print(f"필드 0 수정 후: {editor}")

editor.update_field(1, 2.718)  # 두 번째 필드 (실수) 수정
print(f"필드 1 수정 후: {editor}")

# 문자열 필드는 바이트로 변환 필요
editor.update_field(2, b'Modified')
print(f"필드 2 수정 후: {editor}")

# 네트워크 패킷 처리 예제
class NetworkPacket:
    """네트워크 패킷 가변 버퍼 처리"""
    
    HEADER_FORMAT = '!HH'  # 타입(2바이트), 길이(2바이트), 빅 엔디안
    HEADER_SIZE = struct.calcsize(HEADER_FORMAT)
    
    def __init__(self, packet_type=0, payload=b''):
        self.packet_type = packet_type
        self.payload = bytearray(payload)
        self.update_header()
    
    def update_header(self):
        """헤더 정보 업데이트"""
        self.header = struct.pack(self.HEADER_FORMAT, 
                                 self.packet_type, 
                                 len(self.payload))
    
    def get_full_packet(self):
        """전체 패킷 바이트 반환"""
        self.update_header()
        return self.header + self.payload
    
    def append_payload(self, data):
        """페이로드에 데이터 추가"""
        self.payload.extend(data)
        return self
    
    def modify_payload(self, position, data):
        """페이로드 특정 위치 수정"""
        if position + len(data) <= len(self.payload):
            self.payload[position:position+len(data)] = data
        else:
            raise IndexError("페이로드 범위를 벗어남")
    
    def extract_string(self, start, length, encoding='utf-8'):
        """페이로드에서 문자열 추출"""
        if start + length <= len(self.payload):
            return self.payload[start:start+length].decode(encoding)
        return None
    
    def __len__(self):
        return self.HEADER_SIZE + len(self.payload)
    
    def __str__(self):
        return (f"NetworkPacket(type={self.packet_type}, "
                f"length={len(self.payload)}, "
                f"payload={self.payload[:20]}...)")

# 네트워크 패킷 사용 예제
print("\n네트워크 패킷 예제:")

packet = NetworkPacket(1, b'Initial payload')
print(f"패킷 생성: {packet}")
print(f"전체 패킷: {packet.get_full_packet()}")

# 페이로드 수정
packet.modify_payload(0, b'MODIFIED')
print(f"페이로드 수정 후: {packet}")

# 페이로드 추가
packet.append_payload(b' appended data')
print(f"페이로드 추가 후: {packet}")

# 문자열 추출
if len(packet.payload) >= 8:
    extracted = packet.extract_string(0, 8)
    print(f"추출된 문자열: {extracted}")

# 이미지 픽셀 데이터 처리
class ImageBuffer:
    """이미지 픽셀 데이터 버퍼"""
    
    def __init__(self, width, height, channels=3):
        self.width = width
        self.height = height
        self.channels = channels
        self.buffer = bytearray(width * height * channels)
        self.view = memoryview(self.buffer)
    
    def get_pixel_offset(self, x, y):
        """픽셀 위치 계산"""
        if 0 <= x < self.width and 0 <= y < self.height:
            return (y * self.width + x) * self.channels
        raise IndexError("이미지 범위를 벗어남")
    
    def set_pixel(self, x, y, color):
        """픽셀 색상 설정"""
        offset = self.get_pixel_offset(x, y)
        
        if len(color) != self.channels:
            raise ValueError(f"색상은 {self.channels}개 채널이어야 함")
        
        self.buffer[offset:offset+self.channels] = color
    
    def get_pixel(self, x, y):
        """픽셀 색상 가져오기"""
        offset = self.get_pixel_offset(x, y)
        return self.buffer[offset:offset+self.channels]
    
    def draw_rectangle(self, x1, y1, x2, y2, color):
        """사각형 그리기"""
        for y in range(max(0, y1), min(self.height, y2)):
            for x in range(max(0, x1), min(self.width, x2)):
                self.set_pixel(x, y, color)
    
    def save_to_file(self, filename):
        """바이너리 파일로 저장"""
        with open(filename, 'wb') as f:
            f.write(self.buffer)
    
    def __str__(self):
        return (f"ImageBuffer({self.width}x{self.height}, "
                f"채널:{self.channels}, "
                f"크기:{len(self.buffer)}바이트)")

# 이미지 버퍼 사용 예제
print("\n이미지 버퍼 예제:")

# 10x10 RGB 이미지 생성
image = ImageBuffer(10, 10, 3)
print(f"이미지 생성: {image}")

# 픽셀 설정 (빨간색)
red = bytes([255, 0, 0])
image.set_pixel(5, 5, red)

# 사각형 그리기 (파란색)
blue = bytes([0, 0, 255])
image.draw_rectangle(2, 2, 8, 8, blue)

# 특정 픽셀 값 확인
pixel = image.get_pixel(5, 5)
print(f"픽셀 (5,5): {list(pixel)}")

# 파일 저장
image.save_to_file('image_data.bin')

# 파일 크기 확인
print(f"저장된 파일 크기: {os.path.getsize('image_data.bin')}바이트")
```

## 바이너리 파일 메모리 매핑

### mmap을 이용한 고성능 파일 접근
```python
import mmap
import contextlib

# 기본 메모리 매핑
def basic_mmap_example():
    """메모리 매핑 기본 사용법"""
    
    # 테스트 파일 생성
    test_data = b'X' * 100 + b'\n' + b'Y' * 100 + b'\n' + b'Z' * 100
    with open('mmap_test.dat', 'wb') as f:
        f.write(test_data)
    
    # 메모리 매핑
    with open('mmap_test.dat', 'r+b') as f:
        # 파일 전체를 메모리에 매핑
        with mmap.mmap(f.fileno(), 0) as mm:
            print(f"매핑 크기: {len(mm)}바이트")
            print(f"초기 데이터 (처음 50바이트): {mm[:50]}")
            
            # 데이터 수정
            mm[0:10] = b'MODIFIED!!'
            
            # 검색
            pos = mm.find(b'Y' * 10)
            if pos != -1:
                print(f"'Y'*10 발견 위치: {pos}")
                mm[pos:pos+10] = b'CHANGED!!!'
            
            # 슬라이싱
            middle_slice = mm[50:150]
            print(f"중간 슬라이스 (50:150): {middle_slice[:30]}...")
            
            # 파일에 자동 반영됨
    
    # 수정된 파일 확인
    with open('mmap_test.dat', 'rb') as f:
        print(f"\n수정된 파일 내용 (처음 60바이트): {f.read(60)}")

basic_mmap_example()

# 대용량 파일 처리
def process_large_file_with_mmap():
    """메모리 매핑을 이용한 대용량 파일 처리"""
    
    # 대용량 테스트 파일 생성 (100MB)
    print("\n대용량 파일 처리 테스트 준비 중...")
    
    chunk_size = 1024 * 1024  # 1MB
    total_size = 100 * 1024 * 1024  # 100MB
    
    with open('large_file.bin', 'wb') as f:
        for i in range(total_size // chunk_size):
            # 패턴 데이터 생성
            pattern = f"CHUNK{i:04d}:".encode('utf-8')
            chunk = pattern + b'X' * (chunk_size - len(pattern))
            f.write(chunk)
    
    print(f"테스트 파일 생성 완료: {os.path.getsize('large_file.bin'):,}바이트")
    
    # 메모리 매핑으로 처리
    with open('large_file.bin', 'r+b') as f:
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_WRITE) as mm:
            print(f"\n메모리 매핑으로 파일 처리 시작...")
            
            # 특정 패턴 검색 및 수정
            target = b'CHUNK0050:'
            position = mm.find(target)
            
            if position != -1:
                print(f"패턴 발견 위치: {position:,}바이트")
                
                # 해당 영역 수정
                mm[position:position+len(target)] = b'MODIFIED50:'
                print(f"패턴 수정 완료")
            
            # 여러 위치 동시 수정
            modifications = [
                (b'CHUNK0100:', b'UPDATED100'),
                (b'CHUNK0200:', b'UPDATED200'),
            ]
            
            for old, new in modifications:
                pos = mm.find(old)
                if pos != -1:
                    mm[pos:pos+len(old)] = new
            
            print(f"다중 수정 완료")
    
    # 변경사항 확인
    with open('large_file.bin', 'rb') as f:
        f.seek(position)
        sample = f.read(20)
        print(f"수정된 데이터 샘플: {sample}")
    
    # 임시 파일 정리
    os.remove('large_file.bin')
    print("테스트 파일 정리 완료")

# 주석 처리 (실행 시 시간이 오래 걸릴 수 있음)
# process_large_file_with_mmap()

# 읽기 전용 메모리 매핑
def read_only_mmap_example():
    """읽기 전용 메모리 매핑"""
    
    # 읽기 전용 파일 생성
    data = b"읽기 전용 테스트 데이터\n" * 1000
    with open('readonly.dat', 'wb') as f:
        f.write(data)
    
    # 읽기 전용으로 매핑
    with open('readonly.dat', 'rb') as f:
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            print(f"\n읽기 전용 메모리 매핑")
            print(f"파일 크기: {len(mm):,}바이트")
            
            # 효율적인 검색
            count = mm.count(b'테스트')
            print(f"'테스트' 문자열 발견 횟수: {count}")
            
            # 효율적인 슬라이싱
            first_100 = mm[:100]
            last_100 = mm[-100:]
            
            print(f"처음 100바이트: {first_100[:30]}...")
            print(f"마지막 100바이트: ...{last_100[-30:]}")
            
            # 수정 시도 (오류 발생)
            try:
                mm[0] = 65  # 'A'
            except TypeError as e:
                print(f"수정 시도 오류 (예상됨): {e}")

read_only_mmap_example()

# 부분 메모리 매핑
def partial_mmap_example():
    """파일의 일부만 메모리 매핑"""
    
    # 대용량 파일 생성
    with open('partial_test.dat', 'wb') as f:
        for i in range(100):
            chunk = f"섹션{i:03d}:".encode('utf-8') + b'DATA' * 100 + b'\n'
            f.write(chunk)
    
    file_size = os.path.getsize('partial_test.dat')
    print(f"\n파일 크기: {file_size:,}바이트")
    
    # 특정 섹션만 매핑
    with open('partial_test.dat', 'r+b') as f:
        # 섹션 50의 위치 계산 (간단한 예제)
        section_size = len(f"섹션000:DATA" * 100 + b'\n')
        offset = 50 * section_size
        length = section_size
        
        if offset + length > file_size:
            length = file_size - offset
        
        print(f"\n섹션 50 매핑:")
        print(f"  오프셋: {offset:,}바이트")
        print(f"  길이: {length:,}바이트")
        
        # 부분 매핑
        with mmap.mmap(f.fileno(), length, offset=offset) as mm:
            print(f"  매핑된 데이터 크기: {len(mm):,}바이트")
            
            # 매핑된 부분에서 검색
            if mm.find(b'섹션050:') != -1:
                print(f"  섹션 50 발견!")
            
            # 수정
            if len(mm) > 10:
                mm[7:10] = b'999'  # 섹션 번호 수정
    
    # 수정된 부분 확인
    with open('partial_test.dat', 'rb') as f:
        f.seek(offset)
        section_data = f.read(50)
        print(f"  수정된 섹션 데이터: {section_data}")

partial_mmap_example()

# 공유 메모리 매핑 (프로세스 간 통신)
def shared_memory_example():
    """익명 메모리 매핑을 이용한 공유 메모리"""
    
    # 익명 메모리 매핑 생성
    size = 1024  # 1KB
    with mmap.mmap(-1, size, access=mmap.ACCESS_WRITE) as shared_mem:
        print(f"\n공유 메모리 생성: {len(shared_mem)}바이트")
        
        # 데이터 쓰기
        message = b"프로세스 간 통신 메시지"
        shared_mem[0:len(message)] = message
        
        print(f"쓴 메시지: {message}")
        
        # 다른 위치에 데이터 추가
        import json
        data = {'id': 1, 'name': '테스트', 'value': 123.45}
        json_str = json.dumps(data)
        json_bytes = json_str.encode('utf-8')
        
        offset = 100  # 오프셋 위치에 저장
        shared_mem[offset:offset+len(json_bytes)] = json_bytes
        
        # 데이터 읽기
        shared_mem.seek(0)
        read_message = shared_mem.read(len(message))
        print(f"읽은 메시지: {read_message}")
        
        shared_mem.seek(offset)
        read_json = shared_mem.read(len(json_bytes))
        decoded_data = json.loads(read_json.decode('utf-8'))
        print(f"읽은 JSON 데이터: {decoded_data}")

shared_memory_example()

# 데이터베이스 스타일 인덱스 파일 처리
class MappedIndexFile:
    """메모리 매핑을 사용한 인덱스 파일"""
    
    def __init__(self, filename, record_size=32):
        self.filename = filename
        self.record_size = record_size
        
        # 파일이 없으면 생성
        if not os.path.exists(filename):
            with open(filename, 'wb') as f:
                f.write(b'\x00' * record_size * 1000)  # 초기 공간
        
        self.file = open(filename, 'r+b')
        self.mmap = mmap.mmap(self.file.fileno(), 0)
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    def close(self):
        if self.mmap:
            self.mmap.close()
        if self.file:
            self.file.close()
    
    def get_record(self, index):
        """인덱스로 레코드 읽기"""
        offset = index * self.record_size
        if offset + self.record_size > len(self.mmap):
            return None
        
        self.mmap.seek(offset)
        return self.mmap.read(self.record_size)
    
    def set_record(self, index, data):
        """인덱스에 레코드 쓰기"""
        if len(data) > self.record_size:
            raise ValueError(f"데이터가 너무 큽니다 ({len(data)} > {self.record_size})")
        
        offset = index * self.record_size
        
        # 필요하면 파일 확장
        if offset + self.record_size > len(self.mmap):
            self._extend_file(offset + self.record_size)
        
        self.mmap.seek(offset)
        self.mmap.write(data.ljust(self.record_size, b'\x00'))
    
    def _extend_file(self, new_size):
        """파일 확장"""
        self.mmap.close()
        self.file.close()
        
        # 파일 크기 조정
        with open(self.filename, 'r+b') as f:
            f.seek(0, 2)  # 파일 끝
            current_size = f.tell()
            
            if new_size > current_size:
                f.write(b'\x00' * (new_size - current_size))
        
        # 다시 매핑
        self.file = open(self.filename, 'r+b')
        self.mmap = mmap.mmap(self.file.fileno(), 0)
    
    def find_record(self, pattern):
        """패턴으로 레코드 검색"""
        pos = self.mmap.find(pattern)
        if pos != -1:
            index = pos // self.record_size
            offset = index * self.record_size
            self.mmap.seek(offset)
            return index, self.mmap.read(self.record_size)
        return None

# MappedIndexFile 사용 예제
print("\nMappedIndexFile 예제:")

with MappedIndexFile('index_file.dat', record_size=64) as index_file:
    # 레코드 저장
    records = [
        b"RECORD001:사용자데이터1",
        b"RECORD002:사용자데이터2",
        b"RECORD003:사용자데이터3",
    ]
    
    for i, record in enumerate(records):
        index_file.set_record(i, record)
        print(f"레코드 {i} 저장: {record[:20]}...")
    
    # 레코드 읽기
    for i in range(3):
        record = index_file.get_record(i)
        if record:
            print(f"레코드 {i} 읽기: {record.rstrip(b'\x00').decode('utf-8')}")
    
    # 레코드 검색
    result = index_file.find_record(b"RECORD002")
    if result:
        index, record = result
        print(f"검색 결과 - 인덱스 {index}: {record.rstrip(b'\x00').decode('utf-8')}")
```

## 결론

Python의 고급 파일 처리 기능은 다양한 데이터 형식과 사용 사례를 효율적으로 다룰 수 있는 강력한 도구들을 제공합니다. 압축된 데이터 파일의 경우 `gzip`, `bz2`, `lzma`, `zipfile`, `tarfile` 모듈들을 상황에 맞게 선택하여 사용할 수 있으며, 각각은 특정한 압축률과 성능 특성을 가지고 있습니다.

고정 크기 레코드 처리는 데이터베이스나 로그 파일 같은 구조화된 데이터를 다룰 때 필수적인 기술입니다. `struct` 모듈을 활용하면 바이너리 데이터를 정확한 형식으로 패킹하고 언패킹할 수 있으며, 인덱스 파일을 함께 사용하면 대용량 데이터에서도 빠른 검색이 가능합니다.

바이너리 데이터 수정에는 `bytearray`와 `memoryview`가 핵심 도구입니다. `bytearray`는 가변적인 바이트 시퀀스를 제공하고, `memoryview`는 메모리 복사 없이 데이터의 뷰를 생성하여 대용량 데이터 처리의 성능을 크게 향상시킵니다. 특히 이미지 처리나 네트워크 패킷 조작과 같은 작업에서 유용합니다.

바이너리 파일 메모리 매핑(`mmap`)은 대용량 파일을 효율적으로 처리하는 가장 강력한 방법 중 하나입니다. 파일을 가상 메모리에 매핑함으로써 파일 I/O의 오버헤드를 줄이고, 마치 메모리 내 데이터처럼 접근할 수 있습니다. 이는 데이터베이스 인덱스, 대용량 로그 파일 분석, 공유 메모리 통신 등에 활용될 수 있습니다.

경로 다루기는 `os.path`와 `pathlib` 모듈을 통해 직관적이고 플랫폼 독립적인 방식으로 처리할 수 있습니다. `pathlib`의 객체 지향 접근 방식은 코드 가독성을 높이고 오류 가능성을 줄여줍니다.

이러한 고급 파일 처리 기술들을 효과적으로 조합하면 대용량 데이터 처리, 고성능 애플리케이션 개발, 시스템 프로그래밍 등 다양한 분야에서 Python의 능력을 최대한 활용할 수 있습니다. 각 도구의 특성과 적절한 사용 시나리오를 이해하는 것이 성공적인 데이터 처리 시스템 구축의 핵심입니다.