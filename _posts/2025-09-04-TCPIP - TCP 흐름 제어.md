---
layout: post
title: TCPIP - TCP 흐름 제어
date: 2025-09-04 19:25:23 +0900
category: TCPIP
---
# TCP 흐름 제어

## TCP 흐름 제어의 본질과 필요성

데이터 통신에서 송신자와 수신자는 본질적으로 다른 처리 속도를 가집니다. 송신자는 고속 네트워크 카드를 통해 데이터를 빠르게 밀어낼 수 있지만, 수신자의 애플리케이션은 데이터를 처리하거나 소켓 버퍼에서 읽는 속도가 이를 따라가지 못할 수 있습니다. 수신 측 운영체제는 이 도착하는 데이터를 임시로 보관하기 위해 **수신 버퍼**를 할당합니다. 흐름 제어가 없다면, 이 버퍼는 순식간에 오버플로우되어 도착한 패킷들이 강제로 폐기되게 됩니다.

```
흐름 제어 없는 상황:
송신자 (빠름)         수신자 (느림)
  ████████  →        [버퍼: ████] ← 애플리케이션 (느린 읽기)
  1 Gbps             ↓ 오버플로우!
                     ██ (폐기)
                     
결과: 패킷 손실 → 재전송 → 대역폭 낭비 → 악순환
```

흐름 제어는 **수신자의 버퍼 용량을 절대 초과하지 않는 선에서 송신률을 조절**하는, 신뢰성 있는 데이터 전송의 필수 안전장치입니다.

## 핵심 개념: 흐름 제어 vs 혼잡 제어

```
┌─────────────────┬──────────────────┬──────────────────┐
│                 │  흐름 제어       │  혼잡 제어       │
├─────────────────┼──────────────────┼──────────────────┤
│ 목적            │ 수신자 보호      │ 네트워크 보호    │
│ 제어 대상       │ 수신 버퍼        │ 네트워크 용량    │
│ 윈도우          │ rwnd             │ cwnd             │
│ 결정 주체       │ 수신자           │ 송신자           │
│ 신호            │ ACK의 WIN 필드   │ 패킷 손실/지연   │
│ 범위            │ 종단 간          │ 경로 전체        │
└─────────────────┴──────────────────┴──────────────────┘

실제 전송 윈도우:
  EffectiveWindow = min(rwnd, cwnd)
  
발신자                    네트워크                    수신자
  |                          |                          |
  |  [cwnd: 100KB]           |      [rwnd: 50KB]       |
  |   네트워크가 허용         |      수신자가 허용       |
  |                          |                          |
  |  실제 사용: min(100, 50) = 50KB                    |
  |--------------------------------------------------------|
```

## 핵심 기반: 슬라이딩 윈도우 프로토콜의 상세 해부

TCP 흐름 제어는 슬라이딩 윈도우 프로토콜 위에서 구현됩니다. 이는 세그먼트의 각 바이트에 고유 번호를 부여하는 **시퀀스 번호**와 **확인응답 번호** 시스템에 기반한 정교한 상태 머신입니다.

```
상세한 송신 버퍼 및 윈도우 상태 다이어그램

송신 버퍼 (바이트 단위, 시퀀스 번호 기준)
[1000........2000][2001........3000][3001........4000][4001........5000][5001........6000]
|-----------------|-----------------|-----------------|-----------------|-----------------|
<-- ACKed (완료)-><- Sent,UnACKed-><-- Sendable ---><--- Not Sendable (윈도우 밖) ---->
                  ↑                 ↑                 ↑
            윈도우 시작 (2001)   Next Seq (3001)  윈도우 끝 (4000)
            (LastByteAcked+1)                    (윈도우시작+rwnd)

수신 버퍼 (수신자 측)
[1000........2000][2001....][빈공간][........5000]
|-----------------|---------|-------|-------------|
<-- 읽음(App)--->.<-버퍼됨->.<rwnd>.<-윈도우밖->
                  ↑         ↑
            NextByteExpected  RcvBuffer 끝
            (ACK 번호로 송신)

키 포인트:
* 윈도우 시작: 마지막으로 ACK 받은 바이트 + 1
* 윈도우 끝: 윈도우 시작 + min(rwnd, cwnd)
* Sendable: 즉시 전송 가능 (윈도우 내, 미전송)
* Sent,UnACKed: 전송했으나 ACK 대기 중
```

**동적 업데이트 시나리오:**

```
시간 T0: 초기 상태
  송신자: LastByteAcked=2000, rwnd=2000
  윈도우: [2001 ────────── 4000]
  
시간 T1: 1000 바이트 전송
  송신: SEQ=2001, LEN=1000 (바이트 2001-3000)
  상태: Sent,UnACKed [2001-3000]
        Sendable [3001-4000]

시간 T2: ACK 수신
  수신: ACK=3001, WIN=3000
  해석: "3000까지 받았음, 3000바이트 공간 있음"
  
  업데이트:
    LastByteAcked = 3000
    rwnd = 3000
    새 윈도우: [3001 ────────── 6000]
    
  효과: 윈도우가 1000 슬라이드 + 1000 확장
  
          이전              새로운
  [2001──4000]      [3001────6000]
        →→→ (슬라이드)
              ←→→ (확장)
```

## TCP 세그먼트 헤더에서의 구현

```
TCP 헤더 구조 (흐름 제어 관련 필드)

 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|          Source Port          |       Destination Port        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                        Sequence Number                        |  ← 이 세그먼트의 첫 바이트 번호
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                    Acknowledgment Number                      |  ← 다음 기대 바이트 (누적 ACK)
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|  Data |           |U|A|P|R|S|F|                               |
| Offset| Reserved  |R|C|S|S|Y|I|            Window             |  ← rwnd (수신 가능 바이트 수)
|       |           |G|K|H|T|N|N|                               |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

핵심 필드:
1. Sequence Number (32비트): 송신 데이터의 시작 위치
2. Acknowledgment Number (32비트): 수신자가 다음에 받을 바이트
3. Window (16비트): 수신자의 가용 버퍼 공간 (rwnd)
4. ACK 플래그: 1이면 ACK Number와 Window 유효

제약사항:
  Window 필드 = 16비트 = 최대 65,535 바이트
  → 고속 네트워크에서 부족! (Window Scaling으로 해결)
```

## 역사적 발전: 고전부터 현대까지

### 1. 정지-대기 (Stop-and-Wait) - 1970년대

가장 초보적인 형태로, 하나씩 보내고 ACK를 기다립니다.

```
시간축:
송신자                          수신자
  |                               |
  |--- [SEQ=1, 데이터] ---------->|
  |                               |
  |                       (처리 중)
  |                               |
  |<-------- [ACK=2] -------------|
  |                               |
  |--- [SEQ=2, 데이터] ---------->|
  |                               |
  
링크 활용률 계산:
  U = (전송시간) / (전송시간 + RTT)
  
  예: 1KB 패킷, 1Gbps 링크, 100ms RTT
    전송시간 = 8,192 bits / 1Gbps = 0.008ms
    U = 0.008 / (0.008 + 100) ≈ 0.00008 = 0.008%
    
  결과: 1Gbps 링크를 80Kbps로만 사용! (비효율)
```

**장점:**
- 구현이 극도로 단순
- 버퍼 요구사항 최소 (1 패킷 분량)
- 순서 보장 자동

**단점:**
- 링크 활용률 극악
- 고지연 네트워크에서 실용성 없음
- 현대 네트워크에서는 사용 불가

### 2. Go-Back-N (GBN) - 1980년대

슬라이딩 윈도우를 도입하여 여러 패킷을 연속 전송합니다.

```
정상 동작:
송신자 (윈도우=4)               수신자
  |                               |
  |--[1]------------------------->|
  |--[2]------------------------->|
  |--[3]------------------------->|
  |--[4]------------------------->|
  |                               |
  |<--------[ACK=5]---------------|  (1-4 모두 수신)
  |                               |
  윈도우 슬라이드 → [5,6,7,8]

오류 발생:
  |--[1]------------------------->|
  |--[2]-------X (손실)           |
  |--[3]------------------------->| (폐기!)
  |--[4]------------------------->| (폐기!)
  |                               |
  |<--------[ACK=2]---------------|  (1만 확인)
  |<--------[ACK=2]---------------|  (중복)
  |<--------[ACK=2]---------------|  (중복)
  |                               |
  (타임아웃!)                     |
  |--[2]------------------------->|  ┐
  |--[3]------------------------->|  ├ 모두 재전송
  |--[4]------------------------->|  ┘

문제점:
  패킷 2 하나 손실 → 3,4도 재전송 (불필요)
  수신자는 순서 안맞으면 무조건 폐기
```

**장점:**
- 송신자가 여러 패킷 연속 전송 (높은 처리량)
- 수신자 버퍼 단순 (순서대로만 받음)

**단점:**
- 하나의 손실이 많은 재전송 유발
- 대역폭 낭비 심각
- 고속 네트워크에서 비효율적

### 3. Selective Repeat (SR) - 1980년대 후반

오류 복구를 정밀화한 설계입니다.

```
선택적 재전송:
송신자 (윈도우=4)               수신자 (버퍼링 가능)
  |                               |
  |--[1]------------------------->| [1] 버퍼
  |--[2]-------X (손실)           |
  |--[3]------------------------->| [3] 버퍼 (순서 안맞지만 보관)
  |--[4]------------------------->| [4] 버퍼
  |                               |
  |<--------[ACK=2]---------------|  "1 받음, 2 기대"
  |<--------[SACK 3-4]------------|  "3,4도 받음" (선택적)
  |                               |
  |--[2]------------------------->|  2만 재전송!
  |                               |
  |                       [1][2][3][4] 완성 → 애플리케이션
  |<--------[ACK=5]---------------|
  
수신 버퍼 상태 (순서 안맞아도 저장):
  [1][_][3][4] → 2 도착 → [1][2][3][4] → 전달
  
효율성:
  GBN: 손실 1개 → 재전송 3개 (2,3,4)
  SR:  손실 1개 → 재전송 1개 (2만)
```

**장점:**
- 손실된 패킷만 선택적 재전송
- 대역폭 효율 극대화
- 현대 TCP의 기반

**단점:**
- 수신자 버퍼 복잡 (순서 안맞는 패킷 관리)
- 구현 복잡도 증가

### 4. TCP의 하이브리드 접근 (1980년대-현재)

TCP는 GBN의 누적 ACK와 SR의 선택적 재전송을 결합합니다.

```
기본 동작 (누적 ACK):
  |--[100-199]-------------------->|
  |--[200-299]-------X             |
  |--[300-399]-------------------->|
  |                                |
  |<--------[ACK=200]--------------|  "199까지 받음"
  |<--------[ACK=200]--------------|  중복 (300 받음)
  |<--------[ACK=200]--------------|  중복 (300은 버퍼링)
  |<--------[ACK=200]--------------|  3번째 중복!
  |                                |
  |--[200-299]-------------------->|  빠른 재전송!
  |<--------[ACK=400]--------------|  "모두 받음"

SACK 옵션 사용 시 (RFC 2018):
  |<--[ACK=200, SACK 300-400]------|  명시적 정보
  |                                |
  송신자: "아하, 200-299만 없구나"
  |--[200-299]-------------------->|  정확히 재전송
```

## 현대 TCP의 고급 메커니즘

### 1. 윈도우 스케일링 (Window Scaling, RFC 1323)

16비트 윈도우의 한계를 극복합니다.

```
문제: 고속 장거리 네트워크

BDP = Bandwidth × RTT
예: 1 Gbps × 100ms = 12.5 MB

기존 TCP:
  최대 윈도우 = 65,535 바이트 = 64 KB
  최대 처리량 = 64 KB / 0.1s = 640 KB/s = 5.12 Mbps
  
  1 Gbps 링크에서 0.5%만 사용! (대참사)

해결: Window Scaling

┌─────────────────────────────────────────────┐
│ SYN 세그먼트에서 협상:                       │
│   Option: WS=7 (Scale Factor)               │
│                                             │
│ 실제 윈도우 크기:                            │
│   RealWindow = TCP_Header_Window << WS      │
│              = 65,535 << 7                  │
│              = 8,388,480 바이트 = 8 MB      │
└─────────────────────────────────────────────┘

예시:
  헤더 Window 필드: 10000 (값)
  Scale Factor: 7
  실제 윈도우: 10000 × 2^7 = 1,280,000 바이트
  
최대값:
  Scale Factor 최대: 14
  최대 윈도우: 65,535 × 2^14 = 1,073,725,440 (1GB)

제약사항:
  - 연결 설정(SYN) 시에만 협상 가능
  - 양측 모두 지원해야 함
  - 중간 방화벽이 제거할 수 있음
```

### 2. 타임스탬프 옵션 (TCP Timestamps, RFC 1323)

정확한 RTT 측정과 PAWS(Protection Against Wrapped Sequences)를 제공합니다.

```
TCP 옵션 필드:
  Kind=8, Length=10
  TSval (Timestamp Value): 4바이트
  TSecr (Timestamp Echo Reply): 4바이트

동작:
  송신자 → 수신자: SEQ=X, TSval=100
  수신자 → 송신자: ACK=X, TSecr=100
  
  RTT = 현재시간 - TSval (받은 TSecr 기준)

장점:
1. 재전송 모호성 해결:
   원본[SEQ=X, TS=100] → 손실
   재전송[SEQ=X, TS=200] → 도착
   ACK[TSecr=200] → RTT 정확 측정
   
2. PAWS (시퀀스 번호 랩어라운드 보호):
   32비트 시퀀스가 순환해도 타임스탬프로 구별
   
   고속 네트워크에서:
     1 Gbps: 34초마다 시퀀스 순환
     10 Gbps: 3.4초마다 순환!
     
   타임스탬프로 오래된 패킷 거부

예시:
  정상: SEQ=100, TS=5000 → 수락
  지연: SEQ=100, TS=1000 → 거부 (오래됨)
```

### 3. 지연 ACK (Delayed ACK, RFC 1122)

ACK 전송을 최적화하여 오버헤드를 줄입니다.

```
즉시 ACK (비효율):
  수신: [데이터 1] → [ACK 1] (40바이트)
  수신: [데이터 2] → [ACK 2] (40바이트)
  수신: [데이터 3] → [ACK 3] (40바이트)
  
  문제: 작은 데이터에 큰 헤더

지연 ACK:
  수신: [데이터 1] → (대기)
  수신: [데이터 2] → [ACK 2] (1-2 확인)
  수신: [데이터 3] → (대기)
  
  또는 타이머 만료 (보통 200ms):
  수신: [데이터 1] → (200ms 대기) → [ACK 1]

규칙 (RFC 1122):
  - 최대 2개 세그먼트까지 지연 가능
  - 최대 지연 시간: 500ms (보통 40-200ms)
  - Out-of-order 패킷: 즉시 ACK (빠른 재전송 유도)
  - Push 플래그: 즉시 ACK

상호작용 예시:
  송신: [1][2][3][4]
  수신: [1] 도착 (대기)
        [2] 도착 → [ACK 3] 전송
        [3] 도착 (대기)
        [4] 도착 → [ACK 5] 전송
        
  ACK 수 감소: 4 → 2 (50% 절감)

부작용:
  Nagle's Algorithm과 상호작용 시 지연 증가
  (TCP_NODELAY로 해결)
```

### 4. 선택적 확인응답 (SACK, RFC 2018)

여러 패킷 손실 시 효율적인 복구를 제공합니다.

```
TCP 옵션:
  Kind=5, Length=가변
  Left Edge: 블록 시작 시퀀스
  Right Edge: 블록 끝 시퀀스 + 1

예시 시나리오:
  전송: [100][200][300][400][500][600][700]
          ✓    ✗    ✗    ✓    ✓    ✗    ✓
  
  수신자 버퍼:
    [100][___][___][400][500][___][700]
  
  SACK 정보:
    ACK=200 (100까지 연속 수신)
    SACK: {400-600, 700-800}
    
  송신자 해석:
    손실: 200-300, 300-400, 600-700
    → 이 3개만 재전송

Without SACK (Reno):
  [200] 재전송 → ACK → [300] 재전송 → ACK → ...
  여러 RTT 필요

With SACK:
  [200][300][600] 동시 재전송
  1 RTT로 복구 완료!

SACK 블록 형식:
  ┌────────────┬────────────┐
  │ Left Edge  │ Right Edge │ 블록 1
  ├────────────┼────────────┤
  │ Left Edge  │ Right Edge │ 블록 2
  ├────────────┼────────────┤
  │ Left Edge  │ Right Edge │ 블록 3
  └────────────┴────────────┘
  
  최대 3-4개 블록 (옵션 크기 제한)
```

### 5. D-SACK (Duplicate SACK, RFC 2883)

불필요한 재전송을 감지하고 학습합니다.

```
시나리오: 지연된 ACK로 인한 재전송

  송신: [100-200]
  수신: [100-200] → ACK 지연 중
  송신: (타임아웃) [100-200] 재전송
  수신: [100-200] 중복 수신!
  
  D-SACK 응답:
    ACK=200
    SACK: {100-200}  ← 이미 ACK된 범위!
    
  송신자: "아, 불필요한 재전송이었구나"
  → RTO 조정, 통계 업데이트

활용:
  1. 네트워크 재정렬 감지
  2. ACK 손실 vs 데이터 손실 구분
  3. RTO 알고리즘 개선
  4. 혼잡 제어 파라미터 튜닝

예시:
  ┌────────────┬──────────────────────┐
  │ 패킷 전송  │  수신자 상태         │
  ├────────────┼──────────────────────┤
  │ [1-100]    │ 수신, ACK 지연       │
  │ [101-200]  │ 수신, 버퍼링         │
  │ (RTO)      │                      │
  │ [1-100]재전│ 중복! D-SACK 전송   │
  │            │ ACK=200,SACK{1-100}  │
  └────────────┴──────────────────────┘
```

### 6. Karn's Algorithm - RTT 측정 문제 해결

재전송된 패킷에 대한 RTT 측정 모호성을 해결합니다.

```
문제: 재전송 모호성

  원본 패킷[SEQ=100] 전송 (T=0)
  → 손실
  재전송[SEQ=100] (T=3s)
  → 도착
  ACK[100] 수신 (T=4s)
  
  질문: 어느 전송에 대한 ACK?
  
  경우 1: 원본에 대한 ACK
    RTT = 4 - 0 = 4초 (과대 평가)
    
  경우 2: 재전송에 대한 ACK
    RTT = 4 - 3 = 1초 (정확할 수도)
    
  잘못된 RTT → 잘못된 RTO → 추가 재전송/지연

Karn's Algorithm 해결책:

  규칙 1: 재전송된 세그먼트는 RTT 측정에 사용 안함
  규칙 2: 재전송 발생 시 RTO를 2배로 증가 (백오프)
  
  의사코드:
    if (재전송 아님):
        RTT = 측정값
        SRTT, RTO 업데이트
    else:
        RTT 측정 건너뛰기
        RTO = RTO × 2

예시:
  초기 RTO = 1s
  
  [100] 전송 (T=0)
  (타임아웃 1s)
  [100] 재전송 (T=1), RTO=2s
  (타임아웃 2s)
  [100] 재전송 (T=3), RTO=4s
  ACK 수신 (T=3.5)
  
  → RTT 측정 안함
  → 다음 새 세그먼트로 RTT 측정
```

### 7. Jacobson's Algorithm - 정교한 RTO 계산

RTT의 변동성을 고려한 정확한 RTO 산정입니다.

```
기존 방식 (부정확):
  RTO = 2 × RTT
  
  문제: RTT 변동 무시

Jacobson's Algorithm (RFC 6298):

  변수:
    SRTT: Smoothed RTT
    RTTVAR: RTT Variation
    RTO: Retransmission Timeout
    
  초기화 (첫 측정 R):
    SRTT = R
    RTTVAR = R / 2
    RTO = SRTT + 4 × RTTVAR
    
  이후 측정 R:
    α = 1/8, β = 1/4
    
    RTTVAR = (1-β) × RTTVAR + β × |SRTT - R|
    SRTT = (1-α) × SRTT + α × R
    RTO = SRTT + 4 × RTTVAR
    
  경계:
    RTO_min = 1초 (RFC 권고)
    RTO_max = 60초

예시 계산:
  측정값: 100ms, 110ms, 90ms, 120ms
  
  R=100ms (첫 측정):
    SRTT = 100ms
    RTTVAR = 50ms
    RTO = 100 + 4×50 = 300ms
    
  R=110ms:
    RTTVAR = 0.75×50 + 0.25×|100-110| = 40ms
    SRTT = 0.875×100 + 0.125×110 = 101.25ms
    RTO = 101.25 + 4×40 = 261.25ms
    
  안정적인 RTT → RTO 감소
  변동 큰 RTT → RTO 증가

지수 백오프 (재전송 시):
  RTO_new = RTO_old × 2
  
  1차 재전송: RTO = 1s
  2차 재전송: RTO = 2s
  3차 재전송: RTO = 4s
  4차 재전송: RTO = 8s
  ...
  최대: 60s
```

## 현대적 고급 메커니�즘

### 8. TCP Fast Open (TFO, RFC 7413)

3-way 핸드셰이크 중에 데이터를 전송하여 지연을 줄입니다.

```
기존 TCP (1.5 RTT):
  클라이언트                서버
     |                        |
     |------ SYN ------------>|
     |                        |
     |<--- SYN-ACK -----------|
     |                        |
     |------ ACK ------------>|
     |                        |
     |------ Data ----------->|  ← 여기서야 데이터!
     
  최소 1.5 RTT 후 데이터 전송

TFO 첫 연결 (쿠키 획득):
     |                        |
     |-- SYN + TFO Cookie 요청->|
     |                        |
     |<- SYN-ACK + TFO Cookie -|
     |                        |
     |------ ACK ------------>|

TFO 이후 연결 (0-RTT):
     |                        |
     |-- SYN + Cookie + Data ->|  ← 즉시 데이터!
     |                        |
     |    (서버가 바로 처리)   |
     |<- SYN-ACK + Data -------|
     |                        |
     |------ ACK ------------>|

시간 절약:
  기존: 1.5 RTT
  TFO: 0 RTT (데이터 관점)
  
  100ms RTT: 150ms → 0ms 절약
  → 웹 페이지 로딩 속도 향상

보안 고려사항:
  - 쿠키로 클라이언트 인증
  - Replay attack 방지 필요
  - Idempotent 요청에만 사용 (GET)
```

### 9. Tail Loss Probe (TLP, RFC 8985)

연결 종료 시 손실된 마지막 패킷을 빠르게 복구합니다.

```
문제: 마지막 패킷 손실

  송신: [1][2][3][4][5]
          ✓ ✓ ✓ ✓ ✗
  
  수신: ACK[5] (1-4 확인)
        → 더 이상 패킷 없음
        → 중복 ACK 생성 안됨
        → 빠른 재전송 불가
        → RTO 대기 (1초+)
        
  결과: 긴 지연

TLP 해결:
  송신: [1][2][3][4][5]
          ✓ ✓ ✓ ✓ ✗
          
  타이머 (2 × SRTT):
    → 새 ACK 없음 감지
    → [5] 프로브 재전송
    
  수신: [5] 도착
        → ACK[6]
        
  지연: 2 × SRTT (예: 200ms)
        vs 기존 RTO (1초+)

의사코드:
  if (미확인 데이터 존재 && 새 ACK 없음):
      TLP_timer = 2 × SRTT
      TLP_timer 만료 시:
          마지막 세그먼트 재전송
          또는 새 데이터 전송 (probe)

효과:
  - 짧은 연결 성능 향상
  - HTTP 요청/응답 속도 개선
  - RTO 대기 회피
```

### 10. RACK (Recent ACKnowledgment, RFC 8985)

시간 기반 손실 감지로 더 빠르고 정확한 재전송을 제공합니다.

```
기존 방식 (중복 ACK):
  - 3개 중복 ACK 대기
  - 재정렬된 패킷과 구분 어려움

RACK (시간 기반):
  핵심: 전송 시간을 추적
  
  ┌────────┬─────────┬─────────────┐
  │ 패킷   │ 전송시간│ 상태        │
  ├────────┼─────────┼─────────────┤
  │ [100]  │ T=0     │ ACKed T=0.1 │
  │ [200]  │ T=0.01  │ 손실        │
  │ [300]  │ T=0.02  │ ACKed T=0.12│
  └────────┴─────────┴─────────────┘
  
  분석:
    [300] ACK 도착 (T=0.12)
    [300] 전송 시간: T=0.02
    [200] 전송 시간: T=0.01
    
    [300]이 [200]보다 나중 전송인데 먼저 ACK
    → RTT + reordering_window 후에도
      [200] ACK 없음
    → [200] 손실로 판단
    
  재전송 조건:
    if (후발 패킷 ACK 도착 &&
        선발 패킷 ACK 없음 &&
        경과 시간 > RTT + reorder_window):
        선발 패킷 재전송

장점:
  1. 재정렬 허용 (reorder_window)
  2. 중복 ACK 불필요
  3. 더 빠른 손실 감지
  4. SACK과 시너지

예시 타임라인:
  T=0:    [1] 전송
  T=0.01: [2] 전송
  T=0.02: [3] 전송
  T=0.10: [1] ACK
  T=0.12: [3] ACK  ← [2]보다 늦게 전송, 먼저 ACK
  
  T=0.12 + RTT + reorder_window:
    [2] 여전히 ACK 없음
    → [2] 재전송
```

### 11. Pacing - 버스트 전송 완화

패킷을 균등하게 분산시켜 버퍼 오버플로우를 방지합니다.

```
문제: 버스트 전송

  윈도우 업데이트 시:
  송신: [1][2][3][4][5][6][7][8] ← 한번에!
        ████████████████████████
        
  라우터 큐:
    [████████] 순간적 포화
    → 패킷 손실
    → 재전송
    
Pacing 적용:

  송신: [1] [2] [3] [4] [5] [6] [7] [8]
        █   █   █   █   █   █   █   █
        
  시간 간격 = RTT / cwnd
  
  예: RTT=100ms, cwnd=10 패킷
    간격 = 100ms / 10 = 10ms
    
  타임라인:
    T=0:   [1]
    T=10:  [2]
    T=20:  [3]
    ...

구현 (Linux):
  송신 큐에서 pacing_rate 제어
  
  pacing_rate = cwnd × MSS / RTT
  
  예:
    cwnd = 100 패킷
    MSS = 1460 바이트
    RTT = 100ms
    
    rate = 100 × 1460 / 0.1
         = 1.46 MB/s
         = 11.68 Mbps

장점:
  - 큐 빌드업 감소
  - 손실률 감소
  - 더 안정적인 지연
  - BBR 등 현대 알고리즘의 핵심

TSQ (TCP Small Queues):
  송신 큐 크기 제한으로 pacing 보조
  
  Max queue = 1ms worth of data
  → 과도한 버퍼링 방지
```

## Zero Window와 Persist Timer

수신자 버퍼가 가득 찬 상황의 처리입니다.

```
시나리오: 수신자 버퍼 소진

  수신자:
    버퍼: [████████] (가득)
    애플리케이션: (느린 읽기)
    
  송신자에게:
    ACK=1000, WIN=0  ← "더 이상 못 받음"

송신자 대응:

  1. 전송 중지
  2. Persist Timer 시작
  
  Persist Timer 동작:
    초기: 5초
    만료 시: Window Probe 전송
      → 1바이트 데이터
      → 수신자 윈도우 업데이트 유도
      
  백오프:
    5초 → 10초 → 20초 → ... → 최대 60초
    
타임라인:

  T=0:  수신 ACK[WIN=0]
        Persist Timer = 5s
        
  T=5:  Probe[1 byte] 전송
        
  수신자:
    ACK[WIN=0]  ← 여전히 가득
    
  T=10: Probe 재전송
        Persist Timer = 10s
        
  T=20: Probe 재전송
        
  수신자:
    (애플리케이션이 드디어 읽음)
    ACK[WIN=8000]  ← 공간 생김!
    
  송신자:
    Persist Timer 취소
    정상 전송 재개

왜 필요한가?

  문제 시나리오:
    1. 수신자: ACK[WIN=0]
    2. (애플리케이션이 읽음)
    3. 수신자: ACK[WIN=8000] 전송
    4. 이 ACK 손실!
    5. 송신자: 계속 대기 (데드락)
    
  Persist Probe:
    주기적 확인으로 데드락 방지

구현 상세:
  
  초기값: 5-6초
  
  백오프 공식:
    next = current × 2
    max = 60초
    
  프로브 내용:
    - 1바이트 데이터
    - 또는 이전 보낸 마지막 바이트 재전송
    - 목적: ACK (+ 윈도우 업데이트) 유도
```

## Silly Window Syndrome (SWS) 상세

작은 윈도우의 비효율성과 해결책입니다.

```
문제 발생:

  수신자:
    애플리케이션이 1바이트씩 읽음
    → 1바이트 공간 생김
    → ACK[WIN=1] 광고
    
  송신자:
    1바이트 데이터 전송
    TCP 헤더: 40바이트
    IP 헤더: 20바이트
    데이터: 1바이트
    
    오버헤드: 60 / 1 = 6000%!

타임라인:
  
  T=0: 수신 버퍼 [████████] 가득
  
  T=1: App 1바이트 읽음
       → ACK[WIN=1]
       
  T=2: 송신 [61바이트 패킷] (데이터 1)
  
  T=3: App 1바이트 읽음
       → ACK[WIN=1]
       
  T=4: 송신 [61바이트 패킷] (데이터 1)
  
  → 무한 반복

해결책 1: 수신자 측 (Clark's Solution)

  수신자가 윈도우를 광고하는 조건:
  
  if (빈공간 >= max(MSS, 버퍼/2)):
      새 윈도우 광고
  else:
      WIN=0 유지
      
  예:
    버퍼: 8KB
    MSS: 1460바이트
    
    1바이트 읽음:
      빈공간 = 1 < max(1460, 4096)
      → WIN=0 유지
      
    4KB 읽음:
      빈공간 = 4096 >= max(1460, 4096)
      → WIN=4096 광고

해결책 2: 송신자 측 (Nagle's Algorithm)

  규칙:
    if (미확인_데이터 > 0 && 
        전송_데이터 < MSS):
        대기 (coalesce)
    else:
        전송
        
  예외:
    - 첫 세그먼트: 즉시 전송
    - Push 플래그: 즉시 전송
    - 긴급 데이터: 즉시 전송

  동작 예시:
    App이 100바이트씩 10번 write:
    
    Without Nagle:
      [100][100][100]...[100] → 10 패킷
      
    With Nagle:
      [100] 즉시
      (나머지 대기)
      ACK 도착
      [1000] 한번에 → 2 패킷
      
    절감: 10 → 2 패킷

상호작용 문제:

  Nagle + 지연 ACK:
  
  T=0:   Client write(100)
         → [100] 전송
         
  T=1:   Server 수신
         → 지연 ACK (200ms 대기)
         
  T=2:   Client write(100)
         → Nagle 대기 (ACK 기다림)
         
  T=201: Server ACK 전송
         
  T=202: Client [100] 전송
         
  → 200ms 지연!

해결:

  1. TCP_NODELAY:
     Nagle 비활성화
     → 실시간 앱에 사용
     
  2. TCP_CORK (Linux):
     명시적으로 coalescing 제어
     
  3. TCP_QUICKACK:
     지연 ACK 일시 비활성화

선택 가이드:

  ┌──────────────┬───────────┬──────────┐
  │ 애플리케이션 │ Nagle    │ 지연 ACK │
  ├──────────────┼───────────┼──────────┤
  │ 대량 전송    │ ON       │ ON       │
  │ 실시간 게임  │ OFF      │ OFF      │
  │ SSH/Telnet   │ OFF      │ OFF      │
  │ HTTP         │ ON       │ ON       │
  │ RPC          │ OFF      │ ON       │
  └──────────────┴───────────┴──────────┘
```

## 버퍼 관리와 튜닝

### 동적 수신 버퍼 조정 (Linux Auto-tuning)

```
전통적 방식:
  고정 버퍼 크기 (예: 64KB)
  → 작은 RTT: 낭비
  → 큰 RTT: 부족

Linux Auto-tuning (2.4+):

  목표: BDP(Bandwidth-Delay Product) 채우기
  
  BDP = Bandwidth × RTT
  
  동적 조정:
    1. RTT 측정
    2. 처리량 측정
    3. 버퍼 크기 = BDP × 2
    
  예:
    100 Mbps, 100ms RTT
    BDP = 100 Mbps × 0.1s = 1.25 MB
    버퍼 = 2.5 MB

설정 (sysctl):

  # 최소, 기본, 최대 (바이트)
  net.ipv4.tcp_rmem = 4096 131072 6291456
  net.ipv4.tcp_wmem = 4096  16384 4194304
  
  # Auto-tuning 활성화
  net.ipv4.tcp_moderate_rcvbuf = 1

동작:
  
  연결 시작:
    버퍼 = 기본값 (131KB)
    
  데이터 전송:
    if (버퍼 > 80% 사용):
        버퍼 × 2 (최대까지)
        WIN 업데이트
        
  유휴 상태:
    if (장시간 미사용):
        버퍼 축소

효과:
  
  Short-lived 연결:
    작은 버퍼 → 메모리 절약
    
  Long-lived 연결:
    큰 버퍼 → 높은 처리량
    
  자동 적응:
    네트워크 조건에 맞춤

주의사항:
  
  1. 시스템 메모리 고려:
     max × 동시 연결 수 < 총 메모리
     
  2. 공정성:
     큰 버퍼 = 많은 대역폭
     → 작은 흐름 불리
     
  3. 버퍼블로트:
     과도한 버퍼 → 높은 지연
```

### Send Buffer와 Application Blocking

```
송신 버퍼 역할:

  애플리케이션         커널              네트워크
       |                |                   |
   write(data)          |                   |
       |                |                   |
       v                |                   |
  [송신 버퍼]           |                   |
       |                |                   |
       |← cwnd/rwnd 제한|                   |
       |                |                   |
       +--------------->|------------------>|

버퍼 가득 시:

  시나리오:
    송신 버퍼: 256KB
    네트워크: 느림 (cwnd=10KB)
    
  App write(100KB):
    → 버퍼에 복사 (256KB 중 100KB 사용)
    → 즉시 반환 (non-blocking)
    
  App write(200KB):
    → 156KB 남음
    → 200KB 요청
    → 블로킹! (충분한 공간 대기)

블로킹 조건:

  if (가용_버퍼 < write_크기):
      if (소켓_블로킹_모드):
          sleep(버퍼 공간 생길 때까지)
      else:
          EAGAIN 반환

해제 조건:

  ACK 도착 → 버퍼 공간 확보
  → 블로킹된 write() 깨어남

Non-blocking I/O:

  설정:
    fcntl(sock, F_SETFL, O_NONBLOCK)
    
  동작:
    write() 즉시 반환
    - 성공: 쓴 바이트 수
    - 실패: -1, errno=EAGAIN
    
  폴링:
    select()/poll()/epoll()
    POLLOUT 이벤트 대기

최적화:

  1. SO_SNDBUF 크기 조정:
     setsockopt(sock, SOL_SOCKET, SO_SNDBUF, ...)
     
  2. 큰 전송:
     여러 작은 write()보다
     하나의 큰 write()가 효율적
     
  3. Scatter-gather I/O:
     writev()로 복사 최소화
```

## 특수 환경의 고려사항

### 고속 장거리 네트워크 (Long Fat Networks)

```
정의:
  BDP > 100KB인 네트워크
  
  예:
    1 Gbps × 100ms = 12.5 MB
    10 Gbps × 50ms = 62.5 MB

문제:

  1. 윈도우 스케일링 필수:
     65KB로는 턱없이 부족
     
  2. 패킷 손실 영향 증폭:
     하나 손실 → 긴 복구 시간
     
  3. 버퍼 요구량 증가:
     GB 단위 버퍼 필요

최적화:

  1. 윈도우 스케일링:
     WS=8 이상 (1MB+)
     
  2. SACK 필수:
     여러 손실 효율 복구
     
  3. 큰 초기 윈도우:
     IW10 (10 MSS)
     
  4. 적극적 혼잡 제어:
     CUBIC, BBR

예시 설정:

  # 큰 버퍼
  net.ipv4.tcp_rmem = 4096 87380 16777216
  net.ipv4.tcp_wmem = 4096 65536 16777216
  
  # 메모리 증가
  net.core.rmem_max = 16777216
  net.core.wmem_max = 16777216
  
  # SACK
  net.ipv4.tcp_sack = 1

BDP 계산:

  Required Buffer = BDP × 2
  
  1 Gbps, 100ms:
    BDP = 12.5 MB
    Buffer = 25 MB
    
  10 Gbps, 100ms:
    BDP = 125 MB
    Buffer = 250 MB
```

### 데이터센터 환경

```
특성:
  - 짧은 RTT (< 1ms)
  - 높은 대역폭 (10-100 Gbps)
  - 낮은 버퍼 (스위치)
  - Incast 트래픽

Incast 문제:

  시나리오:
    클라이언트 → 100개 서버 요청
    → 100개 동시 응답
    
  [S1][S2]...[S100]
     ↓  ↓      ↓
     [스위치] ← 버퍼 32KB
        ↓
    [클라이언트]
    
  결과:
    버퍼 오버플로우
    → 다수 패킷 손실
    → 타임아웃 (200ms+)
    → 높은 지연

해결책:

  1. DCTCP (Data Center TCP):
     ECN 기반 혼잡 제어
     작은 큐 유지
     
  2. 짧은 RTO:
     RTO_min = 1ms
     (기본 200ms에서 감소)
     
  3. 작은 초기 윈도우:
     IW=2 (과도한 버스트 방지)
     
  4. 우선순위 큐:
     중요 트래픽 우선 처리

DCTCP 동작:

  스위치:
    큐 > K → ECN 마킹
    (K = 버퍼의 20%)
    
  수신자:
    ECN 받음 → ECE 플래그
    
  송신자:
    ECE 비율(α) 계산
    cwnd = cwnd × (1 - α/2)
    
  효과:
    작은 큐 유지
    낮은 지연 (< 100μs)

설정 예시:

  # DCTCP 활성화
  net.ipv4.tcp_congestion_control = dctcp
  
  # ECN 필수
  net.ipv4.tcp_ecn = 1
  
  # 짧은 RTO
  net.ipv4.tcp_min_rto = 2  # 2ms
```

### 모바일/무선 환경

```
특성:
  - 변동성 큰 대역폭
  - 높은 지연 변동
  - 랜덤 손실 (간섭)
  - 핸드오버

문제:

  1. 랜덤 손실:
     TCP: 손실 = 혼잡
     → 과도한 감소
     
  2. 지연 변동:
     RTT: 50ms → 500ms
     → RTO 계산 어려움
     
  3. 핸드오버:
     기지국 변경
     → 일시적 단절

최적화:

  1. 손실 구분:
     ECN, 지연 패턴 분석
     → 무선 손실 vs 혼잡
     
  2. 적응적 RTO:
     큰 RTTVAR 허용
     
  3. 빠른 복구:
     TLP, RACK
     낮은 RTO_min

Westwood+ 예시:

  무선 손실 감지:
    ACK 간격으로 대역폭 추정
    
  손실 시:
    ssthresh = BWE × RTT_min
    cwnd = ssthresh
    
    (Reno의 cwnd/2 대신)
    
  효과:
    무선 오류 → 작은 감소
    혼잡 → BWE 자연 감소

타임라인:

  T=0: cwnd=100, 대역폭=80 Mbps
  
  T=1: 무선 간섭으로 손실
  
  Reno:
    cwnd = 50 (과도)
    
  Westwood+:
    BWE = 80 Mbps
    RTT = 100ms
    ssthresh = 80 × 0.1 = 1 MB ≈ 85 패킷
    cwnd = 85 (적절)
```

## 모니터링과 디버깅

### ss 명령어로 상태 확인

```bash
# 기본 정보
ss -ti

출력:
ESTAB  0  0  192.168.1.1:22  10.0.0.1:54321
  ts sack cubic wscale:7,7 rto:204 rtt:3.8/2.4
  cwnd:10 ssthresh:7 bytes_sent:1234
  send 25.3Mbps rcv_space:29200 rcv_ssthresh:65535
  
핵심 필드:
  rto: 재전송 타임아웃 (204ms)
  rtt: 평균/변동 (3.8ms / 2.4ms)
  cwnd: 혼잡 윈도우 (10 MSS)
  ssthresh: 슬로우 스타트 임계값
  rcv_space: 수신 버퍼 (29200 바이트)
  rcv_ssthresh: 수신 윈도우 임계값
  send: 예상 전송률

# 메모리 정보
ss -tm

출력:
  skmem:(r0,rb131072,t0,tb87040,f0,w0,o0,bl0,d0)
  
  r: 수신 큐에 있는 바이트
  rb: 수신 버퍼 할당 크기
  t: 송신 큐에 있는 바이트
  tb: 송신 버퍼 할당 크기
  f: forward allocation
  w: wmem allocated
  o: opt mem
  bl: backlog
  d: sock drop

# 특정 연결 필터
ss -ti dst 10.0.0.1

# 모든 TCP 연결
ss -ta
```

### netstat 레거시 도구

```bash
# 기본 정보
netstat -antp

# 수신/송신 큐 확인
netstat -an | grep ESTABLISHED

출력:
tcp  Recv-Q Send-Q Local          Foreign        State
tcp    0    8192  192.168.1.1:22 10.0.0.1:54321 ESTABLISHED

Recv-Q: 수신 버퍼에 있으나 앱이 안 읽은 바이트
Send-Q: 송신 버퍼에 있으나 ACK 안된 바이트

문제 징후:
  Recv-Q 계속 증가 → 애플리케이션 느림
  Send-Q 계속 가득 → 네트워크/수신자 느림

# 통계
netstat -s | grep -i retrans

출력:
  1234 segments retransmitted
  56 fast retransmits
  78 retransmits in slow start
```

### tcpdump로 실시간 분석

```bash
# 윈도우 크기 확인
tcpdump -i eth0 'tcp' -n -v

출력:
IP 192.168.1.1.22 > 10.0.0.1.54321: 
  Flags [.], seq 1000:2000, ack 5000, win 29200, length 1000
  
  win 29200: 수신 윈도우 = 29200 바이트
  length 1000: 페이로드 크기

# Zero Window 감지
tcpdump -i eth0 'tcp[tcpflags] & tcp-ack != 0' -n | grep "win 0"

출력:
  win 0 → 수신자 버퍼 가득
  → Persist Timer 동작 확인

# Window Update 추적
tcpdump -i eth0 -n -vv 'tcp' | grep -E 'win|ack'

# SACK 옵션 확인
tcpdump -i eth0 -n -v 'tcp' | grep -i sack

출력:
  sack 1 {3000:4000} → 블록 정보

# 윈도우 스케일 협상
tcpdump -i eth0 'tcp[tcpflags] & tcp-syn != 0' -v

출력:
  SYN 패킷에서:
    wscale 7 → Scale Factor = 7
    실제 윈도우 = win × 2^7
```

### 성능 카운터와 메트릭

```bash
# 시스템 전체 TCP 통계
cat /proc/net/snmp | grep ^Tcp

출력:
Tcp: RtoAlgorithm RtoMin RtoMax MaxConn ActiveOpens
Tcp: PassiveOpens AttemptFails EstabResets CurrEstab
Tcp: InSegs OutSegs RetransSegs InErrs OutRsts

핵심 지표:
  RetransSegs: 재전송된 세그먼트 수
  InSegs: 수신한 세그먼트
  OutSegs: 전송한 세그먼트
  
  재전송률 = RetransSegs / OutSegs

# 상세 통계
cat /proc/net/netstat | grep TcpExt

출력:
TcpExt: SyncookiesSent SyncookiesRecv SyncookiesFailed
  TCPHPHits TCPPureAcks TCPHPAcks TCPRenoRecovery
  TCPSackRecovery TCPSACKReneging TCPFACKReorder
  TCPTSReorder TCPFullUndo TCPPartialUndo
  TCPLossUndo TCPLostRetransmit
  
중요 카운터:
  TCPSackRecovery: SACK 기반 복구 횟수
  TCPLostRetransmit: 손실된 재전송
  TCPRenoRecovery: Reno 복구 횟수
  TCPFastRetrans: 빠른 재전송 횟수

# 소켓별 상세 정보
cat /proc/net/tcp

출력 (16진수):
  sl  local_address rem_address   st tx_queue rx_queue
   0: 0100007F:0016 00000000:0000 0A 00000000:00000000

해석:
  st: 상태 (0A = LISTEN)
  tx_queue: 송신 큐 크기
  rx_queue: 수신 큐 크기
```

### 실시간 모니터링 스크립트

```bash
#!/bin/bash
# TCP 흐름 제어 모니터링

watch -n 1 '
echo "=== TCP Window Statistics ==="
ss -ti | grep -E "wscale|cwnd|rcv_space" | head -5

echo ""
echo "=== Buffer Queue Status ==="
ss -tn state established | awk "{
  if (NR>1) {
    recv=\$2; send=\$3
    if (recv>0) recv_count++
    if (send>0) send_count++
    total_recv+=recv; total_send+=send
  }
} END {
  print \"Recv-Q blocked:\", recv_count, \"connections\"
  print \"Send-Q blocked:\", send_count, \"connections\"
  print \"Total Recv-Q:\", total_recv, \"bytes\"
  print \"Total Send-Q:\", total_send, \"bytes\"
}"

echo ""
echo "=== Retransmission Rate ==="
netstat -s | grep -E "segments|retransmit" | head -3
'

# 출력 예시:
=== TCP Window Statistics ===
  wscale:7,7 rto:204 rtt:3.8/2.4 cwnd:10 rcv_space:29200
  wscale:8,8 rto:208 rtt:5.2/1.8 cwnd:15 rcv_space:131072

=== Buffer Queue Status ===
Recv-Q blocked: 2 connections
Send-Q blocked: 5 connections
Total Recv-Q: 8192 bytes
Total Send-Q: 245760 bytes

=== Retransmission Rate ===
    123456 segments sent
    1234 segments retransmitted
    0.99% retransmission rate
```

## 흐름 제어와 혼잡 제어의 통합

실제 송신 윈도우는 두 제어 메커니즘의 최솟값으로 결정됩니다.

```
통합 공식:

  EffectiveWindow = min(rwnd, cwnd)
  
  실제 전송 가능 = EffectiveWindow - 
                   (Sent but UnACKed)

시나리오 분석:

1. 수신자가 병목:
   rwnd = 32 KB (수신 버퍼)
   cwnd = 100 KB (네트워크 여유)
   
   → EffectiveWindow = 32 KB
   → 수신자 처리 속도에 제한됨

2. 네트워크가 병목:
   rwnd = 100 KB (수신 버퍼 여유)
   cwnd = 10 KB (혼잡 감지)
   
   → EffectiveWindow = 10 KB
   → 네트워크 혼잡에 제한됨

3. 균형 상태:
   rwnd = 64 KB
   cwnd = 64 KB
   
   → EffectiveWindow = 64 KB
   → 최적 상태

동적 상호작용:

  타임라인:
  
  T=0:  rwnd=64KB, cwnd=10KB (슬로우 스타트)
        Effective=10KB
        
  T=1:  ACK 도착, cwnd→20KB
        Effective=20KB
        
  T=2:  cwnd→40KB
        Effective=40KB
        
  T=3:  cwnd→80KB, rwnd=64KB
        Effective=64KB (rwnd 제한)
        
  T=4:  App이 데이터 읽음, rwnd→128KB
        cwnd=80KB
        Effective=80KB (cwnd 제한)
        
  T=5:  패킷 손실!, cwnd→40KB
        Effective=40KB

시각화:

  시간 →
  
  Effective Window:
  ┌────────────────────────────────────┐
  │     EffWin = min(rwnd, cwnd)       │
  │                                    │
  │ rwnd ─────────────────             │
  │                       ╲            │
  │                        ─────────   │
  │                                    │
  │ cwnd    ╱╲    ╱╲                  │
  │        ╱  ╲  ╱  ╲    ╱            │
  │       ╱    ╲╱    ╲  ╱             │
  │      ╱            ╲╱              │
  │                                    │
  │ Eff  ────────┐    ┌──────         │
  │              ↓    ↑                │
  │           rwnd  cwnd               │
  │           제한  제한               │
  └────────────────────────────────────┘
```

## 애플리케이션 레벨 최적화

### 대용량 파일 전송

```python
# 비효율적 방식
with open('large_file.dat', 'rb') as f:
    while True:
        chunk = f.read(1024)  # 작은 chunk
        if not chunk:
            break
        sock.send(chunk)  # 많은 시스템 콜

# 최적화된 방식
with open('large_file.dat', 'rb') as f:
    while True:
        chunk = f.read(1048576)  # 1MB chunk
        if not chunk:
            break
        sock.sendall(chunk)  # 전체 전송 보장

# 더 나은 방식: sendfile
import os
sock.sendfile(file_fd, offset=0, count=file_size)
# 커널에서 직접 전송 (zero-copy)

성능 비교:
  1KB chunk: 100만 시스템 콜, 느린 처리
  1MB chunk: 1000 시스템 콜, 빠른 처리
  sendfile: 최소 시스템 콜, 복사 없음
```

### 실시간 스트리밍

```python
# TCP_NODELAY로 Nagle 비활성화
import socket

sock = socket.socket()
sock.setsockopt(socket.IPPROTO_TCP, 
                socket.TCP_NODELAY, 1)

# 작은 패킷도 즉시 전송
for frame in video_stream:
    sock.send(frame)  # 지연 없이 전송

# TCP_CORK (Linux): 반대 전략
sock.setsockopt(socket.IPPROTO_TCP,
                socket.TCP_CORK, 1)

# 여러 write 축적
sock.send(header)
sock.send(body)
sock.send(footer)

# 한번에 전송
sock.setsockopt(socket.IPPROTO_TCP,
                socket.TCP_CORK, 0)

사용 시나리오:
  TCP_NODELAY: 게임, VoIP, SSH
  TCP_CORK: HTTP 응답, 벌크 전송
```

### 버퍼 크기 튜닝

```python
import socket

sock = socket.socket()

# 송신 버퍼 크기 설정 (4MB)
sock.setsockopt(socket.SOL_SOCKET, 
                socket.SO_SNDBUF, 
                4194304)

# 수신 버퍼 크기 설정 (4MB)
sock.setsockopt(socket.SOL_SOCKET,
                socket.SO_RCVBUF,
                4194304)

# 현재 크기 확인
sndbuf = sock.getsockopt(socket.SOL_SOCKET,
                         socket.SO_SNDBUF)
rcvbuf = sock.getsockopt(socket.SOL_SOCKET,
                         socket.SO_RCVBUF)

print(f"Send buffer: {sndbuf} bytes")
print(f"Recv buffer: {rcvbuf} bytes")

# 주의: OS가 2배로 설정할 수 있음
# (내부 오버헤드 포함)

권장 크기:
  LAN (1ms RTT):
    Buffer = 1-2 MB
    
  WAN (50ms RTT):
    Buffer = BDP × 2
    예: 100 Mbps → 1.25 MB → 2.5 MB
    
  Long Fat Network (100ms):
    Buffer = 10+ MB
```

## 고급 주제: Application-Limited 상태

```
정의:
  애플리케이션이 데이터를 충분히 빠르게 
  제공하지 못하는 상태

시나리오:

  송신 버퍼: [_________] (비어있음)
  cwnd: 100 KB (여유)
  rwnd: 64 KB (여유)
  
  하지만: 애플리케이션이 write() 안함
  
  결과: 네트워크 유휴

문제점:

  1. cwnd 측정 부정확:
     네트워크 용량을 제대로 측정 못함
     
  2. 버퍼 오판:
     Auto-tuning이 버퍼 축소
     
  3. 대역폭 낭비:
     가용 대역폭 미사용

감지:

  if (송신큐 < EffectiveWindow × 0.5):
      Application-Limited 상태
      
  표시:
    ss 출력에서 app_limited 플래그

대응:

  혼잡 제어:
    cwnd 업데이트 보류
    (부정확한 측정 방지)
    
  Auto-tuning:
    버퍼 축소 지연
    
  애플리케이션:
    더 큰 chunk로 write
    비동기 I/O 사용

예시:

  정상 상태:
    App write: ████████ (지속적)
    송신큐: [████████] (가득)
    처리량: 최대
    
  App-Limited:
    App write: █   █   █ (간헐적)
    송신큐: [█   █   █] (비어있음)
    처리량: 낮음 (앱 탓)
```

## ACK 관련 고급 메커니즘

### ACK Compression/Stretching

```
ACK Compression (압축):

  원인: 중간 라우터의 큐잉
  
  이상적:
    [Data1] → (10ms) → [ACK1]
    [Data2] → (10ms) → [ACK2]
    [Data3] → (10ms) → [ACK3]
    
  실제 (큐에서 대기):
    [Data1] → [ACK1] ╲
    [Data2] → [ACK2] ─┤ (큐 대기)
    [Data3] → [ACK3] ╱
    
    → [ACK1][ACK2][ACK3] 한번에!

효과:

  버스트 전송 유발:
    송신자: 3개 ACK 동시 수신
    → cwnd 공간 3개 생성
    → [Data4][Data5][Data6] 동시 전송
    → 네트워크 버스트!

ACK Stretching (늘림):

  원인: 중간 노드의 ACK 지연
  
  정상:
    100ms 간격으로 ACK
    
  Stretching:
    200-300ms 간격으로 ACK
    → 송신자가 느린 네트워크로 오해
    → cwnd 성장 둔화

대응:

  1. Pacing:
     ACK 도착과 무관하게 
     일정 속도로 전송
     
  2. TSO (TCP Segmentation Offload):
     NIC에서 분할 전송
     
  3. BBR:
     ACK 타이밍 무시하고
     모델 기반 전송
```

### ACK Decimation

```
정의:
  매 N개 패킷마다 한 번만 ACK

전통적 (매 패킷 ACK):
  [P1] → [ACK1]
  [P2] → [ACK2]
  [P3] → [ACK3]
  ...
  
  ACK 오버헤드: 100%

Decimation (N=4):
  [P1] [P2] [P3] [P4] → [ACK4]
  [P5] [P6] [P7] [P8] → [ACK8]
  
  ACK 오버헤드: 25%

구현 (Linux):
  
  tcp_adv_win_scale 파라미터로 제어
  
  ACK 전송 조건:
    - 윈도우의 1/N 수신
    - 타이머 만료
    - Out-of-order 수신

장점:
  - ACK 트래픽 감소
  - 역방향 대역폭 절약
  - CPU 사용량 감소

단점:
  - 손실 감지 지연
  - cwnd 증가 둔화
  - 빠른 재전송 영향

균형:
  적절한 N 값 (2-4)
  → 효율과 응답성 균형
```

## Head-of-Line Blocking 상세

```
TCP의 근본적 문제:

  순서 보장 → HOL 블로킹
  
시나리오:

  전송: [1][2][3][4][5]
          ✓ ✗ ✓ ✓ ✓
          
  수신 버퍼:
    [1][_][3][4][5]
    
  애플리케이션:
    read() → [1] 반환
    read() → 블로킹! (2 대기)
    
  [3][4][5]는 도착했지만 전달 불가!

타임라인:

  T=0:   [1][2][3][4][5] 전송
  T=10:  [1] 수신 → App에 전달
  T=11:  [3][4][5] 수신 → 버퍼링
         [2] 손실
  T=11:  App read() → 블로킹
  T=15:  중복 ACK 전송
  T=20:  빠른 재전송 [2]
  T=30:  [2] 수신 → App에 전달
  T=30:  [2][3][4][5] 모두 전달

  블로킹 시간: 19ms

영향:

  HTTP/1.1:
    하나의 연결로 여러 리소스
    → 하나 지연시 모두 대기
    
  해결: HTTP/2 multiplexing
    하지만 TCP 레벨에서는 여전히 HOL

QUIC의 해결:

  스트림별 독립:
  
  스트림 1: [1][2][X][4]
  스트림 2: [A][B][C][D]
  
  TCP: 패킷 2 손실 → 모두 블로킹
  QUIC: 스트림 1만 블로킹
        스트림 2는 계속 전달!

측정:

  HOL 블로킹 시간 =
    손실 감지 + 재전송 + RTT
    
  평균: 50-200ms
  
  영향:
    지연 민감 앱에 치명적
    (VoIP, 게임, 실시간 비디오)
```

## 미래 발전 방향

### TCP의 진화

```
역사적 발전:

  1981: TCP RFC 793
        - 기본 흐름 제어
        - Stop-and-Wait 수준
        
  1988: Jacobson's Algorithm
        - 정교한 RTT/RTO
        - 혼잡 제어 도입
        
  1996: SACK (RFC 2018)
        - 선택적 재전송
        
  2003: Window Scaling (RFC 1323)
        - 고속 네트워크 지원
        
  2014: TFO (RFC 7413)
        - 0-RTT 데이터
        
  2016: BBR
        - 모델 기반 제어
        
  2020+: RACK, TLP
        - 시간 기반 손실 감지

현재 트렌드:

  1. 더 정확한 측정:
     - 타임스탬프
     - 하드웨어 타이밍
     
  2. 적응적 제어:
     - 머신러닝
     - 동적 파라미터
     
  3. 애플리케이션 통합:
     - SO_ATTACH_REUSEPORT_EBPF
     - 프로그래머블 스택
```

### QUIC로의 전환

```
TCP의 한계:

  1. 커널 구현:
     → 업데이트 느림
     → OS 의존성
     
  2. HOL 블로킹:
     → 멀티플렉싱 제한
     
  3. 핸드셰이크:
     → 1.5 RTT 오버헤드

QUIC 장점:

  1. 유저스페이스:
     빠른 반복 개발
     
  2. 스트림 독립:
     HOL 블로킹 해결
     
  3. 0-RTT:
     즉시 연결

흐름 제어 비교:

  TCP:
    연결 단위 rwnd
    모든 데이터 공유
    
  QUIC:
    연결 레벨 + 스트림 레벨
    
    Connection Flow Control:
      전체 연결의 제한
      
    Stream Flow Control:
      각 스트림의 제한
      
  예:
    Connection rwnd: 1 MB
    Stream 1 rwnd: 256 KB
    Stream 2 rwnd: 256 KB
    Stream 3 rwnd: 256 KB
    
    유연한 자원 배분!

마이그레이션:

  HTTP/3:
    TCP → QUIC 전환
    
  점진적 도입:
    서버: TCP + QUIC 동시 지원
    클라이언트: QUIC 시도 → 실패 시 TCP
```

## 실무 체크리스트

### 성능 최적화

```
┌─────────────────────────────────────────────┐
│ 1. 시스템 설정                               │
├─────────────────────────────────────────────┤
│ □ Window Scaling 활성화                     │
│ □ SACK 활성화                               │
│ □ 타임스탬프 활성화                         │
│ □ 적절한 버퍼 크기 (BDP 기반)              │
│ □ Auto-tuning 활성화                       │
└─────────────────────────────────────────────┘

┌─────────────────────────────────────────────┐
│ 2. 애플리케이션 레벨                         │
├─────────────────────────────────────────────┤
│ □ 큰 chunk 크기 (1MB+)                     │
│ □ 적절한 소켓 옵션                          │
│   - TCP_NODELAY (실시간)                   │
│   - TCP_CORK (벌크)                        │
│ □ 비동기 I/O 사용                          │
│ □ sendfile/splice 활용                    │
└─────────────────────────────────────────────┘

┌─────────────────────────────────────────────┐
│ 3. 모니터링                                 │
├─────────────────────────────────────────────┤
│ □ 재전송률 < 1%                            │
│ □ Recv-Q/Send-Q 낮게 유지                  │
│ □ RTT 변동 모니터링                        │
│ □ 버퍼 사용률 확인                          │
└─────────────────────────────────────────────┘

환경별 권장 설정:

데이터센터:
  net.ipv4.tcp_rmem = 4096 87380 16777216
  net.ipv4.tcp_wmem = 4096 65536 16777216
  net.ipv4.tcp_congestion_control = dctcp
  net.ipv4.tcp_ecn = 1

WAN:
  net.ipv4.tcp_rmem = 4096 131072 6291456
  net.ipv4.tcp_wmem = 4096 16384 4194304
  net.ipv4.tcp_congestion_control = bbr
  net.core.default_qdisc = fq

모바일:
  net.ipv4.tcp_congestion_control = westwood
  net.ipv4.tcp_frto = 2
```

## 결론

TCP 흐름 제어는 신뢰성 있는 데이터 전송의 근간입니다. 1970년대 Stop-and-Wait에서 시작하여, 슬라이딩 윈도우, SACK, Window Scaling, 그리고 현대의 동적 버퍼 조정에 이르기까지 지속적으로 진화해왔습니다.

**핵심 개념 요약:**

```
수신자: "내 버퍼에 N 바이트 공간 있음" (rwnd)
          ↓ (ACK의 WIN 필드)
송신자: "min(rwnd, cwnd) 만큼만 전송"
          ↓
네트워크: 안전한 전송, 손실 최소화
          ↓
애플리케이션: 신뢰성 있는 데이터 수신

본질: 수신자 중심의 종단 간 제어
```

흐름 제어와 혼잡 제어의 조화로운 통합은 TCP가 40년 이상 인터넷의 주요 전송 프로토콜로 자리잡을 수 있었던 핵심 요소입니다. Window Scaling으로 고속 네트워크를 지원하고, SACK으로 효율적인 복구를 제공하며, 동적 버퍼 조정으로 다양한 환경에 적응합니다.

현대에 이르러서는 TFO, TLP, RACK 같은 기법으로 더 빠르고 효율적인 전송을 제공하며, QUIC 같은 차세대 프로토콜에서도 그 기본 원리는 여전히 유효합니다. 하지만 HOL 블로킹 같은 근본적 한계는 QUIC의 스트림별 흐름 제어로 해결되고 있습니다.

성공적인 TCP 통신을 위해서는 시스템 레벨의 적절한 설정, 애플리케이션 레벨의 최적화, 그리고 지속적인 모니터링이 필요합니다. 각 환경(데이터센터, WAN, 모바일)에 맞는 튜닝을 통해 최적의 성능을 달성할 수 있습니다.