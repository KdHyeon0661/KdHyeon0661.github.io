---
layout: post
title: Docker - 컨테이너별 리소스 제한
date: 2025-03-29 19:20:23 +0900
category: Docker
---
# Docker 컨테이너별 리소스 제한 (CPU · Memory · blkio/IO)

## 왜 리소스 제한인가? (운영 관점 재정의)

| 리스크 | 구체적 현상 | 제한으로 얻는 효과 |
|---|---|---|
| CPU 과점유 | 한 컨테이너가 100% 코어를 선점 → 지연/타임아웃 | 공정한 분배(share), 절대 한도(cpus) |
| 메모리 폭주 | OOM Killer가 무작위 종료, 커널 불안정 | 상한선 설정, 스왑 정책 조율 |
| 디스크 IO 폭주 | 큐 지연, tail latency 증가 | blkio 가중치·속도 스로틀 |
| 멀티 테넌시 | noisy neighbor, SLA 훼손 | 코어 고정(cpuset), 우선순위 차등 |

핵심은 “**파손 격리**(fault isolation)”와 “**예측 가능성**(predictability)”이다.

---

## CPU 제한 — cpus, shares, cpuset, quota/period

### 한 줄 요약

- **절대 상한**: `--cpus=N` (가장 직관적, 내부적으로 `quota/period` 셋업)
- **상대 가중치**: `--cpu-shares` (경합 있을 때만 의미)
- **코어 고정**: `--cpuset-cpus=LIST` (NUMA/캐시 친화적)
- **미세조정**: `--cpu-quota`, `--cpu-period` (레거시/정밀 케이스)

### 실전 옵션 비교

| 목적 | 권장 옵션 | 비고 |
|---|---|---|
| “최대 1.5코어만 쓰게” | `--cpus=1.5` | `quota/period` 자동 계산 |
| “다른 컨테이너보다 낮은 우선순위” | `--cpu-shares=512` | 기본 1024와 상대 비교 |
| “0,1번 코어에만 핀닝” | `--cpuset-cpus=0,1` | 지연 민감 워크로드에 유리 |
| “정밀한 주기·할당” | `--cpu-period=100000 --cpu-quota=150000` | 고급 제어, 실수 방지 주의 |

### 수식으로 이해(개념 고도화)

- Build-in 관계:
  $$
  \text{cpus} = \frac{\text{cpu\_quota}}{\text{cpu\_period}}
  $$
  예: `period=100000µs`, `quota=150000µs` → $$\text{cpus}=1.5$$

- CPU shares는 **절대 한도 아님**. 동일 호스트에서 두 컨테이너 A:1024, B:512면 경합 시 A가 B의 2배 CPU time을 받는다. 유휴 CPU가 있으면 둘 다 **풀로 쓴다**.

### 예제

```bash
# 절대 상한: 1.5 vCPU

docker run --name web --cpus="1.5" myorg/web:latest

# 상대 가중치: 경합 시 낮은 우선순위

docker run --name batch --cpu-shares=256 myorg/batch:latest

# 코어 핀닝(0,1번 코어)

docker run --name rt --cpuset-cpus="0,1" myorg/rt:latest

# 고급: period/quota 직접 지정

docker run --cpu-period=100000 --cpu-quota=200000 myorg/worker:latest  # 2 vCPU
```

### 측정/검증

```bash
docker stats
# 또는 cgroup v2에서 실제 한도 확인

cat /sys/fs/cgroup/docker/<CID>/cpu.max     # 예: "150000 100000"
cat /sys/fs/cgroup/docker/<CID>/cpu.weight  # shares 매핑(1~10000)
```

> **cgroup v2 메모**: shares(1024 스케일)는 `cpu.weight`(1~10000)로 매핑. 현대 배포판은 v2가 기본.

---

## 메모리 제한 — 상한선, 스왑, OOM 정책

### 핵심 옵션

| 옵션 | 의미 | 권장 기본 |
|---|---|---|
| `--memory` | 물리 RAM 상한 | 워크로드 객체 크기+여유 20~30% |
| `--memory-swap` | RAM+Swap 총량 | RAM*1.25~*1.5 또는 `--memory`로 스왑 금지 |
| `--oom-kill-disable` | OOM 시 종료 금지 | **권장하지 않음**(호스트 위험) |
| `--kernel-memory` | v1 일부에서 사용 | v2에선 제거/의미 축소 |

### 스왑 전략

- **스왑 금지(하드 제한)**: `--memory-swap` = `--memory`
- **부분 허용**: `--memory-swap` = `--memory + 여유`
- **무제한 스왑**: `--memory-swap=-1` → 지연 폭증 위험

### 예제

```bash
# 512MiB 고정, 스왑 금지

docker run --memory="512m" --memory-swap="512m" myorg/api:latest

# 1GiB + 스왑 총 1.5GiB

docker run --memory="1g" --memory-swap="1.5g" myorg/etl:latest
```

### 관찰/디버깅

```bash
docker stats
# cgroup v2

cat /sys/fs/cgroup/docker/<CID>/memory.max
cat /sys/fs/cgroup/docker/<CID>/memory.current
cat /sys/fs/cgroup/docker/<CID>/memory.events    # oom, oom_kill 카운터
```

> **OOM-Killer**는 컨테이너 내부 프로세스를 죽인다. 앱이 fork 폭주/캐시 과잉이면 **최대 resident set**을 줄이고 메모리 재활용 로직을 넣자.
> `--oom-kill-disable`은 호스트 안정성을 해친다(권장 X).

---

## 디스크 IO — blkio 가중치, 장치별 스로틀

### 두 계층을 구분

- **blkio.weight(가중치)**: 경합 시 비율 배분(10~1000, 기본 500)
- **장치별 스로틀**: `/dev/sdX` 단위로 **절대 속도**(B/s) 또는 IOPS 제한

### 예제

```bash
# 경합 시 IO 우선순위 낮춤

docker run --blkio-weight=300 myorg/report:latest

# 장치별 read/write 대역폭 제한

docker run \
  --device-read-bps /dev/sda:2mb \
  --device-write-bps /dev/sda:1mb \
  myorg/archiver:latest
```

> 경로 주의: 컨테이너 루트가 `overlay2`라 하더라도 **실제 하부 물리 디바이스**(예: `/dev/nvme0n1p2`)에 적용된다. `lsblk`, `findmnt /var/lib/docker`로 확인하고 같은 디바이스에 스로틀을 건다.

### cgroup v2의 IO 인터페이스

- v2에선 `io.weight`, `io.max` 파일로 관리된다. Docker가 내부 매핑을 수행하므로 일반적인 사용자는 CLI 옵션으로 충분하지만, 트러블슈팅 시 참고:

```bash
cat /sys/fs/cgroup/docker/<CID>/io.max     # "8:0 rbps=2097152 wbps=1048576" 등
cat /sys/fs/cgroup/docker/<CID>/io.weight
```

---

## 종합 실행 예 — “안전 기본값” 템플릿

```bash
docker run -d --name app1 \
  --cpus="1.0" \
  --cpu-shares=512 \
  --cpuset-cpus="0,1" \
  --memory="1g" \
  --memory-swap="1.25g" \
  --blkio-weight=300 \
  --device-read-bps /dev/nvme0n1p2:2mb \
  --device-write-bps /dev/nvme0n1p2:1mb \
  --pids-limit=512 \
  --ulimit nofile=65535:65535 \
  --log-driver=json-file --log-opt max-size=10m --log-opt max-file=5 \
  myorg/app:stable
```

> 추가: `--read-only --tmpfs /tmp`(보안/성능), `--cap-drop=ALL`(보안), `--health-*`(가용성)

---

## docker compose로 선언형 관리

### Compose v2 (비 Swarm)에서의 리소스

- `mem_limit`, `cpus`, `cpuset`, `pids_limit` 등 **서비스 레벨** 키를 사용
- `deploy.resources.limits`는 **Swarm 모드**에서만 완전 반영(Compose 단독에선 대부분 무시됨)

```yaml
version: "3.9"
services:
  api:
    image: myorg/api:2.1.0
    cpus: "1.5"
    cpuset: "0-2"
    mem_limit: "1g"
    pids_limit: 512
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"
    # blkio 스로틀은 compose에서 표준 키가 부족 → run-opts는 deploy/device_*로 Swarm에서 선언
```

### Swarm 모드(옵션)

```yaml
services:
  api:
    image: myorg/api:2.1.0
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 512M
```
> **reservations**는 스케줄링 힌트로 활용(실행 중 강제 상한은 `limits`).

---

## 벤치·검증 — 재현 가능한 테스트

### CPU/메모리 스트레스

```bash
# CPU 2개 워커, 60초 스트레스

docker run --rm --cpus=1.5 progrium/stress --cpu 2 --timeout 60

# 메모리 할당 후 유지(600MiB), 60초

docker run --rm --memory=512m polinux/stress \
  --vm 1 --vm-bytes 600M --timeout 60
```

### 디스크 IO(FIO)

```bash
# 호스트에 fio 설치 후 컨테이너 볼륨/경로 대상으로 수행

fio --name=randread --filename=/var/lib/docker/... --bs=4k --iodepth=16 --rw=randread --runtime=60
```

### 관찰

```bash
docker stats
pidof dockerd | xargs -I{} cat /proc/{}/cgroup
sudo cat /sys/fs/cgroup/docker/<CID>/cpu.max
sudo cat /sys/fs/cgroup/docker/<CID>/memory.events
```

---

## 고급 토픽 — NUMA, RT, GC, JIT, 핀닝 전략

- **NUMA/캐시 친화**: `--cpuset-cpus`로 연속 코어를 묶어 L3 공유를 극대화.
- **RT/저지연**: 컨테이너 내부 RT 스케줄이 필요한 경우 커널 설정과 capabilities 확인. 일반 웹/배치에선 불필요.
- **JVM/GC**: `-XX:MaxRAMPercentage`/`-XX:InitialRAMPercentage`로 컨테이너 메모리 인식 조정.
- **.NET/GC**: `DOTNET_GCHeapHardLimit`/`DOTNET_GCHeapHardLimitPercent` 고려.
- **Python**: 대형 Numpy/ML은 `/dev/shm` 사이즈(`--shm-size`)와 메모리 상한을 함께 조정.

---

## 운영 관점 체크리스트

### CPU

- [ ] 상한: `--cpus`로 명시
- [ ] 우선순위: `--cpu-shares`로 차등
- [ ] 핀닝: `--cpuset-cpus`(지연 민감/캐시 효과)
- [ ] 검증: `cpu.max`/`docker stats`

### 메모리

- [ ] 상한: `--memory`
- [ ] 스왑 정책: `--memory-swap`(과대 스왑 금지)
- [ ] `/dev/shm`: `--shm-size`(브라우저/ML/빌드)
- [ ] OOM 로그: `memory.events`

### IO

- [ ] blkio 가중치: `--blkio-weight`
- [ ] 장치별 스로틀: `--device-*-bps`
- [ ] 실제 디바이스 경로 확인: `lsblk`, `findmnt`

### 보안/안정

- [ ] `--pids-limit`, `ulimit nofile`
- [ ] `--read-only --tmpfs /tmp`
- [ ] 로그 회전: `--log-opt max-size/max-file`

---

## systemd와 함께 쓰기 (운영 표준화)

```ini
[Service]
ExecStart=/usr/bin/docker run --name api \
  --cpus=1.25 --cpu-shares=512 --cpuset-cpus=0-1 \
  --memory=1g --memory-swap=1280m \
  --blkio-weight=300 \
  --device-read-bps /dev/nvme0n1p2:2mb \
  --device-write-bps /dev/nvme0n1p2:1mb \
  --pids-limit=512 \
  myorg/api:2.2.0
ExecStop=/usr/bin/docker stop -t 15 api
ExecStopPost=/usr/bin/docker rm -f api
Restart=always
```

---

## 트러블슈팅 — “안 걸리는 함정” 바로잡기

| 증상 | 원인 | 해결 |
|---|---|---|
| `--cpu-shares`가 안 먹는 느낌 | 경합이 없을 때는 의미 없음 | 부하/경합 상황에서만 차등 |
| 디스크 제한이 안 되는 것처럼 보임 | 잘못된 디바이스에 스로틀 | `lsblk`로 실제 파티션 확인 후 일치 지정 |
| 메모리 제한인데도 OOM 빈번 | 페이지 캐시/네이티브 확장/자식 프로세스 | 상한 상향, 코드에서 캐시 크기 제한, pids-limit 병행 |
| 스왑 너무 커서 지연↑ | `--memory-swap=-1` | swap 총량 제한 또는 스왑 금지 |
| compose에서 limits가 무시 | Swarm 전용 필드 사용 | Compose v2는 `mem_limit`, `cpus`, `cpuset` 사용 |

---

## 수식/관점 정리 (빠른 복습)

- CPU 절대 상한:
  $$
  \text{cpus}=\frac{\text{quota}}{\text{period}},\quad \text{기본 period}=100\,000\ \mu s
  $$
- Shares(가중치) 비율:
  $$
  \text{비율}=\frac{W_i}{\sum_j W_j} \quad (\text{경합 시})
  $$
  단, idle 코어가 있으면 모두 가속한다.

---

## 템플릿: “SLA 안전값” 스타터

```bash
docker run -d --name svc \
  --cpus=1 \
  --cpu-shares=512 \
  --memory=512m --memory-swap=640m \
  --pids-limit=256 \
  --blkio-weight=300 \
  --log-driver=json-file --log-opt max-size=10m --log-opt max-file=5 \
  myorg/svc:stable
```

- 웹 API: `cpus=1~2`, `mem=512m~1g`, `pids=256~512`
- 배치/ETL: `cpus=2~4`, IO 스로틀 적극 사용
- 빌드/CI: `cpuset`로 코어 고정, `shm-size` 조정

---

## 부록 A) 빠른 랩 스크립트

```bash
# A: 리소스 제한 컨테이너 2개 기동

docker run -d --name a --cpus=1 --memory=512m --blkio-weight=300 alpine sleep 300
docker run -d --name b --cpus=2 --memory=1g   --blkio-weight=700 alpine sleep 300

# B: CPU 경합 유발

docker exec -d a sh -c "apk add --no-cache stress-ng && stress-ng --cpu 2 --timeout 60"
docker exec -d b sh -c "apk add --no-cache stress-ng && stress-ng --cpu 4 --timeout 60"

# C: 관찰

docker stats --no-stream
```

---

## 부록 B) cgroup v1 ↔ v2 맵핑(요약)

| 리소스 | v1 | v2 |
|---|---|---|
| CPU 상한 | `cpu.cfs_quota_us/cpu.cfs_period_us` | `cpu.max` |
| CPU shares | `cpu.shares` | `cpu.weight` |
| 메모리 | `memory.limit_in_bytes` | `memory.max` |
| IO throttle | `blkio.throttle.*` | `io.max` |
| IO weight | `blkio.weight` | `io.weight` |

> 대부분의 최신 리눅스(예: Ubuntu 22.04+)는 v2가 기본. Docker가 내부 변환하므로 CLI만 잘 써도 된다.

---

## 요약

- **CPU**: `--cpus`(절대), `--cpu-shares`(상대), `--cpuset-cpus`(핀닝)
- **메모리**: `--memory`(상한), `--memory-swap`(총량), 스왑 남용 금지
- **IO**: `--blkio-weight`(비율), `--device-*-bps`(절대 스로틀)
- **운영 기본**: `--pids-limit`, `ulimit nofile`, 로그 회전, 보안 플래그
- **Compose**: v2에선 `mem_limit/cpus/cpuset` 사용, Swarm `deploy.resources`는 Swarm에서만
- **검증**: `docker stats` + cgroup 파일 직접 확인

이 가이드의 템플릿을 시작점으로, 워크로드 특성(지연/처리량/혼합)에 맞춰 숫자를 미세 조정하면 **예측 가능한 성능**과 **안정적인 멀티 테넌시**를 구현할 수 있다.
