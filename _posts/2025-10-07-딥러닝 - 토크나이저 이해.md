---
layout: post
title: 딥러닝 - 토크나이저 이해
date: 2025-10-07 15:25:23 +0900
category: 딥러닝
---
# 토크나이저 이해 총정리  

## 0) 서브워드 토크나이저가 필요한 이유

### 0.1 토큰화 단계의 목적
- **텍스트 → 정수 시퀀스**로 변환하여 신경망이 처리하도록 함.
- 전통 단어 토큰화의 문제:  
  - **OOV**(사전에 없는 단어) = `<unk>` 남발 → 정보 손실  
  - **어형 변화/합성어**가 많은 언어에서 단어 수 폭증(희소성↑)
- **서브워드**(글자와 단어 사이 단위)로 분해하면:  
  - **OOV 거의 0**(byte-level이면 진짜 0)  
  - 자주 쓰는 단어는 1~2 토큰, 희귀 단어는 여러 토큰으로 분해(길이↑)

### 0.2 기본용어
- **Vocab(사전)**: 토큰 집합(토큰→ID 사상 포함)  
- **Pre-tokenization(사전 분절)**: 공백/구두점 기준으로 큰 조각 내는 단계(옵션)  
- **Normalization**: 대소문자, 유니코드 정규화(NFKC 등)  
- **Detokenization**: 토큰 시퀀스를 원문으로 재조립(공백/특수문자 보존)

---

## 1) BPE(Byte Pair Encoding) — 알고리즘부터 구현까지

### 1.1 핵심 아이디어
- 코퍼스의 **문자 시퀀스**에서 **가장 자주 등장하는 인접 쌍**을 합쳐 **새 토큰**으로 만든다.  
- 이를 **K회** 반복하면, 빈도가 높은 글자쌍·음절·접사·자주 등장 단어가 **짧은 표현**을 얻게 된다.

초기 단어 표기(예: 단어 경계 `</w>` 붙임):  
`"lower"` → `l o w e r </w>`  
`"lowest"` → `l o w e s t </w>`  
`"newer"` → `n e w e r </w>`

반복적으로 가장 빈번한 pair(예: `e r`) → `er` 토큰, 다음에 `low` … 이런 식으로 병합.

### 1.2 수학적/직관적 관점
- 입력 문장 집합 \(\mathcal{D}\)가 있을 때, BPE는 **pair 합성 규칙** \(\mathcal{M}\)을 학습한다.  
- 인퍼런스에는 \(\mathcal{M}\)을 **그리디**하게 적용하여 긴 매치를 우선 합침.  
- 최적화 목적을 식으로 쓰면 복잡해지지만, 직관적으로는  
  **코드 길이(총 토큰 수)**를 줄이는 방향으로 빈도 높은 쌍을 계속 만든다고 볼 수 있다.

### 1.3 훈련 알고리즘(개요)
1) 코퍼스의 단어들을 문자 리스트로 분해, 단어 경계 기호 `</w>` 부여  
2) 모든 **인접 문자쌍** 빈도 집계  
3) 최빈쌍을 **새 토큰**으로 병합(사전에 추가)  
4) 2~3을 **merge 횟수**만큼 반복 → `merges.txt` 같은 규칙표 생성

### 1.4 인퍼런스(토큰화)
- 단어의 문자 시퀀스에 대해, **훈련된 병합 규칙**을 순서대로 적용  
- 더 이상 병합할 쌍이 없을 때까지 반복 → 최종 서브워드 시퀀스

> **OOV?**  
> BPE의 기본형은 **최소 단위가 “문자”**이므로,  
> **코퍼스에 없던 단어도** 문자 단위로 **항상 분해** 가능 → 사실상 OOV 없음.  
> 다만 **초기 문자 사전**에 없는 글자(희귀 유니코드)는 `<unk>`가 될 수 있다 → 해결은 **byte-level**.

---

### 1.5 미니 BPE 트레이너/토크나이저 (순수 Python, 교육용)

```python
# 간단한 BPE 구현: 학습(merge 규칙 / vocab) + 인퍼런스
# 교육용으로 작은 코퍼스에만 사용하세요(느립니다).
from collections import Counter, defaultdict

END = "</w>"

def word_to_symbols(word):
    return list(word) + [END]

def get_stats(corpus):
    """corpus: list of tokenized words e.g., [['l','o','w','</w>'], ...]"""
    pairs = Counter()
    for word in corpus:
        for i in range(len(word)-1):
            pairs[(word[i], word[i+1])] += 1
    return pairs

def merge_corpus(corpus, pair):
    """pair: ('e','r') -> 'er'로 병합"""
    a, b = pair
    merged = []
    for word in corpus:
        i = 0; new = []
        while i < len(word):
            if i < len(word)-1 and word[i]==a and word[i+1]==b:
                new.append(a+b)
                i += 2
            else:
                new.append(word[i])
                i += 1
        merged.append(new)
    return merged

def train_bpe(texts, num_merges=50):
    # 1) 단어 빈도 수집
    vocab_counter = Counter()
    for line in texts:
        for w in line.strip().split():
            vocab_counter[w] += 1
    # 2) 문자 단위로 corpus 생성(단어 중복은 빈도만큼 확장)
    corpus = []
    for w, f in vocab_counter.items():
        corpus += [word_to_symbols(w) for _ in range(f)]
    # 3) merge 반복
    merges = []
    for _ in range(num_merges):
        stats = get_stats(corpus)
        if not stats:
            break
        pair, cnt = stats.most_common(1)[0]
        if cnt < 2:  # 더 이상 유의미한 병합 없음
            break
        corpus = merge_corpus(corpus, pair)
        merges.append(pair)
    # 4) vocab 구성(코퍼스에서 관측된 토큰들 집합)
    tok_counter = Counter()
    for word in corpus:
        tok_counter.update(word)
    vocab = {tok: i for i, tok in enumerate(sorted(tok_counter))}
    # merges는 순서가 중요 → 인퍼런스에서 순서대로 적용
    return merges, vocab

def apply_bpe(word, merges):
    # 단어를 문자+END로 시작해서, merges 순으로 가능한 병합 실행
    w = word_to_symbols(word)
    for (a,b) in merges:
        i = 0
        while i < len(w)-1:
            if w[i]==a and w[i+1]==b:
                w = w[:i] + [a+b] + w[i+2:]
            else:
                i += 1
    # END 제거
    if w and w[-1]==END:
        w = w[:-1]
    return w

# 데모
texts = [
    "low lowest new newer wide widest",
    "low low low newer newest lowest"
]
merges, vocab = train_bpe(texts, num_merges=20)
print("merges:", merges[:10])
for w in ["lowest", "newest", "aaaab"]:
    print(w, "→", apply_bpe(w, merges))
```

- 핵심 포인트:
  - **문자 기반**이라 “aaaab” 같은 **처음 보는 단어**도 항상 분해 가능.  
  - 사전은 **관측된 토큰** 위주이지만, 인퍼런스는 **병합 규칙**으로 동작 → 토큰이 사전에 없으면 새로 생길 수 있음.  
    (실전 라이브러리는 규칙과 사전을 함께 관리하여 ID 할당을 고정)

---

### 1.6 Byte-Level BPE (GPT-2 계열의 핵심)
- **입력 전체를 바이트(0..255) 단위**로 본다 → 어떤 유니코드 글자든 **항상** 바이트로 표현 가능.  
- 첫 vocab은 **256 바이트**(또는 “보이는 문자로의 1:1 매핑”) + 특수토큰.  
- 이후 **바이트쌍 병합**을 반복 → 최종은 바이트-수준 BPE.  
- 장점: **진짜 OOV=0**. 이모지·제로-위드스·이국어 글자도 모두 커버.  
- 단점: 사람에게 덜 직관적(토큰이 글자 경계를 깨뜨릴 수 있음), 토큰 길이가 약간 늘 수 있음.

> **실전 팁**:  
> - 범언어/코드/이모지 혼합 코퍼스 → **byte-level BPE**가 가장 안전.  
> - 도메인/언어가 좁고 깨끗하면 문자/어절 BPE/SentencePiece가 토큰 효율이 더 좋을 때도 있다.

---

## 2) SentencePiece — Unigram/BPE, 정규화, 서브워드 정규화

> **SentencePiece**는 “**원시 텍스트(raw)**”를 그대로 받아  
> 공백/전처리 없이 **모델 내부 정규화+토크나이즈**를 수행하는 구조가 특징입니다.  
> (즉, “공백 토큰화 + BPE” 식이 아니라, **문장 단위**에서 직접 서브워드 분절을 학습/적용)

### 2.1 모델 종류
- **Unigram Language Model**(기본):  
  - 후보 서브워드 집합 \(\mathcal{V}\)와 그 확률 \(p(w)\)를 두고,  
    문장 \(x\)의 가능한 분절 \(\mathcal{S}(x)\) 중 **우도 최대** 분절을 찾는다.
  - 학습은 **EM 유사 절차**로 \(p(w)\)와 \(\mathcal{V}\)를 갱신(희귀 토큰 제거).
- **BPE 모드**: BPE 스타일 merge 규칙을 학습(옵션).

### 2.2 유니그램 LM 수식(핵심)
- 문장 \(x\)의 모든 분절 시퀀스 \(s\in\mathcal{S}(x)\)에 대해  
  $$ P(x) = \sum_{s\in \mathcal{S}(x)} \prod_{w\in s} p(w) $$
- 학습 목표는 전체 코퍼스 \(\mathcal{D}\)에 대한 \(-\log P(\mathcal{D})\) 최소화.  
- 실제 구현은 \(\mathcal{V}\) 후보를 크게 시작 → **빈도/우도 기여 낮은 서브워드**를  
  반복 제거하며 \(p(w)\)를 재추정(효율적 근사).

- **Viterbi(최대우도)** 분절:  
  $$ s^\* = \arg\max_{s\in \mathcal{S}(x)} \sum_{w\in s} \log p(w) $$  
  DP로 구함(문자 위치 i에서 시작하는 모든 후보 서브워드 w의 점수 비교).

### 2.3 서브워드 정규화(Subword Regularization)
- 학습 시 인코더에서 **여러 가능한 분절**을 **확률적으로 샘플**(unigram 샘플링, **nbest**/temperature)  
- 효과: 데이터 증강처럼 동작 → **강건성/일반화** 개선  
- 추론(평가/서빙) 시에는 보통 **Viterbi(가장 가능성 높은 분절)** 사용.

### 2.4 정규화/커버리지
- **Normalization Rule**: NFKC/커스텀 규칙으로 기호/공백/대소문자 정규화  
- **Character Coverage**: 자주 등장하는 문자집합을 **사전에 반드시 포함**하도록 하는 비율(예: 0.9995)  
  - 한국어/이모지/혼용 코퍼스에서 **coverage**를 높게 둘수록 `<unk>` 발생을 억제.  
- **Byte Fallback**(옵션): 사전에 없는 문자를 **바이트 덩어리로 대체** → `<unk>` 0에 수렴

> **한국어 주의**  
> - Hangul Jamo 분해(NFD/NFKD)로 **자모 단위**가 토큰화되면 학습이 난해해질 수 있음.  
> - 기본 NFKC는 **자모 재조합** 경향이라 비교적 안전하나,  
>   커스텀 사전/규칙을 쓰는 경우 **정규화→디토크나이즈** 일관성 꼭 검증할 것.

---

### 2.5 SentencePiece 사용 예 (파이썬)

```python
# pip install sentencepiece
import sentencepiece as spm
from pathlib import Path

# 1) 코퍼스를 파일로 준비
corpus_txt = Path("corpus.txt")
corpus_txt.write_text(
    "나는 자연어 토크나이저를 공부한다.\n"
    "BPE와 SentencePiece는 서브워드 기반이다.\n"
    "이모지😊와 영어 Mixed-case, 숫자123도 포함!\n"
)

# 2) 유니그램 모델 학습
spm.SentencePieceTrainer.Train(
    input=str(corpus_txt),
    model_prefix="spm_unigram",
    model_type="unigram",
    vocab_size=2000,
    character_coverage=0.9995,   # 한국어/기호 커버
    normalization_rule_name="nmt_nfkc",  # 일반적 안전지대
    input_sentence_size=1000000,  # 샘플링 학습 시 사용
    shuffle_input_sentence=True
)

# 3) 로드 & 인코딩
sp = spm.SentencePieceProcessor()
sp.load("spm_unigram.model")

text = "토크나이저😊를 테스트합니다!!!"
ids = sp.encode(text, out_type=int)        # Viterbi (deterministic)
pieces = sp.encode(text, out_type=str)
print("pieces:", pieces)
print("ids   :", ids)
print("detok :", sp.DecodeIds(ids))

# 4) 서브워드 정규화(샘플 인코딩)
pieces_nbest = sp.SampleEncodeAsPieces(text, nbest_size=8, alpha=0.2)
print("sampled pieces:", pieces_nbest)
```

- `vocab_size`: 서브워드 수(특수토큰 포함)  
- `character_coverage`: `<unk>`를 줄이려면 **높게** (단, 너무 높이면 vocab 낭비 가능)  
- `SampleEncodeAsPieces`: **학습 시** 데이터 증강처럼 사용(훈련 루프에서 on-the-fly)

> **BPE 모드로 학습**하고 싶다면 `model_type="bpe"`로 변경.  
> 단, SentencePiece의 BPE는 **pre-tokenization 없이** raw에서 병합을 학습한다는 점이 일반 BPE 파이프와 차이.

---

### 2.6 유니그램 Viterbi 토큰화(토이 DP) — 내부 원리 데모

```python
# 간단한 Viterbi 세그멘테이션 (교육용)
import math

def viterbi_segment(text, vocab_prob):
    """
    text: 문자열
    vocab_prob: {subword: prob} (logprob 사용 권장)
    반환: (best_score, segmentation)
    """
    N = len(text)
    # dp[i] = (best_neg_logprob upto i, last_token_start)
    dp = [(float("inf"), -1)] * (N+1)
    dp[0] = (0.0, -1)
    # 최대 서브워드 길이 제한(속도)
    max_len = max(len(w) for w in vocab_prob)
    logp = {w: -math.log(p) for w, p in vocab_prob.items()}
    for i in range(1, N+1):
        best = (float("inf"), -1)
        for l in range(1, min(max_len, i)+1):
            w = text[i-l:i]
            if w in logp:
                cand = (dp[i-l][0] + logp[w], i-l)
                if cand[0] < best[0]:
                    best = cand
        dp[i] = best
    # backtrack
    if dp[N][1] == -1:
        return float("inf"), [text]
    seg = []
    i = N
    while i > 0:
        j = dp[i][1]
        seg.append(text[j:i])
        i = j
    seg.reverse()
    return dp[N][0], seg

toy_vocab = {"토":0.1, "크":0.1, "토크":0.2, "나":0.1, "이저":0.05, "나이":0.05, "저":0.05, "를":0.1, "테스트":0.15, "합니다":0.1}
score, seg = viterbi_segment("토크나이저를테스트합니다", toy_vocab)
print(score, seg)
```

- 실제 SentencePiece는 더 크고 복잡한 후보집합/정규화/스코어링/샘플링을 다룬다.  
- 위 코드는 **개념 이해**용으로만 참고.

---

## 3) OOV 처리 전략 총정리

### 3.1 `<unk>`(UNKNOWN) 토큰
- 사전에 없는 토큰/문자가 오면 `<unk>`로 대체 → **정보 손실**  
- 문장에 `<unk>`가 많아지면 모델 성능/생성 품질 급락  
- SentencePiece는 `<unk>`를 기본 탑재하되, **coverage**/**byte fallback**으로 발생률 최소화 권장

### 3.2 Char-Fallback
- BPE 기본형은 **문자 단위**에서 시작 → **단어 OOV 없음**  
- 다만 **문자 사전**에 없는 희귀 유니코드는 `<unk>` → **byte-fallback** or **char coverage**↑

### 3.3 Byte-Fallback / Byte-Level BPE
- 어떤 문자도 **UTF-8 바이트**로 표현 가능 → **절대 실패하지 않는 경로**  
- SentencePiece도 `--byte_fallback=true`(옵션)로 비사전 문자에 대해 바이트 토큰을 사용  
- GPT-류는 **처음부터 바이트-레벨 BPE** → 진짜 OOV=0

### 3.4 BPE-Dropout / Unigram 샘플링
- 인코딩 중 일부 merge를 랜덤하게 **건너뛰어**(**BPE-Dropout**) 다양한 분절을 노출  
- SentencePiece **Unigram 샘플링**과 유사한 효과 → **강건성↑**,  
  희귀/신조어/OOV 근처에서 성능 손실을 줄임

### 3.5 숫자/기호/케밥케이스/URL
- 숫자 묶음(`1234567890`)을 **한 토큰** 또는 일정 길이로 쪼개기(도메인별 규칙)  
- URL/이메일/코드 토큰화 → byte-level이 가장 안전, 아니면 정규식 pre-tokenization으로 **별도 보존**

---

## 4) Hugging Face `tokenizers`로 BPE 학습/사용

> 빠르고 병렬화된 Rust 백엔드를 파이썬에서 사용.  
> (PyTorch와 함께 쓰기 좋고, Triton/ONNX로 내릴 때도 흔히 같이 사용됩니다.)

```python
# pip install tokenizers
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.normalizers import NFKC
from tokenizers.processors import TemplateProcessing

# 1) 토크나이저 골격
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.normalizer = NFKC()
tokenizer.pre_tokenizer = Whitespace()

trainer = BpeTrainer(
    vocab_size=16000, min_frequency=2, special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
)

# 2) 학습
files = ["corpus.txt"]  # 많은 파일 가능
tokenizer.train(files, trainer)

# 3) 후처리: 문장 경계 토큰 등
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[("[CLS]", tokenizer.token_to_id("[CLS]")),
                    ("[SEP]", tokenizer.token_to_id("[SEP]"))]
)

# 4) 사용
enc = tokenizer.encode("BPE 토크나이저를 테스트합니다.")
print(enc.tokens)
print(enc.ids)

# 5) 저장/로드
tokenizer.save("bpe_tokenizer.json")
tok2 = Tokenizer.from_file("bpe_tokenizer.json")
```

- 이 구성은 **공백 기반 pre-tokenization** → 한국어처럼 형태소 경계가 약한 언어에서는  
  공백이 적게 쓰이면 서브워드 길이가 길어지는 경향 → **SentencePiece**(raw 기반)도 검토

---

## 5) PyTorch 파이프라인: 패딩/마스크/특수토큰

```python
# 예시: HF Tokenizers + PyTorch DataLoader
import torch
from torch.utils.data import Dataset, DataLoader

class TxtDataset(Dataset):
    def __init__(self, path, tokenizer, max_len=256):
        self.lines = [l.strip() for l in open(path, encoding="utf-8") if l.strip()]
        self.tok = tokenizer; self.max_len = max_len
        self.pad_id = tokenizer.token_to_id("[PAD]")
    def __len__(self): return len(self.lines)
    def __getitem__(self, i):
        out = self.tok.encode(self.lines[i])
        ids = out.ids[:self.max_len]
        attn = [1]*len(ids)
        return torch.tensor(ids), torch.tensor(attn)

def collate_pad(batch, pad_id):
    # batch: list of (ids, attn)
    ids, attn = zip(*batch)
    maxL = max(x.size(0) for x in ids)
    def pad(x): 
        return torch.cat([x, torch.full((maxL - x.size(0),), pad_id, dtype=torch.long)])
    ids = torch.stack([pad(x) for x in ids])
    attn= torch.stack([torch.cat([a, torch.zeros(maxL - a.size(0), dtype=torch.long)]) for a in attn])
    return ids, attn

tokenizer = Tokenizer.from_file("bpe_tokenizer.json")
dataset = TxtDataset("corpus.txt", tokenizer)
loader = DataLoader(dataset, batch_size=8, shuffle=True,
                    collate_fn=lambda b: collate_pad(b, tokenizer.token_to_id("[PAD]")))

for ids, attn in loader:
    # ids: [B, T], attn: [B, T]
    # 모델 입력으로 그대로 사용 (예: Transformer)
    break
```

- **특수토큰 설계**: `[PAD] [UNK] [CLS] [SEP] [MASK]`(MLM) / `<s> </s>`(GPT류) 등 **모델에 맞게**  
- **Truncation/Stride**: 길이 초과 시 **슬라이딩 윈도** 전략으로 문맥 보존  
- **Detokenization 일관성**: 저장/로드, 정규화 규칙 변경 시 **회귀 테스트**

---

## 6) 한국어·이모지·혼합 데이터에서의 설계 팁

1) **문자 커버리지**: SentencePiece `character_coverage=0.9995~0.9999` 권장  
2) **정규화**: `nmt_nfkc`(안전지대). 자모 분해/조합 규칙 **임의 변경 금지**(Detok 위험)  
3) **숫자/기호**: 자주 나오는 단위/접두사(`kg`, `%`, `-`)는 병합되도록 코퍼스/빈도 확보  
4) **이모지/이국어 글자** 많으면: **byte-level BPE** 또는 SP **byte_fallback=true**  
5) **공백 토큰화만**으로 충분한지 검토. 한국어는 공백이 문법 경계를 완벽히 반영하지 않음 →  
   **SentencePiece(Unigram)**가 더 안정적일 때가 많다.  
6) **Detok 가독성**이 중요(요약/대화)하면: 공백/구두점 붙이는 규칙을 **post-processor**에서 명시

---

## 7) 품질/성능 측정 체크리스트

- **토큰당 문자수 / 문장당 토큰수** 분포:  
  - $$\mathrm{bpc}=\frac{\text{총 바이트}}{\text{총 토큰}}$$, $$\mathrm{tps}=\frac{\text{총 토큰}}{\text{문장 수}}$$  
  - 도메인에서 **너무 길면** vocab↑ 또는 규칙 개선 필요
- **OOV 비율**: `<unk>` 등장률(문자/토큰 기준). byte fallback이면 ~0  
- **Detok 손실**: 원문 재구성 정확도(공백/이모지 포함)  
- **속도**: 토큰/초(TPS), 멀티스레딩(Tokenizer는 CPU 바운드)  
- **버전 고정**: 모델과 **동일 토크나이저 버전**을 저장(파일 해시/옵션 포함)

---

## 8) 고급 주제 — BPE-Dropout / Mixed Routing / 규칙 결합

- **BPE-Dropout**: 인코딩 시, 각 병합(rule)을 확률 \(p\)로 드롭 → **더 세분화된 분절**을 섞어 학습  
  - SentencePiece의 **Unigram 샘플링**과 유사한 regularization 효과  
- **Mixed Tokenization**: code-switching(한국어+영어+이모지) 상황에는  
  - **영어/코드** 영역: byte-level 선호  
  - **한국어 본문**: SentencePiece(Unigram)  
  - → **스위칭 비용**/디토크 관리 복잡. 보통 **하나의 강건한 토크나이저**로 통일 권장.
- **특수 규칙 주입**: URL/이메일/해시태그는 **사전 분리/보존** 후 서브워드 적용(정규식 pre-tokenization)

---

## 9) 예시 시나리오 — “신조어/이모지 많은 SNS 분류” 설계안

- 요구: **OOV=0**, 빠른 인퍼런스, Detok 중요 낮음  
- 선택: **byte-level BPE(16k~32k)**, NFKC 정규화, 공백 pre-tokenization **없음**  
- 장점: 어떤 이상한 글자도 깨지지 않음  
- 단점: 토큰 시퀀스가 길어질 수 있음 → **모델 context**를 1.2~1.5배 늘리거나,  
  **vocab↑** 및 **BPE-Dropout** 약간 도입으로 완화

---

## 10) 예시 시나리오 — “한국어 QA/RAG” 설계안

- 요구: Detok 가독성, 한국어 본문 정확 분절, 영어·숫자 적당히 대응  
- 선택: **SentencePiece Unigram(32k)**, `character_coverage=0.9995`,  
  `byte_fallback=true`, `nmt_nfkc` 정규화, 학습시 `SampleEncode` 사용  
- 효과: 한국어 본문에서 자연스러운 서브워드, 드물게 등장한 문자/이모지는 바이트 fallback로 안전 처리

---

## 11) 끝으로 — 실전 운영 팁

1) **토크나이저 아카이브에 모든 설정 저장**: vocab/merges/model 파일 + 정규화 규칙 + 버전/해시  
2) **학습/서빙 동일성 테스트**: 100문장 골라 **encode→decode**가 원문과 100% 일치하는지  
3) **데이터 변동 감시**: 새 스크래퍼/언어 추가 시 `<unk>`/fallback 증가 여부를 모니터링  
4) **재학습 시 호환**: 토크나이저 업데이트는 **모델 재학습**과 함께(토큰 경계가 바뀌면 가중치가 흔들림)  
5) **성능 회귀**: 토크나이저 변경 전후로 **downstream 지표**(EM/F1/ROUGE/BLEU 등) 비교

---

## 부록 A. 바이트-레벨 BPE 훈련 스케치(교육용)

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.normalizers import NFKC
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.processors import ByteLevel as ByteLevelProc

tok = Tokenizer(BPE(unk_token="[UNK]"))
tok.normalizer = NFKC()
tok.pre_tokenizer = ByteLevel(add_prefix_space=False)  # 공백도 바이트로 취급
trainer = BpeTrainer(
    vocab_size=32000, min_frequency=2,
    special_tokens=["[PAD]","[UNK]","<s>","</s>"]
)
tok.train(["corpus.txt"], trainer)
tok.post_processor = ByteLevelProc(trim_offsets=False)  # Detok 일관성

enc = tok.encode("이모지😊 + Arabic: سلام + CJK: 漢字")
print(enc.tokens)  # 바이트 기반 병합 결과
```

---

## 부록 B. “숫자·URL 유지” 프리토크나이저(정규식) 예시

```python
import re
PAT = re.compile(r"(https?://\S+|[\w\.-]+@[\w\.-]+|\d[\d,\.]*%?)")

def regex_pretokenize(text):
    parts = []
    last = 0
    for m in PAT.finditer(text):
        if m.start() > last:
            parts += text[last:m.start()].split()  # 나머지는 공백 분리
        parts.append(m.group(0))  # URL/이메일/숫자 덩어리 보존
        last = m.end()
    if last < len(text):
        parts += text[last:].split()
    return parts

print(regex_pretokenize("가격은 12,345원(약 10.5%)이고 웹사이트는 https://ex.am/ple"))
```

이렇게 **특수 토큰 덩어리**는 보존하고, 나머지에 BPE/SentencePiece를 적용하면  
URL/숫자 같은 패턴이 **<unk> 없이** 안정적으로 다뤄진다.

---

# 마무리

- **BPE**: 빈도 높은 쌍을 병합해 **짧은 코드 길이**를 달성. **문자 기반**이라 단어 OOV는 없음.  
  **Byte-level BPE**로 가면 유니코드 전체를 덮어 **진짜 OOV=0**까지 가능.  
- **SentencePiece(Unigram)**: **확률적 분절**로 데이터에 맞는 서브워드를 학습,  
  **샘플링(서브워드 정규화)**로 **강건성** 향상. 정규화/coverage/byte fallback 옵션으로 OOV 제어.  
- **OOV 전략**은 제품 품질에 직결: `<unk>` 최소화, fallback 경로 확보, Detok 회귀 테스트를 **항상 자동화**하세요.  
- 한국어/이모지/멀티링구얼에서는 **byte-fallback** 또는 **byte-level**을 안전한 기본값으로 두고,  
  필요 시 **SentencePiece Unigram**으로 토큰 효율을 끌어올리는 **하이브리드 사고**가 좋습니다.

실제 업무에서는 위 코드들을 조합해 **토크나이저 학습 스크립트 → 저장 → PyTorch 파이프라인 연동 → 회귀테스트**까지  
**일관된 빌드 체인**을 만들어 두는 것이, 모델 정확도만큼이나 중요합니다.  
이렇게 해두면 신조어/새로운 데이터가 들어와도 **겁먹지 않는 인코딩**을 유지할 수 있어요.