---
layout: post
title: 객체지향설계 - 피드백을 통한 지속적 개선
date: 2025-07-25 23:20:23 +0900
category: 객체지향설계
---
# 피드백을 통한 지속적 개선 (Continuous Improvement via Feedback)

## 0. 한눈 요약 (핵심 문장 5개)
1. **짧고 신뢰도 높은 피드백 루프**를 설계하면, 작은 변경을 빠르게 검증·학습·반영할 수 있다.  
2. 루프는 **계측→수집→탐지→분류→조치→측정→반영**의 닫힌 고리여야 한다.  
3. 제품/운영/코드 품질 피드백을 **데이터 계약**과 **표준 지표**로 통합해 비교 가능하게 만든다.  
4. 배포는 **작게·점진적(FF/카나리)**, 실험은 **가설·지표·기간**이 명확해야 한다.  
5. **블레임리스 문화**와 **런북/포스트모템**이 없으면 피드백은 학습으로 연결되지 않는다.

---

## 1. 피드백 기반 개선의 원리

- **제어 이론 관점**: 현재 상태(출력)를 관측→오차 계산→제어 입력(개선)을 가한다. 루프 지연이 짧을수록 안정 제어가 가능하다.  
- **소프트웨어 관점**: 코드·운영·제품 신호를 빠르게 수집·해석·행동하여 **학습 속도**를 최대화한다.

루프의 기대 개선 속도를 단순화하면:

$$
\text{LearningRate} \propto \frac{\text{Signal Quality}}{\text{Loop Time}} \times \text{Actionability}
$$

- Signal Quality ↑ (계측 품질·표준화), Loop Time ↓ (CI 속도·릴리즈 소형화), Actionability ↑ (가설·책임자·런북).

---

## 2. 받아야 할 신호(피드백 유형 확장)

| 범주 | 예시 신호 | 목적 | 표준화 팁 |
|---|---|---|---|
| 개발 | PR 리뷰, 정적 분석, 커버리지, 린트 | 코드 품질 | PR 체크리스트, 최소 기준 강제 |
| 테스트 | 단위/통합/계약/E2E 결과, 뮤테이션 점수 | 회귀 방지 | 속도·결정성 관리, flaky 제거 |
| 관찰성 | 로그/메트릭(p95/p99)/트레이스, 에러(RUM+Sentry) | 가용성/성능 | 공통 태그(서비스·버전·릴리즈) |
| 사용자 | NPS/CSAT, 인터뷰, 세션 리플레이 | 사용성 | 샘플 편향 방지, 동의·익명화 |
| 제품 | 전환율, DAU/MAU, 기능 채택률 | 가치 검증 | 이벤트 스키마 계약(버전링) |
| 비즈 | 매출/COGS, CAC/LTV, 규제 리포트 | 지속성 | 재무·규제 데이터 파이프라인 |
| 프로세스 | DORA, PR 대기시간, 롤백 수 | 팀 효율 | 주간 대시보드·회고 입력 |

---

## 3. 피드백 루프 설계(7단계)

1) **계측(Instrument)** → 2) **수집(Collect)** → 3) **탐지(Detect)** →  
4) **분류·우선순위(Triage)** → 5) **조치(Act/Experiment)** →  
6) **측정(Measure)** → 7) **반영(Share & Learn)**

### 3.1 아키텍처(개념도)

```
[App/Service] --(OTel/SDK)--> [Collector/Agent]
      --> [Queue/Kafka] --> [Storage: Metrics/Logs/Traces/Lake]
      --> [Dashboards/Alerting] --> [Pager/Slack/Tickets]
                               --> [Exp Platform/FF] -> [Rollout/Canary]
                          <-- [Postmortem/ADR/Runbook Knowledge Base]
```

### 3.2 데이터 계약(이벤트 스키마) 예
- 공통 필드: `service`, `env`, `version`, `release`, `user_id`(해시), `session_id`, `trace_id`, `feature_flags`.
- 이벤트 버전링: `event_name:v2` 식으로 **하위 호환** 원칙 문서화.

---

## 4. 계측(코드 예제)

### 4.1 Java (Micrometer + OTel)

```java
// build.gradle: io.micrometer, opentelemetry-exporter-otlp 의존성 가정
Counter checkoutCounter = Counter.builder("app.checkout.success")
    .tag("service", "checkout").register(Metrics.globalRegistry);

Timer timer = Timer.builder("http.server.requests")
    .tag("path", "/api/checkout").register(Metrics.globalRegistry);

// 사용
checkoutCounter.increment();
timer.record(() -> service.checkout(req));
```

### 4.2 Python (FastAPI + structlog)

```python
import structlog, time
log = structlog.get_logger(service="catalog", env="prod")

def timed(fn):
    def w(*a, **kw):
        t=time.time()
        try: return fn(*a, **kw)
        finally: log.info("latency_ms", fn=fn.__name__, ms=int((time.time()-t)*1000))
    return w

@timed
def list_products(category): ...
```

### 4.3 C# (.NET, OpenTelemetry)

```csharp
var builder = WebApplication.CreateBuilder(args);
builder.Services.AddOpenTelemetry()
  .WithMetrics(m => m.AddAspNetCoreInstrumentation().AddPrometheusExporter())
  .WithTracing(t => t.AddAspNetCoreInstrumentation().AddOtlpExporter());
```

---

## 5. 수집 파이프라인(운영 스니펫)

### 5.1 GitHub Actions - PR 빠른 피드백

```yaml
name: ci-pr
on: [pull_request]
jobs:
  fast-checks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with: { distribution: temurin, java-version: 21 }
      - run: ./gradlew clean test -x integrationTest
      - run: ./gradlew checkstyleMain pmdMain spotbugsMain
```

### 5.2 Prometheus Alerting (SLO 기반)

{% raw %}
```yaml
groups:
- name: api-slo
  rules:
  - alert: ErrorBudgetBurnHigh
    expr: (sum(rate(http_requests_total{status=~"5.."}[5m])) by (service))
          /
          (sum(rate(http_requests_total[5m])) by (service)) > 0.05
    for: 15m
    labels: {severity: page}
    annotations:
      runbook: "https://kb/runbooks/api-slo"
      summary: "5xx > 5% for 15m on {{ $labels.service }}"
```
{% endraw %}

---

## 6. 탐지·분류·우선순위

### 6.1 심각도/영향 행렬

| Severity | 설명 | 예 |
|---|---|---|
| P0 | 전체 장애/데이터 손실 | 로그인 전체 불가 |
| P1 | 핵심 기능 큰 영향 | 결제 실패율 급증 |
| P2 | 부분 기능 문제 | 특정 브라우저 오류 |
| P3 | 경미/개선 | UI 정렬 불량 |

### 6.2 티켓 템플릿(요약)

```
Type: BUG/PERF/INCIDENT | Severity: P0..P3 | Env: prod/stage
Summary, Steps, Expected/Actual, Evidence(log/trace), Impact, Owner, ETA
```

---

## 7. 조치(Act) — 리팩토링·설정·실험

### 7.1 Feature Flag(런타임 토글)

```java
public interface FeatureFlags { boolean enabled(String key, String userId); }

if (flags.enabled("checkout.v2", user.id())) {
    // new flow
} else {
    // old flow
}
```

### 7.2 카나리 배포(Kubernetes)

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
spec:
  http:
  - route:
    - destination: { host: api, subset: v1 }  weight: 90
    - destination: { host: api, subset: v2 }  weight: 10
```

---

## 8. 측정(Measure) — 실험·품질 지표

### 8.1 A/B 테스트 핵심 수식

- 표본 크기(대략치):

$$
n \approx 2 \cdot \frac{(z_{1-\alpha/2} + z_{1-\beta})^2 \cdot \bar{p}(1-\bar{p})}{\Delta^2}
$$

- 전환율 \(p_A, p_B\)의 차이 유의성(근사 z-test):

$$
z = \frac{p_B - p_A}{\sqrt{\bar{p}(1-\bar{p})(\frac{1}{n_A}+\frac{1}{n_B})}}
,\quad \bar{p}=\frac{n_A p_A + n_B p_B}{n_A+n_B}
$$

### 8.2 에러 예산(Error Budget) & 번율(Burn Rate)

- 월 SLO(가용성) 99.9% → 에러 예산 \(= 0.1\%\).  
- 번율:

$$
\text{BurnRate} = \frac{\text{Observed Error Rate}}{\text{Allowed Error Rate}}
$$

- 4시간 창에 BurnRate>2 → **즉시 완화** 트리거.

---

## 9. 반영(Share & Learn) — 문서화·지식 축적

- **포스트모템**: 요약, 영향, 타임라인, 근본원인, 단기/장기 조치, 학습.  
- **ADR(Architecture Decision Record)**: 의사결정 맥락·대안·결정·결과 기록.  
- **런북**: 경보별 대응 절차(점검 순서, 롤백, 연락처).

---

## 10. 제품/데이터 분석 실무 예

### 10.1 제품 이벤트 SQL(채택률)

```sql
-- 7일 이내 기능 채택률
WITH users AS (
  SELECT DISTINCT user_id FROM events WHERE event_date >= current_date - 7
),
adopt AS (
  SELECT user_id FROM events
  WHERE event = 'feature_x_used' AND event_date >= current_date - 7
)
SELECT
  (SELECT count(*) FROM adopt)::float / (SELECT count(*) FROM users) AS adoption_rate;
```

### 10.2 전환 퍼널(단계별 이탈)

```sql
WITH s AS (...) -- step1
, t AS (...) -- step2 (join with s)
, u AS (...) -- step3 (join with t)
SELECT
  count(*) s1, (SELECT count(*) FROM t) s2, (SELECT count(*) FROM u) s3;
```

---

## 11. DORA + 운영 지표

| 지표 | 정의 | 계산 힌트 |
|---|---|---|
| 배포 빈도 | 일정 기간 배포 수 | CD 로그 집계 |
| 변경 리드타임 | 첫 커밋→프로덕션 배포까지 | Git+CD 타임스탬프 |
| 변경 실패율 | 배포 중/후 롤백·핫픽스 비율 | 배포 메타데이터 |
| MTTR | 장애→복구 소요 | Incident 타임라인 |

MTTR 근사:

$$
\text{MTTR} = \frac{1}{N}\sum_{i=1}^{N} (t^{\text{recover}}_i - t^{\text{incident}}_i)
$$

---

## 12. 경보 노이즈 튜닝(실전)

- 지표: **정밀도/재현율** 유사 개념으로 경보 품질 측정.  
- 규칙: 지연(For), 히스테리시스(상승/하강 임계), 집계 윈도 조정, **페이지는 P0/P1만**.  
- 소유자/런북 없는 경보는 제거.

---

## 13. 보안·프라이버시

- 로그 **민감정보 금지**(PII 마스킹·토큰화·필터).  
- 데이터 사용 목적·보존기간·권한 관리 문서화.  
- 세션 리플레이/UX 도구는 **명시적 동의**와 샘플링.

---

## 14. 엔드투엔드 시나리오(요약)

**가설**: 새 결제 흐름(Checkout v2)이 전환율을 +2%p 올린다.  
**계측**: `checkout_start`, `checkout_success`, `feature_flags` 기록.  
**배포**: 카나리 10% → 에러율/latency 가드레일 모니터.  
**실험**: AB(FF 기준 분배), 7일, 기본 지표 전환율/가드레일 에러율.  
**결과**: z-test 유의, BurnRate<1.0 유지.  
**반영**: 100% 롤아웃, 포스트모템(이슈 없음) 대신 ADR 기록.

---

## 15. 30/60/90 실행 계획

- **0–30일**: 이벤트 스키마 계약·공통 태그, PR 빠른 체크 파이프라인, 핵심 SLO 정의.  
- **30–60일**: 대시보드/경보 정비(런북 링크), FF 도입, 카나리 템플릿 배포.  
- **60–90일**: AB 실험 프로세스 정착, 포스트모템/ADR 루틴화, DORA 대시보드 주간 공유.

---

## 16. 체크리스트(운영·제품·개발 통합)

- [ ] 계측이 **표준 스키마**로 일관화되어 있는가?  
- [ ] CI가 PR에서 **분 단위** 피드백을 주는가?  
- [ ] 경보마다 **소유자·런북**이 연결되어 있는가?  
- [ ] 배포는 **작게/점진적**이고 롤백이 즉시 가능한가?  
- [ ] 실험은 가설/지표/기간/파워가 정의되었는가?  
- [ ] DORA와 제품 핵심 지표가 **주간 공유**되는가?  
- [ ] 포스트모템이 **블레임리스**로 정기 운영되는가?  
- [ ] 개인정보는 계측·로그에서 **마스킹/최소화**되는가?

---

## 17. 템플릿 모음 (복붙용)

### 17.1 실험(AB) 템플릿

```
Hypothesis: If we [change X], then [metric Y] will improve by Z%.
Primary Metric: (definition, window)
Guardrails: error_rate < 1%, p95 < 500ms
Population & Duration: N, days
Analysis Plan: z-test / Bayesian, min detectable effect
Rollout/Stop: success criteria / rollback criteria
Owner: Team/Person, Due
Notes:
```

### 17.2 포스트모템(블레임리스)

```
Title, When, Severity(P0..P3)
Summary
Impact (scope, duration)
Timeline (UTC+시간대 명시)
Root Cause (systemic)
Mitigation (short-term)
Long-term Actions (Owner, Due)
Learnings & Prevention
```

---

## 18. 도구 레퍼런스(선정 기준)

- **관찰성**: Prometheus+Grafana(자체 호스팅) / Datadog(NewRelic)  
- **에러**: Sentry/Rollbar(릴리즈별 이슈 묶음)  
- **실험/FF**: LaunchDarkly/Split.io / 오픈소스는 OpenFeature+OpenFF  
- **로그**: Loki / ELK / OpenSearch  
- **CI**: GitHub Actions/GitLab CI, 캐시 최적화(Gradle/NPM)  
- **분석**: Amplitude/Mixpanel/PostHog, 쿼리 레이크(Snowflake/BigQuery)

---

## 19. 마무리

피드백은 **센서**, 루프는 **근육**, 문서화는 **기억**이다.  
작은 배치·짧은 루프·명확한 가드레일·블레임리스 학습이 결합될 때, 팀은 실패 비용을 최소화하면서 학습 속도를 극대화한다. 오늘 한 가지 신호의 **계측→조치→측정→공유**를 끝까지 닫아보라. 그것이 조직 전체의 지속적 개선을 여는 첫 고리다.