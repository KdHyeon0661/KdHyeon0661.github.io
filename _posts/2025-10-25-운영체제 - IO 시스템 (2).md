---
layout: post
title: 운영체제 - IO 시스템 (2)
date: 2025-10-25 23:25:23 +0900
category: 운영체제
---
# Kernel I/O Subsystem, Request → Hardware, STREAMS, Performance

## Kernel I/O Subsystem

### 커널 I/O의 큰 그림

```
App ──▶ libc/rt ──▶ Syscall ──▶ VFS/Socket/Char ──▶ FS/Net/Char Driver
                                      │
                                      ├─▶ Page Cache / Buffer Cache / Readahead / Writeback
                                      └─▶ Block Layer (request queue, schedulers, blk-mq)
                                              └─▶ Bus/Device Driver ──▶ Controller ──▶ DMA/IRQ
```

- **VFS (Virtual File System)**: inode/dentry/파일 연산 추상화로 ext4, XFS, NTFS, APFS 등을 통일 인터페이스로 노출.
- **페이지 캐시(Page Cache)**: 파일 데이터 캐시(읽기 히트·리드어헤드, 쓰기 지연·writeback).
- **버퍼 캐시(Buffer Cache)**: 블록 디바이스의 메타블록 캐시(현대 Linux는 페이지 캐시와 통합 관리).
- **블록 계층(Block Layer)**: 요청 병합/정렬/스케줄링, 멀티큐(blk-mq).
- **캐릭터(문자) 드라이버**: 터미널·센서·GPU 등 스트림성 장치(블록 스케줄링 없음).
- **네트워크 스택**: 소켓 API → TCP/UDP/QUIC → NIC 드라이버(NAPI, XDP/AF_XDP).

> 핵심 목표: **정합성(무결성/내구)**, **성능(지연·처리량)**, **격리(QoS/보안)**, **관측 가능성**의 균형.

---

### VFS 핵심 자료구조(간단 스케치)

- **super_block**: 파일시스템 인스턴스(마운트 1개).
- **inode**: 파일 메타데이터(크기, 권한, 블록 맵, 타임스탬프).
- **dentry**: 경로명 캐시(디렉터리 엔트리) — lookup 가속.
- **file**: 열려 있는 파일 핸들(오프셋, 플래그, f_op).

#### (개념 코드) 간단 VFS‐like 디스패치

```c
// vfs_like.h — 개념용(진짜 커널코드 아님)
typedef struct file_ops {
  ssize_t (*read)(void* inode, void* buf, size_t len, off_t *pos);
  ssize_t (*write)(void* inode, const void* buf, size_t len, off_t *pos);
  int     (*fsync)(void* inode);
} file_ops;

typedef struct file {
  void *inode;
  file_ops *fop;
  off_t pos;
} file;

ssize_t vfs_read(file* f, void* buf, size_t len) {
  return f->fop->read(f->inode, buf, len, &f->pos);
}
```

---

### 페이지 캐시 / readahead / writeback

- **읽기**: 페이지 캐시 히트 → 즉시 반환. 미스 → 백엔드 블록 I/O 발행 → 완료 시 캐시에 적재.
- **리드어헤드**: 순차 패턴을 감지하면 **앞쪽 페이지**를 미리 가져와 히트율↑.
- **쓰기**: dirty 페이지를 메모리에 모아두고 **writeback 데몬**(Linux: `flush-*`)이 정해진 정책으로 디스크 반영.
- **동기화**: `fsync()`/`fdatasync()`는 해당 파일의 dirty 페이지/저널 커밋을 보장.

#### (리드어헤드 직관 모델)

도달 확률이 \(p\)인 선행 \(k\)페이지를 당겨오면 기대 히트 이득은
$$
E[\text{hit}] \approx \sum_{i=1}^{k} p^i
$$
메모리/대역폭 비용과 **균형점**을 찾는 것이 튜닝 포인트.

---

### 블록 계층: 요청 큐와 blk-mq

- **요청 병합(merge)**: 인접 LBA 요청 결합 → 큰 I/O로 장치 효율↑.
- **plugging**: 잠시 큐에 묶어두었다가 한꺼번에 제출 → 병합 기회↑.
- **스케줄러**: HDD(위치의존)에는 정렬형(Deadline/BFQ), NVMe에는 지연 제어형(mq-deadline/Kyber) 선호.
- **blk-mq (multi-queue)**: CPU/NUMA 코어별 제출·완료 큐로 락 경합↓, IRQ를 큐/코어에 바인딩.

#### (개념 코드) 요청 병합/정렬

```python
# rq_merge.py — 인접 LBA 병합 예시

reqs = [(100,4),(104,4),(200,8),(208,8)]  # (LBA, nr_pages)
merged=[]
for lba,n in sorted(reqs):
    if merged and merged[-1][0]+merged[-1][1]==lba:
        merged[-1]=(merged[-1][0], merged[-1][1]+n)
    else:
        merged.append((lba,n))
print("merged:", merged)  # [(100,8),(200,16)]
```

---

### 보호/보안/전원

- **권한/ACL**: VFS가 DAC/MAC(SELinux, AppArmor) 적용.
- **무결성**: FS 저널, 체크섬(Btrfs/ZFS), FUA/flush로 순서 보장.
- **전원관리**: ALPM/NVMe APST(자율 전원 상태 전환), 링크 절전, 인터럽트 coalescing 조절.

---

### 스풀링(Spooling)과 이름 있는 파이프

- 프린터/배치 잡은 **스풀 디렉터리**에 큐잉 → 백엔드 서브시스템이 순차 처리, 실패 재시도/순서 제어.

---

## Transforming I/O Requests to Hardware Operations

### 경로: 사용자 호출 → 하드웨어

1) **Syscall**: `read()` / `write()` / `io_uring` 제출
2) **VFS/FS**: 오프셋→블록 매핑, 캐시 확인, dirty/clean 판정
3) **BIO/req**: 논리 블록 요청 생성, 병합·정렬·plugging
4) **스케줄러/blk-mq**: 큐 할당, 우선순위/데드라인 적용
5) **드라이버**: 장치 명령(SCSI/NVMe) 구성, **SG-list** → DMA 맵핑(IOMMU)
6) **컨트롤러**: 명령 수신→DMA 수행
7) **완료**: DMA → MSI-X 인터럽트 → 완료 큐 → 상위로 완료 전파

---

### SCSI vs NVMe 명령 구성

- **SCSI**: CDB(Command Descriptor Block) + LUN → HBA/어레이가 내부로 매핑.
- **NVMe**: **Submission Queue Entry (SQE)** 에 opcode/NSID/SLBA/NumBlocks/PRP(SGL)를 써 넣음. 완료는 **CQE**.

```c
// nvme_sqe_concept.c — (설명용) NVMe 읽기 SQE 필드 개념
struct nvme_sqe_read {
  uint8_t  opcode;  // 0x02 (READ)
  uint8_t  flags;
  uint16_t cid;     // command id
  uint32_t nsid;    // namespace id
  uint64_t rsvd2;
  uint64_t mptr;    // metadata ptr
  uint64_t prp1;    // data ptr 1
  uint64_t prp2;    // data ptr 2 or PRP list
  uint64_t slba;    // starting LBA
  uint16_t nlb;     // # of logical blocks - 1
  // ...
};
```

- **PRP/SGL**: 물리적으로 흩어진 유저 버퍼를 **연결 리스트**로 전달 → 장치가 SG-DMA 수행.
- **배리어**: FLUSH/FUA 비트로 내구 보장(저널/DB 필수).

---

### IOMMU와 Scatter/Gather

- 유저 버퍼를 **핀**(pin)하고 **DMA-safe** 주소(IOVA)를 매긴 후 **SG-list** 생성.
- IOMMU 테이블은 장치별 **격리**를 제공(오작동/공격 완화).

---

### 인터럽트 처리와 하위 반

- **ISR(Top half)**: CQE를 훑어 완료 인덱스 갱신, 타임스탬프 기록.
- **Bottom half**: 완료 처리(상위 레이어 통지), 다음 요청 제출.
- **Coalescing**: 일정 시간/개수 단위로 **인터럽트 묶기** → CPU 절약 vs p99 트레이드오프.

---

### Flush/Write Ordering

- 로그/WAL/저널 순서를 맞추기 위해
  1) **데이터 쓰기**
  2) **배리어(FLUSH/FUA)**
  3) **메타데이터 커밋**
- 잦은 flush는 처리율↓, tail↑. **그룹 커밋**으로 완화.

---

### 실습: io_uring으로 “파일→소켓” Zero-Copy 파이프라인(개념)

```c
// uring_sendfile_like.c — 하나의 링에서 read→send 연쇄(링크드 SQE)
// 빌드: gcc uring_sendfile_like.c -luring
#define _GNU_SOURCE
#include <liburing.h>
#include <fcntl.h>
#include <unistd.h>
#include <string.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/socket.h>
#include <arpa/inet.h>

int main(int argc, char** argv){
  if(argc<3){ fprintf(stderr,"usage: %s <file> <ip:port>\n",argv[0]); return 1; }
  char ip[64]; int port; sscanf(argv[2], "%63[^:]:%d", ip, &port);
  int s=socket(AF_INET,SOCK_STREAM,0);
  struct sockaddr_in dst={.sin_family=AF_INET,.sin_port=htons(port)}; inet_pton(AF_INET, ip, &dst.sin_addr);
  connect(s,(struct sockaddr*)&dst,sizeof(dst));

  int fd=open(argv[1],O_RDONLY);
  struct io_uring ring; io_uring_queue_init(256,&ring,0);
  const size_t BS=1<<17; void* buf=aligned_alloc(4096, BS);
  off_t off=0; for(;;){
    struct io_uring_sqe* r=io_uring_get_sqe(&ring);
    io_uring_prep_read(r, fd, buf, BS, off);
    r->flags |= IOSQE_IO_LINK;  // 다음 SQE와 링크: read 끝나면 send 수행
    struct io_uring_sqe* w=io_uring_get_sqe(&ring);
    io_uring_prep_send(w, s, buf, BS, 0);
    io_uring_submit(&ring);
    struct io_uring_cqe* cqe; io_uring_wait_cqe(&ring,&cqe);
    int n=cqe->res; io_uring_cqe_seen(&ring,cqe);
    if(n<=0) break; off += n;
    // 두 번째 cqe는 send 결과
    io_uring_wait_cqe(&ring,&cqe); io_uring_cqe_seen(&ring,cqe);
  }
  free(buf); close(fd); close(s); io_uring_queue_exit(&ring);
  return 0;
}
```

---

## STREAMS

### STREAMS란?

- **System V STREAMS**: **문자 장치/네트워크 프로토콜**을 **모듈 체인**으로 구성하는 프레임워크.
- 구성 요소:
  - **Stream head**: 사용자 인터페이스(`/dev/...` 파일을 통한 `read/write/ioctl`).
  - **Modules**: 필터/프로토콜 레이어(TTY 라인 규율, PPP, TCP 등).
  - **Driver**: 하단 장치.
  - **Queues**: 각 모듈마다 **read-side / write-side** 큐.
  - **Messages(Mblk)**: 데이터/제어 메시지 타입으로 통과.

```
[App] ─ stream head ─ M1 ─ M2 ─ ... ─ Driver
             ▲  ▲
      read queue write queue  (각 모듈마다 존재)
```

- **동적 구성**: `I_PUSH`/`I_POP` ioctl로 모듈을 **런타임에 삽입/제거**.
- **흐름 제어**: 큐 하이워터/로워워터 기반 back-pressure.

> 현대 Linux는 **네이티브 STREAMS를 채택하지 않았고**, Solaris/illumos, AIX 등에서 역사적으로 널리 사용. BSD는 `tty`/소켓 계층을 별도 방식으로 발전.

---

### STREAMS 메시지 타입(개략)

- **M_DATA**: 데이터 페이로드
- **M_PROTO/M_PCPROTO**: 프로토콜 제어
- **M_FLUSH**: 큐 플러시
- **M_IOCTL**: 제어 연산 전달
- **M_ERROR/M_HANGUP**: 에러/끊김

#### (개념 코드) 간이 STREAMS 모듈 파이프 시뮬

```python
# streams_pipe.py — 메시지 큐로 모듈 체인 스케치

from collections import deque

class Msg:  # M_DATA만 사용
    def __init__(self, data): self.data=data

class Module:
    def __init__(self, name, fn):
        self.name=name; self.fn=fn
        self.q_in=deque(); self.q_out=None
    def push(self, msg):
        self.q_in.append(msg)
    def run(self):
        while self.q_in:
            m=self.q_in.popleft()
            out=self.fn(m)   # 변환/필터
            if self.q_out and out: self.q_out.push(out)

def upper(m): return Msg(m.data.upper())
def add_crc(m): return Msg(m.data + b"|crc")

# 구성: head -> upper -> add_crc -> driver(sink)

head=Module("head", lambda m: m)
m1=Module("upper", upper); m2=Module("crc", add_crc); drv=Module("drv", lambda m: m)

head.q_out=m1; m1.q_out=m2; m2.q_out=drv
head.push(Msg(b"hello"))
for mod in (head,m1,m2,drv): mod.run()
print(drv.q_in[0].data)  # b'HELLO|crc'
```

---

### STREAMS vs 소켓/네트워크 스택

- **STREAMS 장점**: 모듈러 파이프, 제어/데이터의 메시지화, 흐름 제어 통일.
- **소켓 계층 장점**: POSIX 보급, 단순 FD API, 플랫폼 전반의 최적화(eBPF/XDP/DPDK 등 연계).
- 현대 시스템은 **소켓+Netfilter/BPF** 또는 **io_uring + ULP(TLS)** 와 같은 조합으로 모듈성을 해결.

---

## Performance

### 관측 지표

- **지연/처리량/큐 길이/이용률**
  $$L=\lambda W$$ (Little)
  M/M/1에서:
  $$
  W_q=\frac{\rho}{\mu-\lambda}, \quad \rho=\frac{\lambda}{\mu}<1
  $$
- **히트율**(페이지 캐시, D-cache/TLB), **readahead 효율**, **dirty ratio**.
- **p99/p999 지연**: 큐딥/GC/flush/인터럽트 동조로 민감.

### 도구 모음(리눅스 예)

- **`iostat -x`, `vmstat`, `sar`, `pidstat`**: I/O, CPU, 메모리 개괄.
- **`blktrace`/`btt`**: 블록 레벨 타임라인/병합/큐잉.
- **`perf`, `ftrace`, `trace-cmd`**: 커널 함수, 스케줄, 인터럽트.
- **eBPF/bcc/bpftrace**: **특정 경로**(readahead, writeback, rq merge, NVMe CQE) 추적.

```bash
# blktrace 예

sudo blktrace -d /dev/nvme0n1 -o - | sudo blkparse -i -
# bpftrace 예: 블록 layer 요청 완료 지연

sudo bpftrace -e 'tracepoint:block:block_rq_complete { @lat[args->dev] = hist(nsecs - @start[args->rq]); } tracepoint:block:block_rq_issue { @start[args->rq] = nsecs; }'
```

### 병목 패턴과 개선 전략

1) **작은 랜덤 쓰기** → RAID-5/6 RMW 지옥 → **정렬/배치(128KiB+)**, RAID-10 고려
2) **flush 폭탄**(WAL/fsync 빈번) → **그룹 커밋**/`commit=` 주기 조정, 저널 모드 점검
3) **인터럽트 폭주** → **coalescing**/NAPI 큐 배치, 폴링 전환
4) **큐딥 과다** → 평균 대역폭↑지만 **p99↑** → SLO 기반 최적 QD 탐색
5) **페이지 캐시 thrash** → `O_DIRECT` 또는 readahead/dirty 제한 튜닝
6) **NUMA 원격 접근** → IRQ/큐/스레드 **노드 로컬화**
7) **장치 내부 GC 간섭**(SSD) → 주기적 `fstrim`, 쓰기 정렬, 과도한 fill-to-full 회피(여유 공간 유지)

---

### 간단 큐잉 시뮬: 이용률과 p99

```python
# mm1_tail.py — M/M/1 평균/분산과 tail 직관

import random, math
def simulate(lmbda=900, mu=1000, n=200000):
    t=0.0; busy_until=0.0; waits=[]
    for _ in range(n):
        t += random.expovariate(lmbda)             # arrival
        svc = random.expovariate(mu)               # service
        start = max(t, busy_until)
        waits.append((start - t) + svc)            # sojourn time
        busy_until = start + svc
    waits.sort()
    avg = sum(waits)/len(waits)
    p99 = waits[int(0.99*len(waits))]
    return avg, p99
for u in (0.5,0.7,0.85,0.9,0.95,0.98):
    mu=1000; l=mu*u
    avg,p99=simulate(l,mu)
    print(f"ρ={u:.2f}  E[T]={avg*1e3:.2f}ms  p99={p99*1e3:.2f}ms")
```
**관찰**: \(\rho\)가 1에 가까워질수록 p99가 급증 → **큐딥/배치/coalescing** 등 모든 최적화는 **이용률 관리**와 세트로 가야 함.

---

### 파일 I/O 실험: readahead vs O_DIRECT

```c
// read_profile.c — 순차/랜덤, readahead on/off, O_DIRECT 비교(요약형)
#define _GNU_SOURCE
#include <fcntl.h>
#include <sys/time.h>
#include <sys/stat.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

static double now(){ struct timeval tv; gettimeofday(&tv,0); return tv.tv_sec+tv.tv_usec/1e6; }

int main(int argc,char**argv){
  if(argc<3){ fprintf(stderr,"usage: %s <file> <mode:seq|rand> [direct]\n",argv[0]); return 1; }
  int flags=O_RDONLY; if(argc>3) flags|=O_DIRECT;
  int fd=open(argv[1], flags); if(fd<0){perror("open"); return 1;}
  posix_fadvise(fd,0,0,POSIX_FADV_SEQUENTIAL); // seq 힌트
  size_t BS=1<<20, N=1024; void* buf; posix_memalign(&buf,4096,BS);
  struct stat st; fstat(fd,&st); off_t fsz=st.st_size;
  double t0=now(); size_t cnt=0;
  if(!strcmp(argv[2],"seq")){
    for(off_t off=0; off<fsz && cnt<N; off+=BS, cnt++){
      if(pread(fd,buf,BS,off)<0){perror("pread"); break;}
    }
  } else {
    for(;cnt<N;cnt++){
      off_t off = (off_t)((rand()%(fsz/BS))*BS);
      if(pread(fd,buf,BS,off)<0){perror("pread"); break;}
    }
  }
  double t1=now();
  printf("mode=%s direct=%d MiB/s=%.1f\n", argv[2], !!(flags&O_DIRECT),
         (cnt*BS)/(t1-t0)/1024.0/1024.0);
  free(buf); close(fd); return 0;
}
```

---

### 네트워크 I/O p99 안정화 팁(요약)

- **epoll ET + non-blocking** + **고정 버퍼 풀**(alloc/free 제거).
- **TCP_NODELAY**(소량 지연 민감) vs **Nagle**(소량 묶기) — 워크로드에 맞춤.
- NIC **Interrupt moderation**(rx/tx-usecs)과 **RPS/RSS** 조정으로 코어 부담 분산.
- **io_uring** `SQPOLL`/링크드 -SQE로 시스템콜/컨텍스트 전환 최소화.

---

## 실전 체크리스트(한 장 요약)

1) **접근 패턴 파악**: 순차/랜덤, 읽기/쓰기 비율, I/O 크기, 내구 요구.
2) **API 선택**: `mmap`/`readv`/`sendfile`/`io_uring` 등 워크로드 적합 조합.
3) **캐시 정책**: readahead/dirty ratio/`O_DIRECT`/`posix_fadvise` 힌트.
4) **블록 스케줄러/큐딥**: NVMe는 mq-deadline/Kyber·QD 실험, HDD는 Deadline/BFQ.
5) **배리어 관리**: flush 빈도 → 그룹 커밋, 로그/데이터/메타 경계 설계.
6) **NUMA/IRQ 핀**: 큐·IRQ·스레드의 노드 로컬화, 대용량 버퍼는 **hugepage** 고려.
7) **관측/추적**: `iostat/blktrace/perf/eBPF`로 병목을 수치화(특히 **p99**).

---

## 핵심 요약

- **12.4** 커널 I/O 서브시스템은 VFS/페이지 캐시/블록 계층/드라이버의 **파이프라인**으로, 병합·정렬·스케줄링과 캐시 정책이 성능을 좌우한다.
- **12.5** 사용자 요청은 BIO→blk-mq→드라이버→DMA/IRQ로 변환되며, NVMe의 **SQ/CQ + PRP/SGL** 모델이 현대 표준이다. **IOMMU/배리어**는 안전한 고성능의 열쇠.
- **12.6** STREAMS는 모듈러 메시지 파이프 프레임워크로 역사적 의의가 크며, 현대에는 소켓+BPF/DPDK 등으로 유사 모듈성을 달성한다.
- **12.7** 성능은 **이용률 관리, 배치/병합, 캐시·큐 딥·인터럽트 조율**에 달려 있다. 관측(eBPF/블록트레이스)과 실험으로 **p99**를 안정화하자.
