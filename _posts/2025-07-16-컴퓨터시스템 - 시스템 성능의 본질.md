---
layout: post
title: 컴퓨터시스템 - 시스템 성능의 본질
date: 2025-07-16 18:20:23 +0900
category: 컴퓨터시스템
---
# 시스템 성능의 본질: Amdahl의 법칙, 동시성과 병렬성

## 개요 — “빨라지는가?”를 정량화하는 네 가지 시선

1. **Amdahl의 법칙**: 직렬 구간이 있는 한 무한 확장은 불가능하다(강한 스케일링).
2. **Gustafson–Barsis 법칙**: 문제 크기를 늘리면 코어를 채울 수 있다(약한 스케일링).
3. **Queueing/Little의 법칙**: 동시성은 대기행렬(지연↔처리량)의 균형 문제다.
4. **USL(보편적 확장성 법칙)**: 실제 확장 저해 요인을 **경합(α)**·**일관성 비용(β)**로 분해한다.

이 글은 위 네 축을 **동시성 vs 병렬성**의 역할과 엮어, **엔드-투-엔드 성능**을 설계·검증하는 절차를 제공한다.

---

## Amdahl의 법칙 — 직렬 구간이 만드는 단단한 천장

### 공식과 해석

$$
S(N) \;=\; \frac{1}{(1-P) + \frac{P}{N}}
$$

- \(S(N)\): \(N\)개 자원(코어/스레드) 사용 시 **속도 향상 배율**
- \(P\): **병렬화 가능한 비율** (0~1)
- \(1-P\): **직렬 비율** (필연적 병목)

**상한**( \(N\to\infty\) ):
$$
\lim_{N\to\infty} S(N) = \frac{1}{1-P}
$$

즉, **직렬 10%**만 있어도 최대 속도 향상은 **\(\frac{1}{0.1}=10\)배**를 넘기 어렵다.

### 역문제(측정치로 P 추정)

실험에서 \(S(N)\)을 얻었다면:

$$
\frac{1}{S(N)} = (1-P) + \frac{P}{N} = 1 - P\Big(1-\frac{1}{N}\Big)
$$

따라서,

$$
P = \frac{1 - \frac{1}{S(N)}}{1 - \frac{1}{N}}
$$

> 측정만 있으면 **병렬화 가능성의 현실적 상한**을 추정할 수 있다.

### “오버헤드”를 포함한 현실형 Amdahl

스레드 생성/동기화/통신 등의 오버헤드 \(O(N)\) 를 포함하면:

$$
T(N) = T_1 \big( (1-P) + \frac{P}{N} \big) + O(N), \quad
S(N) = \frac{T_1}{T(N)}
$$

오버헤드는 보통 \(O(N) = a + b(N-1) + cN(N-1)\) 같은 선형·쌍곡(상호작용) 항으로 근사한다.
이해 포인트: **작은 작업을 너무 잘게 쪼개면 오히려 느려진다.**

### Karp–Flatt 메트릭(ε) — “직렬+오버헤드”의 합성 관측치

$$
\epsilon(N) = \frac{\frac{1}{S(N)} - \frac{1}{N}}{1 - \frac{1}{N}}
$$

\(\epsilon\)이 **N과 상관없이 일정**하면 주로 직렬 비율 지배,
**N이 커질수록 증가**하면 오버헤드/상호작용(락 경합, 캐시 일관성) 지배.

---

## Gustafson–Barsis 법칙 — 문제를 키워 코어를 채운다(약한 스케일링)

Amdahl은 **문제 크기 고정**(강한 스케일링). 반대로 문제 크기를 늘려 **같은 시간**에 더 큰 일을 하려면:

$$
S_G(N) \;=\; N - (1-P)(N-1)
$$

- \(P\)는 **병렬로 늘어난 부분의 비율**(작업 크기와 함께 커질 수 있음).

**해석**: 데이터/해상도/정확도를 올리며 코어를 채우는 **HPC·배치 분석**에 유리.
실무 팁: **“성능 향상”**을 **“같은 시간에 더 큰 문제 처리”**로 재정의하라.

---

## 동시성 vs 병렬성 — 개념과 목표가 다르다

| 구분 | 동시성(Concurrency) | 병렬성(Parallelism) |
|---|---|---|
| 정의 | 여러 작업을 **교차 수행**해 대기/블로킹을 숨김 | 여러 작업을 **실제로 동시에** 계산 |
| 필요 자원 | 단일 코어도 가능(스케줄링/문맥 전환) | 다중 코어/멀티프로세서 필요 |
| 주요 목표 | **응답성·지연** 개선 | **처리량·총시간** 단축 |
| 주무대 | I/O 경합, 네트워크 서버, GUI | 수치계산, 데이터 병렬, 배치 처리 |
| 핵심 이슈 | 큐 길이·백프레셔·우선순위 역전 | 로드밸런스·캐시/메모리 대역폭·동기화 비용 |

> **둘은 대체재가 아니라 보완재**다. 고성능 서버는 보통 “**동시성(프론트)** + **병렬성(백엔드 계산)**”을 조합한다.

---

## 코드로 보는 차이 — 계산 병렬화 vs 연결 동시성

### 병렬성(계산 가속) — OpenMP 루프 병렬화 (C)

```c
// gcc -O2 -fopenmp omp_pi.c -o omp_pi
#include <omp.h>
#include <stdio.h>

static long n = 1000000000L;
int main(){
    double h = 1.0 / n, sum = 0.0;
    #pragma omp parallel
    {
        double local = 0.0;
        #pragma omp for
        for (long i=0;i<n;i++){
            double x = (i + 0.5) * h;
            local += 4.0 / (1.0 + x*x);
        }
        #pragma omp atomic
        sum += local;
    }
    double pi = sum * h;
    printf("pi=%.12f\n", pi);
}
```

- **핵심**: CPU 바운드 계산을 코어 수만큼 나눠 **총 소요시간**을 줄인다.
- 주의: 루프 분할·캐시 지역성·리덕션 비용·스케줄 정책(`static/dynamic`)이 실제 \(P\)와 \(O(N)\)를 가른다.

### 동시성(응답성/처리량) — epoll 기반 에코 서버 요지 (C/Linux)

```c
// gcc -O2 -Wall -Wextra -pedantic -o echosrv echosrv.c
// 핵심 아이디어만: 다수 연결을 1~소수 스레드로 "동시에" 핸들
int ep = epoll_create1(0);
struct epoll_event ev = { .events = EPOLLIN, .data.fd = listen_fd };
epoll_ctl(ep, EPOLL_CTL_ADD, listen_fd, &ev);
for(;;){
    struct epoll_event out[1024];
    int n = epoll_wait(ep, out, 1024, -1);
    for(int i=0;i<n;i++){
        if(out[i].data.fd == listen_fd){ /* accept loop */ }
        else { /* recv->send; non-blocking; EAGAIN 처리 */ }
    }
}
```

- **핵심**: I/O를 논블로킹 이벤트로 **교차 처리**해 대기를 숨기고 **동시 연결 수**를 키운다.
- 병렬 계산이 없어도 동시성으로 **지연·처리량**을 대폭 개선할 수 있다.

### Python에서의 선택 — GIL 고려

- CPU 바운드 → `multiprocessing` 또는 C 확장/NumPy(벡터화)
- I/O 바운드 → `asyncio`/`trio`(동시성)

```python
# CPU 바운드: 멀티프로세싱으로 병렬성

from multiprocessing import Pool
def f(x): return x*x
with Pool() as p:
    print(sum(p.map(f, range(10_000_000))))
```

```python
# I/O 바운드: asyncio로 동시성

import asyncio, aiohttp
async def fetch(s, url):
    async with s.get(url) as r: return await r.text()
async def main(urls):
    async with aiohttp.ClientSession() as s:
        return await asyncio.gather(*(fetch(s,u) for u in urls))
asyncio.run(main(["https://example.org"]*1000))
```

---

## 측정 → 해석 → 설계로 환류: 스피드업·효율·ε·USL

### 핵심 지표

- **스피드업**: \( S(N)=\frac{T(1)}{T(N)} \)
- **효율**: \( E(N)=\frac{S(N)}{N} \)
- **Karp–Flatt**: \( \epsilon(N)=\frac{\frac{1}{S}-\frac{1}{N}}{1-\frac{1}{N}} \)

### 보편적 확장성 법칙(USL, Gunther)

관측 스루풋 \(X(N)\) 을 정규화 \(C(N)=\frac{X(N)}{X(1)}\) 하면:

$$
C(N) \;=\; \frac{N}{1 + \alpha(N-1) + \beta N(N-1)}
$$

- \(\alpha\): **경합(contention)** — 공유 자원(락/메모리/버스)으로 인한 직렬화
- \(\beta\): **일관성(coherency)** — 상호간 통신/캐시 일관성/복제 갱신 비용

> \(\beta>0\)이면 **어느 시점 이후 역전(throughput 감소)**가 필연 → “최적 코어 수”가 존재.

#### (참고) USL 파라미터 추정 스니펫 (Python, 최소제곱)

```python
import numpy as np
from math import inf

# 관측치: (N, throughput)

obs = np.array([
    (1, 1000), (2, 1850), (4, 3300), (8, 5500),
    (12, 6800), (16, 7200), (24, 6900)
], dtype=float)

N = obs[:,0]
X = obs[:,1]
C = X / X[0]

# 단순 그리드 탐색(설명용)

best, best_err = None, inf
for a in np.linspace(0, 0.2, 401):
    for b in np.linspace(0, 0.01, 401):
        pred = N / (1 + a*(N-1) + b*N*(N-1))
        err = np.sum((pred - C)**2)
        if err < best_err:
            best, best_err = (a,b), err
print("alpha, beta ≈", best)
```

- 결과의 \(\alpha,\beta\) 해석을 통해 **락 제거(α↓)**, **커뮤니케이션 축소(β↓)** 전략을 결정한다.

---

## 동시성과 큐잉 — Little의 법칙으로 “막힘” 읽기

**Little의 법칙**

$$
L = \lambda W
$$

- \(L\): 시스템 내 평균 체류 작업 수(대기+서비스)
- \(\lambda\): 도착률(throughput)
- \(W\): 평균 체류 시간(지연)

**해석**: **큐 길이**(대기중 요청 수)가 선형적으로 지연을 키운다.
스레드를 무작정 늘려도 **임계구역/DB/디스크**가 병목이면 큐만 길어진다 → **백프레셔** 필수.

### 간단 M/M/1 근사

도착률 \(\lambda\), 서비스율 \(\mu\) (\(\rho=\lambda/\mu <1\)):

$$
W = \frac{1}{\mu-\lambda}, \quad L=\frac{\lambda}{\mu-\lambda}
$$

부하 \(\rho\)가 1에 가까워지면 지연은 **폭증**.
운영 팁: “CPU 80%”는 안전해 보여도, **대기열 지연**은 이미 시작됐을 수 있다.

---

## 병렬 설계 체크리스트 — 어디서 새는가?

1. **직렬 구간 제거**: I/O·파서·GC·락·전역 구조체 등.
2. **작업 크기 튜닝**: 너무 작으면 오버헤드↑, 너무 크면 불균형/캐시 미스↑.
3. **데이터 지역성**: AoS→SoA, 타일링, prefetch-friendly 패턴.
4. **락 회피**: 샤딩·락프리(CAS)·배치 갱신·함수형(불변) 구조.
5. **캐시 일관성/거짓 공유**: 64B 패딩, 쓰기 집중 핫라인 분리.
6. **메모리 대역폭/NUMA**: first-touch, 바인딩, 스트림 수 제한.
7. **통신량/동기화**: 원자 갱신 최소화, 트리형 리덕션, 파이프라인화.
8. **효율적 I/O**: 배치/비동기/zero-copy(sendfile, io_uring).
9. **백프레셔**: 큐 길이 상한, 소비자 속도 기준의 생산 제한.
10. **측정 루틴**: p50/p95/p99, S(N), E(N), ε(N), USL(α,β) 주기적 재평가.

---

## 사례 연구 — 이미지 처리 파이프라인 리팩터링

### 초기 상태

- 입력: 이미지 1,000장/초 (네트워크 수신)
- 단계: 디코드 → 전처리 → 추론 → 후처리 → 저장
- 증상: 코어 32개 환경에서 처리량 3,800 img/s, p99 지연 ↑

### 계측과 해석

- `perf`: 추론 60%, 전처리 20%, 디코드 10%, 나머지 10%
- Amdahl \(P \approx 0.9\)로 추정했으나, USL 적합 결과 \(\alpha=0.06\), \(\beta=0.002\)
  → **락 경합(α)**과 **공유 텐서/캐시 교차 오염(β)**가 주요 원인

### 해결

- **파이프라인 병렬화**(스테이지 분리, 링버퍼) + **스테이지 내부 데이터 병렬**
- **SoA 변환**으로 전처리 벡터화, **배치 추론**으로 오버헤드 상쇄
- **NUMA 바인딩/first-touch**, 핫라인 패딩, 락을 per-queue로 샤딩

### 결과

- 처리량: 3,800 → 8,900 img/s
- USL 재적합: \(\alpha=0.02\), \(\beta=0.0005\)
- Amdahl 관점의 \(P\)도 상승(실효 직렬 구간 축소)

---

## 실습 — 측정 데이터로 P, ε, USL 구하기

### 예제 데이터

| N | T(N) [s] | S(N)=T(1)/T(N) |
|---|---:|---:|
| 1 | 100 | 1.00 |
| 2 | 58  | 1.72 |
| 4 | 35  | 2.86 |
| 8 | 22  | 4.55 |
| 16| 17  | 5.88 |

**P 추정 (N=16)**

$$
P = \frac{1 - \frac{1}{S}}{1 - \frac{1}{N}}
= \frac{1 - \frac{1}{5.88}}{1 - \frac{1}{16}}
\approx \frac{0.830}{0.9375}
\approx 0.885
$$

**ε 추정 (N=16)**

$$
\epsilon = \frac{\frac{1}{S}-\frac{1}{N}}{1-\frac{1}{N}}
= \frac{0.170 - 0.0625}{0.9375}
\approx 0.1147
$$

→ 직렬+오버헤드 합성 비율이 약 11.5%. 우선순위: 직렬 구간 제거·락 회피.

**USL 적합**: §5.2의 스니펫으로 \(\alpha,\beta\)를 추정 → 설계 의사결정에 반영.

---

## 동시성과 병렬성을 “결합”하는 설계 패턴

1. **Fan-out/Fan-in**: 작업을 분할하여 병렬 처리 후 리덕션.
2. **Pipeline**: 단계별로 동시 진행(각 스테이지 큐와 백프레셔).
3. **Actor/메시지 패싱**: 공유 상태 최소화, 캐시 친화.
4. **Work-stealing 풀**: 불균형/버스트 대응.
5. **Bulk-synchronous(미니배치)**: 동기화 횟수 감소, 배치 이점.

ASCII 예시(파이프라인):

```
[Network RX] -> [Decode] -> [Preproc] -> [Infer(batch)] -> [Postproc] -> [Sink]
     ||            ||           ||            ||              ||          ||
    epoll        vector       SoA/Tiled     N cores         lock-free     fs
```

---

## 흔한 오해와 실전 주의사항

- **스레드=속도**는 아님: 스레드는 동시성의 그릇일 뿐, 병렬 가속은 **CPU 바운드**일 때만.
- **코어 수만큼 항상 선형**? → Amdahl, 메모리 대역폭, NUMA, 캐시 일관성에 막힌다.
- **락만 없애면 된다?** → 그 순간 **일관성/통신 비용(β)**가 튀어 오른다.
- **마이크로옵티가 전부?** → 병렬 구조/파이프라인/배치 설계가 더 큰 승수.
- **p50만 본다?** → 사용자 체감은 보통 **p95/p99**가 지배.

---

## 부록 — 수식 모음과 빠른 변환

**Amdahl 상한**

$$
S_{\max} = \frac{1}{1-P}
$$

**효율**

$$
E(N)=\frac{S(N)}{N}
$$

**Karp–Flatt**

$$
\epsilon(N)=\frac{\frac{1}{S}-\frac{1}{N}}{1-\frac{1}{N}}
$$

**Gustafson–Barsis**

$$
S_G(N)=N-(1-P)(N-1)
$$

**USL**

$$
C(N)=\frac{N}{1+\alpha(N-1)+\beta N(N-1)}
$$

**Little**

$$
L=\lambda W
$$

---

## 참고 구현 스케치 — 스레드풀 + 파이프라인(현대 C++)

```cpp
// g++ -O2 -pthread pipe_pool.cpp -o pipe_pool
#include <bits/stdc++.h>

using namespace std;

struct ThreadPool {
    vector<thread> ws;
    deque<function<void()>> q;
    mutex m; condition_variable cv; bool stop=false;
    ThreadPool(size_t n){
        while(n--) ws.emplace_back([this]{
            unique_lock<mutex> lk(m);
            for(;;){
                if(!q.empty()){ auto f=move(q.front()); q.pop_front(); lk.unlock(); f(); lk.lock(); }
                else if(stop) break;
                else cv.wait(lk);
            }
        });
    }
    ~ThreadPool(){
        { lock_guard<mutex> lk(m); stop=true; }
        cv.notify_all(); for(auto& t:ws) t.join();
    }
    template<class F> void submit(F&& f){
        { lock_guard<mutex> lk(m); q.emplace_back(forward<F>(f)); }
        cv.notify_one();
    }
};

int main(){
    ThreadPool pool(thread::hardware_concurrency());
    // 파이프라인 3단계: decode -> compute -> sink (큐로 연결)
    concurrent_queue<string> q1, q2; // 구현은 생략(락프리/뮤텍스 큐)
    atomic<bool> done=false;

    // producer
    thread prod([&]{ for(int i=0;i<100000;i++) q1.push("item"+to_string(i)); done=true; });

    // stage1: decode
    for(int i=0;i<4;i++) pool.submit([&]{
        string x;
        while(!done || q1.try_pop(x)){
            if(x.empty()) continue;
            // ... decode ...
            q2.push(move(x));
        }
    });

    // stage2: compute (병렬)
    atomic<long long> cnt=0;
    vector<future<void>> futs;
    for(int i=0;i<8;i++) futs.emplace_back(async(launch::async,[&]{
        string y;
        while(!done || q2.try_pop(y)){
            if(y.empty()) continue;
            // ... heavy compute ...
            cnt++;
        }
    }));
    prod.join(); for(auto& f:futs) f.get();
    cerr << "processed=" << cnt.load() << "\n";
}
```

- 포인트: **스테이지 동시성**(큐) + **스테이지 내부 병렬성**(풀) 결합.
- 실제에선 **백프레셔**(큐 용량 제한), **배치 처리**, **NUMA 바인딩** 등을 추가한다.

---

## 결론 — “똑똑하게 나누고, 동시에 실행하되, 병목을 없애라”

1. **Amdahl**로 상한을 계산하고 **직렬 구간을 먼저 제거**하라.
2. **Gustafson**으로 “같은 시간에 더 큰 문제”를 정의해 코어를 채워라.
3. **동시성**으로 대기를 숨기고 **병렬성**으로 계산을 당겨라.
4. **USL(α,β)**과 **ε(N)**으로 무엇을 줄여야 할지 증거 기반으로 결정하라.
5. **측정→해석→설계**를 주기적으로 환류하여, 성능을 **우연**이 아니라 **구조**로 만든다.

> 같은 하드웨어에서도, 구조와 분할·배치·동기화 전략에 따라 **배 이상**의 차이가 난다.
> 성능 최적화는 “더 많은 스레드”가 아니라 **올바른 추상화와 데이터 흐름**에서 시작된다.
