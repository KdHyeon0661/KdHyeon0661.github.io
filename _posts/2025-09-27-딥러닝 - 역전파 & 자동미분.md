---
layout: post
title: 딥러닝 - 역전파 & 자동미분
date: 2025-09-27 17:25:23 +0900
category: 딥러닝
---
# 1.6 역전파(Backpropagation) & 자동미분(Autograd)
**연쇄법칙 · 파라미터 업데이트 · autograd 개념 · 그래프/메모리 주의**

## A. 큰 그림 한 장으로
- **목표:** 손실 $$\mathcal{L}(\theta)$$ 를 **작게** 하는 파라미터 $$\theta$$ 를 찾기.
- **수단:** 그라디언트 $$\nabla_\theta \mathcal{L}$$ 를 **연쇄법칙**으로 계산(역전파) → 옵티마이저로 업데이트.
- **도구:** PyTorch **autograd** 가 **계산 그래프**를 만들고 **VJP**(vector-Jacobian product)로 미분을 자동 수행.

요약 수식:
$$
\theta \leftarrow \theta - \eta\,\nabla_\theta \mathcal{L}(\theta)
$$

---

## B. 연쇄법칙(Chain Rule) — 스칼라에서 벡터까지

### B-1. 스칼라 합성함수
$$
y = f(g(x)),\quad \frac{dy}{dx} = f'(g(x))\,g'(x)
$$

### B-2. 다변수/벡터 버전 (야코비안)
$$
\mathbf{y} = \mathbf{f}(\mathbf{u}),\quad \mathbf{u}=\mathbf{g}(\mathbf{x})
\quad\Rightarrow\quad
\frac{\partial \mathbf{y}}{\partial \mathbf{x}}
= \frac{\partial \mathbf{y}}{\partial \mathbf{u}}\frac{\partial \mathbf{u}}{\partial \mathbf{x}}
$$

**역전파**는 결국 **출력→입력**으로 이 **야코비안의 곱**을 효율적으로 계산하는 과정이다(중간 야코비안을 전부 explicitly 만들지 않고 **VJP**로 계산).

---

## C. 계산 그래프(Computational Graph)와 autograd

### C-1. 계산 그래프
- 노드: 텐서(값)  
- 엣지: 연산(함수)  
- **forward**: 입력→출력으로 값을 계산, autograd가 **필요한 중간값**을 저장  
- **backward**: 출력의 손실 미분이 입력·파라미터로 **역류**

### C-2. PyTorch autograd 핵심 규칙
- 텐서에 `requires_grad=True` 면 **연산 기록**을 시작(leaf 텐서의 `.grad`에 누적).
- 기본은 **한 번 backward 후 그래프 해제**(메모리 회수). **두 번 이상** 미분하려면
  - `retain_graph=True` (그래프 유지)  
  - 또는 `create_graph=True` (고차 미분용으로 **미분을 위한 그래프** 생성)
- `.backward(gradient)` 의 **`gradient`** 인자는 출력이 스칼라가 아닐 때 “출력에 곱해질 벡터”를 넣어 **VJP**를 지정.

---

## D. 손으로 풀어보는 역전파 (미니 예제)

### D-1. 스칼라 예: \(y=(x_1x_2 + x_3)^2\)

직접 미분:
$$
y = (a)^2,\quad a = x_1x_2 + x_3\\
\frac{\partial y}{\partial a} = 2a,\quad
\frac{\partial a}{\partial x_1}=x_2,\ 
\frac{\partial a}{\partial x_2}=x_1,\
\frac{\partial a}{\partial x_3}=1
$$
따라서
$$
\frac{\partial y}{\partial x_1}=2a\,x_2,\quad
\frac{\partial y}{\partial x_2}=2a\,x_1,\quad
\frac{\partial y}{\partial x_3}=2a
$$

#### PyTorch로 검증
```python
import torch
torch.manual_seed(0)

x = torch.tensor([2.0, -3.0, 0.5], requires_grad=True)  # x1, x2, x3
a = x[0]*x[1] + x[2]
y = a**2
y.backward()                 # dy/dx 계산

print("x.grad =", x.grad)    # [dy/dx1, dy/dx2, dy/dx3]
# 수식과 일치하는지 확인
```

---

## E. 파라미터 업데이트 — SGD의 한 사이클

### E-1. 한 에폭의 미니배치 step
1) `forward` → 2) `loss` → 3) `backward()` → 4) **클립**(선택) → 5) `optimizer.step()` → 6) `optimizer.zero_grad()`  
중요: **그라디언트는 기본 “누적”**. 다음 step 전에 반드시 **초기화**(zero)하자.

```python
import torch, torch.nn as nn

model = nn.Sequential(nn.Linear(10, 64), nn.ReLU(), nn.Linear(64, 1))
opt = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
crit = nn.MSELoss()

for xb, yb in loader:                     # 예: DataLoader
    pred = model(xb)
    loss = crit(pred, yb)
    opt.zero_grad(set_to_none=True)       # 권장: set_to_none=True가 메모리/속도 유리
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 선택: 폭주 방지
    opt.step()
```

> **왜 set_to_none=True?**  
> `.grad`를 0으로 채우는 대신 **None** 으로 두면 autograd가 in-place add를 하지 않아서 **속도/메모리**에 유리하고, “누적인지 0인지” 실수를 줄여준다.

---

## F. autograd 사용 패턴 — 핵심 스니펫 모음

### F-1. 그라디언트가 필요한 텐서와 아닌 텐서
```python
x = torch.randn(4, requires_grad=True)   # leaf
y = 3*x + 2
z = y.pow(2).sum()                       # 스칼라
z.backward()
print(x.grad)                            # OK (leaf tensor)

with torch.no_grad():                    # 평가/추론 구간: 그래프 비생성
    y_pred = model(x_eval)
```

### F-2. 입력에 대한 그라디언트 (예: saliency)
```python
x = torch.randn(1, 3, 224, 224, requires_grad=True)
logits = model(x)
cls = logits[0, 5]       # 6번 클래스 로짓
cls.backward()
saliency = x.grad.abs()  # 입력 민감도
```

### F-3. 그래프 재사용/고차미분
```python
y = model(x).sum()
# 두 번 backward가 필요하면 retain_graph=True
y.backward(retain_graph=True)

# 고차 미분(예: gradient penalty) → create_graph=True
g = torch.autograd.grad(y, x, create_graph=True)[0]  # dy/dx (그래프 유지)
penalty = (g.norm(2, dim=(1,2,3)) - 1).pow(2).mean()
penalty.backward()
```

### F-4. detach로 그래프 끊기 (정규화항/EMA/로깅)
```python
with torch.no_grad():
    ema.copy_(0.999*ema + 0.001*param)     # EMA 갱신은 미분 불필요

val = some_tensor.detach()                  # 값만 복사(그래프 분리)
log_scalar = val.item()                     # 숫자 추출(그래프 완전 분리)
```

> **주의:** `.item()` 은 **스칼라 값만**; 벡터/행렬에서 루프 내부 `.item()` 반복은 **느림**.

---

## G. 그래프/메모리 주의사항

### G-1. 기본 규칙
- **autograd는 fwd 중 필요한 중간 텐서**를 저장한다(역전파 때 사용).
- backward가 끝나면 기본으로 **그래프가 해제**된다(메모리 반환).

### G-2. 흔한 실수 & 해결
1) **in-place 연산**으로 저장값이 변해 버리기  
   - 예: `x += y` / `relu_()` 가 **필요한 중간값**을 망가뜨리면 오류  
   - 해결: in-place 최소화, 필요한 경우 **함수 문서**의 in-place 지원 여부 확인
2) **그래프 붙은 텐서를 리스트에 누적**(학습 내내)  
   - 메모리 누수. 로깅/버퍼링은 **`.detach()`** 해서 저장
3) **gradient 누적** 잊음  
   - `opt.zero_grad()` (또는 set_to_none=True) 호출 위치 점검
4) **작은 배치에서 BatchNorm**  
   - 통계 불안정 → **LayerNorm/GroupNorm** 또는 큰 유효 배치(누적)·SyncBN

### G-3. 체크포인팅(activation checkpointing)
- 메모리를 줄이기 위해 일부 활성값을 저장하지 않고 **역전파 시 재계산**:
```python
from torch.utils.checkpoint import checkpoint

def block(x):
    x = layer1(x); x = torch.relu(x); x = layer2(x)
    return x

out = checkpoint(block, x)   # fwd 일부 재계산 ↔ 메모리↓, 시간↑
```

### G-4. Anomaly Detection(문제 지점 추적)
```python
torch.autograd.set_detect_anomaly(True)
loss.backward()   # 에러가 나면 역전파 경로 중 문제 연산을 추적해준다.
```

---

## H. 수동 파생 vs autograd 검증 — 간단 MLP

### H-1. 이론: 2층 MLP + MSE
$$
\begin{aligned}
\mathbf{h} &= \phi(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1)\\
\hat{y} &= \mathbf{W}_2\mathbf{h} + b_2\\
\mathcal{L} &= \frac{1}{2}\|\hat{y}-y\|_2^2
\end{aligned}
$$
연쇄법칙:
$$
\frac{\partial \mathcal{L}}{\partial \hat{y}}=(\hat{y}-y),\quad
\frac{\partial \mathcal{L}}{\partial \mathbf{W}_2}=(\hat{y}-y)\,\mathbf{h}^\top,\quad
\frac{\partial \mathcal{L}}{\partial \mathbf{h}}=\mathbf{W}_2^\top(\hat{y}-y)
$$
활성 $$\phi=\mathrm{ReLU}$$ 이면 $$\phi'(\mathbf{z})=\mathbb{1}[\mathbf{z}>0]$$:
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W}_1}=\left(\big(\mathbf{W}_2^\top(\hat{y}-y)\big)\odot \phi'(\mathbf{z})\right)\mathbf{x}^\top
$$

### H-2. 코드로 autograd와 대조
```python
import torch, torch.nn as nn, torch.nn.functional as F
torch.manual_seed(0)

x = torch.randn(5, 3)         # B=5, in=3
y = torch.randn(5, 1)

W1 = torch.randn(4, 3, requires_grad=True)
b1 = torch.randn(4,    requires_grad=True)
W2 = torch.randn(1, 4, requires_grad=True)
b2 = torch.randn(1,    requires_grad=True)

# forward
z = x @ W1.t() + b1          # (5,4)
h = F.relu(z)
y_hat = h @ W2.t() + b2      # (5,1)
loss = 0.5*((y_hat - y)**2).mean()

# backward
loss.backward()

# autograd가 계산한 기울기 확인
for name, t in [("W1", W1), ("b1", b1), ("W2", W2), ("b2", b2)]:
    print(name, t.grad.norm().item())
```

> 숙제: 위 수식을 이용해 **수동 파생**으로도 하나 계산해 보고 autograd 값과 비교(또는 `gradcheck`로 검산).

---

## I. 커스텀 autograd 연산 만들기

### I-1. `torch.autograd.Function`
- **정확한 forward/backward** 를 스스로 정의한다(희소/특수 연산, 사용자 정의 수치안정 트릭 등).

```python
import torch

class SwishFn(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        s = torch.sigmoid(x)
        y = x * s
        ctx.save_for_backward(s, y)  # backward 때 쓰기 위해 저장
        return y
    @staticmethod
    def backward(ctx, grad_out):
        s, y = ctx.saved_tensors
        # dy/dx = s + x*s*(1-s) = s*(1 + x*(1-s))  또는 y + s*(1 - y)
        grad_x = grad_out * (y + s*(1 - y))
        return grad_x

def swish(x): return SwishFn.apply(x)

x = torch.randn(8, requires_grad=True)
y = swish(x).sum()
y.backward()
print(x.grad)  # 커스텀 backward 결과
```

### I-2. 수치 검증 `gradcheck`
```python
x = torch.randn(4, dtype=torch.double, requires_grad=True)
torch.autograd.gradcheck(SwishFn.apply, (x,))
```

---

## J. 고차 미분, JVP/VJP, 헤시안·HV(헤시안-벡터 곱)

### J-1. VJP (autograd의 기본)
`autograd.grad(outputs, inputs, grad_outputs=v)` 는
$$
v^\top \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
$$
을 계산한다(백워드에서 **v를 흘린다**).

```python
x = torch.randn(10, requires_grad=True)
y = model(x)            # y: (m,)
v = torch.randn_like(y) # 임의의 방향
vjp = torch.autograd.grad(y, x, grad_outputs=v)[0]
```

### J-2. JVP (정방향 미분; 일부 버전에선 functional API)
- 최신 PyTorch에선 `torch.autograd.functional.jvp(f, x, v)` 가능(버전 의존).  
- 또는 **이중 미분 트릭**으로 구현 가능(지면상 생략).

### J-3. 2계 미분(예: gradient penalty, meta-learning)
```python
x = torch.randn(3, requires_grad=True)
y = (x**2).sum()
g = torch.autograd.grad(y, x, create_graph=True)[0]     # dy/dx = 2x
h = torch.autograd.grad(g.sum(), x)[0]                  # d/dx (2x⋅1) = 2
print(h)  # ~ tensor([2.,2.,2.])
```

### J-4. Hessian-vector product (Pearlmutter 방식)
$$
\mathbf{H}\mathbf{v} = \nabla^2 f(\mathbf{x})\,\mathbf{v}
$$
은 고차미분 1회로 가능:
```python
def hvp(f, x, v):
    g = torch.autograd.grad(f(x), x, create_graph=True)[0]  # ∇f
    Hv = torch.autograd.grad(g, x, grad_outputs=v)[0]
    return Hv
```

---

## K. Mixed Precision(AMP) & Gradient Scaling

### K-1. 왜 필요한가?
- FP16/BF16로 **속도/메모리** 장점.  
- FP16의 **언더플로우**를 막기 위해 **GradScaler**로 손실을 임시 스케일.

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
for xb, yb in loader:
    opt.zero_grad(set_to_none=True)
    with autocast(dtype=torch.float16):   # 또는 bfloat16
        pred = model(xb)
        loss = crit(pred, yb)
    scaler.scale(loss).backward()
    scaler.unscale_(opt)                  # (선택) 클립 전 unscale
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    scaler.step(opt)
    scaler.update()
```

> **주의:** AMP 구간 바깥에서 `.item()` 을 자주 부르면 **CPU-GPU sync**로 느려질 수 있음. 로깅 최소화.

---

## L. 순환/RNN류에서의 BPTT와 `detach`

### L-1. Truncated BPTT
긴 시퀀스의 전체 길이를 한 번에 역전파하면 메모리 폭발 → **구간 단위**로 잘라 역전파(Truncated BPTT).
```python
state = None
for t in range(T):        # 긴 시퀀스
    out, state = rnn(x[t], state)
    loss_t = crit(out, y[t])
    loss_t.backward()
    # 다음 구간의 그래프 누적 방지:
    state = (state[0].detach(), state[1].detach())  # LSTM 예
    opt.step(); opt.zero_grad(set_to_none=True)
```

---

## M. 모듈·텐서 hook으로 중간 그라디언트 관찰/수정

```python
handle = some_tensor.register_hook(lambda g: torch.clamp(g, -1, 1))  # grad clip per-tensor
# ...
handle.remove()

def fwd_hook(module, inp, out):
    print(module.__class__.__name__, out.shape)

h = layer.register_forward_hook(fwd_hook)
# ...
h.remove()
```

> Hook은 **디버깅/분석**에는 유용하지만, 남발하면 **성능·메모리**에 영향.

---

## N. 분산(DDP)과 backward
- `DistributedDataParallel`(DDP)에서는 `loss.backward()` 시 각 파라미터의 `.grad` 가 **AllReduce** 되어 동기화된다.
- **Gradient accumulation** 을 하면 **여러 step 후**에만 AllReduce 하게 조절 가능(FindUnusedParameters 주의).
- 각 에폭마다 **`sampler.set_epoch(epoch)`** 로 셔플 시드를 바꿔야 데이터 중복/편향이 줄어든다.

---

## O. 역전파 트러블슈팅 체크리스트

1) **loss가 NaN/Inf**  
   - 입력/로짓 범위 확인, 수치안정 손실(BCEWithLogits/CE), AMP unscale/clip 순서, 학습률↓  
2) **grad가 0만 나옴**  
   - ReLU dead, `requires_grad` 누락, `detach()` 남용, in-place로 그래프 파괴  
3) **RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation**  
   - in-place 제거 또는 비 in-place로 교체  
4) **메모리 누수**  
   - 그래프 붙은 텐서 로깅/리스트 누적, `retain_graph=True` 남용  
5) **2번 backward 에러**  
   - `retain_graph=True` 또는 **처음 backward에서 `create_graph=True`** (고차미분용)

---

## P. 통합 예제 — 실전형 학습 루프(모든 포인트 모음)

```python
import torch, torch.nn as nn
from torch.cuda.amp import autocast, GradScaler

class Net(nn.Module):
    def __init__(self, d_in=784, d_h=512, d_out=10):
        super().__init__()
        self.fc1 = nn.Linear(d_in, d_h)
        self.fc2 = nn.Linear(d_h, d_h)
        self.fc3 = nn.Linear(d_h, d_out)
        self.act = nn.GELU()
        self.norm = nn.LayerNorm(d_h)
    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.act(self.fc1(x))
        x = self.norm(self.act(self.fc2(x)))
        return self.fc3(x)                       # 로짓 (CE와 결합)

model = Net().cuda()
opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-2)
sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=100)
crit = nn.CrossEntropyLoss(label_smoothing=0.1)
scaler = GradScaler()

accum_steps = 2
for epoch in range(20):
    model.train()
    opt.zero_grad(set_to_none=True)
    for step, (xb, yb) in enumerate(train_loader, start=1):
        xb = xb.cuda(non_blocking=True); yb = yb.cuda(non_blocking=True)
        with autocast(dtype=torch.bfloat16):
            logits = model(xb)
            loss = crit(logits, yb) / accum_steps
        scaler.scale(loss).backward()
        if step % accum_steps == 0:
            scaler.unscale_(opt)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(opt); scaler.update()
            opt.zero_grad(set_to_none=True)
    sched.step()

    # 평가
    model.eval()
    total, correct = 0, 0
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.cuda(); yb = yb.cuda()
            pred = model(xb).argmax(1)
            correct += (pred==yb).sum().item()
            total += yb.numel()
    print(f"epoch {epoch+1} val acc: {correct/total:.3f}")
```

**여기서 확인할 항목**
- 로짓 + CE (softmax는 loss 내부)  
- AMP + GradScaler + per-step grad clip  
- **그라디언트 누적**(accumulation)  
- `.zero_grad(set_to_none=True)`  
- 평가 시 `torch.no_grad()` 와 `model.eval()`  
- LayerNorm/GELU 조합(소실/폭주 안정)

---

## Q. 핵심 공식·개념 요약

- **연쇄법칙**
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{x}}
= \frac{\partial \mathcal{L}}{\partial \mathbf{y}}
\frac{\partial \mathbf{y}}{\partial \mathbf{x}}
$$

- **역전파(VJP)**  
  출력 감도 벡터 $$\mathbf{v}$$ 를 곱해 **효율적**으로 그라디언트를 산출:
$$
\mathbf{v}^\top \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
$$

- **업데이트(예: SGD)**
$$
\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}
$$

- **고차 미분**  
  `create_graph=True` 로 **미분을 위한 그래프** 생성 → 2계 미분/메타러닝/GP 등.

- **수치안정 & 메모리**
  - BCEWithLogits/CE(로짓 입력)  
  - AMP + GradScaler  
  - checkpointing, detach, no_grad, in-place 최소화

---

## R. 실무 체크리스트

- [ ] **로스는 안정형**(BCEWithLogits/CE)으로, **확률 전 변환 금지**  
- [ ] **zero_grad(set_to_none=True)** 로 누적 방지  
- [ ] AMP 사용 시 **scale → unscale → (clip) → step → update** 순서  
- [ ] **in-place** 주의, 필요한 중간값 저장 연산인지 확인  
- [ ] **detach/no_grad** 로 로깅·EMA·평가에서 그래프 분리  
- [ ] 긴 시퀀스는 **Truncated BPTT** 또는 **checkpoint**  
- [ ] 의심스러우면 **set_detect_anomaly(True)** 로 추적  
- [ ] 그라디언트/로짓 범위 모니터링(폭주/소실 조기 감지)  

---

### 마무리
- 역전파는 **연쇄법칙의 체계적 적용**이며, PyTorch autograd는 이를 **VJP**로 자동화한다.  
- **그래프/메모리의 생명주기**(기록→사용→해제)를 이해하면, **속도·안정성·메모리** 모두 잡을 수 있다.  
- 이 장의 코드는 “**왜 이 순서로, 무엇을 주의하며**” 역전파를 돌리는지에 대한 **실전 템플릿**이다.