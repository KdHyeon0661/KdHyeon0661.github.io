---
layout: post
title: 이산수학 - 기댓값과 분산
date: 2025-06-16 22:20:23 +0900
category: 이산수학
---
# 기댓값과 분산 (Expectation & Variance)

## 왜 중요한가?

- **기댓값**: 분포의 **중심**(장기 평균).
- **분산/표준편차**: 분포의 **퍼짐**(흩어짐).
- 머신러닝(손실의 기대 최소화), 통계 추정(바이어스–분산), 금융(수익·리스크), 운영/대기행렬(평균 체류시간 분산) 등 **모든 정량 분야의 핵심 요약 통계**입니다.

---

## 정의

### 이산 확률변수 \(X\)

$$
E[X] = \sum_{x} x \, P(X=x), \qquad
\mathrm{Var}(X) = E[(X - \mu)^2], \quad \mu = E[X].
$$

등가식:
$$
\mathrm{Var}(X) = E[X^2] - (E[X])^2.
$$

### 연속 확률변수 \(X\) (밀도 \(f\))

$$
E[X] = \int_{-\infty}^{\infty} x f(x)\,dx, \qquad
\mathrm{Var}(X) = \int_{-\infty}^{\infty} (x-\mu)^2 f(x)\,dx.
$$

### 일반 변환 \(g\) (LOTUS, 무의식의 법칙)

이산:
$$
E[g(X)] = \sum_x g(x) P(X=x).
$$
연속:
$$
E[g(X)] = \int g(x) f(x)\,dx.
$$

> 특히 선형 \(g(x)=ax+b\)에 대해
> $$E[aX+b]=aE[X]+b,\quad \mathrm{Var}(aX+b)=a^2\mathrm{Var}(X).$$

---

## 기댓값과 분산의 핵심 성질

### 기댓값(선형성)

독립 여부와 무관:
$$
E[aX + bY + c] = aE[X] + bE[Y] + c.
$$

### 분산(선형성 X, 그러나 다음은 성립)

- 상수의 분산:
$$
\mathrm{Var}(c)=0.
$$

- 스케일링:
$$
\mathrm{Var}(aX)=a^2\mathrm{Var}(X).
$$

- 합의 분산(일반형):
$$
\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2\,\mathrm{Cov}(X,Y).
$$
독립이면 \(\mathrm{Cov}(X,Y)=0\)이므로
$$
\mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y).
$$

---

## 공분산·상관계수

- 공분산:
$$
\mathrm{Cov}(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY]-E[X]E[Y].
$$

- 상관계수:
$$
\rho_{XY} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}, \quad -1\le\rho\le 1.
$$

- 독립 ⇒ 공분산 0(역은 일반적으로 거짓).
  (정규 쌍 등 특수 경우엔 무상관 ⇒ 독립이 되기도 함)

---

## 전기댓값·전분산 법칙 (조건부 기대/분산)

### 전기댓값의 법칙 (Law of Total Expectation)

$$
E[X] = E\big[\,E[X\mid Y]\,\big].
$$

### 전분산의 법칙 (Law of Total Variance)

$$
\boxed{\mathrm{Var}(X) = E\big[\mathrm{Var}(X\mid Y)\big] + \mathrm{Var}\big(E[X\mid Y]\big).}
$$

> **불확실성의 분해**: 평균적 내부 변동 + 집단 간 평균 차이 변동.

### 지시변수(Indicator) 테크닉

지시변수 \(I_A\) (사건 \(A\)이면 1, 아니면 0):
$$
E[I_A]=P(A),\quad \mathrm{Var}(I_A)=P(A)(1-P(A)).
$$
복잡한 개수 세기를 **지시변수 합의 기대**로 단순화:
$$
E\Big[\sum_i I_{A_i}\Big] = \sum_i P(A_i).
$$

---

## 대표 분포의 \(E\)와 \(\mathrm{Var}\)

- 베르누이 \(X\sim \mathrm{Bernoulli}(p)\):
$$
E[X]=p,\quad \mathrm{Var}(X)=p(1-p).
$$

- 이항 \(X\sim \mathrm{Binomial}(n,p)\):
$$
E[X]=np,\quad \mathrm{Var}(X)=np(1-p).
$$

- 포아송 \(X\sim \mathrm{Poisson}(\lambda)\):
$$
E[X]=\lambda,\quad \mathrm{Var}(X)=\lambda.
$$

- 기하 \(X\sim \mathrm{Geom}(p)\) (성공까지 시도 횟수, \(1,2,\dots\)):
$$
E[X]=\frac{1}{p},\quad \mathrm{Var}(X)=\frac{1-p}{p^2}.
$$

- 균등 \(X\sim \mathrm{Unif}(a,b)\):
$$
E[X]=\frac{a+b}{2},\quad \mathrm{Var}(X)=\frac{(b-a)^2}{12}.
$$

- 지수 \(X\sim \mathrm{Exp}(\lambda)\):
$$
E[X]=\frac{1}{\lambda},\quad \mathrm{Var}(X)=\frac{1}{\lambda^2}.
$$

- 정규 \(X\sim \mathcal{N}(\mu,\sigma^2)\):
$$
E[X]=\mu,\quad \mathrm{Var}(X)=\sigma^2.
$$

---

## 샘플 평균, 표본분산, 바이어스–분산

### i.i.d. 표본 \(X_1,\dots,X_n\) (평균 \(\mu\), 분산 \(\sigma^2\))

샘플 평균:
$$
\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i,\quad
E[\bar{X}]=\mu,\quad \mathrm{Var}(\bar{X})=\frac{\sigma^2}{n}.
$$

> **평균하면 분산이 \(1/n\)** 로 줄어든다(분산 감소의 기본 원리).

### 표본분산 (불편/편향)

- 불편 추정량:
$$
S^2 = \frac{1}{n-1}\sum_{i=1}^{n} (X_i-\bar{X})^2,\qquad E[S^2]=\sigma^2.
$$

- 편향 있는(편의형) 분산:
$$
\tilde{S}^2 = \frac{1}{n}\sum_{i=1}^{n} (X_i-\bar{X})^2
= \frac{n-1}{n}S^2,\quad E[\tilde{S}^2]=\frac{n-1}{n}\sigma^2.
$$

### 바이어스–분산 분해(예측/추정)

모델 추정량 \(\hat{\theta}(D)\)에 대해
$$
E\big[(\hat{\theta}-\theta)^2\big] = \underbrace{\big(E[\hat{\theta}]-\theta\big)^2}_{\text{Bias}^2}
+ \underbrace{\mathrm{Var}(\hat{\theta})}_{\text{Variance}}
+ \text{(불가피 잡음)}.
$$

---

## 불평등: 마르코프·체비셰프·젠슨

- 마르코프(비음수 \(X\)):
$$
P(X\ge a) \le \frac{E[X]}{a},\quad a>0.
$$

- 체비셰프:
$$
P(|X-\mu|\ge k\sigma) \le \frac{1}{k^2}.
$$

- 젠슨(볼록 \(\phi\)):
$$
\phi(E[X]) \le E[\phi(X)].
$$

---

## 벡터 확장: 공분산 행렬

확률벡터 \(\mathbf{X}\in\mathbb{R}^d\):

- 평균벡터:
$$
E[\mathbf{X}] = (E[X_1],\dots,E[X_d])^\top.
$$

- 공분산행렬:
$$
\Sigma = \mathrm{Cov}(\mathbf{X}) = E\big[(\mathbf{X}-E[\mathbf{X}])(\mathbf{X}-E[\mathbf{X}])^\top\big].
$$

- 선형형태의 분산:
$$
\mathrm{Var}(\mathbf{a}^\top\mathbf{X}) = \mathbf{a}^\top \Sigma \mathbf{a}.
$$

- 독립이면 \(\Sigma\)가 대각행렬(상관 0). 무상관이라도 독립은 아님(정규 제외).

---

## 실전 모델링 예시

### 혼합(미분류 집단) — 전기댓값·전분산

예: 특정 고객군 \(Y\in\{\text{신규},\text{기존}\}\)에 따라 구매금액 \(X\)의 분포가 다를 때,
$$
E[X]=E\big[E[X\mid Y]\big],\quad
\mathrm{Var}(X)=E[\mathrm{Var}(X\mid Y)]+\mathrm{Var}(E[X\mid Y]).
$$
→ **집단 내부 변동 + 집단 간 평균 차이**로 분산 설명.

### A/B 테스트

전환 지시변수 \(I\sim \mathrm{Bern}(p)\):
$$
E[I]=p,\quad \mathrm{Var}(I)=p(1-p).
$$
표본평균 \(\bar{I}\)의 분산은 \(p(1-p)/n\). 신뢰구간·검정의 기본.

### 포트폴리오 분산(두 자산)

가중합 \(R=w_1R_1+w_2R_2, w_1+w_2=1\):
$$
\mathrm{Var}(R) = w_1^2\sigma_1^2 + w_2^2\sigma_2^2 + 2w_1w_2\,\rho\,\sigma_1\sigma_2.
$$
→ 상관 \(\rho\)가 작을수록 리스크 분산 효과 ↑.

---

## 계산 안정성: 스트리밍(온라인) 분산 (Welford)

큰 데이터/스트림에서 **한 번의 패스**로 안정적으로 평균·분산 추정:

```python
def welford_variance(stream):
    n = 0
    mean = 0.0
    M2 = 0.0  # sum of squared deviations
    for x in stream:
        n += 1
        delta = x - mean
        mean += delta / n
        M2 += delta * (x - mean)
    if n < 2:
        return mean, float('nan'), float('nan')
    sample_var = M2 / (n - 1)  # 불편
    pop_var = M2 / n           # 모분산(편의)
    return mean, sample_var, pop_var
```

장점: **수치적으로 안정**(큰 수의 차 감산 회피), **스트리밍 가능**.

---

## 코드 예제: 주사위/혼합/체비셰프 검증

```python
import numpy as np

# (A) 주사위: 이론 vs 시뮬

X = np.arange(1, 7)
P = np.ones(6)/6
E_theory = np.sum(X*P)
E2_theory = np.sum((X**2)*P)
Var_theory = E2_theory - E_theory**2

N = 1_00000
samp = np.random.randint(1, 7, size=N)
E_emp = samp.mean()
Var_emp = samp.var(ddof=0)  # 모분산

print("Die E:", E_emp, "≈", E_theory)
print("Die Var:", Var_emp, "≈", Var_theory)

# (B) 혼합 분포: 전분산 확인
# Y=0 with prob q: N(0,1); Y=1 with prob 1-q: N(3, 2^2)

q = 0.4
n = 200000
Y = (np.random.rand(n) > q).astype(int)
Xmix = np.where(Y==0, np.random.normal(0, 1, n), np.random.normal(3, 2, n))

# 이론

E_X_given_Y = np.array([0, 3])
Var_X_given_Y = np.array([1**2, 2**2])
E_theo = q*E_X_given_Y[0] + (1-q)*E_X_given_Y[1]
Var_theo = q*Var_X_given_Y[0] + (1-q)*Var_X_given_Y[1] + \
           q*(E_X_given_Y[0]-E_theo)**2 + (1-q)*(E_X_given_Y[1]-E_theo)**2

print("Mixture E emp/theo:", Xmix.mean(), E_theo)
print("Mixture Var emp/theo:", Xmix.var(ddof=0), Var_theo)

# (C) 체비셰프 검증: k=3

mu = Xmix.mean(); sigma = Xmix.std()
lhs = np.mean(np.abs(Xmix - mu) >= 3*sigma)  # 좌변 확률
rhs = 1/9  # 1/k^2
print("Chebyshev LHS (emp) ≤ RHS:", lhs, "<=", rhs)
```

---

## 자주 하는 실수 & 체크리스트

1) **분산 선형성 착각**: \(\mathrm{Var}(X+Y)\neq \mathrm{Var}(X)+\mathrm{Var}(Y)\) (항상) — **공분산 항** 확인!
2) **표본분산 분모**: 불편 추정은 \(n-1\). 라이브러리 `var(ddof=1)`인지 체크.
3) **독립 vs 무상관**: \(\mathrm{Cov}=0\) ⇒ 독립 아님(예외: 다변량 정규).
4) **스케일/시프트**: \(\mathrm{Var}(aX+b)=a^2\mathrm{Var}(X)\), \(b\)는 분산에 영향 없음.
5) **LOTUS 잊지 말기**: \(E[g(X)]\)는 \(g(E[X])\)가 아님(볼록이면 젠슨으로 부등호).
6) **혼합 모델**: 전분산 법칙으로 분산이 커질 수 있음(집단 평균 차이 때문).
7) **수치 안정성**: 큰 값 평균/분산 직접식은 **소수점 소실** → Welford 사용.
8) **범위–분산 감**: 체비셰프로 대략적 이상치 확률 상한 얻기.

---

## 모멘트생성함수(MGF)와 분산

MGF \(M_X(t)=E[e^{tX}]\) 존재 시
$$
E[X]=M_X'(0),\quad E[X^2]=M_X''(0),\quad \mathrm{Var}(X)=M_X''(0)-\big(M_X'(0)\big)^2.
$$
로그-MGF(누적생성함수) \(\kappa(t)=\log M_X(t)\):
$$
\kappa'(0)=E[X],\quad \kappa''(0)=\mathrm{Var}(X).
$$

---

## 추가 예제

### 동전 베팅 게임

앞면이면 \(+1\), 뒷면이면 \(-1\). \(X\in\{-1,1\}\), \(P(X=1)=p\).
$$
E[X]=2p-1,\quad \mathrm{Var}(X)=1-(2p-1)^2=4p(1-p).
$$

### 최대/최소의 기대 (i.i.d. 균등)

\(X_i\sim \mathrm{Unif}(0,1)\), \(M=\max X_i\), \(m=\min X_i\).
$$
E[M]=\frac{n}{n+1},\quad E[m]=\frac{1}{n+1}.
$$
(분산도 밀도 유도 가능)

### 선형 조합의 분산(가중 평균)

독립 \(X_i\), \(Y=\sum w_i X_i\), \(\sum w_i=1\):
$$
E[Y]=\sum w_i \mu_i,\quad \mathrm{Var}(Y)=\sum w_i^2 \sigma_i^2 \quad (\text{독립}).
$$
상관 있으면 \(2\sum_{i<j} w_i w_j\,\mathrm{Cov}(X_i,X_j)\) 추가.

---

## 빠른 실습 문제

1) \(X\sim \mathrm{Poisson}(\lambda)\). \(Y=2X+3\).
   \(E[Y],\ \mathrm{Var}(Y)\)는?
   **풀이**: \(E[Y]=2\lambda+3,\ \mathrm{Var}(Y)=4\lambda\).

2) 베르누이 \(p\)의 표본 \(n\)개 평균 \(\bar{X}\)의 분산?
   **답**: \(p(1-p)/n\).

3) 혼합: 확률 \(q\)로 \(N(\mu_1,\sigma_1^2)\), \(1-q\)로 \(N(\mu_2,\sigma_2^2)\).
   \(E[X]\)와 \(\mathrm{Var}(X)\)?
   **답**:
   $$E[X]=q\mu_1+(1-q)\mu_2,$$
   $$\mathrm{Var}(X)=q\sigma_1^2+(1-q)\sigma_2^2+q(1-q)(\mu_1-\mu_2)^2.$$

4) 체비셰프: 평균 \(\mu\), 표준편차 \(\sigma\)에서 \(|X-\mu|\ge 4\sigma\) 확률 상한?
   **답**: \(\le 1/16\).

---

## 요약

- **기댓값**: 선형성 100% 유효.
- **분산**: 스케일 제곱, 합은 **공분산 항** 주의.
- **전기댓값/전분산 법칙**: 혼합·계층 데이터의 필수 도구.
- **표본 평균**은 분산 \(1/n\)로 줄어듦, **표본분산 \(n-1\)**.
- **Welford**로 안정적 온라인 분산.
- **불평등**(마르코프/체비셰프/젠슨)으로 보수적 위험 상한.
- 대표 분포들의 \(E,\mathrm{Var}\) 공식은 반드시 암기해 현장 적용.

---

## 부록) 간단 실험: CLT 감각 잡기

```python
import numpy as np

# 임의 비대칭 분포(지수)에서 표본평균 분포 관찰

lam = 2.0
n = 50
R = 20000

means = np.mean(np.random.exponential(1/lam, size=(R, n)), axis=1)

print("Theo mean:", 1/lam, "Emp mean:", means.mean())
print("Theo var(mean):", (1/lam**2)/n, "Emp var(mean):", means.var(ddof=0))
# means 히스토그램을 그리면 정규형태에 가까워짐 (중심극한정리)

```

> 평균의 분산이 이론대로 \((\sigma^2/n)\) 근처로 떨어지며, 히스토그램은 거의 **정규**에 가깝습니다.
